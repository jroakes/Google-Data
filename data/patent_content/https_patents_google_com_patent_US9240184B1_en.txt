US9240184B1 - Frame-level combination of deep neural network and gaussian mixture models - Google Patents
Frame-level combination of deep neural network and gaussian mixture models Download PDFInfo
- Publication number
- US9240184B1 US9240184B1 US13/765,002 US201313765002A US9240184B1 US 9240184 B1 US9240184 B1 US 9240184B1 US 201313765002 A US201313765002 A US 201313765002A US 9240184 B1 US9240184 B1 US 9240184B1
- Authority
- US
- United States
- Prior art keywords
- sequence
- gmm
- states
- feature vector
- emission probabilities
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/14—Speech classification or search using statistical models, e.g. Hidden Markov Models [HMMs]
- G10L15/142—Hidden Markov Models [HMMs]
Definitions
- a goal of automatic speech recognition (ASR) technology is to map a particular utterance to an accurate textual representation, or other symbolic representation, of that utterance. For instance, ASR performed on the utterance “my dog has fleas” would ideally be mapped to the text string “my dog has fleas,” rather than the nonsensical text string “my dog has freeze,” or the reasonably sensible but inaccurate text string “my bog has trees.”
- Various technologies including computers, network servers, telephones, and personal digital assistants (PDAs), can be employed to implement an ASR system, or one or more components of such a system.
- Communication networks may in turn provide communication paths and links between some or all of such devices, supporting ASR capabilities and services that may utilize ASR capabilities.
- an example embodiment presented herein provides a method comprising: transforming an audio input signal, using one or more processors of a system, into a first sequence of feature vectors and a second sequence of feature vectors, both the first and second sequences of feature vectors corresponding in common to a sequence of temporal frames of the audio input signal, wherein each respective feature vector of the first sequence and a corresponding respective feature vector of the second sequence bear quantitative measures of acoustic properties of a corresponding, respective temporal frame of the sequence of temporal frames of the audio input signal; processing the first sequence of feature vectors with a neural network (NN) implemented by the one or more processors of the system to generate a NN-based set of emission probabilities for a plurality of hidden Markov models (HMMs) implemented by the one or more processors of the system; processing the second sequence of feature vectors with a Gaussian mixture model (GMM) implemented by the one or more processors of the system to generate a GMM-based set of emission probabilities for the plurality of HMMs
- an example embodiment presented herein provides a method comprising: transforming an audio input signal, using one or more processors of a system, into a first sequence of feature vectors and a second sequence of feature vectors, both the first and second sequences of feature vectors corresponding in common to a sequence of temporal frames of the audio input signal, wherein each respective feature vector of the first sequence and a corresponding respective feature vector of the second sequence bear quantitative measures of acoustic properties of a corresponding, respective temporal frame of the sequence of temporal frames of the audio input signal; processing each respective feature vector of the first sequence with a neural network (NN) implemented by the one or more processors of the system to determine, for each respective state of a multiplicity of states of hidden Markov models (HMMs) implemented by the one or more processors of the system, a respective NN-based conditional probability of emitting the respective feature vector of the first sequence given the respective state; processing each respective feature vector of the second sequence with a Gaussian mixture model (GMM) implemented by the one or more
- an example embodiment presented herein provides a system comprising: one or more processors; memory; and machine-readable instructions stored in the memory, that upon execution by the one or more processors cause the system to carry out operations comprising: transforming an audio input signal into a first sequence of feature vectors and a second sequence of feature vectors, wherein both the first and second sequences of feature vectors correspond in common to a sequence of temporal frames of the audio input signal, and wherein each respective feature vector of the first sequence and a corresponding respective feature vector of the second sequence bear quantitative measures of acoustic properties of a corresponding, respective temporal frame of the sequence of temporal frames of the audio input signal, processing the first sequence of feature vectors with a neural network (NN) implemented by the system to generate a NN-based set of emission probabilities for a plurality of hidden Markov models (HMMs) implemented by the system, processing the second sequence of feature vectors with a Gaussian mixture model (GMM) implemented by the system to generate a GMM-based set of emission
- NN
- an example embodiment presented herein provides an article of manufacture including a computer-readable storage medium, having stored thereon program instructions that, upon execution by one or more processors of a system, cause the system to perform operations comprising: transforming an audio input signal into a first sequence of feature vectors and a second sequence of feature vectors, wherein both the first and second sequences of feature vectors correspond in common to a sequence of temporal frames of the audio input signal, and wherein each respective feature vector of the first sequence and a corresponding respective feature vector of the second sequence bear quantitative measures of acoustic properties of a corresponding, respective temporal frame of the sequence of temporal frames of the audio input signal; processing the first sequence of feature vectors with a neural network (NN) implemented by the system to generate a NN-based set of emission probabilities for a plurality of hidden Markov models (HMMs) implemented by the system; processing the second sequence of feature vectors with a Gaussian mixture model (GMM) implemented by the system to generate a GMM-based set of emission probabil
- NN
- an example embodiment presented herein provides an article of manufacture including a computer-readable storage medium, having stored thereon program instructions that, upon execution by one or more processors of a system, cause the system to perform operations comprising: transforming an audio input signal into a first sequence of feature vectors and a second sequence of feature vectors, wherein both the first and second sequences of feature vectors correspond in common to a sequence of temporal frames of the audio input signal, and wherein each respective feature vector of the first sequence and a corresponding respective feature vector of the second sequence bear quantitative measures of acoustic properties of a corresponding, respective temporal frame of the sequence of temporal frames of the audio input signal; processing each respective feature vector of the first sequence with a neural network (NN) implemented by the system to determine, for each respective state of a multiplicity of states of hidden Markov models (HMMs) implemented by the system, a respective NN-based conditional probability of emitting the respective feature vector of the first sequence given the respective state; processing each respective feature vector of the second sequence with a
- NN neural
- FIG. 1 is a flowchart illustrating an example method in accordance with an example embodiment.
- FIG. 2 is a block diagram of an example network and computing architecture, in accordance with an example embodiment.
- FIG. 3A is a block diagram of a server device, in accordance with an example embodiment.
- FIG. 3B depicts a cloud-based server system, in accordance with an example embodiment.
- FIG. 4 depicts a block diagram of a client device, in accordance with an example embodiment.
- FIG. 5 depicts a block diagram of an ASR system, in accordance with an example embodiment.
- FIG. 6 is a schematic illustration of processing of feature vectors with a neural network (NN) to determine NN-based emission probabilities for hidden Markov models, in accordance with an example embodiment.
- NN neural network
- FIG. 7 is a schematic illustration of applying NN-based emission probabilities determined by a neural network to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 8 is a schematic illustration of processing of feature vectors with a Gaussian mixture model (GMM) to determine GMM-based emission probabilities for hidden Markov models, in accordance with an example embodiment.
- GMM Gaussian mixture model
- FIG. 9 is a schematic illustration of applying GMM-based emission probabilities determined by a Gaussian mixture model to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 10 is a schematic illustration of processing of feature vectors with a neural network and with a Gaussian mixture model, and merging the NN-based emission probabilities with the GMM-based emission probabilities to determine merged emission probabilities for hidden Markov models, in accordance with an example embodiment.
- FIG. 11 is a schematic illustration of applying merged emission probabilities determined by a neural network and a Gaussian mixture model to hidden Markov models to determine speech content represented in feature vectors, in accordance with an example embodiment.
- FIG. 12 is a block diagram depicting an example ASR system, and illustrates operational aspects of merged emission probabilities for hidden Markov models, in accordance with an example embodiment.
- An automatic speech recognition (ASR) system can be a processor-based system configured to recognize a spoken utterance in an audio input signal, and responsively carry out an action associated with, or corresponding to, recognition of the utterance.
- the spoken utterance could be a word, multiple words, a phrase, multiple phrases, a sentence, multiple sentences, or other segment of speech, for example.
- the source of the spoken utterance could be a live person speaking in real time into a microphone, telephone, or other audio input/transmission device/system, for example, that then produces and supplies the audio signal as input to the ASR system.
- the source of the spoken utterance could also be previously-recorded speech that is played out via an audio output device/system, for example, and supplied as the audio input signal to the ASR system.
- the action associated with, or corresponding to, recognition of the utterance could be translation of the recognized utterance into text, and output and/or recording of the text.
- the action could also be generation of a response to the recognized utterance, such as synthesis of a reply (e.g., via a voice/sound synthesis device/system), or carrying out a command.
- Other responses are possible as well.
- An ASR system may operate by receiving an input audio signal, processing the audio input signal (e.g., using a digital signal processor) to generate a quantified representation of the signal, and then performing pattern recognition in which the quantified representation of the input signal is matched in some manner against a stored body of similarly quantified representations.
- the stored body often referred to as a “corpus,” is typically a large collection of speech samples that have been digitally processed, deconstructed, and categorized into a finite set of small, fundamental units of speech sounds, as well as possibly a finite set of larger speech segments (e.g., words, phrases, etc.).
- the fundamental units may also have associated “tags” or labels that can be used to identify them for purposes of generating text or other output from strings or sequences of units, for example.
- the small fundamental units could be phonemes. There are, for example, approximately 40 phonemes in spoken English. Spoken words (or other segments of speech) can be constructed from appropriate sequences of subsets of these phonemes. For example, phonemes may occur in particular triplet combinations referred to as “triphones.” In a triphone, a given phoneme can appear in the context of a preceding phoneme and a following (subsequent) phoneme. Accordingly, the fundamental units could instead be, or in addition include, triphones. It is also possible to recognize “quinphones” (groups of five phonemes), as well as other grouping sizes.
- the phonemes (or other small fundamental speech units) of the corpus can be represented and stored in one or another quantitative form. Accordingly, by processing the audio input signal in short units that can be quantitatively compared with stored phonemes or sequences of the stored phonemes, a matching technique can be employed to identify a likely sequence or sequences of stored phonemes that corresponds to the processed audio input signal. In this way, the spoken utterance in the input audio signal can be recognized as corresponding to a synthesized utterance reconstructed from the corpus of stored speech sounds.
- the architecture of an ASR system may include a signal processing component, a pattern classification component, an acoustic model component, a language model component, and a dictionary component (among other possible components).
- the signal processing component receives the audio input signal, digitally samples it within a sequence of time frames, and processes the frame samples to generate a corresponding sequence of “feature vectors.”
- Each feature vector includes a set of measured and/or derived elements that characterize the acoustic content of the corresponding time frame.
- the acoustic content represented in a feature vector can correspond to some portion of one or more fundamental speech units (e.g., phoneme, triphone, etc.), and thus can be used for matching against the speech units of the corpus.
- fundamental speech units e.g., phoneme, triphone, etc.
- the pattern classification component receives a sequence of feature vectors as input, and can apply the acoustic model, language model, and dictionary in order to carry out the recognition process.
- the acoustic model can access the corpus and can implement the search/comparison process to determine optimal sequences of phonemes, triphones, or other fundamental speech units.
- the language model includes rules of the spoken language (e.g., grammar, syntax, etc.) that can be applied to help guide and/or constrain the recognition process, while the dictionary component may provide semantic constraints at the word level.
- the corpus may also include identifications or “labels” of its contents, so that the synthesized utterances reconstructed from the corpus of stored speech sounds can be rendered in text or other formats suitable for enabling the ASR system to generate a response (or responses) to recognition of spoken utterances.
- the output of the pattern classification component is the recognized speech carried in the utterance.
- the form of the output could be a text string or an action corresponding to the recognized speech, for example.
- HMM hidden Markov model
- phonemes, triphones, or other fundamental speech units are modeled probabilistically as respective groupings of HMM states. More specifically, each fundamental speech unit is seen as temporally evolving according to some sequence of temporal phases of the speech unit. It has been observed empirically, for example, that phonemes manifest in speech across three acoustic phases: a start, a middle, and an end. A given phoneme (or other fundamental speech unit) therefore can be reasonably modeled with three states, one corresponding to each acoustic phase. Transitions between states are governed by transition probabilities of the model.
- each state has an associated “emission probability” for “emitting” an output corresponding to the acoustic phase of the phoneme.
- a HMM for the given phoneme or other fundamental speech unit
- a HMM for a given phoneme or other fundamental speech unit
- a respective probability of producing (emitting) the acoustic phase associated with the next state It will be appreciated that a HMM for modeling a fundamental speech unit is not necessarily limited to three states, and that HMMs with greater than or fewer than three states are possible.
- Sequential feature vectors derived from an audio input stream represent a stream of observed acoustic data
- sequential states of one or more HMMs may be concatenated to represent probable sequences of phonemes, triphones, or other fundamental speech units in the corpus that correspond to the observed acoustic data.
- the term “concatenated HMMs” will be used to refer to a concatenation of respective groupings of HMM states, where each respective grouping models a fundamental speech unit (as defined above).
- the states and models are “hidden” in the sense that, while the possible states and their associated transition and emission probabilities may be known, the specific state sequences associated with any given observed sequence of feature vectors is not a priori known.
- Recognition of utterances (speech) in the audio input signal therefore can be thought of as determining the most probable sequence (or sequences) of states of one or more concatenated HMMs that would produce the observed feature vectors.
- the most probable sequence of states then corresponds to the most probable sequence of phonemes, triphones, or other fundamental speech units in the corpus, from which the input utterance can be reconstructed and thereby recognized.
- the determination of the most probable sequences of HMMs and states is carried out one step at a time, with each time step corresponding to the frame of observed data in the input stream as represented quantitatively in a next feature vector. More specifically, at each time step the most likely next HMM state from among a multiplicity of possible HMM states may be determined by computing for each respective state of the multiplicity a conditional probability of observing the data represented in the feature vector at that time step, given the respective state. In the parlance of HMMs, the observable output of a given state is said to be “emitted.”
- the conditional probabilities are the emission probabilities referred to above.
- emission probabilities for each feature vector of the sequence need to be determined.
- GMMs Gaussian mixture models
- NNNs neural networks
- the GMM technique can be thought of as an empirical-statistical classification of the set of quantitative measures included in feature vectors, as derived from a (typically large) sample of observations. More specifically, by treating each type of measured quantity (i.e., feature) of feature vectors as an observational variable, multiple observations over many feature vectors during a training procedure can be used to derive statistical frequencies and related frequency distribution functions for each observational variable, and for a variety of observed feature vectors.
- different feature vectors derived from frames of similar acoustic data may tend to include the features of the defining set in similar proportion.
- similar frames of acoustic data could correspond to a particular fundamental speech unit, such as phoneme or triphone, produced by a range of different sources (e.g., different speakers) and in a range of different contexts (e.g., in different utterances).
- a particular fundamental speech unit such as phoneme or triphone
- sources e.g., different speakers
- contexts e.g., in different utterances
- Repeated observations of the particular fundamental speech unit over the range of sources and contexts may be used during training to map out the observed frequency distribution functions of the defining features as they occur on “average” for the particular fundamental speech unit.
- a particular fundamental speech unit can be thought of as being characterized by a mix of frequency distributions, each frequency distribution characterizing an average or expected contribution of a particular defining feature.
- a body of fundamental speech units can come to be represented by a variety of “average” mixtures of distribution functions, each distribution function associated with an observational variable (i.e., a feature). It has been found empirically that the observed distribution functions can be well-represented as Gaussians. Accordingly, fundamental speech units can be modeled as mixtures of Gaussian distributions of defining features, hence the name “Gaussian mixture model.” In practice, the different speech units to which the different Gaussian mixtures apply may not necessarily be individual phonemes or triphones, but rather empirically-derived clusters of speech units having one or more shared or similar characteristics. During training operation, the parameters of the Gaussians in the various mixtures may be adjusted in an effort to optimize agreement between the models and the known (training) data.
- observed feature vectors derived from an input audio signal can be compared against the GMM distributions in order to determine the conditional probabilities (i.e., emission probabilities) that any state would emit the feature vectors.
- the emission probabilities may then be applied for each feature vector at each time step to predict the most likely next state.
- a neural network functions to generate the emission probabilities for all of the states that need to be considered at each step.
- the input to the neural network is one or more feature vectors, where a sequence of multiple feature vectors can provide additional context for processing by the neural network of a given feature vector of the sequence.
- the neural network is trained to recognize a wide variety of feature vectors, and to associate or identify them with a wide variety of fundamental speech units over a range of sources and contexts.
- the speech units used during training may not necessarily be individual phonemes or triphones, but rather empirically-derived clusters of speech units having one or more shared or similar characteristics.
- the trained NN recognizes (within the limits of its training, for example) the observed feature vectors derived from an input audio signal, and determines the conditional probabilities (i.e., emission probabilities) that any state would emit the feature vectors. The emission probabilities may then be applied for each feature vector at each time step to predict the most likely next state.
- conditional probabilities i.e., emission probabilities
- Both the NN technique and the GMM technique have relative advantages and disadvantages in their respective functions of generating emission probabilities.
- the NN technique can be well-suited to training with large amounts of sample data, and to handling data that may include high degrees of correlation.
- NN implementations may tend to require significant computing resources, both in terms of memory (or storage) and processing power.
- the GMM technique can be less computationally complex and resource-intensive than the NN approach, but may need to incorporate some simplifying assumptions to accommodate correlations in the data.
- each one may tend to yield better or more accurate results than the other for certain fundamental speech units (e.g., phonemes or triphones). In this sense, the two approaches can be considered complementary. Accordingly, there is a need to be able to merge aspects of both techniques to derive the relative advantages of each.
- emission probabilities determined by both a neural network and a GMM are merged at the frame level, and the merged emission probabilities are applied to HMM states. More particularly, an audio input signal is processed to generate a first sequence feature vectors that is input to a neural network, and another corresponding second sequence that is input to a GMM. Each respective feature vector of the first sequence has a corresponding respective feature vector in the second sequence (and vice versa), and each pair of corresponding feature vectors from the first and second sequences correspond in common to temporal frame of the audio input signal.
- the neural network generates a plurality of NN-based emission probabilities for each feature vector of the first sequence, where each NN-based emission probability of the plurality is a conditional probability of emitting the feature vector of the first sequence, given a respective state of a corresponding plurality of HMM states.
- GMM is used to generate a plurality of GMM-based emission probabilities for each feature vector of the second sequence, where each GMM-based emission probability of the plurality is a conditional probability of emitting the feature vector of the second sequence, given a respective state of the corresponding plurality of HMM states.
- a weighted sum of the NN-based and GMM-based emission probabilities for each HMM state is computed.
- the resulting plurality of weighted-sum emission probabilities then corresponds to a frame-level merging of the NN-based and GMM-based emission probabilities.
- the weights used in the weighted sums may be adjusted during a training procedure. More particularly, training with sample data may be used to adjust the weights so as to yield results that are as close as possible (or above a threshold level of accuracy) to the known data.
- an ASR system may include one or more processors, one or more forms of memory, one or more input devices/interfaces, one or more output devices/interfaces, and machine-readable instructions that when executed by the one or more processors cause the ASR system to carry out the various functions and tasks described herein.
- the functions and tasks may form a basis for a method for frame-level merging of NN-based and GMM-based emission probabilities. An example of such a method is described in the current section.
- FIG. 1 is a flowchart illustrating an example method in accordance with an example embodiment.
- the system transforms an audio input signal into a first sequence of feature vectors and a second sequence of feature vectors, where both the first and second sequences of feature vectors correspond in common to a sequence of temporal frames of the audio input signal. More specifically, each respective feature vector of the first sequence has a corresponding respective feature vector in the second sequence, and each pair of corresponding feature vectors of the first and second sequences correspond in common to a respective temporal frame of the sequence of temporal frames of the audio input signal. Further, each corresponding feature vector of the first and second sequences bears quantitative measures of acoustic properties of the corresponding temporal frame. Each temporal frame could contain a portion of the audio input signal digitally sampled within a sliding time window. Each feature vector could include quantitative measures of acoustic properties of the digitally sampled signal within the corresponding time frame.
- the feature vectors could include Mel Filter Cepstral (MFC) coefficients, as described below.
- MFC Mel Filter Cepstral
- Other possible types of quantitative measures of acoustic properties could include Perceptual Linear Predictive (PLP) coefficients, Relative Spectral (RASTA) coefficients, and Filterbank log-energy coefficients. Techniques for determining these types of quantitative measures from sampled speech are generally known. It will be appreciated that feature vectors may not necessarily be restricted to including only one of these (or other) types of quantitative measures, but could also include more than one type.
- first and second sequences of feature vectors could be duplicates of each other. That is each corresponding feature vector of the first and second sequences could be a copy of one another.
- the feature vectors of the first sequence could bear different features than the feature vectors of the second sequence.
- feature vectors of the first sequence could include PLP coefficients
- the feature vectors of the second sequence could include MFC coefficients. Other arrangements are possible as well.
- a neural network processes the first sequence of feature vectors to generate a NN-based set of emission probabilities for a plurality of hidden Markov models (HMMs). More particularly, the NN-based set emission probabilities could be associated with HMMs used to model speech units of the system. Each NN-based emission probability could be associated with a state of a HMM used to model a given one (or a given cluster of similar and/or related) of the speech units; and each state of the HMM could correspond to an acoustic phase of the given one (or a given cluster of similar and/or related) of the speech units.
- HMMs hidden Markov models
- a Gaussian mixture model processes the second sequence of feature vectors to generate a GMM-based set of emission probabilities for the plurality of hidden HMMs.
- the GMM-based set emission probabilities could be associated with HMMs used to model speech units of the system.
- Each NN-based emission probability could be associated with a HMM state used to model a given one (or a given cluster of similar and/or related) of the speech units.
- a determination of which defining types of features to include in the feature vectors of each of the first and second sequences could be based on which type of subsequent processing is used to determine emission probabilities (e.g., step 104 or step 106 ), for example. More particularly, one given set defining feature types (e.g., PLP coefficients) might tend to yield more accurate or better quality results when processed by a neural network (step 104 ), and thus could be used in the first sequence. Similarly, another set of defining feature types (e.g., MFC coefficients) might tend to yield more accurate or better quality results when processed by a GMM (step 106 ), and thus could be used in the second sequence. As noted, the respective sets of defining feature types best suited for each of the first and second sequences could be the same or different.
- the GMM-based emission probabilities and the GMM-based emission probabilities are merged to generate a merged set of emission probabilities for the plurality of HMMs.
- merging the GMM-based emission probabilities and the GMM-based emission probabilities could correspond to computing weighted sums of the two types of emission probabilities.
- the merged set of emission probabilities are applied to the plurality of HMMs in order to determine speech content corresponding to the sequence of temporal frames of the audio input signal.
- the example method employs a neural network (NN), a Gaussian mixture model (GMM), and HMMs
- NN neural network
- GMM Gaussian mixture model
- HMMs HMMs
- the NN could be implemented by a first group of one or more processors
- the GMM could be implemented by a second group of one or more processors
- the HMMs could be implemented by a third group of one or more processors.
- the first, second, and third groups could be the same or different, or any two or more of the first, second, and third groups could include one or more common processors.
- the algorithmic implementations of the NN, GMMs, and/or the HMMs could be considered part of the example method, or could be ancillary to it, being provided as separate algorithmic components, for example.
- each of the HMMs in the plurality could be associated with a respective elemental speech unit, and could have one or more states corresponding to one or more temporal phases of the associated, respective elemental speech unit.
- the plurality of HMMs could collectively correspond to a multiplicity of states.
- each elemental speech unit could be a phoneme, a triphone, or a quinphone.
- the NN-based set of emission probabilities for the plurality of the HMMs could be generated (at step 104 ) by determining NN-based conditional probabilities for each respective feature vector of the first sequence. More specifically, for each respective state of the multiplicity of states, a respective NN-based conditional probability of emitting the respective feature vector of the first sequence, given the respective state, could be determined.
- the GMM-based set of emission probabilities for the plurality of the HMMs could be generated (at step 106 ) by determining GMM-based conditional probabilities for each respective feature vector of the second sequence. More specifically, for each respective state of the multiplicity of states, a respective GMM-based conditional probability of emitting the respective feature vector of the second sequence, given the respective state, could be determined.
- merging at step 108 ) the NN-based set of emission probabilities with the GMM-based set of emission probabilities to generate the merged set of emission probabilities could correspond to merging, on a frame-by-frame basis, the NN-based and GMM-based emission probabilities associated with each pair corresponding feature vectors of the first and second sequences. More specifically, for each respective state of the multiplicity, a weighted sum of the respective NN-based conditional probability and the respective GMM-based conditional probability could be determined on a frame-by-frame basis.
- determining (at step 110 ) the speech content corresponding to the sequence of temporal frames of the audio input signal could correspond to determining a probable sequence of elemental speech units based on a most likely sequence of states of the multiplicity.
- determining speech content could correspond to generating a text string of the speech content, or identifying a computer-executable command based on the speech content. Other actions associated with determining speech content are possible as well.
- the weighted sums corresponding to the merged emission probabilities could include adjustable weights. Further, the weights could be adjusted during a training procedure, using audio input signals of known (predetermined) content and correspondingly known (predetermined) textual translations. More specifically, during training time, a training-time audio input signal could be transformed into a first sequence of feature vectors and a second sequence of feature vectors, where both the first and second sequences of feature vectors correspond in common to a sequence of temporal frames of the training-time audio input signal.
- each respective feature vector of the first sequence could be processed with the NN to determine, for each respective state of the multiplicity HMM states, a respective NN-based conditional probability of emitting the respective feature vector of the first sequence, given the respective state.
- each respective feature vector of the second sequence could be processed with the GMM to determine, for each respective state of the multiplicity HMM states, a respective GMM-based conditional probability of emitting the respective feature vector of the first sequence, given the respective state.
- a respective weighted sum of the respective NN-based conditional probability and the respective GMM-based conditional probability could be determined for each respective state.
- Each respective weighted sum could then be one of a set of weighted-sum emission probabilities for the multiplicity of states.
- the weights could be adjusted iteratively during a training procedure. More specifically, the weighted-sum emission probabilities could be applied to the HMM states to determine predicted speech content of the training-time audio input signal. The predicted speech content could be compared with pre-determined speech content of the training-time audio input signal. The weights of the set of weighted-sum emission probabilities could then be adjusted so as to reduce a difference between the predicted speech content and the pre-determined speech content.
- the difference between predicted speech content and the pre-determined speech content could be used to derive an error signal or a “penalty function.”
- the steps of adjusting the weights, merging the NN-based and GMM-based emission probabilities, and determining the penalty function could be repeated in an iterative loop until the penalty function (or error signal) was reduced below a threshold, for example.
- the weights could be considered to be optimally (or acceptably) adjusted, and their respective, adjusted values used during run-time operation.
- FIG. 1 is meant to illustrate a method in accordance with an example embodiment. As such, various steps could be altered or modified, the ordering of certain steps could be changed, and additional steps could be added, while still achieving the overall desired operation.
- client devices such as mobile phones and tablet computers
- client services are able to communicate, via a network such as the Internet, with the server devices.
- applications that operate on the client devices may also have a persistent, server-based component. Nonetheless, it should be noted that at least some of the methods, processes, and techniques disclosed herein may be able to operate entirely on a client device or a server device.
- This section describes general system and device architectures for such client devices and server devices.
- the methods, devices, and systems presented in the subsequent sections may operate under different paradigms as well.
- the embodiments of this section are merely examples of how these methods, devices, and systems can be enabled.
- FIG. 2 is a simplified block diagram of a communication system 200 , in which various embodiments described herein can be employed.
- Communication system 200 includes client devices 202 , 204 , and 206 , which represent a desktop personal computer (PC), a tablet computer, and a mobile phone, respectively.
- Client devices could also include wearable computing devices, such as head-mounted displays and/or augmented reality displays, for example.
- Each of these client devices may be able to communicate with other devices (including with each other) via a network 208 through the use of wireline connections (designated by solid lines) and/or wireless connections (designated by dashed lines).
- Network 208 may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network.
- IP Internet Protocol
- client devices 202 , 204 , and 206 may communicate using packet-switching technologies. Nonetheless, network 208 may also incorporate at least some circuit-switching technologies, and client devices 202 , 204 , and 206 may communicate via circuit switching alternatively or in addition to packet switching.
- a server device 210 may also communicate via network 208 .
- server device 210 may communicate with client devices 202 , 204 , and 206 according to one or more network protocols and/or application-level protocols to facilitate the use of network-based or cloud-based computing on these client devices.
- Server device 210 may include integrated data storage (e.g., memory, disk drives, etc.) and may also be able to access a separate server data storage 212 .
- Communication between server device 210 and server data storage 212 may be direct, via network 208 , or both direct and via network 208 as illustrated in FIG. 2 .
- Server data storage 212 may store application data that is used to facilitate the operations of applications performed by client devices 202 , 204 , and 206 and server device 210 .
- communication system 200 may include any number of each of these components.
- communication system 200 may comprise millions of client devices, thousands of server devices and/or thousands of server data storages.
- client devices may take on forms other than those in FIG. 2 .
- FIG. 3A is a block diagram of a server device in accordance with an example embodiment.
- server device 300 shown in FIG. 3A can be configured to perform one or more functions of server device 210 and/or server data storage 212 .
- Server device 300 may include a user interface 302 , a communication interface 304 , processor 306 , and data storage 308 , all of which may be linked together via a system bus, network, or other connection mechanism 314 .
- User interface 302 may comprise user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, and/or other similar devices, now known or later developed.
- User interface 302 may also comprise user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, now known or later developed.
- user interface 302 may be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 302 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- Communication interface 304 may include one or more wireless interfaces and/or wireline interfaces that are configurable to communicate via a network, such as network 208 shown in FIG. 2 .
- the wireless interfaces may include one or more wireless transceivers, such as a BLUETOOTH® transceiver, a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard (e.g., 802.11b, 802.11g, 802.11n), a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard, a Long-Term Evolution (LTE) transceiver perhaps operating in accordance with a 3rd Generation Partnership Project (3GPP) standard, and/or other types of wireless transceivers configurable to communicate via local-area or wide-area wireless networks.
- a BLUETOOTH® transceiver e.g., 802.11b, 802.11g, 802.11n
- WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard
- the wireline interfaces may include one or more wireline transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- wireline transceivers such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- USB Universal Serial Bus
- communication interface 304 may be configured to provide reliable, secured, and/or authenticated communications.
- information for ensuring reliable communications e.g., guaranteed message delivery
- a message header and/or footer e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values.
- CRC cyclic redundancy check
- Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, the data encryption standard (DES), the advanced encryption standard (AES), the Rivest, Shamir, and Adleman (RSA) algorithm, the Diffie-Hellman algorithm, and/or the Digital Signature Algorithm (DSA).
- DES data encryption standard
- AES advanced encryption standard
- RSA Rivest, Shamir, and Adleman
- Diffie-Hellman algorithm Diffie-Hellman algorithm
- DSA Digital Signature Algorithm
- Other cryptographic protocols and/or algorithms may be used instead of or in addition to those listed herein to secure (and then decrypt/decode) communications.
- Processor 306 may include one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., digital signal processors (DSPs), graphical processing units (GPUs), floating point processing units (FPUs), network processors, or application specific integrated circuits (ASICs)).
- DSPs digital signal processors
- GPUs graphical processing units
- FPUs floating point processing units
- ASICs application specific integrated circuits
- Processor 306 may be configured to execute computer-readable program instructions 310 that are contained in data storage 308 , and/or other instructions, to carry out various functions described herein.
- Data storage 308 may include one or more non-transitory computer-readable storage media that can be read or accessed by processor 306 .
- the one or more computer-readable storage media may include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with processor 306 .
- data storage 308 may be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 308 may be implemented using two or more physical devices.
- Data storage 308 may also include program data 312 that can be used by processor 306 to carry out functions described herein.
- data storage 308 may include, or have access to, additional data storage components or devices (e.g., cluster data storages described below).
- server device 210 and server data storage device 212 may store applications and application data at one or more locales accessible via network 208 . These locales may be data centers containing numerous servers and storage devices. The exact physical location, connectivity, and configuration of server device 210 and server data storage device 212 may be unknown and/or unimportant to client devices. Accordingly, server device 210 and server data storage device 212 may be referred to as “cloud-based” devices that are housed at various remote locations. One possible advantage of such “cloud-based” computing is to offload processing and data storage from client devices, thereby simplifying the design and requirements of these client devices.
- server device 210 and server data storage device 212 may be a single computing device residing in a single data center. In other embodiments, server device 210 and server data storage device 212 may include multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in diverse geographic locations. For example, FIG. 2 depicts each of server device 210 and server data storage device 212 potentially residing in a different physical location.
- FIG. 3B depicts an example of a cloud-based server cluster.
- functions of server device 210 and server data storage device 212 may be distributed among three server clusters 320 A, 320 B, and 320 C.
- Server cluster 320 A may include one or more server devices 300 A, cluster data storage 322 A, and cluster routers 324 A connected by a local cluster network 326 A.
- server cluster 320 B may include one or more server devices 300 B, cluster data storage 322 B, and cluster routers 324 B connected by a local cluster network 326 B.
- server cluster 320 C may include one or more server devices 300 C, cluster data storage 322 C, and cluster routers 324 C connected by a local cluster network 326 C.
- Server clusters 320 A, 320 B, and 320 C may communicate with network 308 via communication links 328 A, 328 B, and 328 C, respectively.
- each of the server clusters 320 A, 320 B, and 320 C may have an equal number of server devices, an equal number of cluster data storages, and an equal number of cluster routers. In other embodiments, however, some or all of the server clusters 320 A, 320 B, and 320 C may have different numbers of server devices, different numbers of cluster data storages, and/or different numbers of cluster routers. The number of server devices, cluster data storages, and cluster routers in each server cluster may depend on the computing task(s) and/or applications assigned to each server cluster.
- server devices 300 A can be configured to perform various computing tasks of a server, such as server device 210 . In one embodiment, these computing tasks can be distributed among one or more of server devices 300 A.
- Server devices 300 B and 300 C in server clusters 320 B and 320 C may be configured the same or similarly to server devices 300 A in server cluster 320 A.
- server devices 300 A, 300 B, and 300 C each may be configured to perform different functions.
- server devices 300 A may be configured to perform one or more functions of server device 210
- server devices 300 B and server device 300 C may be configured to perform functions of one or more other server devices.
- the functions of server data storage device 212 can be dedicated to a single server cluster, or spread across multiple server clusters.
- Cluster data storages 322 A, 322 B, and 322 C of the server clusters 320 A, 320 B, and 320 C, respectively, may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective server devices, may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.
- server device 210 and server data storage device 212 can be distributed across server clusters 320 A, 320 B, and 320 C
- various active portions and/or backup/redundant portions of these components can be distributed across cluster data storages 322 A, 322 B, and 322 C.
- some cluster data storages 322 A, 322 B, and 322 C may be configured to store backup versions of data stored in other cluster data storages 322 A, 322 B, and 322 C.
- Cluster routers 324 A, 324 B, and 324 C in server clusters 320 A, 320 B, and 320 C, respectively, may include networking equipment configured to provide internal and external communications for the server clusters.
- cluster routers 324 A in server cluster 320 A may include one or more packet-switching and/or routing devices configured to provide (i) network communications between server devices 300 A and cluster data storage 322 A via cluster network 326 A, and/or (ii) network communications between the server cluster 320 A and other devices via communication link 328 A to network 308 .
- Cluster routers 324 B and 324 C may include network equipment similar to cluster routers 324 A, and cluster routers 324 B and 324 C may perform networking functions for server clusters 320 B and 320 C that cluster routers 324 A perform for server cluster 320 A.
- the configuration of cluster routers 324 A, 324 B, and 324 C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 324 A, 324 B, and 324 C, the latency and throughput of the local cluster networks 326 A, 326 B, 326 C, the latency, throughput, and cost of the wide area network connections 328 A, 328 B, and 328 C, and/or other factors that may contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the system architecture.
- FIG. 4 is a simplified block diagram showing some of the components of an example client device 400 .
- client device 400 may be or include a “plain old telephone system” (POTS) telephone, a cellular mobile telephone, a still camera, a video camera, a fax machine, an answering machine, a computer (such as a desktop, notebook, or tablet computer), a personal digital assistant (PDA), a wearable computing device, a home automation component, a digital video recorder (DVR), a digital TV, a remote control, or some other type of device equipped with one or more wireless or wired communication interfaces.
- POTS plain old telephone system
- PDA personal digital assistant
- DVR digital video recorder
- client device 400 may include a communication interface 402 , a user interface 404 , a processor 406 , and data storage 408 , all of which may be communicatively linked together by a system bus, network, or other connection mechanism 410 .
- Communication interface 402 functions to allow client device 400 to communicate, using analog or digital modulation, with other devices, access networks, and/or transport networks.
- communication interface 402 may facilitate circuit-switched and/or packet-switched communication, such as POTS communication and/or IP or other packetized communication.
- communication interface 402 may include a chipset and antenna arranged for wireless communication with a radio access network or an access point.
- communication interface 402 may take the form of a wireline interface, such as an Ethernet, Token Ring, or USB port.
- Communication interface 402 may also take the form of a wireless interface, such as a Wifi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or LTE).
- communication interface 402 may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH® interface, and a wide-area wireless interface).
- User interface 404 may function to allow client device 400 to interact with a human or non-human user, such as to receive input from a user and to provide output to the user.
- user interface 404 may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, still camera and/or video camera.
- User interface 404 may also include one or more output components such as a display screen (which, for example, may be combined with a touch-sensitive panel), CRT, LCD, LED, a display using DLP technology, printer, light bulb, and/or other similar devices, now known or later developed.
- User interface 404 may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 404 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- client device 400 may support remote access from another device, via communication interface 402 or via another physical interface (not shown).
- Processor 406 may comprise one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., DSPs, GPUs, FPUs, network processors, or ASICs).
- Data storage 408 may include one or more volatile and/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor 406 .
- Data storage 408 may include removable and/or non-removable components.
- processor 406 may be capable of executing program instructions 418 (e.g., compiled or non-compiled program logic and/or machine code) stored in data storage 408 to carry out the various functions described herein. Therefore, data storage 408 may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by client device 400 , cause client device 400 to carry out any of the methods, processes, or functions disclosed in this specification and/or the accompanying drawings. The execution of program instructions 418 by processor 406 may result in processor 406 using data 412 .
- program instructions 418 e.g., compiled or non-compiled program logic and/or machine code
- program instructions 418 may include an operating system 422 (e.g., an operating system kernel, device driver(s), and/or other modules) and one or more application programs 420 (e.g., address book, email, web browsing, social networking, and/or gaming applications) installed on client device 400 .
- data 412 may include operating system data 416 and application data 414 .
- Operating system data 416 may be accessible primarily to operating system 422
- application data 414 may be accessible primarily to one or more of application programs 420 .
- Application data 414 may be arranged in a file system that is visible to or hidden from a user of client device 400 .
- Application programs 420 may communicate with operating system 412 through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs 420 reading and/or writing application data 414 , transmitting or receiving information via communication interface 402 , receiving or displaying information on user interface 404 , and so on.
- APIs application programming interfaces
- application programs 420 may be referred to as “apps” for short. Additionally, application programs 420 may be downloadable to client device 400 through one or more online application stores or application markets. However, application programs can also be installed on client device 400 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on client device 400 .
- FIG. 5 depicts a block diagram of an example ASR system 500 in which an example embodiment of frame-level merging of NN-based and GMM-based emission probabilities could be carried out.
- FIG. 5 also shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the ASR system 500 include a feature analysis module 502 , a pattern classification module 504 , an acoustic model 506 , a dictionary 508 , and a language model 510 . These functional components could be implemented as machine-language instructions in a centralized and/or distributed fashion on one or more computing platforms or systems, such as those described above.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- an audio signal bearing an utterance 501 may be input to the ASR system 500 , whereupon the system may generate an output 511 that could include recognized speech in the form of one or more text strings and possibly associated confidence levels.
- the output could also take the form of a computer-executable action or command identified or associated with the recognized speech (or other audio) content of the input utterance 501 .
- the utterance 501 could include an analog or digital representation of human speech, such as a spoken word, multiple words, a phrase, multiple phrases, a sentence, multiple sentences, or other segment of speech, for example.
- the source of the utterance 501 could be a live person speaking in real time into a microphone, telephone, or other audio input/transmission device/system, that then produces and supplies the audio signal as input to the ASR system 500 .
- the audio input/transmission device/system also not explicitly shown in FIG. 5 , could, by way of example, be a client device, such as the ones described above. Additionally or alternatively, an audio input/transmission device/system could be integrated as part of the ASR system 500 .
- the source of the spoken utterance could also be previously-recorded speech that is played out via an audio output device/system, for example, and supplied as the audio input signal to the ASR system.
- the utterance 501 may be received at the analysis module 502 , which may convert utterance 501 into a sequence of one or more feature vectors 503 .
- the conversion of the utterance 501 into the feature vectors 503 is sometimes referred to as feature extraction.
- each of feature vectors 503 may include temporal and/or spectral representations of the acoustic features of at least a portion of utterance 501 .
- the feature vectors 503 may be input to the pattern classification module 504 , which may produce the output 511 .
- the output 511 could be one or more text string transcriptions of utterance 501 . Each transcription may be accompanied by a respective confidence level indicating an estimated likelihood that the transcription is correct (e.g., 80% confidence, 90% confidence, etc.).
- the output 511 could also take the form of an executable application or command determined based on the recognize speech content of the utterance 501 .
- pattern classification module 504 may bring together and/or incorporate functionality of the acoustic model 506 , the dictionary 508 , and the language model 510 .
- the acoustic model 506 is used to model the observed data, as represented in the feature vectors 503 , subject to guidance and constraints supplied by the dictionary 508 and the language model 510 .
- the modeling process determines probabilities that a particular sequence of feature vectors 503 were derived from particular sequences of spoken sub-word sounds. Modeling may also involve probabilistic mapping of sequences of feature vectors to one or more fundamental speech units (e.g., phonemes) from among a stored corpus of fundamental speech units.
- fundamental speech units e.g., phonemes
- the language model 510 may assign probabilities to sequences of phonemes or words, based on the likelihood of a sequence of phonemes or words occurring in an input utterance to the ASR system.
- language model 510 may define the conditional probability of w n (the nth word in a phrase transcribed from an utterance), given the values of the pattern of n ⁇ 1 previous words in the phrase. This conditional probability can be expressed formally as P(w n
- the feature analysis module 502 may sample and quantize utterance 501 within a time sequence of overlapping or non-overlapping temporal frames, and perform spectral analysis on the frames to derive a feature vector associated with each frame.
- each frame could be acquired in a sliding time window that is periodically advanced.
- Each advance of the time window could be in increments measured in fractional units of the width of the time window.
- the width of each frame (and of the sliding time window) could be 25 milliseconds (ms), and the time increment between each frame acquisition could be 10 ms.
- each new 25 ms frame would advance by 10 ms past the end of the previous frame, and the first 15 ms of each new 25 ms frame would overlap with the last 15 ms of the previous frame.
- every two consecutive frames would contain 15 ms of common audio data (e.g. of an utterance).
- Other frame sizes, window sizes, and time increment sizes could be used as well.
- Feature extraction produces a feature vector for each frame of sampled audio data (e.g. of the utterance 501 ).
- each feature vector may include Mel Filter Cepstral (MFC) coefficients of each frame of the utterance 501 , as determined by the feature analysis module 502 .
- MFC coefficients may represent the short-term power spectrum of a portion of utterance 501 , and may be based on, for example, a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency.
- a Mel scale may be a scale of pitches subjectively perceived by listeners to be about equally distant from one another, even though the actual frequencies of these pitches are not equally distant from one another.
- the feature analysis module 502 may further perform noise removal and convert the standard spectral coefficients to MFC coefficients, and then calculate first-order and second-order cepstral derivatives of the MFC coefficients.
- the first-order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive frames.
- the second-order cepstral coefficient derivatives may be calculated based on the slopes of linear regressions performed over windows of two or more consecutive sets of first-order cepstral coefficient derivatives.
- one or more frames of utterance 501 may be represented by a feature vector of MFC coefficients, first-order cepstral coefficient derivatives, and second-order cepstral coefficient derivatives.
- the feature vector may contain 13 coefficients, 13 first-order derivatives, and 13 second-order derivatives, therefore having a length of 39.
- feature vectors may use different combinations of features in other possible embodiments.
- feature vectors could include Perceptual Linear Predictive (PLP) coefficients, Relative Spectral (RASTA) coefficients, Filterbank log-energy coefficients, or some combination thereof.
- PLP Perceptual Linear Predictive
- RASTA Relative Spectral
- Filterbank log-energy coefficients or some combination thereof.
- Each feature vector may be thought of as including a quantified characterization of the acoustic content of a corresponding temporal frame of the utterance 501 (or more generally of an audio input signal).
- the corpus applied in the modeling procedure may include a large collection of stored speech samples that have been digitally processed, deconstructed, and categorized into a set of fundamental units corresponding to speech sounds, such as phonemes.
- the corpus may also include a set of units corresponding to larger speech segments (e.g., words, phrases, etc.).
- the fundamental units may also have associated “tags” or labels that can be used to identify them for purposes of generating text or other output from strings or sequences of units, for example.
- a fundamental unit of speech that is suitable for use in the modeling procedure is a phoneme.
- a phoneme may be considered to be the smallest segment of an utterance that encompasses a meaningful contrast with other segments of utterances.
- a word typically includes one or more phonemes.
- phonemes may be thought of as utterances of letters, although this is not a perfect analogy, as some phonemes may present multiple letters.
- the phonemic spelling for the American English pronunciation of the word “cat” is /k/ /ae/ /t/, and consists of the phonemes /k/, /ae/, and /t/.
- phonemic spelling for the word “dog” is /d/ /aw/ /g/, consisting of the phonemes /d/, /aw/, and /g/.
- Different phonemic alphabets exist, and other phonemic representations are possible.
- Common phonemic alphabets for American English contain about 40 distinct phonemes. Each of these phonemes can be represented in a quantifiable form as a distribution of feature vector values.
- phonemes frequently occur in triplet combinations referred to as triphones, whereby a given phoneme appears in the context of a preceding phoneme and a following (subsequent) phoneme.
- the preceding phoneme is called the “left context”
- the following (subsequent) phoneme is called the “right context.”
- the ordering of the phonemes of a triphone corresponds to the direction in which English is read.
- Triphones capture how acoustic properties of a phoneme can depend on what precedes and follows it in a particular, spoken word.
- the word “dad” when spoken could be considered to be constructed of phoneme sequence [d], [ae], and [d], where the brackets (“[ ]”) identify the particular phoneme being spoken, and the forward-slash (“/”) notation has been dropped for the sake of brevity.
- the first triphone of “dad” would be “#[d]ae.”
- the left-context phoneme “#” signifies the start of the first [d] phoneme with nothing (e.g., a pause) preceding it (e.g, silence), and the right-context phoneme “ae” signifies the a-sound (as in “dad”) following it.
- Both the left and right contexts influence how the [d] phoneme is rendered acoustically.
- the next triphone would be “d[ae]d,” in which the acoustic properties of the [ae] phoneme are influenced by the both the left-context (preceding) phoneme “d” and the right-context (following) phoneme “d.”
- the last triphone would be be “ae[d]#,” in which the acoustic properties of the final [d] phoneme are influenced by the both the left-context (preceding) phoneme “ae” and the right-context (following) phoneme “#” that signifies the end of the word (e.g., no further sound).
- the acoustic model 506 may include a neural network (NN) and one or more hidden Markov models (HMMs).
- NN neural network
- HMMs hidden Markov models
- Such an implementation is referred to herein as a “hybrid neural network/hidden Markov model,” and is abbreviated as “HNN/HMM” (or “HNN/HMMs” in reference to a plurality of HMMs).
- HNN/HMM hidden neural network/hidden Markov model
- one or more HMMs are used to model the fundamental speech units (e.g., phonemes, triphones, etc.), while the neural network is used to determine emission probabilities to apply to the models, based on the observed data (e.g., sequence of feature vectors 503 in the example ASR system 500 ).
- the fundamental speech units of HMMs will be taken to be triphones, since this is the case in practice for certain ASR systems. It will be appreciated, however, that the principles discussed is not limited to triphones, and that other fundamental speech units can be used (e.g. phonemes, quinphones, clusters of similar and/or related speech units, etc.).
- a triphone may be modeled as temporally evolving according to a sequence of temporal phases. It has been observed empirically, for example, that triphones may typically manifest in speech across three acoustic phases: a start, a middle, and an end.
- the HMM for a given triphone therefore can be constructed having three states, one corresponding to each acoustic phase. Transitions between states are governed by transition probabilities of the model, and one or more states could include self-transitions that “loop” back to themselves.
- each state has an associated emission probability for emitting an output corresponding to the acoustic phase of the triphone.
- the HMM for a given triphone is characterized by probabilities of transitioning from a current state to a next state, and upon transitioning, a respective probability of producing (emitting) the acoustic phase associated with the next state.
- the emission probabilities may be determined by the neural network, based on the observed utterance as represented in the feature vectors derived from the utterance.
- the triphone sequence described above could be modeled with three HMM states each.
- the triphone “#[d]ae” could be modeled according to states corresponding to “#[d]ae.1,” “#[d]ae.2,” and “#[d]ae.3,” where the “0.1,” “0.2,” and “0.3” signify a temporal order of the states in the HMM for the triphone “#[d]ae.”
- the triphone “d[ae]d” could be modeled with a HMM having states corresponding to “d[ae]d.1,” “d[ae]d.2,” and “d[ae]d.3,” and the triphone “ae[d]#” could be modeled with a HMM having states corresponding to “ae[d]#.1,” “ae[d]#.2,” “ae[d]#.3.” This description could be generalized to different number of acoustic phases of tri
- the sequential feature vectors 503 derived from the utterance 501 represent a stream of observed acoustic data
- sequential states of one or more concatenated HMMs represent sequences of acoustic phases of triphones in the corpus that probabilistically correspond to the observed acoustic data. While the possible states and their associated transition and emission probabilities of the HMMs may be known, the specific state sequences associated with any given observed sequence of feature vectors is not a priori known (hence the term “hidden”).
- Recognition of speech in the input utterance 501 therefore involves determining the most probable sequence (or sequences) of states of one or more concatenated HMMs that would produce the observed feature vectors 503 . The most probable sequence of states then corresponds to the most probable sequence of triphones (including acoustic phases), from which the output 511 can be determined.
- the determination of the most probable sequences of HMMs and states is carried out one step at a time, where each step corresponds to a feature vector in the sequence 503 , and by extension to a frame of sampled audio data.
- the process can be guided at each new step by the results of the previous step, since the most probable state determined for the previous step may constrain the possible (allowed) states that can be transitioned to on the next step.
- the NN determines a conditional probability that the particular feature vector would be emitted given the allowed next state.
- the NN may be trained before run time to recognize feature vectors as input, and to generate associated conditional probabilities as output. Then, at each time step corresponding to a frame at run time, the NN, based on what it has “learned” during training, generates a posterior conditional probability of being in the particular allowed next state, given the observed run-time feature vector.
- the emission probability for each particular allowed next state is a prior conditional probability of emitting the observed feature vector, given that the HMM is in the particular allowed next state.
- the prior conditional probability i.e., the emission probability—can be related to the posterior conditional probability through Bayes rule.
- the NN may be trained to be able to produce, at run time, the posterior conditional probability p(q k ⁇ x j ), corresponding to the a posteriori probability of the HMM state q k given the acoustic data x j observed at run time.
- the training of the NN may take place before run time, using training data (e.g., from the corpus).
- Bayes rule can be expressed as: p ( x j )/ p ( x j
- q k ) p ( q k )/ p ( q k ⁇ x j ), [1] where p(q k ) gives the prior probabilities for the q k states, and p(x j ) gives the probabilities for the acoustic features.
- the probabilities p(x j ) are the same for all states at run time, and so may be treated as a scaling constant in the expression for Bayes rule. It may therefore be seen that the a priori emission probabilities p(x j
- q k ) for the q k , k 1, . . . , K states follow from Bayes rule (equation [1] above) applied at run time for the HMM states.
- the most probable next state for that time step can be determined as the one that maximizes the combined likelihood of being transitioned to, and emitting the observed feature vector.
- the most probable sequence of states corresponding to a sequence of feature vectors is determined, and from which follows the most probable sequence of fundamental speech units in the corpus and a reconstruction of the utterance in the audio input signal.
- One of the aspects of using a neural network for determining the emission probabilities is that correlations among feature vectors are accounted for naturally in the “learning” process during training Consequently, categorization of feature vectors corresponding to the speech samples of the corpus can avoid simplifying assumptions often required by other analysis techniques, such as Gaussian mixture models, to deal with statistical complexities.
- the ability of neural networks to naturally account for correlations among feature vectors also enables determination of the probabilities for a given input feature vector to include input from a sub-sequence of feature vectors preceding and/or following the given feature vector. Feature vectors preceding and/or following a given feature vector can thereby provide additional context for the neural network.
- ANNs Artificial neural networks
- feed-forward networks may take the form of a multiplicity of interconnected “layers,” each including a set of “nodes.”
- a typical architecture may include an input layer, and output layer, and one or more intervening layers, commonly referred to as “hidden” layers.
- Each node in a given layer may correspond to a mathematical function for computing a scalar output of one or more inputs.
- the nodes of the input layer typically each receive just one input at a given computational step (e.g., time step), the total number of inputs to the neural network being the total number of nodes in the input layer.
- the computed outputs of each input-layer node may then serve as input to each node of the next (forward) layer.
- the nodes of the output layer deliver the output of the neural network, the total number of outputs of neural network being the total number of nodes in the output layer.
- All of the nodes may be the same scalar function, differing only according to possibly different parameter values, for example.
- the mathematical function could take the form of a sigmoid function, in which case each node could compute a sigmoidal nonlinearity of a weighted sum of its inputs. It will be appreciated that other functional forms could be used as well.
- Training a neural network may typically involve adjusting parameter values to achieve, to a given level of confidence, known results from known input data.
- a variety of techniques may be used to train a neural network, including stochastic gradient descent, batch gradient descent, second order methods, Hessian-free optimization, and gradient boost, among possibly others.
- a neural network to speech recognition involves providing one or more feature vectors as input, and delivering emission probabilities as output.
- the effectiveness and/or accuracy of a neural network may depend, at least in part, on the number of nodes per layer, and the number of hidden layers between the input and output layers.
- DNN Deep Neural Network
- a HNN/HMM speech recognition system may include a DNN for generation of emission probabilities.
- a DNN for generation of emission probabilities.
- predicted emission probabilities for a given sequence of input feature vectors may be accurately predicted, correspondingly supporting accurate speech recognition.
- a DNN can learn to accurately predict emission probabilities given run-time feature vectors.
- a DNN, and a neural network in general, including its layers, nodes, and connections between nodes may be implemented as executable instructions stored in one or another form of non-transient computer readable media, and executed by one of more processors of speech synthesis system, for example.
- FIG. 6 is a schematic illustration of processing of feature vectors with a neural network (e.g., a DNN) to determine emission probabilities for hidden Markov models.
- a time sequence of feature vectors 601 is represented by a “staircase” of overlapping rectangles labeled, by way of example, N ⁇ 2, N ⁇ 1, . . . , N, N+1, . . . , N+7, where each label corresponds to a frame time step at which the input audio data was acquired (e.g., digitally sampled).
- each feature vector corresponds to a frame of sampled audio input (e.g., an utterance), and that each frame may be acquired in a sliding time window.
- each frame i.e., the time window
- the time increment between successive windows is 10 ms.
- each next frame overlaps the previous frame by 15 ms.
- the time increment between frames e.g., 10 ms in the present example
- the frame period can be referred to as the frame period
- the inverse of the frame period can be referred as the frame rate (100 frames per second in the present example).
- the feature vectors 601 in FIG. 6 may be the output of sampling and digital processing, such as by the feature analysis module 502 shown in FIG. 5 .
- the frame-like representation of the feature vectors 601 may thus be taken as a visual cue that digital samples of the input utterance 501 may be acquired in time frames using a sliding window, and then subject to feature extraction.
- t ack,i may also be considered the time at which feature extraction is performed, although this is not necessarily a restriction of embodiments described herein.
- the legend at lower right of FIG. 6 reiterates the meanings of t i and t ack,i .
- feature vectors corresponding to frame times t ack,N , t ack,N+1 , . . . , t ack,N+5 are shown at being input to the neural network at neural network time steps t N , t N+1 , . . . , t N+5 .
- each feature vector is shown as being “accompanied” by two preceding (left-context) and two following (right-context) feature vectors corresponding to preceding and following frame acquisition times.
- the input to the neural network 602 at neural network time step t N includes the feature vector labeled N, together with feature vectors labeled N ⁇ 2, N ⁇ 1, N+1, and N+2, corresponding to frame acquisition times t ack,N ⁇ 2 , t ack,N ⁇ 1 , t ack,N , t ack,N+1 , and t ack,N+2 .
- the input to the neural network 602 at neural network time step t N+1 includes the feature vector labeled N+1, together with feature vectors labeled N ⁇ 1, N, N+2, and N+3, corresponding to frame acquisition times t ack,N ⁇ 1 , t ack,N , t ack,N+1 , t ack,N+2 , and t ack,N+3 .
- This pattern is extended in the figure up to neural network time step t N+5 for the feature vector labeled N+5, together with feature vectors labeled N+3, N+4, N+6, and N+7, corresponding to frame acquisition times t ack,N+3 , t ack,N+4 , t ack,N+5 , t ack,N+6 , and t ack,N+7 .
- each feature vector could be accompanied by four preceding and four following feature vectors.
- the number of preceding and following feature vectors need not be equal.
- p k ) for q k , k 1, . . . , K HMM states according, for example, to equation [1].
- the neural network 602 may be considered as operating at the input frame rate.
- K HMM states according, for example, to equation [1].
- the neural network 602 may be considered as operating at the input frame rate.
- a set of K emission probabilities 603 is generated at each of neural network time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- the output set of emission probabilities may apply to the HMM states at just one frame time, corresponding to just one frame of input audio data.
- FIG. 7 depicts a graph of observed acoustic data along a time (horizontal) axis versus HMM states along a vertical axis.
- an example utterance 701 of “cat sat” is input to an audio processing module 702 , which samples the input in frames and outputs a time sequence of feature vectors 703 .
- the feature vectors 703 are then input to a neural network 704 , which outputs respective sets of emission probabilities at each neural network time step.
- the feature vectors 703 may be considered analogous to the feature vectors 601 shown in FIG.
- Output of the emission probabilities at neural network time steps is represented as a series of short vertical arrows at times marked along the horizontal time axis, and occurs at the frame rate.
- a multiplicity of HMMs 705 - 1 , 705 - 2 , 705 - 3 , 705 - 4 , 705 - 5 , and 705 - 6 is represented as a portion of a concatenation of HMM states pictured along the vertical axis in FIG. 7 .
- Each HMM is used to model a respective triphone, and includes three states corresponding to three acoustic phases of the respective triphone.
- Each state is represented as a circle enclosing a state label q k , such as q 1 , q 2 , q 3 , etc.
- An arrow connecting adjacent states signifies a transition between the connected states, while a loop-shaped arrow signifies a “self-transition” that leaves a state unchanged after a given time step.
- the HMM 705 - 1 includes states q 1 , q 2 , and q 3 for modeling the triphone states #[k]ae.1, #[k]ae.2, and #[k]ae.3 of the triphone #[k]ae.
- the HMM 705 - 2 includes states q 4 , q 5 , and q 6 for modeling the triphone states k[ae]t.1, k[ae]t.2, and k[ae]t.3 of the triphone k[ae]t.
- the HMM 705 - 3 includes states q 7 , q 8 , and q 9 for modeling the triphone states ae[t]#.1, ae[t]#.2, and ae[t]#.3 of the triphone ae[t]#;
- the HMM 705 - 4 includes states q 10 , q 11 , and q 12 for modeling the triphone states #[s]ae.1, #[s]ae.2, and #[s]ae.3 of the triphone #[s]ae;
- the HMM 705 - 5 includes states q 4 , q 5 , and q 6 for modeling the triphone states s[ae]t.1, s[ae]t.2, and s[ae]t.3 of the triphone s[ae]t;
- the HMM 705 - 6 includes states q 7 , q 8 , and q 9 for modeling the triphone states ae[t]#.1, ae
- the HMM 705 - 2 for k[ae]t and the HMM 705 - 5 for s[ae]t are made up of the same states q 4 , q 5 , and q 6 .
- This repetition of states is meant to represent how HMM and HMM states may be shared among similar triphones.
- the HMM 705 - 3 for ae[t]# and the HMM 705 - 6 also for ae[t]# are made up of the same states q 7 , q 8 , and q 9 .
- the sharing of states is an example of “clustering” of similar triphones, which may help reduce the number of states that needs to be considered at each time step, as described below.
- the neural network 704 outputs of K emission probabilities for the states of the HMMs at each neural network time step; i.e. at the frame rate.
- K emission probabilities By applying the K emission probabilities to the K HMM states, one of the K states is determined to be most probable at each neural network time step.
- a path 709 through the graph of observed acoustic data versus HMM states is mapped out by connecting successive points in the graph, also at the frame rate.
- the path 709 then represents the most likely sequence of HMMs and HMM states, and thereby yields the sequence of triphones in the corpus that most probably corresponds to the input utterance 701 , as represented in the feature vectors 703 .
- a set of emission probabilities 707 is shown as being output from the neural network 704 at a current neural network time step t N .
- the emission probabilities 707 are labeled as p 1 , p 2 , p 3 , p 4 , . . . , and may be applied to similarly indexed HMM states. Note that p 4 , p 5 , and p 6 are repeated for the HMMs 705 - 2 and 705 - 5 . Similarly, p 7 , p 8 , and p 9 are repeated for the HMMs 705 - 3 and 705 - 6 .
- the HMM state q 4 of the HMM 705 - 5 is the most probable next state in this example.
- the immediately preceding state in this example was also q 4 of the HMM 705 - 5 .
- a legend at the lower right of FIG. 7 reiterates the proportional relation between the a priori emission probabilities and the a posteriori conditional probabilities generated by the neural network.
- HMM 705 - 6 there may be additional HMMs (and states) available to model the input utterance 701 .
- HMMs and states
- 40 phonemes the approximate number for spoken English
- Clustering of similar triphones and/or triphone acoustic phases, plus constraints that may rule out certain sequences of states, can help reduce this number to approximately 8,000 HMM states. Clustering is represented in FIG.
- the acoustic model 506 may also include an implementation (e.g., one or more coded algorithms) of a Gaussian mixture model (GMM).
- GMM Gaussian mixture model
- the singular term GMM applies to a collection or one or more mixtures of Gaussian distributions.
- the same one or more HMMs are used to model the fundamental speech units (e.g., phonemes, triphones, etc.).
- the GMM is used to determine emission probabilities to apply to the models, based on the observed data (i.e., sequence of feature vectors 503 in the example ASR system 500 ).
- the fundamental speech units of HMMs will be taken to be triphones, since this is the case in practice for certain ASR systems. It will be appreciated, however, that the principles discussed is not limited to triphones, and that other fundamental speech units can be used (e.g. phonemes, quinphones, clusters of similar and/or related speech units etc.).
- the determination of the most probable sequences of HMMs and states is again carried out one step at a time, where each step corresponds to a feature vector in the sequence 503 , and by extension to a frame of sampled audio data.
- each step corresponds to a feature vector in the sequence 503 , and by extension to a frame of sampled audio data.
- the particular set of defining features in the feature vectors used in the GMM approach may not necessarily be the same as the set used in NN approach.
- any such distinction between the defining features in feature vectors used in the GMM and NN approaches is may not necessarily be apparent in the sequence 503 , which, at the level of FIG. 5 , can be considered as input to either approach.
- the process of determining the most probable sequence of state corresponding to the input sequence of feature vectors can be guided at each new step by the results of the previous step, since the most probable state determined for the previous step may constrain the possible (allowed) states that can be transitioned to on the next step.
- the GMM determines a conditional probability that the particular feature vector would be emitted given the allowed next state.
- the GMM may be trained before run time to associate feature vectors with conditional probabilities as output. That is, at each time step corresponding to a frame at run time, GMM is used to determine, for each respective HMM state, a conditional probability of observing the feature vector at that time step given the respective HMM state. Thus for each frame, a plurality of GMM-based conditional probabilities is computed, one for each HMM state.
- Gaussian mixture model can be described as a weighted sum of M Gaussian densities, given by the expression:
- x is a D-dimensional continuous-valued vector (i.e., features)
- Each component density is a D-variate Gaussian function of the form:
- g ⁇ ( x ⁇ ⁇ i , ⁇ i ) 1 ( 2 ⁇ ⁇ ⁇ ) D / 2 ⁇ ⁇ ⁇ i ⁇ 1 / 2 ⁇ exp ⁇ ⁇ - 1 2 ⁇ ( x - ⁇ i ) ′ ⁇ ⁇ i - 1 ⁇ ( x - ⁇ i ) ⁇ , [ 3 ] with mean vector ⁇ i and covariance matrix ⁇ i .
- the parameters are adjusted for known fundamental speech units to produce mixtures that probabilistically represent the features as observed in the known fundamental speech units.
- the fundamental speech units could be phonemes, triphones, or clustering of similar triphones and/or triphone acoustic phases.
- GMM techniques are generally well-known, and not discussed in further detail herein.
- the most probable next state for that time step can be determined in the same manner as in the HNN/HMM approach. Namely, most probable next state is the one that maximizes the combined likelihood of being transitioned to, and emitting the observed feature vector.
- the most probable sequence of states corresponding to a sequence of feature vectors is determined, and from which follows the most probable sequence of fundamental speech units in the corpus and a reconstruction of the utterance in the audio input signal.
- the GMM approach may include simplifying assumptions, such as assuming negligible and/or ignorable correlations of features and/or feature vectors, the implementation costs, both in terms of computational complexity and processing resources, may be significantly smaller than those of the HNN/HMM approach. Moreover, in spite of the simplifying assumptions, the GMM approach may nevertheless yield largely equivalent, or even better, predictive results than the HNN/HMM for a certain subset of fundamental speech units.
- FIG. 8 is a schematic illustration of processing of feature vectors with a GMM to determine emission probabilities for hidden Markov models.
- a time sequence of feature vectors 801 is represented again by a “staircase” of overlapping rectangles labeled, by way of example, N ⁇ 2, N ⁇ 1, . . . , N, N+1, . . . , N+7, where each label corresponds to a frame time step at which the input audio data was acquired (e.g., digitally sampled).
- the representation of the feature vectors 801 is the same as that of feature vectors 601 in FIG. 6 , and the previous explanation of feature vectors 601 apply to feature vectors 801 as well. As noted above, however, the specific defining features of the feature vectors 801 may be different than those feature vectors 601 .
- the legend at lower right of FIG. 8 reiterates the meanings of t i and t ack,i .
- feature vectors corresponding to frame times t ack,N , t aCk,N+1 , . . . , t ack,N+5 are shown at being input to the GMM at neural GMM steps t N , t N+1 , . . . , t N+5 .
- each feature vector is shown as being “accompanied” by two preceding (left-context) and two following (right-context) feature vectors corresponding to preceding and following frame acquisition times.
- each feature vector in sequence 801 could be accompanied by four left-context and four right-context feature vectors.
- the number of left-context and right-context feature vectors need not be equal even within just sequence 801 .
- q k ) for q k , k 1, . . . , K HMM states according, for example. As such, the GMM 802 may be considered as operating at the input frame rate.
- a set of K emission probabilities 803 is generated at each of GMM time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- the output set of emission probabilities may apply to the HMM states at just one frame time, corresponding to just one frame of input audio data.
- FIG. 9 Application of emission probabilities to hidden Markov models to determine speech content represented in feature vectors is illustrated schematically in FIG. 9 , which may be considered analogous to FIG. 7 .
- an example utterance 901 of “cat sat” is input to an audio processing module 902 , which samples the input in frames and outputs a time sequence of feature vectors 903 .
- the feature vectors 903 are then input to a GMM 904 , which outputs respective sets of emission probabilities at each neural network time step.
- the feature vectors 903 may be considered analogous to the feature vectors 801 shown in FIG. 8
- the GMM 904 may be considered analogous to the GMM 802 also in FIG. 8 .
- Output of the emission probabilities at GMM time steps is again represented as a series of short vertical arrows at times marked along the horizontal time axis, and occurs at the frame rate.
- a multiplicity of HMMs 905 - 1 , 905 - 2 , 905 - 3 , 905 - 4 , 905 - 5 , and 905 - 6 is again represented as a portion of a concatenation of HMM states pictured along the vertical axis in FIG. 9 .
- each HMM is used to model a respective triphone, and includes three states corresponding to three acoustic phases of the respective triphone.
- the format of the HMMs is the same as in FIG. 7 .
- the GMM 904 outputs of K emission probabilities for the states of the HMMs at each GMM time step; i.e. at the frame rate.
- K emission probabilities for the states of the HMMs at each GMM time step; i.e. at the frame rate.
- applying the K emission probabilities to the K HMM states determines the most probable next state at each GMM time step.
- a path 909 through the graph of observed acoustic data versus HMM states is mapped out by connecting successive points in the graph, also at the frame rate.
- the path 909 then represents the most likely sequence of HMMs and HMM states, and thereby yields the sequence of triphones in the corpus that most probably corresponds to the input utterance 901 , as represented in the feature vectors 903 .
- the path 909 is depicted as being slightly different than the path 709 .
- a set of emission probabilities 907 is shown as being output from the GMM 904 at a current neural network time step t N .
- the emission probabilities 907 are labeled as b 1 , b 2 , b 3 , b 4 , . . . , and may be applied to similarly indexed HMM states. Note that b 4 , b 5 , and b 6 are repeated for the HMMs 905 - 2 and 905 - 5 . Similarly, b 7 , b 8 , and b 9 are repeated for the HMMs 905 - 3 and 905 - 6 .
- the HMM state q 6 of the HMM 905 - 5 is the most probable next state in this example.
- the immediately preceding state in this example was q 5 of the HMM 905 - 5 .
- a legend at the lower right of FIG. 9 defines the conditional probabilities generated by the GMM.
- Each of the HNN/HMM approach and the GMM approach have relative benefits in terms of the quality of their respective speech recognition results and corresponding predicted outputs (e.g., textual renderings of the recognized speech). For example, it has been found empirically that for a common multiplicity of HMM states, the GMM approach has an accuracy of about 64% in recognizing the phoneme /iy/, while the accuracy for the NN approach is about 62%. As another example, the accuracy of the NN approach in recognizing the phoneme /ay/ is about 68%, while the accuracy of the GMM approach is about 55%. There may be other examples as well.
- each approach uses the same set of HMM states to make predictions from the same set of fundamental speech units (e.g., phonemes, triphones, or clusters of similar and/or related speech units)
- differences in the resulting recognition and predictions may be largely attributable to differences in the emission probabilities generated by each approach.
- both the NN and GMM generate emission probabilities for the same HMM states.
- differences between the two approaches in the predicted sequence of HMM states may be attributable to frame-by-frame differences between the NN-based conditional probabilities and the GMM-based conditional probabilities. Since the two approaches may yield better relative predictive accuracy for different fundamental speech units, it would be desirable to merge their respective emission probabilities on a frame-by-frame basis in a way that enhances the overall accuracy compared to just one or the other of the approaches.
- NN-based conditional probabilities and GMM-based conditional probabilities may be merged on a frame-by-frame basis by determining weighted sums of the two types of probabilities. More specifically, for each frame of an audio input signal a feature extraction may generate two parallel sequences of feature vectors. A first sequence may be input to a neural network for computation of NN-based conditional probabilities, and a second sequence may be input to a GMM for computation of GM-based conditional probabilities. As noted above, the two sequences could be duplicates of one another, or each could carry different defining features. In either case, the two sequences would correspond with each other, and with a sequence of frames of in audio input signal, on a frame-by-frame basis.
- both the NN and the GMM may generate emission probabilities for the same multiplicity of HMM states on a frame-by-frame basis.
- On a frame-by-frame basis then, there will be one NN-based conditional probability and one GMM-based conditional probability for each HMM state.
- For each HMM state a weighted sum of the NN-based conditional probability and the GMM-based conditional probability can be computed to yield a merged probability.
- FIG. 10 illustrates an example embodiment of frame-level merging of NN-based and GMM-based conditional probabilities for HMMs.
- a time sequence of feature vectors 1001 is represented again by a “staircase” of overlapping rectangles labeled, by way of example, N ⁇ 2, N ⁇ 1, . . . , N, N+1, . . . , N+7, where each label corresponds to a frame time step at which the input audio data was acquired (e.g., digitally sampled).
- the representation of the feature vectors 1001 is the same as that of feature vectors 601 and 801 in FIGS. 6 and 8 , and the previous explanations of feature vectors 601 and 801 apply to feature vectors 1001 as well.
- explicit labeling of the input frames and context frames has been omitted.
- the single sequence feature vectors 1001 is intended to represent a common frame-level feature extraction for the two sequences referred to above. However, as noted, the specific defining features of each sequence of feature vectors could be different.
- the corresponding feature vectors in each stream may be presented as input at the same time step to a GMM 1002 and a NN 1004 .
- the input feature vector to each of the NN and GMM may be accompanied by different contextual feature vectors (i.e., different numbers of left-context and right-context feature vectors).
- q k ) for q k , k 1, . . . , K HMM states.
- q k ) for q k , k 1, . . . , K HMM states.
- K is generated at each of the same time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- a set of weighted emission probabilities 1007 is generated, state-by-state, as weighted sums of the NN-based and the GMM-based conditional probabilities. These are designated ⁇ N,k (p,b); ⁇ N+1,k (p,b); ⁇ N+2,k (p,b); ⁇ N+3,k (p,b); ⁇ N+4,k (p,b); ⁇ N+5,k (p,b); at each of time steps t N , t N+1 , t N+2 , t N+3 , t N+4 , and t N+5 .
- the k-subscript indicates that a separate weighted sum is determined for each of the K states.
- the embodiments described herein so far include generating and merging emission probabilities for two approaches, namely NN and GMM.
- the approach can be generalized to encompass more than just the two techniques for determining probabilities, and then merging their respective probabilities.
- indexed m 1, . . .
- FIG. 11 Application of the merged emission probabilities to hidden Markov models to determine speech content represented in feature vectors is illustrated schematically in FIG. 11 , which may be considered analogous to FIGS. 7 and 9 .
- an example utterance 1101 of “cat sat” is input to an audio processing module 1102 , which samples the input in frames and outputs a time sequence of feature vectors 1103 .
- the sequence 1103 may be considered as representing two parallel sequences, one for NN processing and the other for GMM processing. This is depicted by the delivery of the feature vectors 1103 as input to both GMM 1104 and NN 1106 .
- the respective output emission probabilities of GMM 1104 and of NN 1106 are then input to a merge module 1108 .
- the output of the merge module 1108 may be considered the merged stream 1007 shown in FIG. 10 .
- the merged emission probabilities are applied to the K HMM states at each time step t i , as represented by the short vertical arrows at times marked along the horizontal time axis in FIG. 11 .
- the merge module 1108 could be machine-executable instructions for an algorithm for computing weighted sums, such as that expressed in equation [4].
- a multiplicity of HMMs 1105 - 1 , 1105 - 2 , 1105 - 3 , 1105 - 4 , 1105 - 5 , and 1105 - 6 is again represented as a portion of a concatenation of HMM states pictured along the vertical axis in FIG. 11 .
- each HMM is used to model a respective triphone, and includes three states corresponding to three acoustic phases of the respective triphone.
- the format of the HMMs is the same as in FIGS. 7 and 9 .
- the merge module 1108 outputs of K emission probabilities for the states of the HMMs at each time step; i.e. at the frame rate.
- K emission probabilities for the states of the HMMs at each time step; i.e. at the frame rate.
- applying the K emission probabilities to the K HMM states determines the most probable next state at each time step.
- a path 1109 through the graph of observed acoustic data versus HMM states is mapped out by connecting successive points in the graph, also at the frame rate.
- the path 1109 then represents the most likely sequence of HMMs and HMM states, and thereby yields the sequence of triphones in the corpus that most probably corresponds to the input utterance 1101 , as represented in the feature vectors 1103 .
- the path 1109 is depicted as being slightly different than each of the path 709 and 909 .
- a set of emission probabilities 1107 is shown as being output from the merge module 1108 at a current neural network time step t N .
- the emission probabilities 1107 are labeled as ⁇ 1 , ⁇ 2 , ⁇ 3 , ⁇ 4 , . . . , and may be applied to similarly indexed HMM states. Note that ⁇ 4 , ⁇ 5 , and ⁇ 6 are repeated for the HMMs 1105 - 2 and 1105 - 5 . Similarly, ⁇ 7 , ⁇ 8 , and ⁇ 9 are repeated for the HMMs 1105 - 3 and 1105 - 6 .
- the HMM state q 5 of the HMM 1105 - 5 is the most probable next state in this example.
- the immediately preceding state in this example was also q 5 of the HMM 1105 - 5 .
- a legend at the lower right of FIG. 11 defines the merged conditional probabilities generated by the merge module 1108 .
- an ASR system that implements frame-level merging of emission probabilities generated by two or more approaches, such NN-based and GMM-based techniques, can be trained by first training separately for each of the two or more approaches, and then training the frame-level merging of emission probabilities. More particularly, training of the frame-level merging may be achieved by iteratively adjusting the weights (e.g., weighting functions w m (k) in equation [5]) so as to optimize speech recognition accuracy (or reduce errors or penalty functions to below one or more thresholds). For speech recognition with emission probabilities determined by a neural network, at least some techniques for training may be generally well-known, and are not discussed further herein.
- Training can be illustrated by considering the general case of M different techniques for determining emission probabilities, where the weighted sums are given by equation [5].
- the probability p t i m (k) may be replaced with a “score” x t i m (k) for the state k at time t i .
- the weighting function w m (k) may be expressed as a k-element vector W m .
- the score x t i m (k) can also be expressed as a k-element vector x t i m .
- training can be considered as determining w m with the large margin criteria.
- classification error can be regularized by the l 2 norm. This can be expressed as the optimization problem:
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- FIG. 12 is a block diagram depicting an example ASR system 1200 that illustrates operational aspects of merged emission probabilities for hidden Markov models, in accordance with an example embodiment. As with FIG. 5 , FIG. 12 shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the ASR system 1200 include a feature analysis module 1202 , a GMM module 1204 , a NN module 1206 , a merge module 1208 , a HMM module 1210 , a merge training module 1212 , and a speech database 1214 .
- These functional components could be implemented as machine-language instructions in a centralized and/or distributed fashion on one or more computing platforms or systems, such as those described above.
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- Various inputs and outputs are also identified, and discussed below.
- the figure is depicted in a way the represents two operational modes: training-time and run-time.
- a thick, horizontal line marks a conceptual boundary between these two modes, with “Training-Time” labeling a portion of FIG. 12 above the line, and “Run-Time” labeling a portion below the line.
- various arrows in the figure signifying information and/or processing flow and/or transmission are shown as dashed lines in the “Training-Time” portion of the figure, and as solid lines in the “Run-Time” portion.
- a training-time utterance 1201 stored in the speech database 1214 is presented as input to the feature analysis module 1202 , which then outputs the sequence 1203 of training-time feature vectors.
- the sequence 1203 could be two parallel sequences that correspond on a frame-by-frame basis to each frame of data in the input training-time utterance 1201 . This is depicted conceptually be delivery of the sequence 1203 to each of the GMM module 1204 and the NN module 1206 .
- the GMM module 1204 generates and outputs the training-time GMM-based state predictions 1205 (i.e., conditional probabilities), while the NN module 1206 generates and outputs the training-time NN-based state predictions 1207 (i.e., conditional probabilities). Both the GMM-based state predictions 1205 and the NN-based state predictions 1207 are input to the merge module 1208 , which then merges the two sets of state prediction according the description of FIG. 10 or of equation [4], for example.
- the output of the merge module 1208 is the training-time merged state predictions 1209 . These could correspond to the merged emission probabilities 1007 in FIG. 10 or merged emission probabilities 1107 in FIG. 11 , for example.
- the training-time merged state predictions 1209 are then input to the HMM module 1210 , which outputs the training-time predicted text 1211 .
- the training-time predicted text 1211 is input to the merge training module 1212 , which also gets target text 1213 from the speech database 1214 .
- the target text 1213 corresponds to a known textual translation of the training-time utterance 1201 , as indicated by the wavey-dashed-line double arrow pointing to each of the target text 1213 and the training-time utterance 1201 . That is, the target text 1213 can be consider the “correct” speech-to-text translation of the training-time utterance 1201 .
- merge module 1212 Having both the target text 1213 and the training-time predicted text 1211 as input, merge module 1212 carry out one or another form of parameter adjustment, such as the optimization algorithm illustrated in Table 1, in order to adjust the weights of the frame-level merging so as to achieve best or optimal agreement between the target text 1213 and the training-time predicted text 1211 .
- this process might be iterative.
- the training procedure represented in FIG. 12 may be carried out repeatedly, over numerous samples of training-time utterances in order to determine what might be considered the best or optimal weights to use in the merging process.
- a run-time utterance 1215 such as might be supplied by a user or retrieved from a stored audio file, is presented as input to the feature analysis module 1202 , which then outputs the sequence 1217 of run-time feature vectors.
- the sequence 1217 could be two parallel sequences that correspond on a frame-by-frame basis to each frame of data in the input run-time utterance 1215 . This is again depicted conceptually be delivery of the sequence 1217 to each of the GMM module 1204 and the NN module 1206 .
- the GMM module 1204 generates and outputs the run-time GMM-based state predictions 1219 (i.e., conditional probabilities), while the NN module 1206 generates and outputs the training-time NN-based state predictions 1221 (i.e., conditional probabilities). Both the GMM-based state predictions 1219 and the NN-based state predictions 1221 are input to the merge module 1208 , which then merges the two sets of state prediction according the description of FIG. 10 or of equation [4], for example.
- the output of the merge module 1208 is the run-time merged state predictions 1223 . These could correspond to the merged emission probabilities 1007 in FIG. 10 or merged emission probabilities 1107 in FIG. 11 , for example.
- the run-time merged state predictions 1223 are then input to the HMM module 1210 , which outputs the run-time predicted text 1225 . Having been trained to achieve optimum weights for use in the merge module 1208 , the HMM module 1210 may yield more accurate run-time predicted text 1225 than might be achieved using either of the GMM-based state predictions 1219 or the NN-based state predictions 1221 individually.
Abstract
Description
p(x j)/p(x j |q k)=p(q k)/p(q k ∥x j), [1]
where p(qk) gives the prior probabilities for the qk states, and p(xj) gives the probabilities for the acoustic features. Before run time, the ASR system may also be trained to generate expected output (e.g., text strings) from known input speech (e.g., utterances), from which relative frequencies of the qk, k=1, . . . , K states, and correspondingly the prior probabilities p(qk) for the qk states may be determined. In addition, the probabilities p(xj) are the same for all states at run time, and so may be treated as a scaling constant in the expression for Bayes rule. It may therefore be seen that the a priori emission probabilities p(xj|qk) for the qk, k=1, . . . , K states follow from Bayes rule (equation [1] above) applied at run time for the HMM states.
with mean vector μi and covariance matrix Σi.
ψt t
where ψt
Ψt
where Ψt
i=1, . . . , T, training can be considered as determining wm with the large margin criteria. More particularly, the classification error can be regularized by the l2 norm. This can be expressed as the optimization problem:
where yt
can be used to count the classification error.
TABLE 1 |
Subgradient descent algorithm for learning merging weights |
Input: {(xt |
{ηt} = t = 1T |
w0 = 0 |
for ti, i = 1, . . . , T do |
|Inference with loss augmented objective: |
|ŷt |
|Compute the subgradient: gt |
|Update the weights: wt |
|
It will be appreciated that the algorithm represented in Table 1, as well as the other various mathematical expressions and formulae described herein, could be implemented in variety of ways as machine-language instructions. The machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
Claims (23)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/765,002 US9240184B1 (en) | 2012-11-15 | 2013-02-12 | Frame-level combination of deep neural network and gaussian mixture models |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261726714P | 2012-11-15 | 2012-11-15 | |
US13/765,002 US9240184B1 (en) | 2012-11-15 | 2013-02-12 | Frame-level combination of deep neural network and gaussian mixture models |
Publications (1)
Publication Number | Publication Date |
---|---|
US9240184B1 true US9240184B1 (en) | 2016-01-19 |
Family
ID=55071465
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/765,002 Active 2033-12-05 US9240184B1 (en) | 2012-11-15 | 2013-02-12 | Frame-level combination of deep neural network and gaussian mixture models |
Country Status (1)
Country | Link |
---|---|
US (1) | US9240184B1 (en) |
Cited By (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150019214A1 (en) * | 2013-07-10 | 2015-01-15 | Tencent Technology (Shenzhen) Company Limited | Method and device for parallel processing in model training |
US20150161988A1 (en) * | 2013-12-06 | 2015-06-11 | International Business Machines Corporation | Systems and methods for combining stochastic average gradient and hessian-free optimization for sequence training of deep neural networks |
US20150170640A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Generating representations of acoustic sequences |
US20150243285A1 (en) * | 2012-09-07 | 2015-08-27 | Carnegie Mellon University, A Pennsylvania Non-Profit Corporation | Methods for hybrid gpu/cpu data processing |
CN105763303A (en) * | 2016-04-19 | 2016-07-13 | 成都翼比特自动化设备有限公司 | Hybrid automatic repeat request algorithm based on prediction |
CN105976812A (en) * | 2016-04-28 | 2016-09-28 | 腾讯科技（深圳）有限公司 | Voice identification method and equipment thereof |
CN108172229A (en) * | 2017-12-12 | 2018-06-15 | 天津津航计算技术研究所 | A kind of authentication based on speech recognition and the method reliably manipulated |
WO2019006088A1 (en) * | 2017-06-28 | 2019-01-03 | The Regents Of The University Of California | Training artificial neural networks with reduced computational complexity |
CN109144719A (en) * | 2018-07-11 | 2019-01-04 | 东南大学 | Cooperation discharging method based on markov decision process in mobile cloud computing system |
US20190150764A1 (en) * | 2016-05-02 | 2019-05-23 | The Regents Of The University Of California | System and Method for Estimating Perfusion Parameters Using Medical Imaging |
US10380997B1 (en) * | 2018-07-27 | 2019-08-13 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US10475442B2 (en) * | 2015-11-25 | 2019-11-12 | Samsung Electronics Co., Ltd. | Method and device for recognition and method and device for constructing recognition model |
US10503580B2 (en) | 2017-06-15 | 2019-12-10 | Microsoft Technology Licensing, Llc | Determining a likelihood of a resource experiencing a problem based on telemetry data |
US10599769B2 (en) * | 2018-05-01 | 2020-03-24 | Capital One Services, Llc | Text categorization using natural language processing |
US10657437B2 (en) * | 2016-08-18 | 2020-05-19 | International Business Machines Corporation | Training of front-end and back-end neural networks |
US20200312307A1 (en) * | 2019-03-25 | 2020-10-01 | Microsoft Technology Licensing, Llc | Dynamic Combination of Acoustic Model States |
US10805317B2 (en) | 2017-06-15 | 2020-10-13 | Microsoft Technology Licensing, Llc | Implementing network security measures in response to a detected cyber attack |
US10832129B2 (en) | 2016-10-07 | 2020-11-10 | International Business Machines Corporation | Transfer of an acoustic knowledge to a neural network |
US10855455B2 (en) * | 2019-01-11 | 2020-12-01 | Advanced New Technologies Co., Ltd. | Distributed multi-party security model training framework for privacy protection |
US10922627B2 (en) | 2017-06-15 | 2021-02-16 | Microsoft Technology Licensing, Llc | Determining a course of action based on aggregated data |
US10930271B2 (en) * | 2013-07-31 | 2021-02-23 | Google Llc | Speech recognition using neural networks |
US20210082311A1 (en) * | 2018-05-11 | 2021-03-18 | Speech Engineering Limited | Computer implemented method and apparatus for recognition of speech patterns and feedback |
US11043218B1 (en) * | 2019-06-26 | 2021-06-22 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
US11062226B2 (en) | 2017-06-15 | 2021-07-13 | Microsoft Technology Licensing, Llc | Determining a likelihood of a user interaction with a content element |
US11132990B1 (en) * | 2019-06-26 | 2021-09-28 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
US11205050B2 (en) * | 2018-11-02 | 2021-12-21 | Oracle International Corporation | Learning property graph representations edge-by-edge |
CN115622608A (en) * | 2022-09-29 | 2023-01-17 | 广州爱浦路网络技术有限公司 | Method, system and medium for optimization of offloading strategies based on low-earth-orbit satellite edge calculation |
US11621011B2 (en) | 2018-10-29 | 2023-04-04 | Dolby International Ab | Methods and apparatus for rate quality scalable coding with generative models |
US11977974B2 (en) | 2017-11-30 | 2024-05-07 | International Business Machines Corporation | Compression of fully connected / recurrent layers of deep network(s) through enforcing spatial locality to weight matrices and effecting frequency compression |
Citations (94)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5007093A (en) * | 1987-04-03 | 1991-04-09 | At&T Bell Laboratories | Adaptive threshold voiced detector |
US5046100A (en) * | 1987-04-03 | 1991-09-03 | At&T Bell Laboratories | Adaptive multivariate estimating apparatus |
US5193142A (en) * | 1990-11-15 | 1993-03-09 | Matsushita Electric Industrial Co., Ltd. | Training module for estimating mixture gaussian densities for speech-unit models in speech recognition systems |
US5502790A (en) | 1991-12-24 | 1996-03-26 | Oki Electric Industry Co., Ltd. | Speech recognition method and system using triphones, diphones, and phonemes |
US5509103A (en) * | 1994-06-03 | 1996-04-16 | Motorola, Inc. | Method of training neural networks used for speech recognition |
US5535305A (en) | 1992-12-31 | 1996-07-09 | Apple Computer, Inc. | Sub-partitioned vector quantization of probability density functions |
US5548684A (en) | 1994-04-22 | 1996-08-20 | Georgia Tech Research Corporation | Artificial neural network viterbi decoding system and method |
US5555344A (en) | 1991-09-20 | 1996-09-10 | Siemens Aktiengesellschaft | Method for recognizing patterns in time-variant measurement signals |
US5692100A (en) | 1994-02-02 | 1997-11-25 | Matsushita Electric Industrial Co., Ltd. | Vector quantizer |
US5734793A (en) * | 1994-09-07 | 1998-03-31 | Motorola Inc. | System for recognizing spoken sounds from continuous speech and method of using same |
US5737486A (en) | 1990-05-10 | 1998-04-07 | Nec Corporation | Pattern recognition method by using reference patterns each of which is defined by predictors |
US5745649A (en) | 1994-07-07 | 1998-04-28 | Nynex Science & Technology Corporation | Automated speech recognition using a plurality of different multilayer perception structures to model a plurality of distinct phoneme categories |
US5754978A (en) * | 1995-10-27 | 1998-05-19 | Speech Systems Of Colorado, Inc. | Speech recognition system |
US5754681A (en) * | 1994-10-05 | 1998-05-19 | Atr Interpreting Telecommunications Research Laboratories | Signal pattern recognition apparatus comprising parameter training controller for training feature conversion parameters and discriminant functions |
US5794197A (en) | 1994-01-21 | 1998-08-11 | Micrsoft Corporation | Senone tree representation and evaluation |
US5839103A (en) * | 1995-06-07 | 1998-11-17 | Rutgers, The State University Of New Jersey | Speaker verification system using decision fusion logic |
US5867816A (en) | 1995-04-24 | 1999-02-02 | Ericsson Messaging Systems Inc. | Operator interactions for developing phoneme recognition by neural networks |
US5930754A (en) | 1997-06-13 | 1999-07-27 | Motorola, Inc. | Method, device and article of manufacture for neural-network based orthography-phonetics transformation |
US5937384A (en) | 1996-05-01 | 1999-08-10 | Microsoft Corporation | Method and system for speech recognition using continuous density hidden Markov models |
US6092039A (en) | 1997-10-31 | 2000-07-18 | International Business Machines Corporation | Symbiotic automatic speech recognition and vocoder |
US6226612B1 (en) * | 1998-01-30 | 2001-05-01 | Motorola, Inc. | Method of evaluating an utterance in a speech recognition system |
US6233550B1 (en) | 1997-08-29 | 2001-05-15 | The Regents Of The University Of California | Method and apparatus for hybrid coding of speech at 4kbps |
US6363289B1 (en) | 1996-09-23 | 2002-03-26 | Pavilion Technologies, Inc. | Residual activation neural network |
US6366883B1 (en) * | 1996-05-15 | 2002-04-02 | Atr Interpreting Telecommunications | Concatenation of speech segments by use of a speech synthesizer |
US20020091522A1 (en) * | 2001-01-09 | 2002-07-11 | Ning Bi | System and method for hybrid voice recognition |
US20020116196A1 (en) * | 1998-11-12 | 2002-08-22 | Tran Bao Q. | Speech recognizer |
US20020128834A1 (en) | 2001-03-12 | 2002-09-12 | Fain Systems, Inc. | Speech recognition system using spectrogram analysis |
US6456969B1 (en) * | 1997-12-12 | 2002-09-24 | U.S. Philips Corporation | Method of determining model-specific factors for pattern recognition, in particular for speech patterns |
US6490555B1 (en) * | 1997-03-14 | 2002-12-03 | Scansoft, Inc. | Discriminatively trained mixture models in continuous speech recognition |
US20020193991A1 (en) * | 2001-06-13 | 2002-12-19 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
US20030009334A1 (en) * | 2001-07-03 | 2003-01-09 | International Business Machines Corporation | Speech processing board for high volume speech processing applications |
US6526380B1 (en) * | 1999-03-26 | 2003-02-25 | Koninklijke Philips Electronics N.V. | Speech recognition system having parallel large vocabulary recognition engines |
US6671669B1 (en) * | 2000-07-18 | 2003-12-30 | Qualcomm Incorporated | combined engine system and method for voice recognition |
US20040064315A1 (en) | 2002-09-30 | 2004-04-01 | Deisher Michael E. | Acoustic confidence driven front-end preprocessing for speech recognition in adverse environments |
US6760699B1 (en) | 2000-04-24 | 2004-07-06 | Lucent Technologies Inc. | Soft feature decoding in a distributed automatic speech recognition system for use over wireless channels |
US20040138885A1 (en) * | 2003-01-09 | 2004-07-15 | Xiaofan Lin | Commercial automatic speech recognition engine combinations |
US20040153319A1 (en) * | 2003-01-30 | 2004-08-05 | Sherif Yacoub | Two-engine speech recognition |
US20040210437A1 (en) * | 2003-04-15 | 2004-10-21 | Aurilab, Llc | Semi-discrete utterance recognizer for carefully articulated speech |
US20040236573A1 (en) | 2001-06-19 | 2004-11-25 | Sapeluk Andrew Thomas | Speaker recognition systems |
US6845357B2 (en) | 2001-07-24 | 2005-01-18 | Honeywell International Inc. | Pattern recognition using an observable operator model |
US20050065790A1 (en) * | 2003-09-23 | 2005-03-24 | Sherif Yacoub | System and method using multiple automated speech recognition engines |
US6898567B2 (en) * | 2001-12-29 | 2005-05-24 | Motorola, Inc. | Method and apparatus for multi-level distributed speech recognition |
US6912499B1 (en) * | 1999-08-31 | 2005-06-28 | Nortel Networks Limited | Method and apparatus for training a multilingual speech model set |
US20050213810A1 (en) * | 2004-03-29 | 2005-09-29 | Kohtaro Sabe | Information processing apparatus and method, recording medium, and program |
US6963837B1 (en) | 1999-10-06 | 2005-11-08 | Multimodal Technologies, Inc. | Attribute-based word modeling |
US6963835B2 (en) | 2003-03-31 | 2005-11-08 | Bae Systems Information And Electronic Systems Integration Inc. | Cascaded hidden Markov model for meta-state estimation |
US20050286772A1 (en) * | 2004-06-24 | 2005-12-29 | Lockheed Martin Corporation | Multiple classifier system with voting arbitration |
US20060136205A1 (en) * | 2004-12-21 | 2006-06-22 | Song Jianming J | Method of refining statistical pattern recognition models and statistical pattern recognizers |
US20060136209A1 (en) | 2004-12-16 | 2006-06-22 | Sony Corporation | Methodology for generating enhanced demiphone acoustic models for speech recognition |
US20060143010A1 (en) * | 2004-12-23 | 2006-06-29 | Samsung Electronics Co., Ltd. | Method, medium, and apparatus recognizing speech |
US7228275B1 (en) * | 2002-10-21 | 2007-06-05 | Toyota Infotechnology Center Co., Ltd. | Speech recognition system having multiple speech recognizers |
US20070136059A1 (en) * | 2005-12-12 | 2007-06-14 | Gadbois Gregory J | Multi-voice speech recognition |
US20070198261A1 (en) * | 2006-02-21 | 2007-08-23 | Sony Computer Entertainment Inc. | Voice recognition with parallel gender and age normalization |
US20070198257A1 (en) * | 2006-02-20 | 2007-08-23 | Microsoft Corporation | Speaker authentication |
US20070288242A1 (en) * | 2006-06-12 | 2007-12-13 | Lockheed Martin Corporation | Speech recognition and control system, program product, and related methods |
US20080059200A1 (en) | 2006-08-22 | 2008-03-06 | Accenture Global Services Gmbh | Multi-Lingual Telephonic Service |
US20080147391A1 (en) | 2006-12-15 | 2008-06-19 | Samsung Electronics Co., Ltd. | Method of and apparatus for transforming speech feature vector |
US20080208577A1 (en) * | 2007-02-23 | 2008-08-28 | Samsung Electronics Co., Ltd. | Multi-stage speech recognition apparatus and method |
US20080249762A1 (en) * | 2007-04-05 | 2008-10-09 | Microsoft Corporation | Categorization of documents using part-of-speech smoothing |
US20090018833A1 (en) * | 2007-07-13 | 2009-01-15 | Kozat Suleyman S | Model weighting, selection and hypotheses combination for automatic speech recognition and machine translation |
US20090048843A1 (en) | 2007-08-08 | 2009-02-19 | Nitisaroj Rattima | System-effected text annotation for expressive prosody in speech synthesis and recognition |
US7496512B2 (en) | 2004-04-13 | 2009-02-24 | Microsoft Corporation | Refining of segmental boundaries in speech waveforms using contextual-dependent models |
US20090138265A1 (en) * | 2007-11-26 | 2009-05-28 | Nuance Communications, Inc. | Joint Discriminative Training of Multiple Speech Recognizers |
US7574357B1 (en) | 2005-06-24 | 2009-08-11 | The United States Of America As Represented By The Admimnistrator Of The National Aeronautics And Space Administration (Nasa) | Applications of sub-audible speech recognition based upon electromyographic signals |
US20090216528A1 (en) * | 2005-06-01 | 2009-08-27 | Roberto Gemello | Method of adapting a neural network of an automatic speech recognition device |
US20090319342A1 (en) * | 2008-06-19 | 2009-12-24 | Wize, Inc. | System and method for aggregating and summarizing product/topic sentiment |
US20100004930A1 (en) * | 2008-07-02 | 2010-01-07 | Brian Strope | Speech Recognition with Parallel Recognition Tasks |
US7660774B2 (en) | 2005-05-31 | 2010-02-09 | Honeywell International Inc. | Nonlinear neural network fault detection system and method |
US20100121638A1 (en) | 2008-11-12 | 2010-05-13 | Mark Pinson | System and method for automatic speech to text conversion |
US20100185436A1 (en) | 2009-01-21 | 2010-07-22 | Al-Zahrani Abdul Kareem Saleh | Arabic poetry meter identification system and method |
US20100198598A1 (en) | 2009-02-05 | 2010-08-05 | Nuance Communications, Inc. | Speaker Recognition in a Speech Recognition System |
US20100217589A1 (en) | 2009-02-20 | 2010-08-26 | Nuance Communications, Inc. | Method for Automated Training of a Plurality of Artificial Neural Networks |
US7826894B2 (en) | 2004-03-22 | 2010-11-02 | California Institute Of Technology | Cognitive control signals for neural prosthetics |
US20100280827A1 (en) * | 2009-04-30 | 2010-11-04 | Microsoft Corporation | Noise robust speech classifier ensemble |
US20100318358A1 (en) * | 2007-02-06 | 2010-12-16 | Yoshifumi Onishi | Recognizer weight learning device, speech recognizing device, and system |
US20100332229A1 (en) * | 2009-06-30 | 2010-12-30 | Sony Corporation | Apparatus control based on visual lip share recognition |
US20110040561A1 (en) * | 2006-05-16 | 2011-02-17 | Claudio Vair | Intersession variability compensation for automatic extraction of information from voice |
US20110224981A1 (en) * | 2001-11-27 | 2011-09-15 | Miglietta Joseph H | Dynamic speech recognition and transcription among users having heterogeneous protocols |
US20110288855A1 (en) | 2002-06-28 | 2011-11-24 | Conceptual Speech Llc | Multi-phoneme streamer and knowledge representation speech recognition system and method |
US20120072215A1 (en) | 2010-09-21 | 2012-03-22 | Microsoft Corporation | Full-sequence training of deep structures for speech recognition |
US20120076361A1 (en) * | 2009-06-03 | 2012-03-29 | Hironobu Fujiyoshi | Object detection device |
US20120084086A1 (en) * | 2010-09-30 | 2012-04-05 | At&T Intellectual Property I, L.P. | System and method for open speech recognition |
US20120101401A1 (en) * | 2009-04-07 | 2012-04-26 | National University Of Ireland | Method for the real-time identification of seizures in an electroencephalogram (eeg) signal |
US20120166194A1 (en) * | 2010-12-23 | 2012-06-28 | Electronics And Telecommunications Research Institute | Method and apparatus for recognizing speech |
US8239195B2 (en) | 2008-09-23 | 2012-08-07 | Microsoft Corporation | Adapting a compressed model for use in speech recognition |
US20120269441A1 (en) * | 2011-04-19 | 2012-10-25 | Xerox Corporation | Image quality assessment |
US8311827B2 (en) | 2007-09-21 | 2012-11-13 | The Boeing Company | Vehicle control |
US20130006991A1 (en) * | 2011-06-28 | 2013-01-03 | Toru Nagano | Information processing apparatus, method and program for determining weight of each feature in subjective hierarchical clustering |
US8352245B1 (en) * | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
US20130226587A1 (en) * | 2012-02-27 | 2013-08-29 | Hong Kong Baptist University | Lip-password Based Speaker Verification System |
US20140149104A1 (en) * | 2012-11-23 | 2014-05-29 | Idiap Research Institute | Apparatus and method for constructing multilingual acoustic model and computer readable recording medium for storing program for performing the method |
US20140278390A1 (en) * | 2013-03-12 | 2014-09-18 | International Business Machines Corporation | Classifier-based system combination for spoken term detection |
US20140358265A1 (en) * | 2013-05-31 | 2014-12-04 | Dolby Laboratories Licensing Corporation | Audio Processing Method and Audio Processing Apparatus, and Training Method |
US20150058004A1 (en) * | 2013-08-23 | 2015-02-26 | At & T Intellectual Property I, L.P. | Augmented multi-tier classifier for multi-modal voice activity detection |
-
2013
- 2013-02-12 US US13/765,002 patent/US9240184B1/en active Active
Patent Citations (99)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5046100A (en) * | 1987-04-03 | 1991-09-03 | At&T Bell Laboratories | Adaptive multivariate estimating apparatus |
US5007093A (en) * | 1987-04-03 | 1991-04-09 | At&T Bell Laboratories | Adaptive threshold voiced detector |
US5737486A (en) | 1990-05-10 | 1998-04-07 | Nec Corporation | Pattern recognition method by using reference patterns each of which is defined by predictors |
US5193142A (en) * | 1990-11-15 | 1993-03-09 | Matsushita Electric Industrial Co., Ltd. | Training module for estimating mixture gaussian densities for speech-unit models in speech recognition systems |
US5555344A (en) | 1991-09-20 | 1996-09-10 | Siemens Aktiengesellschaft | Method for recognizing patterns in time-variant measurement signals |
US5502790A (en) | 1991-12-24 | 1996-03-26 | Oki Electric Industry Co., Ltd. | Speech recognition method and system using triphones, diphones, and phonemes |
US5535305A (en) | 1992-12-31 | 1996-07-09 | Apple Computer, Inc. | Sub-partitioned vector quantization of probability density functions |
US5794197A (en) | 1994-01-21 | 1998-08-11 | Micrsoft Corporation | Senone tree representation and evaluation |
US5692100A (en) | 1994-02-02 | 1997-11-25 | Matsushita Electric Industrial Co., Ltd. | Vector quantizer |
US5548684A (en) | 1994-04-22 | 1996-08-20 | Georgia Tech Research Corporation | Artificial neural network viterbi decoding system and method |
US5509103A (en) * | 1994-06-03 | 1996-04-16 | Motorola, Inc. | Method of training neural networks used for speech recognition |
US5745649A (en) | 1994-07-07 | 1998-04-28 | Nynex Science & Technology Corporation | Automated speech recognition using a plurality of different multilayer perception structures to model a plurality of distinct phoneme categories |
US5734793A (en) * | 1994-09-07 | 1998-03-31 | Motorola Inc. | System for recognizing spoken sounds from continuous speech and method of using same |
US5754681A (en) * | 1994-10-05 | 1998-05-19 | Atr Interpreting Telecommunications Research Laboratories | Signal pattern recognition apparatus comprising parameter training controller for training feature conversion parameters and discriminant functions |
US5867816A (en) | 1995-04-24 | 1999-02-02 | Ericsson Messaging Systems Inc. | Operator interactions for developing phoneme recognition by neural networks |
US5839103A (en) * | 1995-06-07 | 1998-11-17 | Rutgers, The State University Of New Jersey | Speaker verification system using decision fusion logic |
US5754978A (en) * | 1995-10-27 | 1998-05-19 | Speech Systems Of Colorado, Inc. | Speech recognition system |
US5937384A (en) | 1996-05-01 | 1999-08-10 | Microsoft Corporation | Method and system for speech recognition using continuous density hidden Markov models |
US6366883B1 (en) * | 1996-05-15 | 2002-04-02 | Atr Interpreting Telecommunications | Concatenation of speech segments by use of a speech synthesizer |
US6363289B1 (en) | 1996-09-23 | 2002-03-26 | Pavilion Technologies, Inc. | Residual activation neural network |
US6490555B1 (en) * | 1997-03-14 | 2002-12-03 | Scansoft, Inc. | Discriminatively trained mixture models in continuous speech recognition |
US5930754A (en) | 1997-06-13 | 1999-07-27 | Motorola, Inc. | Method, device and article of manufacture for neural-network based orthography-phonetics transformation |
US6233550B1 (en) | 1997-08-29 | 2001-05-15 | The Regents Of The University Of California | Method and apparatus for hybrid coding of speech at 4kbps |
US6475245B2 (en) | 1997-08-29 | 2002-11-05 | The Regents Of The University Of California | Method and apparatus for hybrid coding of speech at 4KBPS having phase alignment between mode-switched frames |
US6092039A (en) | 1997-10-31 | 2000-07-18 | International Business Machines Corporation | Symbiotic automatic speech recognition and vocoder |
US6456969B1 (en) * | 1997-12-12 | 2002-09-24 | U.S. Philips Corporation | Method of determining model-specific factors for pattern recognition, in particular for speech patterns |
US6226612B1 (en) * | 1998-01-30 | 2001-05-01 | Motorola, Inc. | Method of evaluating an utterance in a speech recognition system |
US20020116196A1 (en) * | 1998-11-12 | 2002-08-22 | Tran Bao Q. | Speech recognizer |
US6526380B1 (en) * | 1999-03-26 | 2003-02-25 | Koninklijke Philips Electronics N.V. | Speech recognition system having parallel large vocabulary recognition engines |
US6912499B1 (en) * | 1999-08-31 | 2005-06-28 | Nortel Networks Limited | Method and apparatus for training a multilingual speech model set |
US6963837B1 (en) | 1999-10-06 | 2005-11-08 | Multimodal Technologies, Inc. | Attribute-based word modeling |
US6760699B1 (en) | 2000-04-24 | 2004-07-06 | Lucent Technologies Inc. | Soft feature decoding in a distributed automatic speech recognition system for use over wireless channels |
US6671669B1 (en) * | 2000-07-18 | 2003-12-30 | Qualcomm Incorporated | combined engine system and method for voice recognition |
US20020091522A1 (en) * | 2001-01-09 | 2002-07-11 | Ning Bi | System and method for hybrid voice recognition |
US20020128834A1 (en) | 2001-03-12 | 2002-09-12 | Fain Systems, Inc. | Speech recognition system using spectrogram analysis |
US7233899B2 (en) | 2001-03-12 | 2007-06-19 | Fain Vitaliy S | Speech recognition system using normalized voiced segment spectrogram analysis |
US20020193991A1 (en) * | 2001-06-13 | 2002-12-19 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
US20040236573A1 (en) | 2001-06-19 | 2004-11-25 | Sapeluk Andrew Thomas | Speaker recognition systems |
US20030009334A1 (en) * | 2001-07-03 | 2003-01-09 | International Business Machines Corporation | Speech processing board for high volume speech processing applications |
US6845357B2 (en) | 2001-07-24 | 2005-01-18 | Honeywell International Inc. | Pattern recognition using an observable operator model |
US20110224981A1 (en) * | 2001-11-27 | 2011-09-15 | Miglietta Joseph H | Dynamic speech recognition and transcription among users having heterogeneous protocols |
US6898567B2 (en) * | 2001-12-29 | 2005-05-24 | Motorola, Inc. | Method and apparatus for multi-level distributed speech recognition |
US20110288855A1 (en) | 2002-06-28 | 2011-11-24 | Conceptual Speech Llc | Multi-phoneme streamer and knowledge representation speech recognition system and method |
US20040064315A1 (en) | 2002-09-30 | 2004-04-01 | Deisher Michael E. | Acoustic confidence driven front-end preprocessing for speech recognition in adverse environments |
US7228275B1 (en) * | 2002-10-21 | 2007-06-05 | Toyota Infotechnology Center Co., Ltd. | Speech recognition system having multiple speech recognizers |
US20040138885A1 (en) * | 2003-01-09 | 2004-07-15 | Xiaofan Lin | Commercial automatic speech recognition engine combinations |
US20040153319A1 (en) * | 2003-01-30 | 2004-08-05 | Sherif Yacoub | Two-engine speech recognition |
US6963835B2 (en) | 2003-03-31 | 2005-11-08 | Bae Systems Information And Electronic Systems Integration Inc. | Cascaded hidden Markov model for meta-state estimation |
US20040210437A1 (en) * | 2003-04-15 | 2004-10-21 | Aurilab, Llc | Semi-discrete utterance recognizer for carefully articulated speech |
US20050065790A1 (en) * | 2003-09-23 | 2005-03-24 | Sherif Yacoub | System and method using multiple automated speech recognition engines |
US7826894B2 (en) | 2004-03-22 | 2010-11-02 | California Institute Of Technology | Cognitive control signals for neural prosthetics |
US20050213810A1 (en) * | 2004-03-29 | 2005-09-29 | Kohtaro Sabe | Information processing apparatus and method, recording medium, and program |
US7496512B2 (en) | 2004-04-13 | 2009-02-24 | Microsoft Corporation | Refining of segmental boundaries in speech waveforms using contextual-dependent models |
US20050286772A1 (en) * | 2004-06-24 | 2005-12-29 | Lockheed Martin Corporation | Multiple classifier system with voting arbitration |
US20060136209A1 (en) | 2004-12-16 | 2006-06-22 | Sony Corporation | Methodology for generating enhanced demiphone acoustic models for speech recognition |
US7467086B2 (en) | 2004-12-16 | 2008-12-16 | Sony Corporation | Methodology for generating enhanced demiphone acoustic models for speech recognition |
US20060136205A1 (en) * | 2004-12-21 | 2006-06-22 | Song Jianming J | Method of refining statistical pattern recognition models and statistical pattern recognizers |
US20060143010A1 (en) * | 2004-12-23 | 2006-06-29 | Samsung Electronics Co., Ltd. | Method, medium, and apparatus recognizing speech |
US7660774B2 (en) | 2005-05-31 | 2010-02-09 | Honeywell International Inc. | Nonlinear neural network fault detection system and method |
US20090216528A1 (en) * | 2005-06-01 | 2009-08-27 | Roberto Gemello | Method of adapting a neural network of an automatic speech recognition device |
US8126710B2 (en) | 2005-06-01 | 2012-02-28 | Loquendo S.P.A. | Conservative training method for adapting a neural network of an automatic speech recognition device |
US7574357B1 (en) | 2005-06-24 | 2009-08-11 | The United States Of America As Represented By The Admimnistrator Of The National Aeronautics And Space Administration (Nasa) | Applications of sub-audible speech recognition based upon electromyographic signals |
US20070136059A1 (en) * | 2005-12-12 | 2007-06-14 | Gadbois Gregory J | Multi-voice speech recognition |
US20070198257A1 (en) * | 2006-02-20 | 2007-08-23 | Microsoft Corporation | Speaker authentication |
US20070198261A1 (en) * | 2006-02-21 | 2007-08-23 | Sony Computer Entertainment Inc. | Voice recognition with parallel gender and age normalization |
US8010358B2 (en) | 2006-02-21 | 2011-08-30 | Sony Computer Entertainment Inc. | Voice recognition with parallel gender and age normalization |
US20110040561A1 (en) * | 2006-05-16 | 2011-02-17 | Claudio Vair | Intersession variability compensation for automatic extraction of information from voice |
US20070288242A1 (en) * | 2006-06-12 | 2007-12-13 | Lockheed Martin Corporation | Speech recognition and control system, program product, and related methods |
US20080059200A1 (en) | 2006-08-22 | 2008-03-06 | Accenture Global Services Gmbh | Multi-Lingual Telephonic Service |
US20080147391A1 (en) | 2006-12-15 | 2008-06-19 | Samsung Electronics Co., Ltd. | Method of and apparatus for transforming speech feature vector |
US20100318358A1 (en) * | 2007-02-06 | 2010-12-16 | Yoshifumi Onishi | Recognizer weight learning device, speech recognizing device, and system |
US20080208577A1 (en) * | 2007-02-23 | 2008-08-28 | Samsung Electronics Co., Ltd. | Multi-stage speech recognition apparatus and method |
US20080249762A1 (en) * | 2007-04-05 | 2008-10-09 | Microsoft Corporation | Categorization of documents using part-of-speech smoothing |
US20090018833A1 (en) * | 2007-07-13 | 2009-01-15 | Kozat Suleyman S | Model weighting, selection and hypotheses combination for automatic speech recognition and machine translation |
US20090048843A1 (en) | 2007-08-08 | 2009-02-19 | Nitisaroj Rattima | System-effected text annotation for expressive prosody in speech synthesis and recognition |
US8311827B2 (en) | 2007-09-21 | 2012-11-13 | The Boeing Company | Vehicle control |
US20090138265A1 (en) * | 2007-11-26 | 2009-05-28 | Nuance Communications, Inc. | Joint Discriminative Training of Multiple Speech Recognizers |
US20090319342A1 (en) * | 2008-06-19 | 2009-12-24 | Wize, Inc. | System and method for aggregating and summarizing product/topic sentiment |
US20100004930A1 (en) * | 2008-07-02 | 2010-01-07 | Brian Strope | Speech Recognition with Parallel Recognition Tasks |
US8239195B2 (en) | 2008-09-23 | 2012-08-07 | Microsoft Corporation | Adapting a compressed model for use in speech recognition |
US20100121638A1 (en) | 2008-11-12 | 2010-05-13 | Mark Pinson | System and method for automatic speech to text conversion |
US20100185436A1 (en) | 2009-01-21 | 2010-07-22 | Al-Zahrani Abdul Kareem Saleh | Arabic poetry meter identification system and method |
US20100198598A1 (en) | 2009-02-05 | 2010-08-05 | Nuance Communications, Inc. | Speaker Recognition in a Speech Recognition System |
US20100217589A1 (en) | 2009-02-20 | 2010-08-26 | Nuance Communications, Inc. | Method for Automated Training of a Plurality of Artificial Neural Networks |
US20120101401A1 (en) * | 2009-04-07 | 2012-04-26 | National University Of Ireland | Method for the real-time identification of seizures in an electroencephalogram (eeg) signal |
US20100280827A1 (en) * | 2009-04-30 | 2010-11-04 | Microsoft Corporation | Noise robust speech classifier ensemble |
US20120076361A1 (en) * | 2009-06-03 | 2012-03-29 | Hironobu Fujiyoshi | Object detection device |
US20100332229A1 (en) * | 2009-06-30 | 2010-12-30 | Sony Corporation | Apparatus control based on visual lip share recognition |
US20120072215A1 (en) | 2010-09-21 | 2012-03-22 | Microsoft Corporation | Full-sequence training of deep structures for speech recognition |
US20120084086A1 (en) * | 2010-09-30 | 2012-04-05 | At&T Intellectual Property I, L.P. | System and method for open speech recognition |
US20120166194A1 (en) * | 2010-12-23 | 2012-06-28 | Electronics And Telecommunications Research Institute | Method and apparatus for recognizing speech |
US8352245B1 (en) * | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
US20120269441A1 (en) * | 2011-04-19 | 2012-10-25 | Xerox Corporation | Image quality assessment |
US20130006991A1 (en) * | 2011-06-28 | 2013-01-03 | Toru Nagano | Information processing apparatus, method and program for determining weight of each feature in subjective hierarchical clustering |
US20130226587A1 (en) * | 2012-02-27 | 2013-08-29 | Hong Kong Baptist University | Lip-password Based Speaker Verification System |
US20140149104A1 (en) * | 2012-11-23 | 2014-05-29 | Idiap Research Institute | Apparatus and method for constructing multilingual acoustic model and computer readable recording medium for storing program for performing the method |
US20140278390A1 (en) * | 2013-03-12 | 2014-09-18 | International Business Machines Corporation | Classifier-based system combination for spoken term detection |
US20140358265A1 (en) * | 2013-05-31 | 2014-12-04 | Dolby Laboratories Licensing Corporation | Audio Processing Method and Audio Processing Apparatus, and Training Method |
US20150058004A1 (en) * | 2013-08-23 | 2015-02-26 | At & T Intellectual Property I, L.P. | Augmented multi-tier classifier for multi-modal voice activity detection |
Non-Patent Citations (16)
Title |
---|
"An Introduction to Hybrid HMM/Connectionist Continuous Speech Recognition", Nelson Morgan and Herve Bourlard, IEEE Signal Processing Magazine, May 1995. * |
Anastasakos, T.; McDonough, J.; Makhoul, J., "A compact model for speaker-adaptive training." Spoken Language 1966. ICSLP 96. Proceedings., Fourth International Conference on Oct. 3-6, 1996, vol. 2, pp. 1137-1140, from http://www.asel.udel.edu/icslp/cdrom/vol2/764/a764.pdf. |
Daniel J. Kershaw. "Phonetic Context-Dependency in a Hybrid ANN/HMM Speech Recognition System." St. John's College University of Cambridge. Jan. 28, 1997, pp. i-x, 1-16. |
Eide, Ellen, and Gish, Herbert, "A parametric approach to vocal tract length normalization," Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on May 9, 1996, vol. 1, pp. 346-348, from http://www.ee.columbia.edu/~dpwe/papers/EideG96-vtln.pdf. |
Eide, Ellen, and Gish, Herbert, "A parametric approach to vocal tract length normalization," Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on May 9, 1996, vol. 1, pp. 346-348, from http://www.ee.columbia.edu/˜dpwe/papers/EideG96-vtln.pdf. |
Gales, M.J.F. "Maximum Likelihood Linear Transformations for HMM-Based Speech Recognition." Technical Document CUED/F-INFENG/TR 291, Cambridge University Engineering Department, May 1995, from ftp://svr-ftp.eng.cam.ac.uk/pub/reports/auto-pdf/gales-tr291.pdf. |
Guangsen et al. "Comparison of Smoothing Techniques for Robust Context Dependent Acoustic Modelling in Hybrid NN/HMM Systems." Interspeech 2001, Aug. 2011, pp. 457-460. |
Ming, Ji, and F. Jack Smith. "Improved phone recognition using Bayesian triphone models." Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on. vol. 1. IEEE, 1998, pp. 1-4. |
N. Morgan and H. Bourlard. "An Introduction to Hybrid HMM/Connectionist Continuous Speech Recognition." IEEE Signal Processing Magazine, pp. 25-42, May 1995. |
Notice of Allowance in U.S. Appl. No. 13/560,740 mailed Nov. 7, 2012. |
Office Action in U.S. Appl. No. 13/560,658 mailed Dec. 12, 2012. |
Office Action in U.S. Appl. No. 13/560,706 mailed Nov. 28, 2012. |
Sim et al. "Stream-based context-sensitive phone mapping for cross-lingual speech recognition." Interspeech 2009, Sep. 2009, pp. 3019-3022. |
Sim, Khe Chai. "Discriminative product-of-expert acoustic mapping for cross-lingual phone recognition." Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on IEEE, Dec. 2009, pp. 546-551. |
Young et al. "Tree-based state tying for high accuracy acoustic modelling." Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 307-312. |
Ziegenhain et al. "Triphone tying techniques combining a-priori rules and data driven methods." European Conference on Speech Communication and Technology (EUROSPEECH). vol. 2. 2001, pp. 1-4. |
Cited By (54)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150243285A1 (en) * | 2012-09-07 | 2015-08-27 | Carnegie Mellon University, A Pennsylvania Non-Profit Corporation | Methods for hybrid gpu/cpu data processing |
US9558748B2 (en) * | 2012-09-07 | 2017-01-31 | Carnegie Mellon University | Methods for hybrid GPU/CPU data processing |
US9508347B2 (en) * | 2013-07-10 | 2016-11-29 | Tencent Technology (Shenzhen) Company Limited | Method and device for parallel processing in model training |
US20150019214A1 (en) * | 2013-07-10 | 2015-01-15 | Tencent Technology (Shenzhen) Company Limited | Method and device for parallel processing in model training |
US10930271B2 (en) * | 2013-07-31 | 2021-02-23 | Google Llc | Speech recognition using neural networks |
US11620991B2 (en) | 2013-07-31 | 2023-04-04 | Google Llc | Speech recognition using neural networks |
US9626621B2 (en) | 2013-12-06 | 2017-04-18 | International Business Machines Corporation | Systems and methods for combining stochastic average gradient and hessian-free optimization for sequence training of deep neural networks |
US20150161988A1 (en) * | 2013-12-06 | 2015-06-11 | International Business Machines Corporation | Systems and methods for combining stochastic average gradient and hessian-free optimization for sequence training of deep neural networks |
US9483728B2 (en) * | 2013-12-06 | 2016-11-01 | International Business Machines Corporation | Systems and methods for combining stochastic average gradient and hessian-free optimization for sequence training of deep neural networks |
US10923112B2 (en) | 2013-12-17 | 2021-02-16 | Google Llc | Generating representations of acoustic sequences |
US10535338B2 (en) * | 2013-12-17 | 2020-01-14 | Google Llc | Generating representations of acoustic sequences |
US11721327B2 (en) | 2013-12-17 | 2023-08-08 | Google Llc | Generating representations of acoustic sequences |
US10134393B2 (en) | 2013-12-17 | 2018-11-20 | Google Llc | Generating representations of acoustic sequences |
US20150170640A1 (en) * | 2013-12-17 | 2015-06-18 | Google Inc. | Generating representations of acoustic sequences |
US20190139536A1 (en) * | 2013-12-17 | 2019-05-09 | Google Llc | Generating representations of acoustic sequences |
US9721562B2 (en) * | 2013-12-17 | 2017-08-01 | Google Inc. | Generating representations of acoustic sequences |
US10475442B2 (en) * | 2015-11-25 | 2019-11-12 | Samsung Electronics Co., Ltd. | Method and device for recognition and method and device for constructing recognition model |
CN105763303A (en) * | 2016-04-19 | 2016-07-13 | 成都翼比特自动化设备有限公司 | Hybrid automatic repeat request algorithm based on prediction |
CN105976812A (en) * | 2016-04-28 | 2016-09-28 | 腾讯科技（深圳）有限公司 | Voice identification method and equipment thereof |
US20190150764A1 (en) * | 2016-05-02 | 2019-05-23 | The Regents Of The University Of California | System and Method for Estimating Perfusion Parameters Using Medical Imaging |
US10657437B2 (en) * | 2016-08-18 | 2020-05-19 | International Business Machines Corporation | Training of front-end and back-end neural networks |
US11003983B2 (en) * | 2016-08-18 | 2021-05-11 | International Business Machines Corporation | Training of front-end and back-end neural networks |
US10832129B2 (en) | 2016-10-07 | 2020-11-10 | International Business Machines Corporation | Transfer of an acoustic knowledge to a neural network |
US10503580B2 (en) | 2017-06-15 | 2019-12-10 | Microsoft Technology Licensing, Llc | Determining a likelihood of a resource experiencing a problem based on telemetry data |
US10805317B2 (en) | 2017-06-15 | 2020-10-13 | Microsoft Technology Licensing, Llc | Implementing network security measures in response to a detected cyber attack |
US11062226B2 (en) | 2017-06-15 | 2021-07-13 | Microsoft Technology Licensing, Llc | Determining a likelihood of a user interaction with a content element |
US10922627B2 (en) | 2017-06-15 | 2021-02-16 | Microsoft Technology Licensing, Llc | Determining a course of action based on aggregated data |
US20200285956A1 (en) * | 2017-06-28 | 2020-09-10 | The Regents Of The University Of California | Training Artificial Neural Networks with Reduced Computational Complexity |
WO2019006088A1 (en) * | 2017-06-28 | 2019-01-03 | The Regents Of The University Of California | Training artificial neural networks with reduced computational complexity |
US11620514B2 (en) * | 2017-06-28 | 2023-04-04 | The Regents Of The University Of California | Training artificial neural networks with reduced computational complexity |
US11977974B2 (en) | 2017-11-30 | 2024-05-07 | International Business Machines Corporation | Compression of fully connected / recurrent layers of deep network(s) through enforcing spatial locality to weight matrices and effecting frequency compression |
CN108172229A (en) * | 2017-12-12 | 2018-06-15 | 天津津航计算技术研究所 | A kind of authentication based on speech recognition and the method reliably manipulated |
US10599769B2 (en) * | 2018-05-01 | 2020-03-24 | Capital One Services, Llc | Text categorization using natural language processing |
US11379659B2 (en) | 2018-05-01 | 2022-07-05 | Capital One Services, Llc | Text categorization using natural language processing |
US11810471B2 (en) * | 2018-05-11 | 2023-11-07 | Speech Engineering Limited | Computer implemented method and apparatus for recognition of speech patterns and feedback |
US20210082311A1 (en) * | 2018-05-11 | 2021-03-18 | Speech Engineering Limited | Computer implemented method and apparatus for recognition of speech patterns and feedback |
CN109144719B (en) * | 2018-07-11 | 2022-02-15 | 东南大学 | Collaborative unloading method based on Markov decision process in mobile cloud computing system |
CN109144719A (en) * | 2018-07-11 | 2019-01-04 | 东南大学 | Cooperation discharging method based on markov decision process in mobile cloud computing system |
US10720151B2 (en) | 2018-07-27 | 2020-07-21 | Deepgram, Inc. | End-to-end neural networks for speech recognition and classification |
US10847138B2 (en) * | 2018-07-27 | 2020-11-24 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US10380997B1 (en) * | 2018-07-27 | 2019-08-13 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US10540959B1 (en) | 2018-07-27 | 2020-01-21 | Deepgram, Inc. | Augmented generalized deep learning with special vocabulary |
US20200035224A1 (en) * | 2018-07-27 | 2020-01-30 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US20210035565A1 (en) * | 2018-07-27 | 2021-02-04 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US11676579B2 (en) * | 2018-07-27 | 2023-06-13 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US11621011B2 (en) | 2018-10-29 | 2023-04-04 | Dolby International Ab | Methods and apparatus for rate quality scalable coding with generative models |
US11205050B2 (en) * | 2018-11-02 | 2021-12-21 | Oracle International Corporation | Learning property graph representations edge-by-edge |
US10855455B2 (en) * | 2019-01-11 | 2020-12-01 | Advanced New Technologies Co., Ltd. | Distributed multi-party security model training framework for privacy protection |
US20200312307A1 (en) * | 2019-03-25 | 2020-10-01 | Microsoft Technology Licensing, Llc | Dynamic Combination of Acoustic Model States |
US11670299B2 (en) * | 2019-06-26 | 2023-06-06 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
US11043218B1 (en) * | 2019-06-26 | 2021-06-22 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
US20210358497A1 (en) * | 2019-06-26 | 2021-11-18 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
US11132990B1 (en) * | 2019-06-26 | 2021-09-28 | Amazon Technologies, Inc. | Wakeword and acoustic event detection |
CN115622608A (en) * | 2022-09-29 | 2023-01-17 | 广州爱浦路网络技术有限公司 | Method, system and medium for optimization of offloading strategies based on low-earth-orbit satellite edge calculation |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9240184B1 (en) | Frame-level combination of deep neural network and gaussian mixture models | |
US8484022B1 (en) | Adaptive auto-encoders | |
US8442821B1 (en) | Multi-frame prediction for hybrid neural network/hidden Markov models | |
US11854534B1 (en) | Asynchronous optimization for sequence training of neural networks | |
US9466292B1 (en) | Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition | |
US20200335093A1 (en) | Latency constraints for acoustic modeling | |
US11776531B2 (en) | Encoder-decoder models for sequence to sequence mapping | |
US11769493B2 (en) | Training acoustic models using connectionist temporal classification | |
US11620991B2 (en) | Speech recognition using neural networks | |
US9542927B2 (en) | Method and system for building text-to-speech voice from diverse recordings | |
US8527276B1 (en) | Speech synthesis using deep neural networks | |
US11335333B2 (en) | Speech recognition with sequence-to-sequence models | |
US9183830B2 (en) | Method and system for non-parametric voice conversion | |
US8996366B2 (en) | Multi-stage speaker adaptation | |
US8805684B1 (en) | Distributed speaker adaptation | |
US8965763B1 (en) | Discriminative language modeling for automatic speech recognition with a weak acoustic model and distributed training | |
US9620145B2 (en) | Context-dependent state tying using a neural network | |
US9177549B2 (en) | Method and system for cross-lingual voice conversion | |
US20210050033A1 (en) | Utilizing bi-directional recurrent encoders with multi-hop attention for speech emotion recognition | |
US20230335111A1 (en) | Method and system for text-to-speech synthesis of streaming text | |
Razavi et al. | A Posterior-Based Multistream Formulation for G2P Conversion |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LIN, HUI;LEI, XIN;VANHOUCKE, VINCENT;REEL/FRAME:029795/0968Effective date: 20130211 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |