CN115514957A - Adaptive filter intra prediction mode in image/video compression - Google Patents
Adaptive filter intra prediction mode in image/video compression Download PDFInfo
- Publication number
- CN115514957A CN115514957A CN202210981861.7A CN202210981861A CN115514957A CN 115514957 A CN115514957 A CN 115514957A CN 202210981861 A CN202210981861 A CN 202210981861A CN 115514957 A CN115514957 A CN 115514957A
- Authority
- CN
- China
- Prior art keywords
- block
- prediction
- pixels
- pixel
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/593—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial prediction techniques
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/11—Selection of coding mode or of prediction mode among a plurality of spatial predictive coding modes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/117—Filters, e.g. for pre-processing or post-processing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/182—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a pixel
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/184—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being bits, e.g. of the compressed video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/80—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation
Abstract
Adaptive filter intra prediction modes in image/video compression are disclosed. A method of generating a prediction block for coding a block of a frame using intra prediction. The method includes determining filter coefficients for generating a prediction block using a training region, the training region being adjacent to the block and comprising a plurality of reconstructed pixels, the filter coefficients minimizing a function of differences, each difference being a respective difference between a pixel in the training region and a prediction of the pixel in the training region, and predicting using the filter coefficients; and generates a prediction block using the filter coefficients.
Description
Description of the different cases
The application belongs to divisional application of Chinese invention patent application No.201980090403.0, which has an application date of 2019, 10 and 31.
Technical Field
The present application relates to adaptive filter intra prediction modes in image/video compression.
Background
A digital video stream may represent video using a sequence of frames or still images. Digital video can be used for a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or user-generated video sharing. Digital video streams may contain large amounts of data and consume a large amount of computing or communication resources of a computing device for processing, transmission, or storage of video data. Various methods, including compression and other encoding techniques, have been proposed to reduce the amount of data in a video stream.
Spatial similarity-based coding may be performed by dividing a frame or image into blocks that are predicted based on other blocks within the same frame or image. The difference (i.e., the residual) between the block and the predicted block is compressed and encoded in the bitstream. The decoder uses the difference and the reference frame to reconstruct the frame or picture.
Disclosure of Invention
Aspects, features, elements, and embodiments for encoding and decoding a block using intra prediction edge filtering are disclosed.
One aspect of the disclosed embodiments is a method of generating a prediction block for a block of a coded frame using intra prediction. The method comprises the following steps: determining filter coefficients for generating the prediction block using a training region, the training region being adjacent to the block and comprising a plurality of reconstructed pixels, the filter coefficients minimizing a function of differences, each difference being a respective difference between a pixel in the training region and a prediction of that pixel in the training region, and the prediction using the filter coefficients; and generating a prediction block using the determined filter coefficients.
Another aspect is an apparatus for generating a prediction block for a block of a coded frame using intra prediction. The apparatus includes a memory and a processor. The processor is configured to execute instructions stored in the memory to determine filter coefficients for generating a prediction block using a training region, the training region being adjacent to the block and comprising a plurality of reconstructed pixels, the filter coefficients minimizing a function of differences, each difference being a respective difference between a pixel in the training region and a prediction of that pixel in the training region, and the prediction using the filter coefficients; and generating a prediction block using the determined filter coefficients.
Another aspect is a method of generating a prediction block for decoding a block of a frame using intra prediction. The method includes decoding an adaptive intra-prediction mode from a compressed bitstream, the adaptive intra-prediction mode indicating a training region, the training region including a plurality of reconstructed pixels; determining filter coefficients for generating a prediction block using the training region; and generating a prediction block for the block by recursive extrapolation using the filter coefficients.
These and other aspects of the disclosure are disclosed in the following detailed description of the embodiments, the appended claims and the accompanying drawings.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device that may implement a transmitting station or a receiving station.
Fig. 3 is a diagram of a video stream to be encoded and subsequently decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a flow diagram of a process for generating a prediction block for a block of a coded frame using intra prediction according to an embodiment of the present disclosure.
Fig. 7A is a diagram of a training area according to an embodiment of the present disclosure.
Fig. 7B is a diagram of possible taps according to an embodiment of the present disclosure.
Fig. 8 is a diagram of an optimal adaptive filter according to an embodiment of the present disclosure.
Fig. 9 is a diagram of directional intra prediction modes according to an embodiment of the present disclosure.
Fig. 10A is a diagram of an intra prediction mode having a prediction angle of 90 degrees.
Fig. 10B is a diagram of an intra prediction mode with a 135-degree prediction angle.
FIG. 11 is a diagram of an example of recursive extrapolation, according to an embodiment of the present disclosure.
Detailed Description
As described above, a compression scheme associated with a coded video stream may include using one or more techniques to divide an image into blocks and generate a digital video output bitstream (i.e., an encoded bitstream) to limit the information included in the output bitstream. The received bitstream is decoded to recreate the block and source images based on limited information. Encoding a video stream or a portion thereof (such as a frame or block) may include using spatial similarities in the video stream to improve coding efficiency. For example, a current block of a video stream may be encoded based on identifying differences between previously coded pixel values or differences (e.g., residuals) between combinations of previously encoded pixel values and pixel values in the current block being encoded.
Encoding using spatial similarity may be referred to as intra prediction. Intra-prediction attempts to predict the pixel values of a current block of a frame of a video stream using pixels that are peripheral to the current block, i.e., using pixels that are in the same frame as the current block but outside the current block. Intra prediction may be performed along prediction directions, referred to herein as prediction angles, where each direction may correspond to an intra prediction mode. The intra prediction mode may be signaled by the encoder to the decoder.
Many different intra prediction modes (e.g., available) may be supported. Some intra-prediction modes use a single value for all pixels within a prediction block generated using at least one peripheral pixel. Other intra prediction modes, which may be referred to as directional intra prediction modes, may each have a corresponding prediction angle. The intra-prediction modes may include, for example, a horizontal intra-prediction mode, a vertical intra-prediction mode, and various other directional intra-prediction modes. Thus, the predicted angle may be any angle between 0 and 360 degrees. In some embodiments, the predicted angle may be any angle between 0 and 270 degrees. The available prediction angles may also be a subset of all possible prediction angles. For example, the codec may have available prediction modes corresponding to 50-60 prediction angles among 0 to 360 prediction angles.
In an example, the prediction angle may be encoded as a base angle and an offset. For example, the offset may be in the range of [ -3, +3] degrees from the base angle. For example, the AV1 codec includes eight basic directional intra prediction modes. Therefore, a total of 8 × 6=54 intra prediction angles may be used. The 54 intra prediction angles correspond to eight basic prediction angles and 6 offsets for each basic prediction angle.
Various directional intra-prediction modes may be used to propagate pixel values from previously encoded blocks along angular lines (including horizontal, vertical, and directions offset from horizontal and/or vertical) to predict a block. For example, the propagated pixel values may include peripheral pixels above and/or to the left of the block in the same frame (e.g., when raster scan order is used in encoding).
The current block may be predicted by projecting reference pixels from peripheral pixels. For example, the peripheral pixels may include pixels of the left and upper (i.e., upper) boundaries of the current block at an angle or direction that deviates from horizontal and vertical lines. The reference pixels may be, for example, actual pixel values of peripheral pixels or average pixel values (such as a weighted average) of some peripheral pixels, which are propagated in the angular direction to form the prediction block. The peripheral pixels may be combined in other ways to generate the reference pixel.
Fig. 9 is a diagram 900 of directional prediction modes according to an embodiment of the present disclosure. Fig. 9 illustrates three exemplary directional prediction modes 902, 904, and 906 labeled as zone 1, zone 2, and zone 3, respectively. The illustrated directional prediction modes 902-906 may be used to generate a prediction block having dimensions that conform to the current block 912. The directional prediction mode 902 illustrates an intra prediction mode having a prediction angle between 0 degrees and 90 degrees. The directional prediction mode 904 illustrates an intra prediction mode having a prediction angle between 90 degrees and 180 degrees. The directional prediction mode 906 illustrates an intra prediction mode with a prediction angle between 180 degrees and 270 degrees.
FIG. 9 also illustrates a first pixel 908 in a row above the current block and a second pixel 910 in a column to the left of the current block. The first pixel 908 and the second pixel 910 may be used to generate a prediction block. In some embodiments, directional prediction in zone 1 (i.e., intra prediction mode with prediction angles between 0 and 90) uses the first pixels 908 but may not use the second pixels 910 to generate a predicted block; the directional prediction in zone 2 (i.e., the intra prediction mode with a prediction angle between 90 ° and 180 °) uses the first pixel 908 and the second pixel 910 to generate a prediction block; and directional prediction in zone 3 (i.e., intra prediction mode with a prediction angle between 180 ° and 270 °) uses the second pixels 910 but may not use the first pixels 908 to generate the prediction block.
Fig. 10A is a diagram of an intra prediction mode having a prediction angle of 90 degrees. Fig. 10A illustrates generation of a prediction block for a 4x4 block to be predicted (also referred to as a current block) and corresponds to directional prediction in region 2 of fig. 9 (i.e., a directional prediction mode 904). The intra prediction mode of fig. 10A propagates the peripheral pixels a to D down the columns of the prediction block so that the value of each pixel in the columns is set equal to the value of the peripheral pixels a to D adjacent in the arrow direction.
Fig. 10B is a diagram of an intra prediction mode with a 135 degree prediction angle. Fig. 10B illustrates generation of a prediction block for a 4x4 current block and corresponds to directional prediction in region 2 of fig. 9. The intra-prediction mode of fig. 10B propagates the peripheral pixel values to the right and down the 135 degree line (i.e., line 1006) to form the prediction block. The peripheral pixel values may include, for example, some of the peripheral pixels 1008 (i.e., pixels a-R) from blocks that are adjacent to the 4x4 current block of the frame 1010 to form the prediction block 1002 for the current block. Although the 135 degree intra prediction mode in fig. 10B is illustrated using pixel values of peripheral pixels 1008 to generate the prediction block 1002, for example, linear combinations (e.g., weighted averages) of some (e.g., two, three, or more) of the peripheral pixels may be used to predict pixel values of the prediction block along a line extending through the block. For example, pixel value 1004 to be propagated along line 1006 may be formed from a weighted average of pixel values K, L, and M.
As mentioned above, intra prediction consists mainly of copying boundary pixels (or linear combinations thereof) in certain directions, which may reflect a simple model of potential spatial correlation. Alternatively, the image signal may be viewed as a two-dimensional non-separable markov model, whose corresponding correlation model may better capture subtle directional effects within the block. Thus, an improvement in intra prediction may include a set of prediction modes represented by a three-tap extrapolation filter to achieve intra prediction based on these models. Such models are referred to herein as recursive predictions or recursive extrapolations.
FIG. 11 is a diagram of an example 1100 of recursive extrapolation, according to an embodiment of the present disclosure. Each pixel X of an image (such as pixel 1102) may be considered according to a two-dimensional non-separable markov model with zero mean and unit variance, and its evolutionary recursion may be written as a function of adjacent vertical pixels V (such as pixel 1104), adjacent horizontal pixels H (such as pixel 1106), and adjacent diagonal pixels D (such as pixel 1108), as shown in equation (1):
X＝c v V+c h H+c d D+∈ (1)
in equation (1), V, H, and D are the specified vertical, horizontal, and diagonal neighbors of X, respectively, and e represents the innovation term.
Coefficient c v 、c h And c d Effectively capturing the associated gradients in two-dimensional space, or "directionality" of the image signal. Order to
In block-based video and image compression, unlike the above-described Differential Pulse Code Modulation (DPCM) setup, which predicts each sample from the available neighboring reconstruction, the codec must predict the entire block at a given set of boundary pixels (i.e., pixels that are peripheral but outside the block). As shown with reference to the prediction block 1110, the prediction of an intra pixel cannot use the reconstruction of its immediate neighbors. This difficulty can be circumvented by recursive extrapolation, where the predicted values of the neighbors are fed into a 3-tap filter. In the recursive intra-prediction mode (which may be referred to as filter intra mode in some codecs) the prediction content of a block is defined as the result of a 3-tap filter and is used as an input to the 3-tap filter for future neighbors, starting from a known boundary and applied recursively to the far end of the block.
The prediction block 1110 illustrates peripheral, reconstructed pixels (i.e., shaded pixels). The reconstructed pixels above block 1110 (referred to as the above-neighboring pixels) are labeled x 0，1 To x 0，4 . The reconstructed pixel to the left of block 1110 (referred to as the left-neighboring pixel) is labeled x 1,0 To x 4,0 . The label being x 0，0 Is referred to as the upper left neighboring pixel.
Equation (1) can be used as val (l) = c v x 0，1 +c h x 1，0 +c d x 0，0 To derive a predicted pixel at a location in the prediction block 1110 labeled "1", where val (m) refers to the pixel value at the location labeled "m". Equation (1) can be used as val (2) = c v x 0，2 +c h val(1)+c d x 0，1 To derive the flags in the prediction block 1110 " 2 "is predicted pixel at the location of. That is, in calculating the value at the position "2", the calculated value of the pixel at the position "1" is used. As another example, the calculated values at positions "6", "7" and "10" may be used to derive a predicted value for the pixel at the position labeled "11". Therefore, the calculated value of the pixel at the position "11" is val (ll) = c v val(7)+c h val(10)+c d val(6)。
In a codec implementation, N predefined 3-tap extrapolation filters are available. I.e. N coefficient sets (c) v ，c h ，c d ) Can be used. Which of the N sets to use may be transmitted from the encoder to the decoder, such as by encoding an indication of the set in the compressed bitstream. The predefined set may be derived by training offline for each possible block size.
As described, encoding an image block of a frame (e.g., video frame, image) using recursive extrapolation uses a 3-tap filter with predefined coefficients. Thus, the coefficients are independent and independent of the image signal of which the image block is a part. Furthermore, a filter with more or less than 3 taps may be more suitable (i.e., produce better compression) than the 3-tap filter described above.
When an image block is coded using recursive extrapolation, embodiments of the present disclosure may derive filter coefficients based on the signal of the image. That is, rather than using predefined coefficients (i.e., weights), coefficients used in an extrapolation operation for deriving a prediction block for the current block are calculated (e.g., determined) based on the image signal itself. In addition, embodiments according to the present disclosure may determine (e.g., use, select, etc.) an extrapolation filter having a tap number appropriate for the block. For example, depending on the image signal, embodiments of the present disclosure may select a 2-tap, 3-tap, or 4-tap filter for use in the extrapolation. Furthermore, although the recursive extrapolation described above always uses top, left, and diagonal neighbors, embodiments according to the present disclosure may select from bottom left, top right, and top left neighbors in a heap-out operation. To illustrate these neighboring pixels, and referring again to fig. 11, the lower left, left side, upper left, upper right neighboring pixels of the pixel at position "11" are the pixels at positions "14", "10", "6", "7", and "8", respectively.
Details are described herein after first describing an environment in which the multi-level composite prediction disclosed herein may be implemented.
Fig. 1 is a schematic diagram of a video encoding and decoding system 100. Transmitter station 102 may be, for example, a computer with an internal configuration of hardware, such as that described in fig. 2. However, other suitable implementations of transmitting station 102 are possible. For example, the processing of transmitting station 102 may be distributed among multiple devices.
In one example, the receiving station 106 may be a computer with an internal configuration of hardware, such as that described in FIG. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 may be distributed among multiple devices.
Other implementations of the video encoding and decoding system 100 are possible. For example, one embodiment may omit network 104. In another embodiment, the video stream may be encoded and then stored for transmission to the receiving station 106 or any other device having memory at a later time. In one embodiment, the receiving station 106 receives the encoded video stream (e.g., via the network 104, a computer bus, and/or some communication path) and stores the video stream for later decoding. In an exemplary embodiment, the real-time transport protocol (RTP) is used to transmit encoded video over the network 104. In another embodiment, transport protocols other than RTP may be used (e.g., hypertext transport protocol (HTTP) -based video streaming protocol).
For example, when used in a videoconferencing system, transmitting station 102 and/or receiving station 106 may include the capability to both encode and decode video streams as described below. For example, receiving stations 106 may be video conference participants that receive encoded video bitstreams from a video conference server (e.g., transmitting station 102) for decoding and viewing, and further encode and transmit their own video bitstreams to the video conference server for decoding and viewing by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 that may implement a transmitting station or a receiving station. For example, computing device 200 may implement one or both of transmitting station 102 and receiving station 106 of fig. 1. Computing device 200 may be in the form of a computing system including multiple computing devices or in the form of a single computing device (e.g., a mobile phone, tablet computer, laptop computer, notebook computer, desktop computer, etc.).
The CPU 202 in the computing device 200 may be a central processing unit. Alternatively, the CPU 202 may be another type of device or devices capable of manipulating or processing existing or hereafter developed information. Although the disclosed embodiments may be implemented by a single processor (e.g., CPU 202) as shown, speed and efficiency advantages may be realized using more than one processor.
In an embodiment, the memory 204 in the computing device 200 may be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device. Any other suitable type of storage device may be used as memory 204. The memory 204 may include code and data 206 that is accessed by the CPU 202 using the bus 212. The memory 204 may further include an operating system 208 and application programs 210, the application programs 210 including at least one program that allows the CPU 202 to perform the methods described herein. For example, the application programs 210 may include application 1 through application N, which further include a video coding application that performs the methods described herein. Computing device 200 may also include secondary storage 214, which secondary storage 214 may be, for example, a memory card used with the mobile computing device. Because video communication sessions may contain a large amount of information, they may be stored in whole or in part in secondary storage 214 and loaded into memory 204 for processing as needed.
Although fig. 2 depicts the CPU 202 and memory 204 of the computing device 200 as being integrated into a single unit, other configurations may be utilized. The operations of CPU 202 may be distributed across multiple machines (each machine may have one or more processors) that may be coupled directly or across a local area network or other network. Memory 204 may be distributed across multiple machines, such as a network-based memory or memory in multiple machines that perform operations for computing device 200. Although depicted as a single bus herein, the bus 212 of the computing device 200 may be comprised of multiple buses. In addition, the secondary storage 214 may be directly coupled to other components of the computing device 200 or may be accessible via a network and may include a single integrated unit (such as a memory card) or multiple units (such as multiple memory cards). Computing device 200 may be implemented in a wide variety of configurations.
Fig. 3 is a diagram of an example of a video stream 300 to be encoded and then decoded. The video stream 300 includes a video sequence 302. At the next level, the video sequence 302 includes a plurality of adjacent frames 304. Although three frames are described as adjacent frames 304, video sequence 302 may include any number of adjacent frames 304. The adjacent frames 304 may then be further subdivided into single frames (e.g., frame 306). At the next level, the frame 306 may be divided into a series of slices 308 or planes. For example, the segments 308 may be a subset of frames that allow for parallel processing, for example. The segment 308 may also be a subset of frames that may separate the video data into individual colors. For example, a frame 306 of color video data may include a luminance plane and two chrominance planes. The segments 308 may be sampled at different resolutions.
Regardless of whether the frame 306 is divided into segments 308, the frame 306 may be further subdivided into blocks 310, which blocks 310 may contain data corresponding to, for example, 16 × 16 pixels in the frame 306. The block 310 may also be arranged to include data from one or more segments 308 of pixel data. The block 310 may also have any other suitable size, such as 4 × 4 pixels, 8 × 8 pixels, 16 × 8 pixels, 8 × 16 pixels, 16 × 16 pixels, or larger.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, encoder 400 may be implemented in transmitting station 102, such as by providing a computer software program stored in a memory (e.g., memory 204). The computer software program may include machine instructions that, when executed by a processor, such as CPU 202, cause transmitting station 102 to encode video data in the manner described herein. Encoder 400 may also be implemented as dedicated hardware included, for example, in transmitting station 102. The encoder 400 has the following stages that perform various functions in the forward path (shown by the solid connecting lines) to use the video stream 300 as input to produce an encoded or compressed bitstream 420: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy coding stage 408. The encoder 400 may also include a reconstruction path (shown by the dashed connecting lines) to reconstruct the frame used to encode future blocks. In fig. 4, the encoder 400 has the following stages that perform various functions in the reconstruction path: a dequantization stage 410, an inverse transform stage 412, a reconstruction stage 414, and a loop filtering stage 416. Other structural variations of encoder 400 may be used to encode video stream 300.
When the video stream 300 is presented for encoding, the video stream 300 may be processed in units of blocks. At the intra/inter prediction stage 402, a block may be encoded using intra prediction (also referred to as intra prediction) or inter prediction (also referred to as inter prediction), or a combination of both. In any case, a prediction block may be formed. In the case of intra prediction, all or part of the prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter prediction, all or part of the prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
Next, still referring to fig. 4, the prediction block may be subtracted from the current block at the intra/inter prediction stage 402 to generate a residual block (also referred to as a residual). The transform stage 404 uses a block-based transform to transform the residual into, for example, transform coefficients in the frequency domain. Such block-based transforms include, for example, discrete Cosine Transform (DCT) and Asymmetric Discrete Sine Transform (ADST). Other block-based transforms are possible. Furthermore, a combination of different transforms may be applied to a single residual. In one example of transform application, the DCT transforms the residual block into the frequency domain, where the transform coefficient values are based on spatial frequency. The top left corner of the matrix is the lowest frequency (DC) coefficient and the bottom right corner of the matrix is the highest frequency coefficient. It is noted that the size of the prediction block, and thus the residual block, may be different from the size of the transform block. For example, a prediction block may be divided into smaller blocks to which separate transforms are applied.
The quantization stage 406 uses a quantizer value or quantization stage to convert the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients. For example, the transform coefficients may be divided by the quantizer values and truncated. The quantized transform coefficients are then entropy encoded by entropy encoding stage 408. Entropy coding may be performed using a variety of techniques, including tokens and binary trees. The entropy coded coefficients are then output to the compressed bitstream 420, along with other information used to decode the block, which may include, for example, the type of prediction used, the type of transform, the motion vectors, quantizer values. The information to decode the block may be entropy coded into a block, frame, slice, and/or section header within the compressed bitstream 420. The compressed bitstream 420 may also be referred to as an encoded video stream or an encoded video bitstream, and these terms will be used interchangeably herein.
The reconstruction path in fig. 4 (illustrated by the dashed connecting lines) may be used to ensure that both the encoder 400 and the decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420. The reconstruction path performs functions similar to those performed during the decoding process discussed in more detail below, including dequantizing the quantized transform coefficients at a dequantization stage 410 and inverse transforming the dequantized transform coefficients at an inverse transform stage 412 to produce a block of derivative residues (also referred to as derivative residuals). At the reconstruction stage 414, the predicted block predicted at the intra/inter prediction stage 402 may be added to the derivative residual to create a reconstructed block. A loop filtering stage 416 may be applied to the reconstructed block to reduce distortion such as blocking artifacts.
Other variations of the encoder 400 may be used to encode the compressed bitstream 420. For example, a non-transform based encoder may quantize the residual signal directly for certain blocks or frames without the transform stage 404. In another embodiment, the encoder 400 may have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 may be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program may include machine instructions that, when executed by a processor such as CPU 202, cause receiving station 106 to decode video data in the manner described herein. Decoder 500 may also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106. Similar to the reconstruction path of the encoder 400 discussed above, the decoder 500 in one example includes the following stages that perform various functions to generate the output video stream 516 from the compressed bitstream 420: entropy decoding stage 502, dequantization stage 504, inverse transform stage 506, intra/inter prediction stage 508, reconstruction stage 510, loop filtering stage 512, and deblock filtering stage 514. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 may be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients. Dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by quantizer values), and inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to produce derivative residuals, which may be the same as the derivative residuals created by inverse transform stage 412 in encoder 400. Using header information decoded from the compressed bitstream 420, the decoder 500 may use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400 (e.g., at the intra/inter prediction stage 402). At the reconstruction stage 510, the prediction block may be added to the derivative residual to create a reconstructed block. A loop filtering stage 512 may be applied to the reconstructed block to reduce blocking artifacts. Other filtering may be applied to the reconstructed block. In this example, a deblocking filtering stage 514 is applied to the reconstructed block to reduce block distortion, and the result is output as an output video stream 516. The output video stream 516 may also be referred to as a decoded video stream, and these terms will be used interchangeably herein.
Other variations of the decoder 500 may be used to decode the compressed bitstream 420. For example, the decoder 500 may generate the output video stream 516 without the deblocking filtering stage 514. In some embodiments of the decoder 500, the deblock filtering stage 514 is applied before the loop filtering stage 512. Additionally or alternatively, the encoder 400 includes a deblocking filtering stage in addition to the loop filtering stage 416.
Fig. 6 is a flow diagram of a process 600 for generating a prediction block for coding a block of a frame using intra prediction according to an embodiment of the present invention. In an example, the frame may be a frame of a video sequence. In an example, the frame may be a single image. The block may be the current block being coded. More specifically, the process 600 generates the prediction block for the block by recursive extrapolation. The block may be the largest coded block. The maximum coding block may be referred to by different names, such as macroblock, super-block, maximum coding unit, etc. The size of the largest coded block may be 64 x 64, 128 x 128, smaller or larger. The block may be a subblock of a largest coded block. Thus, the block may have a size of 32 × 32, 16 × 16, 8 × 8, or 4 × 4. The block may be a chroma block. The block may be a luminance block. The block may be co-extensive with a transform block. The prediction block may have the same size as the block. That is, if the size of a block is N × N, the size of a prediction block is also N × N, where N is a positive integer.
At 602, the process 600 uses a training region adjacent to a block to determine filter coefficients for generating a prediction block for the block. The training area is at the periphery of the block (e.g., outside the block). The training region includes a plurality of reconstructed pixels. As described with reference to fig. 7A and 7B, the process 600 may use the training region to determine the filter coefficients. The training area may be or may comprise one or more training areas. The determined filter coefficients minimize a function of the difference, as described further below. Each difference is a respective difference between a pixel in the training region and a prediction of that pixel in the training region.
As described above, intra prediction of a block uses pixels at the periphery of the block. The pixels at the periphery of the block are reconstructed pixels. In the encoder, the reconstructed pixels may be as described with reference to reconstruction stage 414 of fig. 4. In the decoder, reconstructing the pixels may be as described with reference to the reconstruction stage 510 of fig. 5.
Fig. 7A is a diagram 700 of a training area according to an embodiment of the present disclosure. Diagram 700 includes block 702. The block 702 may be a block for which the process 600 determines a prediction block. Block 702 is merely an illustration of the location of a current block to be predicted. Alternatively or equivalently, block 702 illustrates a prediction block to be generated. Block 702 may be any size block. For example, the block 702 may have a size (i.e., dimension) of 4 × 4, 8 × 8, 16 × 16, 32 × 32, 64 × 64, 128 × 128, or any other square or rectangular block size. Block 702 may be a block of a current frame.
The training area of block 702 may be or include at least one of an upper area 704 (i.e., an upper training area), a left area 706 (i.e., a left training area), or a combination thereof. The training area is referred to herein as a context. The upper region 704 includes reconstructed pixels and is located above the block 702 in raster scan order. The left region 706 includes reconstructed pixels and is located to the left of the block 702 in raster scan order. The upper region 704 and the left region 706 are shown as overlapping regions. A particular demarcation and/or boundary separating the upper region 704 and the left region 706 is not necessary, as will become apparent further below. As described further below, the training area is the upper area 704 (i.e., pixels of the upper area 704), the left area 706 (i.e., pixels of the left area 704), or both (pixels of both areas).
The upper region 704 may have w 1 Height of the row, wherein w 1 >1. That is, the upper region 704 may include w 1 (i.e., more than 1) line of reconstructed pixels. The upper region 704 may extend beyond the size of the block 702 by w 3 An additional column, wherein w 3 May be an integer equal to or greater than 0. Extended w 3 This additional column is referred to herein as the extended upper region.
The left region 706 may have a width w 2 Wherein w is 2 >1. That is, the left region 706 may include w 2 (i.e., more than 1) column reconstructed pixels. The left region 706 may extend beyond the size of the block 702 by w 4 An additional row, wherein w 4 May be an integer equal to or greater than 0. Extended w 4 This additional row is referred to herein as the extended left region.
In an example, w 1 、w 2 、w 3 And w 4 May be equal to 4. However, w 1 、w 2 、w 3 And w 4 Each of which may have different (equal or unequal) values, as described below with reference to offline training.
The expanded reconstructed pixels (i.e., the reconstructed pixels of the expanded left region and/or the expanded upper region) may be used for some blocks to be encoded. Generally, when encoding an image, as described above, the image is divided into the largest coded blocks (e.g., macroblocks, super-blocks, etc.) or into coded blocks of a certain size. The largest coded block may be encoded to determine a rate-distortion cost. The largest coded block may then be recursively partitioned into smaller blocks. For example, a 64 × 64 block may be divided into four 32 × 32 blocks; each of the 64 x 64 blocks may be further divided into 32 x 32 sub-blocks, and so on. A rate-distortion value may be determined for the sub-blocks. Thus, for at least some sub-blocks, the right-side neighboring reconstruction column and/or the lower neighboring reconstruction row may be used for the block (or sub-block) that includes the sub-block.
In an example, the online training may be or may include a training region that uses (e.g., analyzes) reconstructed pixels. Three possible training areas (referred to herein as "contexts") may be used: full context, left context, and top context.
The full context includes w above block 702 1 Row and w to the left of block 702 2 And (4) columns. The complete context may also include w 3 An additional column (if available) and w 4 An additional left row (if available).
The left context shown in context 708 includes reconstructed pixels to the left of block 702. More specifically, the left context includes w to the left of block 702 2 And (4) columns. The left context may also include w to the left of block 702 1 An additional row (if available), and w below block 702 4 Additional rows (if available).
The upper context illustrated in context 710 includes reconstructed pixels above block 702. More specifically, the upper context includes w above block 702 1 And (4) a row. The upper context may also include w to the left of block 702 2 An additional column (if available) and w to the right of block 702 3 An additional column (if available).
In an example, the training area may be the left area 706. In an example, the training area may include an upper area 704 and a left area 706. The upper region 704 may include N (where N > 1) rows of reconstructed pixels. The left region 706 may include M (where M > 1) reconstructed pixel columns. In an example, N may be equal to M.
Fitting the optimal filter includes determining optimal coefficients to be used in the extrapolation operation. The optimal filter may be a 2-tap filter, a 3-tap filter, a 4-tap filter, or a 5-tap filter. In an example, fitting the optimal filter may be or may include determining a solution to a least squares problem such as that given in optimization problem (2):
those skilled in the art recognize that "arg min" determines (e.g., calculates, finds, etc.) the (a, β, γ) tuple that minimizes the function value among all possible (a, β, γ) tuples. In the optimization problem (2), x l 、xt l And x t Representing the left, top left and top adjacent pixels, respectively, of pixel x as described with reference to fig. 11. However, as will become more apparent below, other neighboring pixels are also possible. In the optimization problem (2), R denotes a set of pixels for which optimal filter coefficients (a, β, γ) are to be determined. That is, the set R includes some reconstructed pixels of the training region, as described further below. Using coefficients (a, beta, gamma) instead of coefficients as in the reference map 11 Said predefined coefficient (c) v ，c h ，c d ). Thus, the function of the difference between each pixel in the training region and the corresponding prediction for that pixel in the training region is a function of the sum of the squares of the differences between each pixel in the training region and the corresponding prediction for that pixel in the training region.
The parameter λ represents the regularization coefficient, which is biased towards a solution closer to zero, all else being equal. A solution close to zero means that the absolute value of the coefficient is a small value. That is, if the sum of the squared differences (e.g., Σ) x∈R (x-(αx l +βx tl +γx t )) 2 ) With a regularization term (e.g. λ (α) 2 +β 2 +γ 2 ) ) are the same, the coefficients with smaller absolute values are biased (e.g., selected). Such regularization may generally reduce the risk of overfitting. The parameter lambda may be a predetermined value and may be selected empirically. Such regularization can be used so that the optimization problem (2) has a single solution; otherwise, when λ =0 and x = constant (i.e. all pixel values in the training region are the same), x ∈ R, e.g. the optimization problem (2) can have an infinite number of solutions.
Although the sum of squares is used in optimization problem (2), more generally, by minimizing the first reconstructed pixel (e.g., pixel value x) and the second reconstructed pixel (e.g., pixel value x) l 、x tl And x t ) To generate filter coefficients, based on the difference between corresponding (e.g., consecutive) extrapolations of the filter coefficients. In an example, the difference may be a sum of absolute differences. Any other suitable measure of error (i.e., difference) may be used. For a first pixel in the training region, the prediction of the first pixel may include a sum of each filter coefficient applied to a corresponding second pixel, each second pixel being a pixel in the training region that is adjacent to the first pixel.
Optimization problem (2) is an example of determining the optimal coefficients for a 3-tap filter. However, as already mentioned, the optimal filter may be a 2-tap filter, a 4-tap filter, a 5-tap filter, or other sized filter. Thus, in the case of a 2-tap filter, process 600 uses a formula similar to optimization problem (2) to determine the optimal (α, β) coefficients; in the case of a 4-tap filter, the process 600 determines optimal (a, β, γ, δ) coefficients, etc., using a formula similar to the optimization problem (2). For example, as will become more apparent further below, the optimal filter may be a 2-tap filter using the left and upper left neighbors. For example, as will become more apparent further below, the optimal filter may be a 4-tap filter using lower left, upper left, and upper neighbors.
As described above, process 600 uses online training based on pixels that have been reconstructed in a neighborhood of an analysis block to derive (e.g., compute, determine, infer, etc.) coefficients for recursive extrapolation. Process 600 may be compared to using a portion of the reconstructed pixels (i.e., the first reconstructed pixels) as peripheral pixels, as described with reference to, for example, first pixels 908 and/or second pixels 910 of fig. 9; and another portion of the reconstructed pixels (i.e., the second reconstructed pixels) is used as the source pixels for the image block. A portion 712 of the upper region 704 (i.e., the upper training region) is used as an illustration. In this illustration, a 3-tap filter is assumed.
Pixels 716-720 are part of the first reconstructed pixel. Pixel 714 is part of a second reconstructed pixel. Pixels 716, 718, 720 are left, top left, and top adjacent pixels, respectively, of pixel 714. For pixel 714, x corresponds to the value of the source pixel, and the sum (ax) l +βx tl +γx t ) Corresponding to the useAdjacent value x l 、x lt And x t As a prediction of the extrapolated x. Similar to the recursive extrapolation described with reference to FIG. 11, the recursive extrapolation is performed on the remaining pixels in the training area.
As described above, the process 600 may determine the coefficients for the optimal filter. Thus, in addition to the filter coefficients, the process 600 may determine optimal coefficients for at least a subset of the possible tap combinations. The process 600 may then select those coefficient and tap combinations that correspond to the smallest error.
Fig. 7B is a diagram 750 of possible taps according to an embodiment of the present disclosure. Given the current pixel 752 (i.e., the current pixel location), the process 600 may select a combination of available neighboring pixels in the optimization problem (2). The available neighboring pixels may include pixels labeled "0" - "4". That is, the available neighboring pixels may include a lower left pixel (labeled "0"), a left pixel (labeled "1"), an upper left pixel (labeled "2"), an upper pixel (labeled "3"), and an upper right pixel (labeled "4").
Any combination of available pixels labeled "0" - "4" may be selected according to embodiments of the present disclosure. However, in some embodiments described further below, a combination including both pixels labeled "0" and pixels labeled "4" is not used. That is, combinations of pixels that include both the lower left and upper right neighbors are not allowed. Limitations may be imposed because using the top-right pixels requires row-major prediction order, while using the bottom-left pixels requires column-major prediction order. Using both the top-right pixel and the bottom-left pixel for recursive extrapolation may result in dependencies whereby the extrapolation operation requires at least one pixel that has not yet been encoded (e.g., reconstructed).
FIG. 11 is used in conjunction with FIG. 7B to illustrate dependencies. Predicting the pixel at position "1" of FIG. 11 and using the pixels labeled "0" and "4" of FIG. 7B would refer to using X 2,0 And X 0,2 They may both be available. However, predicting the pixel at position "5" in FIG. 11 and using the pixels labeled "0" and "4" in FIG. 7B requires X 3,0 Sum positionThe pixel value at "2"; and predicting the pixel at position "2" of FIG. 11 and using the pixels labeled "0" and "4" in FIG. 7B would require positions "5" and X 0,3 The pixel value of (b). Therefore, predicting the pixel value at position "5" requires the pixel value at position "2" and vice versa, thereby generating a dependency.
In the case where the lower left and upper right neighbors are not used together, a total of 18 tap combinations are possible: nine 2-tap filters are possible (i.e., using pixel combinations ("0", "1"), ("0", "2"), ("0", "3"), ("1", "2"), ("1", "3"), ("1", "4"), ("2", "3"), ("2", "4") and ("3", "4")); seven 3-tap filters are possible (i.e., using pixel combinations ("0", "1", "2"), ("0", "1", "3"), ("1", "2", "4"), ("1", "3", "4") and ("2", "3", "4")), and two 4-tap filters are possible (i.e., using pixel combinations ("0", "1", "2", "3") and ("1", "2", "3", "4")).
As described above, there may be three possibilities for the training area; i.e., using the upper region, using the left region, or using both the left and upper regions. As described above, there are 18 possible filter pixel combinations. Thus, in an embodiment, there are a total of 3 × 18=54 possible filter type and training region combinations. These combinations may be referred to as adaptive filter modes. That is, each combination may be an adaptive filter mode. The adaptive filter modes may be numbered, such as from 0 to 53. Thus, in an example, using a training region neighboring the block to determine filter coefficients for generating a prediction block for the block may include determining (e.g., choosing, selecting, etc.) an adaptive filter pattern that indicates at least one of a training region (i.e., an upper region, a left region, or both) or a neighboring pixel location (i.e., which of the combination of lower left, left side, upper left, upper top, and upper right) to be used in the recursive extrapolation.
When implemented by a decoder, using a training region adjacent to the block to determine filter coefficients for generating a prediction block for the block may include decoding an adaptive filter pattern from a compressed bitstream. The compressed bitstream may be the compressed bitstream 420 of fig. 5. For example, the decoder may receive an index of the adaptive filter pattern and may use the indicated training region and/or the indicated neighboring pixel locations to be used in the recursive extrapolation to calculate optimal filter coefficients using the optimization problem (2). As described above, depending on the indicated neighboring pixel positions, the decoder may calculate 2, 3 or 4 filter coefficients.
Referring again to fig. 6, at 604, the process 600 generates a prediction block using the determined filter coefficients. The prediction block is generated using recursive extrapolation. The recursive extrapolation is similar to the recursive extrapolation described with reference to fig. 11. It will be appreciated, however, that while the recursive extrapolation of fig. 11 uses a 3-tap filter, the process 600 may use the number of taps as determined at 602. It will also be appreciated that while the recursive extrapolation of FIG. 11 may use left, top-left, and top-adjacent pixels in the recursive extrapolation, the process 600 may use a combination of pixels as determined at 602 and as described with reference to the graph 750 of FIG. 7B.
When implemented by an encoder, process 600 may include encoding an adaptive filter pattern in a compressed bitstream. The compressed bitstream may be the compressed bitstream 420 of fig. 4. The coded adaptive filter mode may include or may refer to an indication (such as an index) of the coded adaptive filter mode. The adaptive filter pattern may be entropy coded using a cumulative distribution function. In an example, the adaptive filter pattern may be entropy encoded using the same cumulative distribution function regardless of the block size.
As described above, combinations of 54 filter types and training regions are possible. In addition, a fixed training region size (i.e., w) is used 1 And w 2 May be equal to 4). However, if the different sizes of the left area and/or the upper area are also variable, the total number of combinations increases significantly. The increased number of combinations may be computationally burdensome and/or may require a large amount of signaling overhead. For example, to indicate one of 54 possible combinations, it may be desirable toSix bits are required. Thus, in some embodiments, offline training may be used to reduce the number of combinations of filter types, training regions, and training region sizes.
Offline training is used to select (e.g., find, determine, choose, etc.) the optimal set of adaptive filter patterns that will be used by the encoder and decoder. That is, offline training is used to select a subset of the set of all possible adaptive filter patterns, such that the subset of adaptive filter patterns results in the largest gain (MSE gain or RD cost gain, as described further below) compared to not using the selected adaptive filter pattern.
In an embodiment, the training region size may be fixed, and an optimal number of filter and training region combinations may be selected (e.g., determined, calculated, etc.). In an example, seven optimal filter and training region combinations may be selected. Thus, the process 600 can only use (e.g., test) those optimal combinations.
To determine the optimal filter and training region combination, the offline training process may consider the minimization of the optimization problem (3):
in optimization problem (3), F is the set of all possible 54 filters, as described above; b is a sufficiently large training set of variable-size blocks. P k (F) Is the set of subsets of F of size k. For example, if it is desired to find an optimal combination of 5 filters, k is set to 5.S k Is the combination of k optimal filters resulting from the optimization problem (3). MSE best_mode (b) Is the mean square error obtained when the training block b is predicted using the optimal intra prediction mode (as selected by the encoder). MSE f (b) Is to predict the mean square error of block b using the adaptive intra prediction mode. The optimal intra-prediction mode is one of the available intra-prediction modes to be selected by an intra/inter-prediction stage (such as intra/inter-prediction stage 402 of fig. 4) for coding block b. Available intra-prediction modes do not include descriptions hereinThe adaptive filter model described above. MSE f (b) Is the mean square error obtained if the block b is encoded using an adaptive filter.
The solution to the optimization problem (3) is the first k adaptive filters to minimize the average MSE of the prediction blocks in the training set B when the optimal mode is chosen for each block based on MSE. The "average MSE" refers to the overall MSE of all training blocks, rather than minimizing the MSE for each training block.
Fig. 8 is a diagram of an optimal adaptive filter according to an embodiment of the present disclosure. Curve 810 illustrates an optimal adaptive filter configuration for the purpose of minimizing the predicted MSE given by the optimization problem (3). Note that each adaptive filter of curve 810 has the same shape as that of diagram 750 of fig. 7B.
A point such as point 812 represents the tap position used in the corresponding adaptive filter combination. A label, such as label 814 above the filter configuration (i.e., "above"), indicates the training area (i.e., context) to use. Thus, if the label indicates "above," then the above region (i.e., the above context) will be used; if the label indicates "left", the left region (i.e., left context) will be used; and if no label is indicated, the training area includes both the left area and the top area (i.e., full context). Thus, for example, adaptive filter 816 is such that the upper and upper right pixels will be in the extrapolation operation during recursive filtering, and both the upper and left regions (i.e., the full context) will be used when determining the filter coefficients, such as using (i.e., solving) the optimization problem (1). For example, the adaptive filter 818 is such that during recursive filtering, the left and upper-left pixels will be used in the extrapolation operation, and when determining the filter, only the upper region (i.e., the upper context) will be used.
Thus, each adaptive intra-prediction mode may be represented by three variables; i.e. the number of taps, the position of the taps and the training area.
Instead of solving an MSE-based optimization problem, such as described with reference to optimization problem (3), to determine the optimal adaptive filter configuration, the optimization problem may be based on the rate-distortion (RD) cost of encoding block B of training set B using the optimal intra-prediction mode as compared to encoding block B using an adaptive filter. Therefore, optimization problem (4) can be used instead of optimization problem (3):
in the optimization problem (4), F is the set of all possible 54 filters, as described above; b is a sufficiently large training set of variable-size blocks. RD best_mode (b) Is the rate-distortion value obtained when the training block b is predicted using the optimal (as selected by the encoder) intra prediction mode. RD f (b) Is a rate-distortion value when the block b is predicted using the adaptive intra-prediction mode. The optimal intra prediction mode is one of the available intra prediction modes selected for coding block b by an intra/inter prediction stage, such as intra/inter prediction stage 402 of fig. 4. The available intra-prediction modes do not include the adaptive filter modes described herein.
Whether optimization problem (3) or optimization problem (4) is used, the optimal adaptive filter can be selected. Each optimal adaptive filter may be used as an intra-prediction mode (e.g., an adaptive intra-prediction mode). In an example, five, seven, fewer, or more optimal adaptive filters may be selected for use as intra-prediction modes.
In the above description, the training areas 704-710 of FIG. 7A are depicted as having a fixed size. However, in some embodiments, the optimal size of the training region may be determined for each selected optimal adaptive filter. For example, if seven (7) optimal adaptive filters are selected as described above, the optimization problem (5) can be solved to determine the optimal size of the corresponding training area for each optimal adaptive filter; namely, an upper region size, a left region size, a right region size, and a lower region size.
In optimization problem (5), (w) 1 ，...，w 7 ) A training region size is described for each of the seven optimal adaptive filters selected; b denotes a training set (e.g., transform unit or prediction unit) with one fixed-size block;
Due to trying (w) 1 ，...，w 7 ) The brute force approach of each possible combination of (a) may be computationally very expensive, so in some embodiments, a greedy approximation minimization algorithm may be used. For example, starting from some reasonable initial approximation, a pattern may be iteratively chosen, wherein the pattern provides the best improvement. This approximation may be chosen empirically. For example, a simple initial setting (such as w) may be tested 1 ＝w 2 ＝w 3 ＝w 4 = 4), and the best one of the simple initial settings can be selected as the initial setting. The training region shape parameters for a pattern may then be modified while keeping all other patterns fixed. To further narrow the search space, only training region shape modifications in a small neighborhood around the current approximation of the training region shape in a given mode may be considered. The iterative process may then be repeated until the optimization problem converges to a solution.
In an embodiment, table I lists seven adaptive intra-prediction modes used by an encoder (such as encoder 400 of fig. 4) and a decoder (such as decoder 500 of fig. 5). Column "Num" of table I provides an index for easy reference to the adaptive intra prediction mode; the column "filter size" indicates the number of taps for the adaptive intra prediction mode, and the column "neighboring pixels" refers to neighboring pixels used in recursive extrapolation (as described with reference to FIG. 7B); and the column "training context" refers to the context (as described with reference to fig. 7A) used to derive the coefficient values for the adaptive intra prediction mode (as described with reference to optimization problem (2)).
TABLE I
Num | Filter size | Adjacent | Training context | |
0 | 3- |
1,2,3 | |
|
1 | 3- |
0,1,3 | |
|
2 | 3- |
1,3,4 | |
|
3 | 4- |
0,1,2,3 | |
|
4 | 4- |
1,2,3,4 | |
|
5 | 3- |
0,2,3 | |
|
6 | 3- |
1,2,4 | Complete context |
The lower arrays, pointer _ filter _ intra _ thickness _ ver, adapt _ filter _ intra _ thickness _ hor, adapt _ filter _ intra _ top _ right _ offset, and adapt _ filter _ intra _ bottom _ left _ offset, respectively, correspond to the w _ filter _ intra _ thickness _ ver, w _ filter _ intra _ bottom _ left _ offset described above with reference to FIG. 7A 1 、w 2 、w 3 And w 4 . That is, the array defines parameters describing the size of the training region for fitting the adaptive filter for each transform size and adaptive filter intra mode.
The arrays of adapt _ filter _ intra _ thickness _ ver, adapt _ filter _ intra _ thickness _ hor, adapt _ filter _ intra _ top _ right _ offset and adapt _ filter _ intra _ bottom _ left _ offset describe the corresponding w of each adaptive intra prediction mode of table I by transforming the block size 1 、w 2 、w 3 And w 4 Size.
Lines "{12,9, 6,10,7,6},// TX-16X 16" of adapt _ filter _ intra _ thickness Jior are now used to illustrate how the above array should be understood: for a transform block size of 16x16, the values of W2 for each of the adaptive intra prediction modes 0-7 of table I are 12,9, 6,10,7, and 6, respectively.
Another aspect of the disclosed embodiments is a process for generating a prediction block for decoding a block of a frame using intra-prediction, the process comprising decoding an adaptive intra-prediction mode from a compressed bitstream, the adaptive intra-prediction mode indicating a training region of reconstructed pixels to be used in an extrapolation operation; determining filter coefficients for generating a prediction block using the training region; and generating a prediction block for the block by using recursive extrapolation of the filter coefficients. The prediction block may be added to the decoded residual block to reconstruct the block.
As described above, the adaptive intra-prediction mode may be a plurality of adaptive intra-prediction modes S selected from a set of F possible adaptive intra-prediction modes using off-line training by solving an optimization problem k One of them. In an example, the optimization may be an optimization problem (3). In another example, the optimization problem may be optimization problem (4). In another example, the optimization problem may include optimization problem (3) and optimization problem (5). In another example, the optimization problem may include an optimization problem (4) and an optimization problem (5).
Table II presents the results of a comparison in terms of Bjontegaard rate difference (BD-rate), with negative values indicating coding performance gain. Table II compares the gains obtained when using the first seven optimal adaptive filters with the optimal training region size, the first five optimal adaptive filters with the optimal training region size, the first three optimal adaptive filters with the optimal training region size, and the first seven optimal adaptive filters with a fixed training region size, compared to the baseline without the adaptive intra-prediction mode.
Table III compares the gain of the conventional filter intra mode compared to the proposed adaptive mode from the baseline. The row labeled FILTER _ INTRA illustrates the gain obtained when using the mode FILTER _ mode (i.e., using predefined coefficients) compared to not using the FILTER _ INTRA mode. The line labeled ADAPT FILTER INTRA illustrates the gain obtained when using the adaptive INTRA prediction mode described herein compared to not using the adaptive INTRA prediction mode. The row labeled FILTER _ INTRA + ADAPT _ FILTER _ INTRA illustrates the gain obtained when using the FILTER _ intro mode and the adaptive INTRA-frame prediction mode.
Aspects of the encoding and decoding described above illustrate some encoding and decoding techniques. However, it will be understood that encoding and decoding, as those terms are used in the claims, may refer to compression, decompression, transformation, or any other processing or alteration of data.
The word "example" or "embodiment" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "embodiment" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "embodiment" is intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless otherwise indicated or apparent from the context, the phrase "X comprises a or B" is intended to mean any of its natural inclusive permutations. That is, if X comprises A; x includes B; or X includes both a and B, then "X includes a or B" is satisfied under any of the above examples. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, the use of the term "an embodiment" or "one embodiment" throughout this document should not be taken to mean the same embodiment or implementation unless described as such.
Embodiments of transmitting station 102 and/or receiving station 106 (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby, including by encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term "processor" should be understood to cover any of the above hardware, alone or in combination. The terms "signal" and "data" may be used interchangeably. In addition, portions of transmitting station 102 and receiving station 106 need not necessarily be implemented in the same manner.
Further, in an aspect, transmitting station 102 or receiving station 106 may be implemented, for example, using a computer or processor having a computer program that, when executed, performs any of the various methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be utilized which may contain other hardware for performing any of the methods, algorithms, or instructions described herein.
Transmitting station 102 and receiving station 106 may be implemented, for example, on computers in a videoconferencing system. Alternatively, transmitting station 102 may be implemented on a server, and receiving station 106 may be implemented on a device separate from the server, such as a handheld communication device. In this case, transmitting station 102 may use encoder 400 to encode the content into an encoded video signal and transmit the encoded video signal to a communication device. In turn, the communication device may then use the decoder 500 to decode the encoded video signal. Alternatively, the communication device may decode content stored locally on the communication device (e.g., content not sent by transmitting station 102). Other transmitter station 102 and receiving station 106 embodiments are available. For example, the receiving station 106 may be a generally stationary personal computer rather than a portable communication device, and/or a device including the encoder 400 may also include the decoder 500.
Additionally, all or a portion of embodiments of the present disclosure may take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be any apparatus that can, for example, tangibly embody, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
The above-described embodiments, implementations, and aspects have been described in order to allow easy understanding of the present disclosure and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (19)
1. A device for decoding a block of a frame using intra prediction, the device comprising:
a processor that:
decoding an adaptive intra-frame prediction mode of a set of adaptive filter modes from a compressed bitstream, the adaptive intra-frame prediction mode indicating a number of filter coefficients and a relative position of a pixel to be predicted in a subset of neighboring pixels with respect to the pixel to be predicted,
wherein the set of adaptive filter modes includes a first adaptation mode and a second adaptation mode,
wherein the first and second adaptation modes indicate the same number of coefficients, and
wherein the first adaptation mode indicates a first set of first relative positions of a first subset of neighboring pixels that is different from a second set of second relative positions of a second subset of neighboring pixels indicated by the second adaptation mode;
determining filter coefficients for generating a prediction block for the block; and
generating a prediction block for the block by using recursive extrapolation of the filter coefficients and the relative position.
2. The apparatus of claim 1, wherein the relative positions of the pixels to be predicted comprise lower left, left side, upper left, upper right and upper right neighboring pixels of the pixels to be predicted.
3. The apparatus of claim 1, wherein the number of filter coefficients is one of 2, 3, or 4.
4. The apparatus of claim 1, wherein determining filter coefficients for generating the prediction block comprises:
using a training region adjacent to the block for determining the filter coefficients, wherein the training region of the block comprises reconstructed pixels.
5. The apparatus of claim 4, wherein the first and second electrodes are disposed on opposite sides of the substrate,
wherein the training area comprises adjacent rows and adjacent rows,
wherein the adjacent rows comprise at least one upper row of reconstructed pixels above an upper edge periphery of the block or a left side column of left reconstructed pixels above a left side edge periphery of the block, and
wherein the training region further comprises at least two reconstruction rows.
6. The device of claim 5, wherein determining the filter coefficients using the training region adjacent to the block comprises:
obtaining the filter coefficients by fitting an optimal filter to differences between the neighboring row of pixels and respective predictions of the neighboring row of pixels, wherein the respective predictions of the neighboring row of pixels are obtained using respective relative positions of the neighboring row of pixels.
7. The apparatus of claim 6, wherein fitting the optimal filter comprises a regularization factor.
8. The apparatus of claim 4, wherein the adaptive intra prediction mode indicates a size of the training region.
9. The apparatus of claim 8, wherein the size of the training area comprises at least one of: a first height of an upper region peripheral to an upper edge of the block, a first width of a right region peripheral to a right side edge of the block, a second height of a bottom region peripheral to a bottom edge of the block, or a second width of a left region peripheral to a left side edge of the block.
10. The apparatus of claim 4, wherein the training area comprises at least one of: four rows of pixels adjacent to a top edge of the block, four columns of pixels adjacent to a right side edge of the block, four rows of pixels adjacent to a bottom edge of the block, or four columns of pixels adjacent to a left side edge of the block.
11. A method for generating a prediction block for coding a block of a frame using intra prediction, the method comprising:
determining a pixel value indicative of at least one training area and neighboring pixelsA configured adaptive intra-prediction mode of a location, wherein the adaptive intra-prediction mode is a plurality of adaptive intra-prediction modes S k One of the blocks, and wherein the training region is adjacent to the block and includes a plurality of reconstructed pixels;
determining filter coefficients for obtaining respective predicted pixels of neighboring pixels within the training area when applied to the defined respective configurations of the neighboring pixels in accordance with the configuration of the neighboring pixels,
wherein the filter coefficients minimize a function of differences, each difference being a respective difference between a pixel in the training region and a prediction of that pixel in the training region; and
predicting each pixel of the prediction block by applying the filter coefficients to a configuration of neighboring pixels of the predicted pixel, thereby generating the prediction block by recursive extrapolation using the filter coefficients.
12. The method of claim 11, further comprising:
encoding the adaptive intra prediction mode in a compressed bitstream.
13. The method of claim 11, wherein determining the filter coefficients for generating a prediction block for the block using the training region neighboring the block comprises:
decoding the adaptive intra prediction mode from the compressed bitstream.
14. The method of claim 11, wherein the first and second light sources are selected from the group consisting of,
wherein each neighboring pixel is selected from a group of neighboring pixels comprising a left neighbor, an upper-left neighbor, an upper neighbor, a lower-left neighbor, and an upper-right neighbor, an
Wherein each filter coefficient is applied to a respective one of the one or more neighboring pixels.
15. The method of claim 14, wherein the one or more neighboring pixels do not include both the lower-left neighbor and the upper-right neighbor.
16. The method of claim 11, wherein the training area comprises at least one of a first area above the block or a second area to the left of the block.
17. The method of claim 11, wherein, for a first pixel in the training region, the prediction of the first pixel comprises a sum of each filter coefficient applied to a respective second pixel, each second pixel being a pixel in the training region adjacent to the first pixel.
18. The method of claim 11, wherein the function of the difference between each pixel in the training region and the prediction of that pixel in the training region is a function of the sum of the squares of the differences between each pixel in the training region and the prediction of that pixel in the training region.
19. The method of claim 11, wherein the plurality of adaptive intra prediction modes S k Is selected from a set F of possible adaptive intra-prediction modes using off-line training by solving an optimization problem given by:
where B is a variable size training block set, MSE best_mode (b) Is the mean square error, MSE, obtained when the training block b is predicted using the optimal intra prediction mode f (b) Is to predict the mean square error of the training block b using an adaptive intra prediction mode F, which is the set of all possible adaptive intra prediction modes, and P k (F) Is the set of subsets of size k in F.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/287,969 | 2019-02-27 | ||
US16/287,969 US10778972B1 (en) | 2019-02-27 | 2019-02-27 | Adaptive filter intra prediction modes in image/video compression |
PCT/US2019/059030 WO2020176143A1 (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction modes in image/video compression |
CN201980090403.0A CN113491129B (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction mode in image/video compression |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980090403.0A Division CN113491129B (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction mode in image/video compression |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115514957A true CN115514957A (en) | 2022-12-23 |
Family
ID=68654881
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980090403.0A Active CN113491129B (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction mode in image/video compression |
CN202210981861.7A Pending CN115514957A (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction mode in image/video compression |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980090403.0A Active CN113491129B (en) | 2019-02-27 | 2019-10-31 | Adaptive filter intra prediction mode in image/video compression |
Country Status (4)
Country | Link |
---|---|
US (3) | US10778972B1 (en) |
EP (1) | EP3932075A1 (en) |
CN (2) | CN113491129B (en) |
WO (1) | WO2020176143A1 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220303541A1 (en) * | 2019-06-20 | 2022-09-22 | Interdigital Vc Holdings France, Sas | Method and device for picture encoding and decoding using position dependent intra prediction combination |
KR20210133395A (en) * | 2020-04-29 | 2021-11-08 | 삼성전자주식회사 | An image encoder, an image sensing device, and an operating method of the image encoder |
DE102021117397A1 (en) * | 2020-07-16 | 2022-01-20 | Samsung Electronics Co., Ltd. | IMAGE SENSOR MODULE, IMAGE PROCESSING SYSTEM AND IMAGE COMPRESSION METHOD |
US11140394B1 (en) * | 2020-08-27 | 2021-10-05 | Tencent America LLC | Adaptive chroma intra mode coding in video compression |
CN116320410A (en) * | 2021-12-21 | 2023-06-23 | 腾讯科技（深圳）有限公司 | Data processing method, device, equipment and readable storage medium |
WO2024043116A1 (en) * | 2022-08-24 | 2024-02-29 | ソニーセミコンダクタソリューションズ株式会社 | Learning device, inference device, learning method, inference method, encryption device, and decryption device |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP0920216A1 (en) | 1997-11-25 | 1999-06-02 | Deutsche Thomson-Brandt Gmbh | Method and apparatus for encoding and decoding an image sequence |
US8331448B2 (en) | 2006-12-22 | 2012-12-11 | Qualcomm Incorporated | Systems and methods for efficient spatial intra predictabilty determination (or assessment) |
JP5188875B2 (en) | 2007-06-04 | 2013-04-24 | 株式会社エヌ・ティ・ティ・ドコモ | Image predictive encoding device, image predictive decoding device, image predictive encoding method, image predictive decoding method, image predictive encoding program, and image predictive decoding program |
US9008175B2 (en) | 2010-10-01 | 2015-04-14 | Qualcomm Incorporated | Intra smoothing filter for video coding |
JP5478740B2 (en) | 2011-01-12 | 2014-04-23 | 三菱電機株式会社 | Moving picture encoding apparatus, moving picture decoding apparatus, moving picture encoding method, and moving picture decoding method |
US9432699B2 (en) | 2011-05-18 | 2016-08-30 | Nokia Technologies Oy | Methods, apparatuses and computer programs for video coding |
JP5798539B2 (en) | 2012-09-24 | 2015-10-21 | 株式会社Ｎｔｔドコモ | Moving picture predictive coding apparatus, moving picture predictive coding method, moving picture predictive decoding apparatus, and moving picture predictive decoding method |
US9667994B2 (en) | 2012-10-01 | 2017-05-30 | Qualcomm Incorporated | Intra-coding for 4:2:2 sample format in video coding |
WO2014120368A1 (en) * | 2013-01-30 | 2014-08-07 | Intel Corporation | Content adaptive entropy coding for next generation video |
US9451254B2 (en) | 2013-07-19 | 2016-09-20 | Qualcomm Incorporated | Disabling intra prediction filtering |
CN115134610A (en) * | 2015-06-11 | 2022-09-30 | 杜比实验室特许公司 | Method for encoding and decoding image using adaptive deblocking filtering and apparatus therefor |
EP3453174A1 (en) | 2016-05-06 | 2019-03-13 | VID SCALE, Inc. | Method and system for decoder-side intra mode derivation for block-based video coding |
EP3516869B1 (en) | 2016-10-04 | 2022-06-08 | HFI Innovation Inc. | Method and apparatus for intra chroma coding in image and video coding |
CN107896330B (en) * | 2017-11-29 | 2019-08-13 | 北京大学深圳研究生院 | It is a kind of in frame and the filtering method of inter-prediction |
AU2020226574A1 (en) * | 2019-02-22 | 2021-09-30 | Huawei Technologies Co., Ltd. | Method and apparatus for affine based inter prediction of chroma subblocks |
-
2019
- 2019-02-27 US US16/287,969 patent/US10778972B1/en active Active
- 2019-10-31 CN CN201980090403.0A patent/CN113491129B/en active Active
- 2019-10-31 EP EP19808948.4A patent/EP3932075A1/en active Pending
- 2019-10-31 WO PCT/US2019/059030 patent/WO2020176143A1/en unknown
- 2019-10-31 CN CN202210981861.7A patent/CN115514957A/en active Pending
-
2020
- 2020-08-21 US US16/999,109 patent/US11297314B2/en active Active
-
2022
- 2022-03-02 US US17/684,461 patent/US11979564B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
EP3932075A1 (en) | 2022-01-05 |
US20200275095A1 (en) | 2020-08-27 |
US20200382773A1 (en) | 2020-12-03 |
US11979564B2 (en) | 2024-05-07 |
WO2020176143A1 (en) | 2020-09-03 |
US10778972B1 (en) | 2020-09-15 |
US20220191479A1 (en) | 2022-06-16 |
US11297314B2 (en) | 2022-04-05 |
CN113491129B (en) | 2022-08-23 |
CN113491129A (en) | 2021-10-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN113491129B (en) | Adaptive filter intra prediction mode in image/video compression | |
US10798408B2 (en) | Last frame motion vector partitioning | |
US10992939B2 (en) | Directional intra-prediction coding | |
CN110622515A (en) | Directional intra prediction coding | |
US9942568B2 (en) | Hybrid transform scheme for video coding | |
US9544597B1 (en) | Hybrid transform in video encoding and decoding | |
CN110741645B (en) | Blockiness reduction | |
US10506240B2 (en) | Smart reordering in recursive block partitioning for advanced intra prediction in video coding | |
CN107318015B (en) | Video signal encoding method, decoding method and decoding device | |
CN110800299A (en) | Scan order adaptation for entropy coding of blocks of image data | |
US10506256B2 (en) | Intra-prediction edge filtering | |
US9380298B1 (en) | Object-based intra-prediction | |
WO2024096895A1 (en) | Wavefront scan order for transform coefficient coding | |
WO2024096896A1 (en) | Jointly designed context model and scan order for transform coefficient coding | |
WO2024081010A1 (en) | Region-based cross-component prediction |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |