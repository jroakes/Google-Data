US10567464B2 - Video compression with adaptive view-dependent lighting removal - Google Patents
Video compression with adaptive view-dependent lighting removal Download PDFInfo
- Publication number
- US10567464B2 US10567464B2 US15/832,023 US201715832023A US10567464B2 US 10567464 B2 US10567464 B2 US 10567464B2 US 201715832023 A US201715832023 A US 201715832023A US 10567464 B2 US10567464 B2 US 10567464B2
- Authority
- US
- United States
- Prior art keywords
- data
- vantage
- residual data
- subset
- light
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H04L65/607—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/60—Network streaming of media packets
- H04L65/70—Media network packetisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/111—Transformation of image signals corresponding to virtual viewpoints, e.g. spatial image interpolation
- H04N13/117—Transformation of image signals corresponding to virtual viewpoints, e.g. spatial image interpolation the virtual viewpoint locations being selected by the viewers or determined by viewer tracking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/156—Mixing image signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/161—Encoding, multiplexing or demultiplexing different image signal components
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/194—Transmission of image signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/207—Image signal generators using stereoscopic image cameras using a single 2D image sensor
- H04N13/232—Image signal generators using stereoscopic image cameras using a single 2D image sensor using fly-eye lenses, e.g. arrangements of circular lenses
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/243—Image signal generators using stereoscopic image cameras using three or more 2D image sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/282—Image signal generators for generating image signals corresponding to three or more geometrical viewpoints, e.g. multi-view systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/349—Multi-view displays for displaying three or more geometrical viewpoints without viewer tracking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/13—Adaptive entropy coding, e.g. adaptive variable length coding [AVLC] or context adaptive binary arithmetic coding [CABAC]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/196—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding being specially adapted for the computation of encoding parameters, e.g. by averaging previously computed encoding parameters
- H04N19/198—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding being specially adapted for the computation of encoding parameters, e.g. by averaging previously computed encoding parameters including smoothing of a sequence of encoding parameters, e.g. by averaging, by choice of the maximum, minimum or median value
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/30—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using hierarchical techniques, e.g. scalability
- H04N19/33—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using hierarchical techniques, e.g. scalability in the spatial domain
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/44—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/46—Embedding additional information in the video signal during the compression process
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/553—Motion estimation dealing with occlusions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/597—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding specially adapted for multi-view video sequence encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/85—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using pre-processing or post-processing specially adapted for video compression
- H04N19/86—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using pre-processing or post-processing specially adapted for video compression involving reduction of coding artifacts, e.g. of blockiness
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/45—Cameras or camera modules comprising electronic image sensors; Control thereof for generating image signals from two or more image sensors being of different type or operating in different modes, e.g. with a CMOS sensor for moving images in combination with a charge-coupled device [CCD] for still images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/90—Arrangement of cameras or camera modules, e.g. multiple cameras in TV studios or sports stadiums
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/95—Computational photography systems, e.g. light-field imaging systems
- H04N23/957—Light-field or plenoptic cameras or camera modules
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/2224—Studio circuitry; Studio devices; Studio equipment related to virtual studio applications
- H04N5/2226—Determination of depth image, e.g. for foreground/background separation
-
- H04N5/2254—
-
- H04N5/2258—
-
- H04N5/247—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/139—Format conversion, e.g. of frame-rate or size
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/275—Image signal generators from 3D object models, e.g. computer-generated stereoscopic image signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/332—Displays for viewing with the aid of special glasses or head-mounted displays [HMD]
- H04N13/344—Displays for viewing with the aid of special glasses or head-mounted displays [HMD] with head-mounted left-right displays
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/42—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation
- H04N19/436—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation using parallelised computational arrangements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/816—Monomedia components thereof involving special video data, e.g 3D video
Definitions
- the present document relates to the display of video from user-selected viewpoints for use in virtual reality, augmented reality, free-viewpoint video, omnidirectional video, and/or the like.
- Display of a volume of captured video or positional tracking video may enable a viewer to perceive a captured scene from any location and at any viewing angle within a viewing volume.
- a viewpoint can be reconstructed to provide the view of a scene from any location within the viewing volume.
- the user may enjoy an immersive virtual presence within an environment.
- Such a virtual reality experience may be enhanced by providing viewer motion with six degrees of freedom, stereoscopic perception at any interpupillary distance, full motion parallax, and/or correct view-dependent lighting.
- VR virtual reality
- AR augmented reality
- Virtual reality and augmented reality video may include depth cues such as stereopsis, binocular occlusions, vergence, motion parallax and view-dependent lighting, which may enhance the viewer's sense of immersion.
- Many existing lossy video compression techniques such as chroma subsampling, transform coding, and quantization reduce the bit-rate of the video stream by discarding imperceptible information.
- lossy video compression techniques such as chroma subsampling, transform coding, and quantization reduce the bit-rate of the video stream by discarding imperceptible information.
- such known techniques generally do not generally exploit additional aspects of the virtual reality or augmented reality video stream, and therefore do not provide compression ratios sufficient for use with virtual reality or augmented reality video with full motion parallax and view-dependent lighting.
- a video compression scheme may adaptively compress and/or remove such additional data.
- view-dependent lighting may be adaptively removed without degrading the level of immersion provided by the virtual reality or augmented reality experience.
- one or more image capture devices may capture a video stream of a scene.
- the video stream may be stored in a data store.
- Vantage data may be iteratively retrieved from the data store for compression.
- This vantage data may include base vantage data including base vantage color data depicting the scene from a base vantage location, and target vantage data including target vantage color data depicting the scene from a target vantage location.
- a processor may be used to reproject the base vantage data to the target vantage location to obtain reprojected target vantage data, compare the reprojected target vantage data with the target vantage data to obtain residual data, and compress the residual data. Compression of the residual data may include removal of a subset of the residual data that is likely to be less viewer-perceptible than a remainder of the residual data.
- the compressed video stream may then be stored, including the base vantage data and the compressed residual data.
- removing the subset of the residual data may include applying quantization to the residual data. Further, in some embodiments, removing the subset of the residual data may include applying entropy encoding to the residual data. Yet further, removing the subset of the residual data may include identifying an occluded region of the residual data, that is indicative of disocclusion between the base vantage location and the target vantage location, and selecting the subset from outside the occluded region.
- Removing the subset of the residual data may further include generating a mask that delineates the occluded region and a non-occluded region (i.e., an “outside region”) outside the occluded region.
- Removing the subset of the residual data may further include removing all of the residual data in the outside region, or removing only a portion of the residual data in the outside region.
- a Gaussian smoothing kernel may be applied to blend the occluded region with the outside region to generate a blended occluded region, and all of the residual data that lies outside the blended occluded region may be removed.
- the compressed video stream may be decoded by using the mask to apply only the occluded region of the residual data to the base vantage to reproject the base vantage data to the target vantage location.
- Removing the subset of the residual data may include removing reprojection errors from the residual data and/or removing view-dependent lighting from the residual data.
- all view-dependent lighting may be removed from the residual data.
- a subset of the view-dependent lighting that is likely to be user-imperceptible is identified and removed, without removing the remainder of the view-dependent lighting.
- the compressed video stream may be decoded by applying the remainder of the residual data to the base vantage data to reproject the base vantage data to the target vantage location. This may be iteratively done to decode the entire video stream, or at least the portion that is needed in the virtual reality or augmented reality experience.
- FIG. 1 is a diagram of a plenoptic light-field camera, according to one embodiment.
- FIG. 2 is a conceptual diagram of a light-field volume, according to one embodiment.
- FIG. 3 is a conceptual diagram of virtual viewpoint generation from a fully sampled light-field volume.
- FIG. 4 is a conceptual diagram comparing the sizes of a physical capture device, capturing all incoming rays within a limited field-of-view, and the virtual size of the fully sampled light-field volume, according to one embodiment.
- FIG. 5 is a conceptual diagram of a coordinate system for a light-field volume.
- FIG. 6 is a diagram of an array light-field camera, according to one embodiment.
- FIG. 7 is a diagram of a virtual reality capture system according to the prior art, developed by Jaunt.
- FIG. 8 is a diagram of a stereo virtual reality capture system according to the prior art.
- FIG. 9 is a block diagram depicting a capture system according to one embodiment.
- FIG. 10 is a diagram showing a tiled array in an ideal ring configuration of contiguous plenoptic light-field cameras, according to one embodiment.
- FIGS. 11A through 11C are diagrams showing various patterns for joining camera lenses to create a continuous surface on a volume of space, according to various embodiments.
- FIG. 12 is a diagram of a ring configuration with the addition of a top-facing light-field camera, according to one embodiment.
- FIG. 13 is a diagram showing different basic lens designs that can be used in different embodiments, and shows typical field-of-view (FOV) and Numerical Apertures for those designs.
- FOV field-of-view
- FIG. 14 is an exemplary schematic cross section diagram of a double Gauss lens design that can be used in one embodiment.
- FIG. 15 is a diagram showing ring configuration of plenoptic light-field cameras with circular lenses and non-contiguous entrance pupils, according to one embodiment.
- FIGS. 16A through 16C are diagrams depicting a sparsely populated light-field ring configuration that rotates, according to one embodiment.
- FIGS. 17A through 17C are diagrams depicting a fully populated set of lenses and sparsely populated sensors, according to one embodiment.
- FIGS. 18A through 18C are diagrams of a fully populated set of lenses and sparsely populated sensors, according to one embodiment.
- FIG. 19 is a diagram showing a ring configuration of contiguous array light-field cameras, according to one embodiment.
- FIGS. 20A and 20B are diagrams of a fully populated set of objective lens arrays and sparsely populated sensors for array light-field cameras, according to one embodiment.
- FIG. 21 is a diagram showing an array light-field camera using a tapered fiber optic bundle, according to one embodiment.
- FIG. 22 is a diagram showing array light-field cameras using tapered fiber optic bundles in a ring configuration, according to one embodiment.
- FIG. 23 is a diagram showing a tiled light-field camera array in a single layer ring configuration, according to one embodiment.
- FIG. 24 is a diagram showing a tiled light-field camera array in a dual layer ring configuration, according to one embodiment.
- FIGS. 25A through 25B are diagrams comparing a schematic view of a plenoptic light-field camera to a virtual camera array that is approximately optically equivalent.
- FIG. 26 is a diagram showing a possible set of two cylindrical calibration charts that may be used to calibrate a tiled light-field camera array, according to one embodiment.
- FIG. 27 is an image of an example of a virtual reality headset, the Oculus Rift (Development Kit version).
- FIG. 28 is a conceptual drawing showing a virtual camera system and field-of-view that may be used to generate virtual views, according to one embodiment.
- FIG. 29 is a conceptual drawing showing a coordinate system with a virtual camera system based on an ideal lens, according to one embodiment.
- FIG. 30 is a conceptual drawing showing a virtual camera system based on a more complete model of a virtual lens, according to one embodiment.
- FIG. 31 is a diagram showing example output from an optical ray tracer, according to one embodiment.
- FIGS. 32A through 32C are conceptual diagrams showing a rotating sparsely populated tiled array of array light-field cameras, according to one embodiment.
- FIG. 33 is an exemplary image showing a CMOS photosensor mounted in an electronics package.
- FIG. 34 is a diagram showing the relationship between the physical size and field-of-view on the capture surface to the size of a virtual fully sampled light-field volume, according to one embodiment.
- FIGS. 35A through 35D are perspective and side elevation views depicting a tiled array of conventional cameras, according to one embodiment.
- FIG. 36 is a diagram that depicts stitching that may be used to provide an extended vertical field-of-view.
- FIG. 37 is a perspective view depicting a tiled array according to another alternative embodiment.
- FIG. 38 depicts a tiling scheme representing some or all of the view encoded in the video data for a single vantage, including three layers, according to one embodiment.
- FIG. 39 depicts an encoder according to one embodiment.
- FIGS. 40 through 44 depict various vantage encoding schemes according to certain embodiments.
- FIGS. 45A and 45B depict encoding schemes with inter-vantage prediction, according to certain alternative embodiments.
- FIGS. 46A and 46B depict encoding schemes according to further alternative embodiments.
- FIG. 47 depicts a system for generating and compressing tiles, according to one embodiment.
- FIG. 48 depicts a system for tile decoding, compositing, and playback, according to one embodiment.
- FIG. 49 is a diagram depicting how a vantage view may be composed, according to one embodiment.
- FIG. 50 depicts the view of a checkerboard pattern from a known virtual reality headset.
- FIG. 51 depicts a method for capturing volumetric video data, encoding the volumetric video data, decoding to obtain viewpoint video data, and displaying the viewpoint video data for a viewer, according to one embodiment.
- FIG. 52 is a series of graphs depict a tile-based scheme, according to one embodiment.
- FIGS. 53A and 53B depict exemplary tiling schemes, according to certain embodiments.
- FIG. 54 depicts a hierarchical coding scheme, according to one embodiment.
- FIGS. 55A, 55B, 55C, and 55D are a series of views depicting the operation of the hierarchical coding scheme of FIG. 54 in two dimensions, according to one embodiment.
- FIGS. 56A, 56B, 56C, and 56D are a series of views depicting the operation of the hierarchical coding scheme of FIG. 54 in three dimensions, according to another embodiment.
- FIGS. 57A, 57B, 57C, and 57D are a series of graphs depicting the projection of depth layers onto planar image from a spherical viewing range from a vantage, according to one embodiment.
- FIG. 58 is a schematic diagram depicting reprojection error, according to one embodiment.
- FIG. 59 is an image depicting an original vantage, according to one embodiment.
- FIG. 60 is an image depicting an inter-vantage view generated by reprojection, according to one embodiment.
- FIG. 61 is an image depicting the image of FIG. 60 , after removal of view-dependent lighting and reprojection error, with only the occluded region shown, according to one embodiment.
- FIG. 62 is an image depicting the residual data from reprojection of a base vantage to a target vantage location, according to one embodiment.
- FIG. 63 is an image depicting the residual data of FIG. 62 , after removal of all view-dependent lighting and reprojection error, according to one embodiment.
- FIG. 64 is an image depicting the residual data of FIG. 62 , after removal of non-perceptual view-dependent lighting information, according to one embodiment.
- FIG. 65 is diagram depicting an inter-vantage based video compression system that provides complete view-dependent lighting removal, according to one embodiment.
- FIG. 66 is a diagram of an inter-vantage based video compression system that provides perceptually-optimized view-dependent lighting removal, according to another embodiment.
- FIG. 67 is a flow diagram depicting a method for compressing a video stream, which may be volumetric video data to be used for a virtual reality or augmented reality experience, according to one embodiment.
- FIG. 68 is a flow diagram depicting performance of the step of compressing the residual data of the method of FIG. 67 , according to one embodiment.
- the described embodiments may provide for capturing continuous or nearly continuous light-field data from many or all directions facing away from the capture system, which may enable the generation of virtual views that are more accurate and/or allow viewers greater viewing freedom.
- a data acquisition device can be any device or system for acquiring, recording, measuring, estimating, determining and/or computing data representative of a scene, including but not limited to two-dimensional image data, three-dimensional image data, and/or light-field data.
- a data acquisition device may include optics, sensors, and image processing electronics for acquiring data representative of a scene, using techniques that are well known in the art.
- One skilled in the art will recognize that many types of data acquisition devices can be used in connection with the present disclosure, and that the disclosure is not limited to cameras.
- any use of such term herein should be considered to refer to any suitable device for acquiring image data.
- Virtual reality is intended to be a fully immersive experience for users, often having the goal of creating an experience that is as close as possible to “being there.”
- Users typically use headsets with immersive, wide-angle stereo viewing, multidirectional sound, and onboard sensors that can measure orientation, accelerations, and/or position.
- FIG. 27 shows an image of the Oculus Rift Development Kit headset as an example of a virtual reality headset 2700 .
- Viewers using virtual reality and/or augmented reality headsets may move their heads to point in any direction, move forward and backward, and may move their heads side to side. The point of view from which the user views his or her surroundings may change to match the motion of his or her head.
- FIG. 27 depicts some exemplary components of the virtual reality headset 2700 .
- the virtual reality headset 2700 may have a processor 2710 , memory 2720 , a data store 2730 , user input 2740 , and a display screen 2750 .
- Each of these components may be any device known in the computing and virtual reality arts for processing data, storing data for short-term or long-term use, receiving user input, and displaying a view, respectively.
- the user input 2740 may include one or more sensors that detect the position and/or orientation of the virtual reality headset 2700 . By maneuvering his or her head, a user (i.e., a “viewer”) may select the viewpoint and/or view direction from which he or she is to view an environment.
- a user i.e., a “viewer”
- the virtual reality headset 2700 may also have additional components not shown in FIG. 27 . Further, the virtual reality headset 2700 may be designed for standalone operation or operation in conjunction with a server that supplies video data, audio data, and/or other data to the virtual reality headset. Thus, the virtual reality headset 2700 may operate as a client computing device. As another alternative, any of the components shown in FIG. 27 may be distributed between the virtual reality headset 2700 and a nearby computing device such that the virtual reality headset 2700 and the nearby computing device, in combination, define a client computing device.
- Virtual reality content may be roughly divided into two segments: synthetic content and real world content.
- Synthetic content may include applications like video games or computer-animated movies that are generated by the computer.
- Real world content may include panoramic imagery and/or live action video that is captured from real places or events.
- Synthetic content may contain and/or be generated from a 3-dimensional model of the environment, which may be also used to provide views that are matched to the actions of the viewer. This may include changing the views to account for head orientation and/or position, and may even include adjusting for differing distances between the eyes.
- FIGS. 7 and 8 show exemplary capture systems 700 and 800 , respectively.
- FIG. 7 depicts a virtual reality capture system, or capture system 700 , according to the prior art, developed by Jaunt.
- the capture system 700 consists of a number of traditional video capture cameras 710 arranged spherically.
- the traditional video capture cameras 710 are arranged facing outward from the surface of the sphere.
- FIG. 8 depicts a stereo virtual reality capture system, or capture system 800 , according to the prior art.
- the capture system 800 consists of 8 stereo camera pairs 810 , plus one vertically facing camera 820 .
- Image and/or video data is captured from the camera pairs 810 , which are arranged facing outward from a ring.
- the image and/or video data captured is limited to the set of viewpoints in the camera arrays.
- One method of generating intermediate viewpoints is to generate two 360° spherically mapped environments—one for each eye. As the viewer turns his or her head, each eye sees a window into these environments. Image and/or video data from the cameras in the array are stitched onto the spherical surfaces.
- this approach is geometrically flawed, as the center of perspective for each eye changes as the user moves his or her head, and the spherical mapping assumes a single point of view. As a result, stitching artifacts and/or geometric distortions cannot be fully avoided.
- the approach can only reasonably accommodate viewers changing their viewing direction, and does not perform well when the user moves his or her head laterally, forward, or backward.
- Another method to generate intermediate viewpoints is to attempt to generate a 3D model from the captured data, and interpolate between viewpoints based at least partially on the generated model.
- This model may be used to allow for greater freedom of movement, but is fundamentally limited by the quality of the generated three-dimensional model. Certain optical aspects, like specular reflections, partially transparent surfaces, very thin features, and occluded imagery are extremely difficult to correctly model. Further, the visual success of this type of approach is highly dependent on the amount of interpolation that is required. If the distances are very small, this type of interpolation may work acceptably well for some content. As the magnitude of the interpolation grows (for example, as the physical distance between cameras increases), any errors will become more visually obvious.
- Another method of generating intermediate viewpoints involves including manual correction and/or artistry in the postproduction workflow. While manual processes may be used to create or correct many types of issues, they are time intensive and costly.
- a capture system that is able to capture a continuous or nearly continuous set of viewpoints may remove or greatly reduce the interpolation required to generate arbitrary viewpoints. Thus, the viewer may have greater freedom of motion within a volume of space.
- the present document describes several arrangements and architectures that allow for capturing light-field volume data from continuous or nearly continuous viewpoints.
- the viewpoints may be arranged to cover a surface or a volume using tiled arrays of light-field cameras.
- Such systems may be referred to as “capture systems” in this document.
- a tiled array of light-field cameras may be joined and arranged in order to create a continuous or nearly continuous light-field capture surface. This continuous capture surface may capture a light-field volume.
- the tiled array may be used to create a capture surface of any suitable shape and size.
- FIG. 2 shows a conceptual diagram of a light-field volume 200 , according to one embodiment.
- the light-field volume 200 may be considered to be a spherical volume. Rays of light 210 originating outside of the light-field volume 200 and then intersecting with the light-field volume 200 may have their color, intensity, intersection location, and direction vector recorded. In a fully sampled light-field volume, all rays and/or “ray bundles” that originate outside the light-field volume are captured and recorded. In a partially sampled light-field volume or a sparsely sampled light-field volume, a subset of the intersecting rays is recorded.
- FIG. 3 shows a conceptual diagram of virtual viewpoints, or subviews 300 , that may be generated from captured light-field volume data, such as that of the light-field volume 200 of FIG. 2 .
- the light-field volume may be a fully sampled light-field volume; hence, all rays of light entering the light-field volume 200 may have been captured.
- any virtual viewpoint within the light-field volume 200 facing any direction, may be generated.
- two subviews 300 are generated based on two viewpoints. These subviews 300 may be presented to a viewer of a VR system that shows the subject matter captured in the light-field volume 200 . One subview 300 may be generated for each of the viewer's eyes. The ability to accurately generate subviews may be limited by the sampling patterns, acceptance angles, and surface coverage of the capture system.
- the capture system 900 may contain a set of light-field cameras 910 that form a continuous or nearly continuous capture surface 920 .
- the light-field cameras 910 may cooperate to fully or partially capture a light-field volume, such as the light-field volume 200 of FIG. 2 .
- control and readout circuitry 930 For each of the light-field cameras 910 , there is attached control and readout circuitry 930 .
- This control and readout circuitry 930 may control the operation of the attached light-field camera 910 , and can read captured image and/or video data from the light-field camera 910 .
- the capture system 900 may also have a user interface 940 for controlling the entire array.
- the user interface 940 may be physically attached to the remainder of the capture system 900 and/or may be remotely connected to the remainder of the capture system 900 .
- the user interface 940 may include a graphical user interface, displays, digital controls, analog controls, and/or any other controls or feedback devices by which a user can provide input to control the operation of the capture system 900 .
- the capture system 900 may also have a primary controller 950 that communicates with and controls all the light-field cameras 910 .
- the primary controller 950 may act to synchronize the light-field cameras 910 and/or control the individual light-field cameras 910 in a systematic manner.
- the capture system 900 may also include data storage 960 , which may include onboard and/or remote components for recording the captured video and/or image data generated by the light-field cameras 910 .
- the data storage 960 may be physically part of the capture system 900 (for example, in hard drives, flash memory and/or RAM), removable storage (for example, arrays of SD cards and/or other removable flash storage), and/or remotely connected storage (for example, RAID storage connected wirelessly or via a wired connection).
- the capture system 900 may also include data processing circuitry 970 , which may process the image and/or video data as part of the capture system 900 .
- the data processing circuitry 970 may include any type of processing circuitry, including but not limited to one or more microprocessors, ASICs, FPGA's, and/or the like.
- the capture system 900 may simply collect and store raw data, which may be processed by a separate device such as a computing device with microprocessors and/or other data processing circuitry.
- the tiled light-field cameras 910 form an outward-facing ring.
- a tiled light-field camera array 2300 is shown in FIG. 23 .
- the tiled light-field cameras 2310 form a complete 360° ring in a single layer.
- Light-field cameras 2310 that neighbor each other may have overlapping fields-of-view, as shown in the top view on the left.
- Each of the light-field cameras 2310 may have a lens surface 2320 that is the outward-facing surface of a main lens of the light-field camera 2310 .
- the lens surfaces 2320 may be arranged in a ring pattern.
- FIG. 24 Another arrangement of a tiled light-field camera array 2400 , with 2 layers, is shown in FIG. 24 .
- light-field cameras 2410 with lens surfaces 2420 may be arranged in a top layer 2430 that captures a 360° field-of-view that faces partially “up,” and in a bottom layer 2440 may capture a 360° field-of-view that faces partially “down.”
- Light-field cameras 2410 that are adjacent to each other within the top layer 2430 or within the bottom layer 2440 may have overlapping fields-of-view, as shown in the top view on the left.
- light-field cameras 2410 of the top layer 2430 may have fields-of-view that overlap those of their adjacent counterparts in the bottom layer 2440 , as shown in the side view on the right.
- each layer may beneficially possess more or fewer light-field cameras 2310 or light-field cameras 2410 , depending on the field-of-view applicable to each light-field camera.
- many other camera arrangements may be used, which may include additional numbers of layers.
- a sufficient number of layers may be used to constitute or approach a spherical arrangement of light-field cameras.
- the tiled light-field cameras are arranged on the outward facing surface of a sphere or other volume.
- FIG. 11 shows possible configurations for the tiled array. Specifically, FIG. 11A shows a tiling pattern 1100 of light-field cameras that creates a cubic volume.
- FIG. 11B shows a tiling pattern 1120 wherein quadrilateral regions may be warped in order to approximate the surface of a sphere.
- FIG. 11C shows a tiling pattern 1140 based on a geodesic dome. In the tiling pattern 1140 , the tile shape may alternate between pentagons and hexagons. These tiling patterns are outlined in the darker color. In all of the patterns shown, the number of tiles shown is exemplary, and the system may use any number of tiles. In addition, many other volumes and tiling patterns may be constructed.
- the tiles displayed in the tiling pattern 1100 , the tiling pattern 1120 , and the tiling pattern 1140 represent the maximum extent of the light-field capturing surface for a single light-field camera in the tiled array.
- the physical capture surface may closely match the tile size. In other embodiments, the physical capture surface may be substantially smaller than the tile size.
- human natural viewing parameters For many virtual reality and/or augmented reality viewing experiences, “human natural” viewing parameters are desired.
- “human natural” viewing parameters refer specifically to providing approximately human fields-of-view and inter-ocular distances (spacing between the eyes). Further, it is desirable that accurate image and/or video data can be generated for any viewpoint as the viewer moves his or her head.
- the physical size of the capture surface of the tiled array may be determined by the output requirements and fields-of-view of the objective lenses in the capture system.
- FIG. 4 conceptually shows the relationship between a physical capture surface, or capture surface 400 , with an acceptance or capture surface field-of-view 410 and a virtual fully sampled light-field volume 420 .
- a fully sampled light-field volume is a volume where all incoming rays from all directions have been captured. Within this volume (for example, the sampled light-field volume 420 ), any virtual viewpoint may be generated, looking any direction, with any field-of-view.
- the tiled array is of sufficient size and captures a sufficient field-of-view to enable generation of viewpoints that allow VR viewers to freely move their heads within a normal range of neck motion.
- This motion may include tilting, rotating, and/or translational motion of the head.
- the desired radius of such a volume may be 100 mm.
- the field-of-view of the capture surface may be determined by other desired optical properties of the capture system (discussed later).
- the capture surface may be tiled with lenses arranged in a double Gauss or other known lens arrangement. Each lens may have an approximately 20° field-of-view half angle.
- the physical capture surface, or capture surface 400 may be designed to be at least 300 mm in radius in order to accommodate the system design parameters.
- the capture system is of sufficient size to allow users a nearly full range of motion while maintaining a sitting position.
- the desired radius of the fully sampled light-field volume 420 may be 500 mm. If the selected lens has a 45° field-of-view half angle, the capture surface 400 may be designed to be at least 700 mm in radius.
- the tiled array of light-field cameras is of sufficient size and captures sufficient field-of-view to allow viewers to look in any direction, without any consideration for translational motion.
- the diameter of the fully sampled light-field volume 420 may be just large enough to generate virtual views with separations large enough to accommodate normal human viewing.
- the diameter of the fully sampled light-field volume 420 is 60 mm, providing a radius of 30 mm. In that case, using the lenses listed in the example above, the radius of the capture surface 400 may be at least 90 mm.
- a different limited set of freedoms may be provided to VR viewers.
- rotation and tilt with stereo viewing may be supported, but not translational motion.
- the radius of the capture surface 400 is between 75 mm and 150 mm, and the field-of-view on the surface is between 90° and 120°.
- This embodiment may be implemented using a tiled array of light-field cameras in which each objective lens in the objective lens array is a wide-angle lens.
- the light-field cameras in the tiled array are plenoptic light-field cameras.
- a plenoptic light-field camera 100 may capture a light-field using an objective lens 110 , plenoptic microlens array 120 , and photosensor 130 .
- the objective lens 110 may be positioned to receive light through an aperture (not shown).
- Each microlens in the plenoptic microlens array 120 may create an image of the aperture on the surface of the photosensor 130 .
- the plenoptic light-field camera 100 may facilitate the generation of viewpoints within a sampled light-field volume that are not aligned with any of the camera lenses of the capture system. This will be explained in greater detail below.
- FIGS. 25A and 25B show the relationship between a plenoptic light-field camera such as the plenoptic light-field camera 100 of FIG. 1 , and a virtual camera array 2500 that are approximately optically equivalent.
- the objective lens 110 captures light from within an angular field-of-view 2510 .
- the objective lens 110 has an entrance pupil, the optical image of the aperture stop seen through the front of the objective lens 110 .
- the light captured by the objective lens 110 passes through the plenoptic microlens array 120 , where each microlens 2520 in the array creates an N ⁇ N pixel “disk image” on the surface of the photosensor 130 .
- the disk image is an image of the aperture as seen by the microlens 2520 through which the disk image was received.
- the plenoptic light-field camera 100 is approximately optically equivalent to a virtual camera array of N ⁇ N cameras 2530 with the same angular field-of-view 2510 , with the vertex of each camera 2530 located on the surface of the entrance pupil.
- the size of each entrance pupil in the virtual camera array 2500 is approximately 1/Nth the size (in one dimension) of the entrance pupil of the objective lens 110 .
- the term approximately is used in the description above, as optical aberrations and other systemic variations may result in deviations from the ideal virtual system described.
- FIG. 10 shows a tiled array 1000 in a ring configuration where the entrance pupils 1010 from the objective lenses 1020 create a gap-free surface on the tiled array 1000 .
- the entrance pupil 1010 may be large relative to the physical size of each light-field camera 1030 in the tiled array 1000 , as shown in FIG. 10 . Further, in order to provide large viewing angles in as large a volume as possible, it may be beneficial to start with a lens that has a relatively wide field-of-view. Thus, a good lens design choice may include a relatively wide field-of-view paired with a relatively large aperture (as aperture size and entrance pupil size are very closely related).
- FIG. 13 is a diagram 1300 depicting typical fields-of-view and aperture ranges for different types of lens designs.
- a double Gauss lens design 1310 with a low F-number is used for the objective lens.
- different lens types may be used, including any of those illustrated on the diagram 1300 .
- FIG. 14 shows a cross section view of a double Gauss lens design 1400 with a large aperture.
- Double Gauss lenses have a desirable combination of field-of-view and a potentially large entrance pupil.
- 50 mm lenses for 35 mm cameras
- These lenses may use an aperture stop that is greater than or equal to 50 mm on a sensor that is approximately 35 mm wide.
- a tiled array may have plenoptic light-field cameras in which the entrance pupil and aperture stop are rectangular and the entrance pupils of the objective lenses create a continuous or nearly continuous surface on the capture system.
- the aperture stop may be shaped to allow for gap-free tessellation.
- the entrance pupil 1010 may have a square or rectangular shape.
- one or more lens elements may be cut (for example, squared) to allow for close bonding and to match the shape of the aperture stop.
- the layout and packing of the microlens array such as the plenoptic microlens array 120 of FIG. 1 , may be optimized for the shape of the entrance pupil 1010 .
- the plenoptic microlens array 120 may have a square or rectangular shape and packing to match a square or rectangular shape of the entrance pupil 1010 .
- a lens with a relatively wide field-of-view and relatively large entrance pupil is selected as the objective lens, and the lenses are spaced as closely as possible while maintaining the traditional round shape.
- a double Gauss type lens with a large aperture may be a good choice for the objective lens.
- FIG. 15 A tiled array 1500 in a ring configuration using round lenses is shown in FIG. 15 .
- the objective lenses 1520 may be circular, along with the entrance pupils 1510 of the light-field cameras 1530 .
- the entrance pupils 1510 may not be continuous to each other, as shown in the side view on the right-hand side.
- these types of objective lenses may be used in any tiling pattern.
- the light-field cameras are arranged into a geodesic dome using two different lens diameters and the tiling pattern 1140 shown in FIG. 11C . Such an arrangement may help to minimize the spacing between the entrance pupils 1510 in order to enhance the continuity of the light-field data captured.
- one or more top and/or bottom facing cameras may be used in addition to a tiled array in a ring configuration.
- FIG. 12 conceptually depicts a tiled array 1200 with light-field cameras 1210 arranged in a ring-shaped pattern, with a single light-field camera 1220 facing up.
- Another light-field camera 1220 (not shown) may be positioned on the opposite side of the tiled array 1200 and may be oriented in a direction opposite to that of the light-field camera 1220 .
- the upward and/or downward facing light-field camera(s) 1220 may be standard two-dimensional camera(s), light-field camera(s) or a combination thereof.
- Embodiments of this type may capture highly incomplete light-field volume data directly above and below the tiled array 1200 , but may offer significant savings in total system cost and/or complexity.
- the views directly above and below the tiled array 1200 may be considered less important than other directions. For example, a viewer may not require as much detail and/or accuracy when looking up or down as when viewing images at his or her elevation.
- the surface of a capture system may be made to change its rotational position and capture different sets of viewpoints at different times. By changing the rotational position between frames, each successive frame may be used to capture portions of the light-field volume that may not have been captured in the previous frame.
- a sensor array 1600 may be a sparsely populated ring of plenoptic light-field cameras 1610 . Each successive frame may capture a different set of angles than the previous frame.
- a portion of the light-field volume is captured.
- the sensor array 1600 is then rotated to the position shown at time B by rotating the ring, and another portion of the light-field volume is captured.
- the sensor array 1600 is rotated again, by once again rotating the ring, with another capture at time C.
- This embodiment may allow for finer sampling of the light-field volume, more complete sampling of the light-field volume, and/or sampling with less physical hardware.
- the embodiments with changing rotational position are displayed in a ring configuration.
- Rotation may be carried out about one axis, as in FIGS. 16A through 16C , or multiple axes, if desired.
- a spherically tiled configuration may, for example, be rotated about all three orthogonal axes.
- the camera array rotates in the same direction between each capture, as in FIGS. 16A through 16C .
- the camera array oscillates between two or more capture positions and may change the direction of rotation between captures.
- the overall frame rate of the system may be very high so that every rotational position is captured at a sufficient frame rate.
- the overall frame capture rate including time for positions changes, may be greater than or equal to 180 frames per second. This may enable samples to be taken at each rotational position in synchronization with the desired frame rate.
- the entire sensor array 1600 may be attached to a rotary joint, which allows the tiled array to rotate independently of the rest of the system and surroundings.
- the electrical connections may go through a slip ring, or rotary electrical interface, to connect rotating components in the system to non-rotating components.
- the rotation and/or oscillation may be driven by a motor 1620 , which may be a stepper motor, DC motor, or any other suitable motor system.
- the light-field sensors within the capture system may be rotated to capture different sets of viewpoints at different times, while the objective lenses may stay in a fixed position.
- each successive frame may be used to capture portions of the light-field volume that were not captured in the previous frame.
- a sensor array 1700 may include a ring with a full set of objective lenses 1710 with a sparse set of light-field sensors 1720 . At each time of capture, the sensor array 1700 may capture images from a subset of the objective lenses 1710 .
- the objective lenses 1710 may maintain a fixed position while the array of light-field sensors 1720 may rotate.
- a portion of the light-field volume is captured that corresponds to the objective lenses 1710 that are actively used at that time (i.e., the objective lenses 1710 that are in alignment with one of the light-field sensors 1720 ).
- the light-field sensors 1720 are then rotated to the position shown at time B, and another portion of the light-field volume is captured, this time corresponding with the different set of objective lenses 1710 that are in alignment with the light-field sensors 1720 .
- the light-field sensors 1720 are rotated again, with another capture at time C.
- This embodiment may allow for finer sampling of the light-field volume, more complete sampling of the light-field volume, and/or sampling with less physical hardware.
- the embodiments with changing rotational position are displayed in a ring configuration.
- Rotation may be carried out about one axis, as in FIGS. 17A through 17C , or multiple axes, if desired.
- a spherically tiled configuration may, for example, be rotated about all three orthogonal axes.
- the light-field sensor array rotates in the same direction between each capture, as in FIGS. 17A through 17C .
- the light-field sensor array may oscillate between two or more capture positions and may change the direction of rotation between captures, as in FIGS. 18A through 18C .
- FIGS. 18A through 18C depict a sensor array 1800 that may include a ring with a full set of objective lenses 1810 with a sparse set of light-field sensors 1820 , as in FIGS. 17A through 17C .
- the objective lenses 1810 may maintain a fixed position while the array of light-field sensors 1820 rotates.
- the array of light-field sensors 1820 may rotate clockwise from FIG. 18A to FIG. 18B , and then counterclockwise from FIG. 18B to FIG. 18C , returning in FIG. 18C to the relative orientation of FIG. 18A .
- the array of light-field sensors 1820 may thus oscillate between two or more relative positions.
- the array of light-field sensors 1720 and/or the array of light-field sensors 1820 may be attached to a rotary joint, which allows the array of light-field sensors 1720 or the array of tiled light-field sensors 1820 to rotate independently of the rest of the capture system and surroundings.
- the electrical connections may go through a slip ring, or rotary electrical interface, to connect rotating components in the system to non-rotating components.
- the rotation and/or oscillation may be driven by a stepper motor, DC motor, or any other suitable motor system.
- the light-field cameras in the tiled array are array light-field cameras.
- FIG. 6 One example is shown in FIG. 6 .
- FIG. 6 shows the basic configuration of an array light-field camera 600 according to one embodiment.
- the array light-field camera 600 may include a photosensor 610 and an array of M ⁇ N objective lenses 620 .
- Each objective lens 620 in the array may focus light onto the surface of the photosensor 610 and may have an angular field-of-view approximately equivalent to the other objective lenses 620 in the array of objective lenses 620 .
- the fields-of-view of the objective lenses 620 may overlap as shown.
- the objective lenses 620 may cooperate to capture M ⁇ N virtual viewpoints, with each virtual viewpoint corresponding to one of the objective lenses 620 in the array. Each viewpoint may be captured as a separate image. As each objective lens 620 is located at a slightly different position than the other objective lenses 620 in the array, each objective lens 620 may capture approximately the same image, but from a different point of view from those of the other objective lenses 620 . Many variations of the basic design are possible, and any variation may be applied to the embodiments described below.
- FIG. 19 conceptually shows how array light-field cameras 600 as in FIG. 6 may be tiled to form a nearly continuous capture surface 1900 .
- a ring tiling pattern is displayed in the FIG. 19 , any tiling scheme may be used, including but not limited to those of FIGS. 11A, 11B, and 11C .
- each subview may capture image and/or video data using a lens with a field-of-view greater than or equal to 90° and may have a resolution greater than or equal to 1920 ⁇ 1080.
- FIGS. 16A through 16C, 17A through 17C , and/or 18 A through 18 C may be applied to array light-field cameras like the array light-field camera 600 of FIG. 6 . This will be described in greater detail in connection with FIGS. 32A through 32C and FIGS. 10A through 10C .
- the surface of a capture system having array light-field cameras may be made to change its rotational position and capture different sets of viewpoints at different times. By changing the rotational position between frames, each successive frame may be used to capture portions of the light-field volume that may not have been captured in the previous frame, as in FIGS. 16A through 16C .
- a sensor array 3200 may be a sparsely populated ring of array light-field cameras 3210 . Each successive frame may capture a different set of angles than the previous frame.
- a portion of the light-field volume is captured.
- the sensor array 3200 is then rotated to the position shown at time B by rotating the ring, and another portion of the light-field volume is captured.
- the sensor array 3200 is rotated again, by once again rotating the ring, with another capture at time C.
- This embodiment may allow for finer sampling of the light-field volume, more complete sampling of the light-field volume, and/or sampling with less physical hardware. Further, the benefits of the use array light-field cameras may be obtained. For clarity, the embodiments with changing rotational position are displayed in a ring configuration. However, it should be recognized that the principle may be applied to any tiled configuration. Rotation may be carried out about one axis, as in FIGS. 32A through 32C , or multiple axes, if desired. A spherically tiled configuration may, for example, be rotated about all three orthogonal axes.
- the array light-field camera array rotates in the same direction between each capture, as in FIGS. 32A through 32C .
- the array light-field camera array oscillates between two or more capture positions and may change the direction of rotation between captures.
- the overall frame rate of the system may be very high so that every rotational position is captured at a sufficient frame rate.
- the overall frame capture rate including time for positions changes, may be greater than or equal to 180 frames per second. This may enable samples to be taken at each rotational position in synchronization with the desired frame rate.
- the entire sensor array 3200 may be attached to a rotary joint, which allows the tiled array to rotate independently of the rest of the system and surroundings.
- the electrical connections may go through a slip ring, or rotary electrical interface, to connect rotating components in the system to non-rotating components.
- the rotation and/or oscillation may be driven by a stepper motor, DC motor, or any other suitable motor system.
- the light-field sensors of array light-field cameras within the capture system may be rotated to capture different sets of viewpoints at different times, while the arrays of objective lenses may stay in a fixed position.
- each successive frame may be used to capture portions of the light-field volume that were not captured in the previous frame.
- a sensor array 2000 may include a ring with a full set of arrays of objective lenses 2010 with a sparse set of light-field sensors 2020 . At each time of capture, the sensor array 2000 may capture images from a subset of the arrays of objective lenses 2010 .
- the arrays of objective lenses 2010 may maintain a fixed position while the array of light-field sensors 2020 may rotate.
- a portion of the light-field volume is captured that corresponds to the arrays of objective lenses 2010 that are actively used at that time (i.e., the arrays of objective lenses 2010 that are in alignment with one of the light-field sensors 2020 ).
- the light-field sensors 2020 are then rotated to the position shown at time B, and another portion of the light-field volume is captured, this time corresponding with the different set of arrays of objective lenses 2010 that are in alignment with the light-field sensors 2020 .
- the light-field sensors 2020 are rotated again to once again reach the position shown at Time A, and capture may continue to oscillate between the configuration at Time A and that at time B. This may be accomplished via continuous, unidirectional rotation (as in FIGS. 17A through 17C ) or via oscillating motion in which rotation reverses direction between captures, as in FIGS. 18A through 18C .
- This embodiment may allow for finer sampling of the light-field volume, more complete sampling of the light-field volume, and/or sampling with less physical hardware. Further, the benefits of the use array light-field cameras may be obtained. For clarity, the embodiments with changing rotational position are displayed in a ring configuration. However, it should be recognized that the principle may be applied to any tiled configuration. Rotation may be carried out about one axis, as in FIGS. 20A and 20B , or multiple axes, if desired. A spherically tiled configuration may, for example, be rotated about all three orthogonal axes.
- the array of light-field sensors 2020 may be attached to a rotary joint, which allows the array of light-field sensors 2020 to rotate independently of the rest of the capture system and surroundings.
- the electrical connections may go through a slip ring, or rotary electrical interface, to connect rotating components in the system to non-rotating components.
- the rotation and/or oscillation may be driven by a stepper motor, DC motor, or any other suitable motor system.
- FIG. 33 shows an exemplary CMOS photosensor 3300 in a ceramic package 3310 .
- the active area 3320 on the photosensor 3300 there may be space required for inactive die surface, wire bonding, sensor housing, electronic and readout circuitry, and/or additional components. All space that is not active area is part of the package 3310 will not record photons. As a result, when there are gaps in the tiling, there may be missing information in the captured light-field volume.
- tapered fiber optic bundles may be used to magnify the active surface of a photosensor such as the photosensor 3300 of FIG. 33 .
- This concept is described in detail in U.S. Provisional Application Ser. No. 62/148,055 for “Light Guided Image Plane Tiled Arrays with Dense Fiber Optic Bundles for Light-Field and High Resolution Image Acquisition”, filed Apr. 15, 2015, the disclosure of which is incorporated herein by reference in its entirety.
- FIG. 21 A schematic illustration is shown in FIG. 21 , illustrating an array light-field camera 2100 .
- the objective lens array 2120 focuses light on the large end 2140 of a tapered fiber optic bundle 2130 .
- the tapered fiber optic bundle 2130 transmits the images to the photosensor 2110 and decreases the size of the images at the same time, as the images move from the large end 2140 of the tapered fiber optic bundle 2130 to the small end 2150 of the tapered fiber optic bundle 2130 .
- By increasing the effective active surface area of the photosensor 2110 gaps in coverage between array light-field cameras 2100 in a tiled array of the array light-field cameras 2100 may be reduced. Practically, tapered fiber optic bundles with magnification ratios of approximately 3:1 may be easily acquired.
- FIG. 22 conceptually shows how array light-field cameras using fiber optic tapers, such as the array light-field camera 2100 of FIG. 21 , may be tiled to form a tiled array 2200 in a ring configuration.
- Usage of the tapered fiber optic bundles 2130 may increase the amount of available space between the photosensors 2110 , allowing room that may be required for other purposes.
- Array light-field cameras using tapered fiber optic bundles may be used to create capture surfaces that may otherwise be extremely impractical.
- Photosensors are generally rectangular, and customization to specific shapes and/or sizes can be extremely time and cost-intensive.
- tiling options using rectangles can be limited, especially when a goal is to minimize gaps in coverage.
- the large ends of the tapered fiber optic bundles used in the tiled array are cut into a mix of precisely sized and shaped hexagons and pentagons. These tapered fiber optic bundles may then be attached to photosensors and tiled into a geodesic dome as shown in FIG. 11C . Objective lenses may be packed onto the geodesic surface as efficiently as possible.
- each photosensor may capture image and/or video data in regions directly connected to fiber optic bundles that reach the surface of the dome (for example, resulting in pentagonal and hexagonal active areas on the photosensors).
- fiber optic bundles that reach the surface of the dome (for example, resulting in pentagonal and hexagonal active areas on the photosensors).
- the resolution and maximum depth-of-field of virtual views generated from light-field volume data may be limited to the resolution and depth-of-field of the captured subviews.
- subviews in the light-field camera systems described herein have a large depth-of-field.
- the depth-of-field and of the subview is at least partially determined by the focus of the lens system and the size of the aperture.
- the resolution of each subview is limited by the resolution of the photosensor pixels used when capturing that subview as well as the achievable resolution given the optics of the system. It may be desirable to maximize both the depth-of-field and the resolution of the subviews.
- the resolution and depth-of-field of the subviews may need to be balanced against the limitations of the sensor, the limitations of the available optics, the desirability of maximizing the continuity of the capture surface, and/or the desired number of physical subviews.
- the focus of the objective lenses in the capture system may be set to the hyperfocal position of the subviews given the optical system and sensor resolution. This may allow for the creation of virtual views that have sharp focus from a near distance to optical infinity.
- the aperture of each objective lens in the objective lens array may be reduced to increase the depth-of-field of the subviews.
- the aperture size may be set so that a desired close focus distance is achievable when the objective lenses have focus set to their respective hyperfocal distances.
- images for different virtual viewpoints may be generated.
- two images may be generated: one for each eye.
- the images may be generated from viewpoints that are displaced from each other by the ordinary displacement that exists between two human eyes. This may enable the images to present the viewer with the impression of depth.
- Image generation may be continuous, and may occur at any frame rate, such as, for example, 24 frames per second (FPS), 30 FPS, or 60 FPS, so that the images, in sequence, define a video feed for each eye.
- the video feed may be generated in real time as the viewer moves his or her head.
- Accelerometers, position sensors, and/or other sensors may be used to detect the motion and/or position of the viewer's head; the resulting position data may be used to move the viewpoints used to generate the images in general synchronization with the viewer's movements to present the impression of immersion in the captured environment.
- all pixels in all the light-field cameras in the tiled array may be mapped to light-field volume coordinates. This mapping may facilitate the generation of images for different viewpoints within the light-field volume.
- Light-field volume coordinates are shown conceptually in FIG. 5 .
- Light-field volume coordinates are an extended version of standard light-field coordinates that may be used for panoramic and/or omnidirectional viewing, and may be expressed in terms of rho1, theta1, rho2, theta2. These variables may define a coordinate system 500 that is based on the polar coordinates of the intersection of a ray with the surface of two concentric spheres.
- the inner sphere 510 may have a radius r1 that is large enough to intersect with all rays of interest. Any virtual sphere that fully contains the physical capture system may be sufficient.
- the outer sphere 520 may be larger than the inner sphere 510 .
- outer sphere 520 may be of any size larger that the inner sphere 510 , it may be conceptually simplest to make the outer sphere 520 extremely large (r2 approaches infinity) so that rho2 and theta2 may often simply be treated as directional information directly.
- This coordinate system 500 may be relative to the entire tiled light-field capture system.
- a ray 530 intersects the inner sphere 510 at (rho1, theta1) and the outer sphere 520 at (rho2, theta2). This ray 530 is considered to have the 4D coordinate (rho1, theta1, rho2, theta2).
- any coordinate system may be used as long as the location and direction of all rays of interest can be assigned valid coordinates.
- the coordinate system 500 of FIG. 5 represents only one of many coordinate systems that may be used to describe the rays of light in a light-field volume in a manner that is global to the light-field camera array.
- any other known coordinate system may be used, including but not limited to Cartesian and cylindrical coordinate systems.
- the coordinate system 500 for a light-field volume may be considered to exist in a 3-dimensional Cartesian space, and the origin of the coordinate system 500 may be located at the center of the inner sphere 510 and the outer sphere 520 .
- Coordinates may be converted from light-field volume coordinates to Cartesian coordinates by additionally taking into account the radii of the inner sphere 510 and the outer sphere 520 .
- many rays that may be defined in Cartesian coordinates may not be able to be represented in the coordinate system 500 , including all rays that do not intersect the inner sphere 510 .
- a mapping from a pixel position, indexed in a 2D array by x and y, on a camera, camera, to a light-field volume coordinate in the coordinate system 500 is a mapping function: f (camera, x,y ) ⁇ (rho1,theta1,rho2,theta2)
- each pixel, microlens, and subaperture may have a physical size; as a result, each pixel may integrate light not from a single ray, but rather a “ray bundle” consisting of a narrow volume of rays.
- ray bundle consisting of a narrow volume of rays.
- the mapping function may be determined by the design of the capture system. Using a ray tracer or other optical software, a mapping from pixel coordinates to camera-centric world coordinates may be created. In one embodiment, the ray tracer traces a single, representative ray, from the center of each pixel, through the optical system, and out into the world. That representative ray may be parameterized by its intersection with the entrance pupil and direction of travel.
- many rays may be traced for each pixel, intersecting with the pixel in many locations and from many directions.
- the rays that are successfully traced from the pixel and out through the objective lens may be aggregated in some manner (for example, by averaging or fitting a ray using least squares error regression), and a representative ray may be generated.
- the camera-centric world coordinates may then be transformed based on the camera's location within the tiled array, into world coordinates that are consistent to all cameras in the array.
- each transformed ray in the consistent world coordinate space may be traced and intersections calculated for the inner and outer spheres that define the light-field volume coordinates.
- a calibration process may determine the mapping function after the camera is constructed.
- the calibration process may be used to fine-tune a previously calculated mapping function, or it may be used to fully define the mapping function.
- FIG. 26 shows a diagram 2600 with a set of two charts that may be used to calibrate the mapping function. More specifically, the diagram 2600 includes a cylindrical inner calibration chart, or chart 2610 , and a cylindrical outer calibration chart, or chart 2620 . The chart 2610 and the chart 2620 are concentric and axially aligned with the capture system 2630 . Each of the chart 2610 and the chart 2620 contains a pattern so that locations on images may be precisely calculated. For example, the pattern may be a grid or checkerboard pattern with periodic features that allow for global alignment.
- the capture system 2630 may be calibrated as follows:
- the size and shapes of the chart 2610 and the chart 2620 may be varied to include spherical charts, cubic charts, or any other type of surface or combination thereof. Different chart types may be more readily adapted to different coordinate systems.
- Images for virtual reality viewing may be generated from the light-field volume data. These images will be referred to as “virtual views.” To create a virtual view, a virtual lens, virtual focus position, virtual field-of-view and virtual sensor may be used.
- a virtual lens may be centered at the location of the desired virtual viewpoint.
- the virtual lens may contain a virtual aperture that may have any shape or size, and these characteristics may partially determine the depth-of-field and bokeh of the virtual view.
- the virtual focus position and virtual field-of-view of the lens may jointly define a region that will be visible and “in focus” after reconstruction. Notably, the focus and resolution are ultimately limited by the focus and resolution of the capture system, so it is possible to reconstruct an image on a virtual focal plane where nothing is really in focus.
- the virtual sensor may have the same resolution as the desired output resolution for the virtual view.
- a virtual camera system may be used to generate the virtual view. This embodiment is conceptually shown in FIG. 28 , in connection with a coordinate system 2800 having an inner sphere 2810 and an outer sphere 2820 .
- the virtual camera system may have a virtual lens 2830 and a virtual sensor 2840 that can be used to generate the virtual view.
- the configuration of the virtual lens 2830 may determine a virtual focal plane 2850 with a virtual field-of-view.
- an ideal lens is assumed, and the virtual setup may be simplified.
- This embodiment is conceptually shown in FIG. 29 , in connection with a coordinate system 2900 having an inner sphere 2910 and an outer sphere 2920 .
- the sensor pixels may be mapped directly onto the surface of the focal plane and more complicated ray tracing may be avoided.
- the lens may be geometrically simplified to a surface (for example, a circular disc) to define a virtual lens 2930 in three-dimensional Cartesian space.
- the virtual lens 2930 may represent the aperture of the ideal lens.
- the virtual field-of-view and virtual focus distance when taken together, define an “in focus” surface in three-dimensional Cartesian space with the same aspect ratio as the virtual sensor.
- a virtual sensor 2940 may be mapped to the “in focus” surface.
- An algorithm to create the virtual view may then be the following:
- the virtual lens and/or the virtual sensor may be fully modeled as a more complete optical system.
- FIG. 30 illustrates modeling in the context of a coordinate system 3000 having an inner sphere 3010 and an outer sphere 3020 .
- the embodiment may consist of a virtual sensor 3040 and a virtual lens 3030 , each with size and shape in Cartesian coordinates.
- rays in the captured light-field volume may be traced through the virtual lens 3030 and ultimately intersected (or not) with the virtual sensor 3040 .
- the virtual sensor 3040 may consist of virtual optical components, including one or more virtual lenses, virtual reflectors, a virtual aperture stop, and/or additional components or aspects for modeling.
- rays that intersect with the entrance to the virtual lens 3030 may be optically traced through the virtual lens 3030 and onto the surface of the virtual sensor 3040 .
- FIG. 31 shows exemplary output 3100 from an optical ray tracer.
- a set of rays 3110 are refracted through the elements 3120 in a lens 3130 and traced to the intersection point 3140 on a virtual sensor surface 3150 .
- An algorithm to create the virtual view may then be the following:
- optical ray tracers may function with varying levels of complexity as the behavior of light in the physical world is extremely complex.
- the above examples assume that one ray of light from the world equates to a single ray of light after passing through an optical system.
- Many optical modeling programs will model additional complexities such as chromatic dispersion, diffraction, reflections, and absorption.
- FIG. 15 shows a tiled array 1500 in the form of a ring arrangement of light-field cameras 1530 in which gaps exist between the entrance pupils of the light-field cameras 1530 in the tiled array 1500 .
- Light from the world that intersects with the tiled array 1500 in the gaps will not be recorded. While the sizes of the gaps in a light-field capture system may be extremely small relative to those of prior art systems, these gaps may still exist in many embodiments.
- these rays may be synthetically generated.
- rays are synthetically generated using simple interpolation between the closest available samples based on their light-field volume coordinates.
- Simple interpolation may work well when the difference between the location of the available samples and the desired sample is small. Notably, small is a relative term, and dependent on many factors, including the resolution of the virtual view, the location of physical subjects in the world at the time of capture, the application's tolerance for errors, and a host of other factors.
- the simple interpolation may generate a new sample value based on a weighted average of the neighboring rays.
- the weighting function may use nearest neighbor interpolation, linear interpolation, cubic interpolation, median filtering or any other approach now known or later developed.
- rays are synthetically generated based on a three-dimensional model and/or a depth map of the world at the time of capture.
- a depth map and a three-dimensional model may be easily interchangeable.
- the term depth map will be used.
- a depth map may be generated algorithmically from the captured light-field volume.
- Depth map generation from light-field data and/or multiple overlapping images is a complicated problem, but there are many existing algorithms that attempt to solve the problem. See, for example, the above-cited U.S. patent application Ser. No. 14/302,826 for “Depth Determination for Light Field Images”, filed Jun. 12, 2014 and issued as U.S. Pat. No. 8,988,317 on Mar. 24, 2015, the disclosure of which is incorporated herein by reference.
- a virtual synthetic ray may be traced until it reaches an intersection with the depth map.
- the closest available samples from the captured light-field volume may be the rays in the light-field that intersect with the depth map closest to the intersection point of the synthetic ray.
- the value assigned to the synthetic ray may be a new sample value based on a weighted average of the neighboring rays.
- the weighting function may use nearest neighbor interpolation, linear interpolation, cubic interpolation, median filtering, and/or any other approach now known or later developed.
- a pixel infill algorithm may be used if insufficient neighboring rays are found within an acceptable distance. This situation may occur in cases of occlusion. For example, a foreground object may block the view of the background from the perspective of the physical cameras in the capture system. However, the synthetic ray may intersect with the background object in the occluded region. As no color information is available at that location on the background object, the value for the color of the synthetic ray may be guessed or estimated using an infill algorithm. Any suitable pixel infill algorithms may be used.
- One exemplary pixel infill algorithm is “PatchMatch,” with details as described in C. Barnes et al., PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing ACM Transactions on Graphics (Proc. SIGGRAPH), August 2009.
- the algorithms for virtual view generation cited above may not execute efficiently enough, may not execute quickly enough, and/or may require too much data or bandwidth to properly enable viewing applications.
- the captured data may be reorganized or resampled as appropriate.
- the captured data may be resampled into a regularized format.
- the light-field is resampled into a four-dimensional table, with separate dimensions for rho1, theta1, rho2 and theta2.
- the size of the resampled table will depend on many factors, including but not limited to the intended output resolution of the virtual views and the number of discrete viewpoints from which virtual views may be generated.
- the intended linear output resolution of a virtual view may be 1000 pixels, and the field-of-view may be 100°. This may result in a total sampling of 3600 pixels for 360°. In the same embodiment, it may be desired that 100 discrete viewpoints can be generated in a single dimension.
- the size of the four-dimensional table may be 100 ⁇ 100 ⁇ 3600 ⁇ 3600.
- large sections of the table may be empty of data, and the table may be dramatically compressed relative to its nominal size.
- the resampled, regularized data structure may be generated through the use of “splatting” algorithms, “gathering” algorithms, or any other algorithm or technique.
- the resampling process may begin with a four-dimensional table initialized with empty values.
- the values corresponding to each ray in the captured data set may then be added into the table at the data index(es) that best match the four-dimensional coordinates of the ray.
- the adding may use any interpolation algorithm to accumulate the values, including but not limited to a nearest neighbor algorithm, a quadlinear algorithm, a quadcubic algorithm, and/or combinations or variations thereof.
- the value for each data index in the 4D table is calculated by interpolating from the nearest rays in the captured light-field data set.
- the value at each index is a weighted sum of all rays that have coordinates within a four-dimensional hypercube centered at the coordinates corresponding to the data index.
- the weighting function may use nearest neighbor interpolation, linear interpolation, cubic interpolation, median filtering or any other approach now known or later developed.
- holes may be filled in using four-dimensional interpolation techniques in which values for the holes are interpolated based on the values of their neighbors in the four-dimensional table.
- the interpolation may use any type of filter kernel function, including but limited to linear functions, median filter functions, cubic functions, and/or sync functions.
- the filter kernel may be of any size.
- “hole” data may be filled in using pixel infill algorithms.
- a two-dimensional slice of data may be generated by keeping rho1 and theta1 fixed.
- a pixel infill algorithm for example, PatchMatch
- PatchMatch may be applied to fill in the missing data in the two-dimensional slice, and the generated data values may then be added into the four-dimensional table.
- the resampled four-dimensional table may be divided and stored in pieces.
- each piece may correspond with a file stored in a file system.
- the full four-dimensional table may be broken up by evenly in four pieces by storing 1 ⁇ 4 ⁇ 1 ⁇ 4 ⁇ 1 ⁇ 4 ⁇ 1 ⁇ 4 of the full table in each piece.
- One advantage of this type of approach may be that entire pieces may be completely empty, and may thus be discarded.
- Another advantage may be that less information may need to be loaded in order to generate a virtual view.
- a set of virtual views is precomputed and stored.
- a sufficient number of virtual views may be precomputed to enable the display of any needed viewpoint from the precomputed virtual views.
- the viewing software may read and display the precomputed virtual views.
- some precomputed virtual views may be used in combination with real time generation of other virtual views.
- conventional, two-dimensional cameras may be used in order to provide additional spatial resolution, cost reduction, more manageable data storage, processing, and/or transmission, and/or other benefits.
- such conventional cameras may be arranged in a tiled array similar to those described above for light-field cameras.
- Such arrays may also be arranged to provide continuous, or nearly continuous, fields-of-view.
- FIGS. 35A through 35D perspective and side elevation views depict a tiled array 3500 of conventional cameras, according to one embodiment.
- the tiled array 3500 may have three different types of cameras, including upper view cameras 3510 , center view cameras 3520 , and lower view cameras 3530 .
- the upper view cameras 3510 , the center view cameras 3520 , and the lower view cameras 3530 may be arranged in an alternating pattern, with a center view camera 3520 between each upper view camera 3510 and each lower view camera 3530 .
- the tiled array 3500 may have as many of the center view cameras 3520 as it has of the lower view cameras 3530 and the upper view cameras 3510 , combined.
- the larger number of center view cameras 3520 may provide enhanced and/or more complete imaging for the center view, in which the viewer of a virtual reality experience is likely to spend the majority of his or her viewing time.
- the upper view cameras 3510 and the lower view cameras 3530 may each have a relatively large field-of-view 3540 , which may be 120° or larger.
- the center view cameras 3520 may each have a field-of-view 3550 that approximates that of the headset the user will be wearing to view the virtual reality experience. This field-of-view 3550 may be, for example, 90° to 110°.
- the placement of the upper view cameras 3510 and the lower view cameras 3530 may be relatively sparse, by comparison with that of the center view cameras 3520 , as described above.
- a diagram 3600 depicts stitching that may be used to provide an extended vertical field-of-view 3610 .
- a 200° or greater vertical field-of-view 3610 may be obtained at any point along the tiled array 3500 with only “close” stitching. Additional vertical field-of-view may be constructed with “far” stitching.
- the tiled array 3500 may have full support for three angular degrees of freedom and stereo viewing. Further, the tiled array 3500 may provide limited support for horizontal parallax and/or limited stitching, except for extreme cases. Alternative embodiments may provide support for head tilt, vertical parallax, and/or forward/backward motion. One embodiment that provides some of these benefits will be shown and described in connection with FIG. 37 .
- a perspective view depicts a tiled array 3700 according to another alternative embodiment.
- the tiled array 3700 may have three different types of cameras, including upper view cameras 3710 , center view cameras 3720 , and lower view cameras 3730 .
- each of the upper view cameras 3710 and the lower view cameras 3730 may have a field-of-view 3740 that is relatively large, for example, 120° or larger.
- Each of the center view cameras 3720 may have a field-of-view 3750 that is somewhat smaller, for example, 90° to 110°.
- the upper view cameras 3710 , the center view cameras 3720 , and the lower view cameras 3730 may be arranged in three rows, including a top row 3760 , a middle row 3770 , and a bottom row 3780 .
- the upper view cameras 3710 and the center view cameras 3720 may be arranged in an alternating pattern.
- the middle row 3770 only the center view cameras 3720 may be present.
- the lower view cameras 3730 and the center view cameras 3720 may be arranged in an alternating pattern similar to that of the upper view cameras 3710 and the center view cameras 3720 of the top row 3760 .
- the tiled array 3700 may have approximately four times as many of the center view cameras 3720 as of each of the upper view cameras 3710 and the lower view cameras 3730 .
- more complete imaging may be provided for the center views, in which the viewer of a virtual reality experience is likely to spend the majority of his or her viewing time.
- the center view cameras 3720 on the top row 3760 may be tilted upward, and the center view cameras 3720 on the bottom row 3780 may be tilted downward. This tilt may provide enhanced vertical stitching and/or an enhanced vertical field-of-view.
- the tiled array 3700 may have three full degrees of freedom, and three limited degrees of freedom.
- the tiled array 3700 may provide support for head tilt via the enhanced vertical field-of-view, and may further provide limited vertical parallax. Further, the tiled array 3700 may support limited forward/backward movement.
- various alterations may be made in order to accommodate user needs or budgetary restrictions. For example, fewer cameras may be used; in some tiled array embodiments, only ten to twenty cameras may be present. It may be advantageous to use smaller cameras with smaller pixel sizes. This and other modifications may be used to reduce the overall size of the tiled array. More horizontal and/or vertical stitching may be used.
- approximately forty cameras may be used.
- the cameras may be, for example, Pt Grey Grasshopper 3 machine vision cameras, with CMOSIS MCV3600 sensors, USB 3.0 connectivity, and one-inch, 2 k ⁇ 2 k square image sensors, with 90 frames per second (FPS) capture and data transfer capability.
- the data transfer rate for raw image data may be 14.4 GB/s (60 FPS at 12 bits), and a USB 3.0 to PCIE adapter may be used.
- Each USB 3.0 interface may receive the image data for one camera.
- the tiled array may have a total resolution of 160 megapixels.
- Each of the center view cameras may have a Kowa 6 mm lens with a 90° field-of-view.
- Each of the upper view cameras and lower view cameras may have a Fujinon 2.7 mm fisheye lens with a field-of-view of 180° or more. In alternative embodiments, more compact lenses may be used to reduce the overall size of the tiled array.
- tiled arrays Conventional cameras may be arranged in tiled arrays according to a wide variety of tiled arrays not specifically described herein. With the aid of the present disclosure, a person of skill in the art would recognize the existence of many variations of the tiled array 3500 of FIG. 35 and the tiled array 3700 that may provide unique advantages for capturing virtual reality video streams.
- volumetric video data may strain the storage, bandwidth, and/or processing capabilities of client computing systems and/or networks. Accordingly, in at least one embodiment, the volumetric video data may be divided into portions, and only the portion needed, or likely to be needed soon, by a viewer may be delivered.
- a viewer is only able to observe a field-of-view (FoV) inside the viewing volume.
- the system only fetches and renders the needed FoV from the video volume data.
- a spatial random access coding and viewing scheme may be used to allow arbitrary access to a viewer's desired FoV on a compressed volumetric video stream.
- Inter-vantage and inter spatial-layer predictions may also be used to help improve the system's coding efficiency.
- Advantages of such a coding and/or viewing scheme may include, but are not limited to, the following:
- Immersive video may also be referred to as “volumetric video” where there is a volume of viewpoints from which the views presented to the user can be generated.
- volumemetric video may be referred to as “volumetric video” where there is a volume of viewpoints from which the views presented to the user can be generated.
- digital sampling of all view-dependent color and depth information may be carried out for any visible surfaces in a given viewing volume. Such sampling representation may provide sufficient data to render any arbitrary viewpoints within the viewing space. Viewers may enjoy smooth view-dependent lighting transitions and artifacts-free occlusion filling when switching between different viewpoints.
- an image-based rendering system may represent immersive video data by creating a three-dimensional sampling grid over the viewing volume.
- Each point of the sampling grid is called a “vantage.”
- Various vantage arrangements may be used, such as a rectangular grid, a polar (spherical) matrix, a cylindrical matrix, and/or an irregular matrix.
- Each vantage may contain a projected view, such as an omnidirectional view projected onto the interior of a sphere, of the scene at a given coordinate in the sampling grid.
- This projected view may be encoded into video data for that particular vantage. It may contain color, texture, and/or depth information. Additionally or alternatively, the projected view may be created using the virtual view generated from the light-field volume data, as discussed in the previous section.
- the system may perform a barycentric interpolation of color between four vantages whose locations form a tetrahedron that includes the view position for each eye view.
- Other fusion techniques may alternatively or additionally be used to interpolate between vantages.
- the result may be the combination of any number of vantages to generate viewpoint video data for a viewpoint that is not necessarily located at any of the vantages.
- a positional tracking video experience may require more than hundreds of high resolution omnidirectional vantages across the viewing volume. This may require at least two orders of magnitude more storage space, by comparison with conventional two-dimensional videos.
- image-based and/or video-based compression techniques such as JPEG, H.264/AVC and/or HEVC, may be applied to the color and/or depth channels to remove any spatial and temporal redundancies within a single vantage stream, as well as redundancies between different vantage streams.
- a compressed vantage which requires a decoding procedure, may further put computation and memory pressure on the client's system.
- the system and method may only decode and render the region of vantages within a viewer's FoV. Spatial random access may be facilitated by dividing a vantage into multiple tiles. Each tile may be independently and/or jointly encoded with the system's vantage encoder using image-based and/or video-based compression techniques, or encoded through the use of any other compression techniques.
- a tile-based representation may also offer inherent parallelizability for multi-core systems.
- the tiling scheme used for vantage compression may be different from the tiling scheme used for rendering or culling used by the rendering pipeline.
- tiling may be used to expedite delivery, decoding, and/or display of video data, independently of the use of compression.
- Tiling may expedite playback and rendering independently of the manner in which tiling is performed for encoding and/or transmission.
- the tiling scheme used for encoding and transmission may also be used be used for playback and rendering.
- a tiled rendering scheme may help reduce computation complexity and provide stability to meet time-varying demands on the CPU and/or GPU of a computing system.
- FIG. 52 a series of graphs depict a tile-based scheme 5200 , according to one embodiment.
- the tile-based scheme 5200 may allow spatial random access on a single vantage for any field of view within a 360° field, i.e., any viewing direction originating at the vantage.
- FIG. 52 illustrates fetched tiles, on the bottom row, that correspond to various input fields, on the top row, showing a top-down view of the input field-of-view projected on a single spherical vantage.
- Each planar image is projected to a planar image from a single omnidirectional spherical vantage.
- Coding dependencies, system processing, and/or network transmission may introduce spatial random access latency to the system. Spatial random access to different tiles may be needed in certain instances, such as when the viewer switches the FoV in a virtual reality experience by turning his or her head or when the viewer moves to a new region along the vantage sampling grid. To prevent playback discontinuity in such situations, the system and method disclosed herein may pre-load the tiles outside a viewer's FoV. However, this may increase the decoding load on the client system and limit the complexity savings provided by the compression and/or apportionment of the video data. Accordingly, the pre-fetched tiles may instead be provided at a lower spatial resolution, so as to conceal switching latency.
- the system and method provide such different quality representations by displaying the tiles at different resolutions and/or delivering the tiles at different bitrates.
- a multi-spatial resolution layer scheme may be used.
- a system according to the present disclosure may have any number of spatial resolution layers. Further, all tiles need not necessarily have the same number of spatial resolution layers; rather, different tiles may have different numbers of spatial resolution layers. Different tiles may additionally or alternatively have different bit rates, spatial resolutions, frame resolutions, shapes, and/or aspect ratios.
- a spatial layered scheme may also provide error-resilience against data corruption and network packet losses.
- FIG. 38 illustrates a simplified example of tiles with multiple spatial layers.
- a tile 3800 is shown, representing some or all of the view encoded in the video data for a single vantage, including three layers, according to one embodiment.
- the tile 3800 may have a first layer 3810 , a second layer 3820 , and a third layer 3830 .
- the first layer 3810 may be a low resolution layer
- the second layer 3820 may be a medium resolution layer
- the third layer 3830 may be a high resolution layer.
- the first layer 3810 may be transmitted and used to generate and display the viewpoint video data when bandwidth, storage, and/or computational limits are stringent.
- the second layer 3820 may be transmitted and used to generate and display the viewpoint video data when bandwidth, storage, and/or computational limits are moderate.
- the third layer 3830 may be transmitted and used to generate and display the viewpoint video data when bandwidth, storage, and/or computational limits are less significant.
- equirectangular projection can be used to project a given scene onto each vantage.
- a panoramic projection may be formed from a sphere onto a plane. This type of projection may create non-uniform sampling densities. Due to constant spacing of latitude, this projection may have a constant vertical sampling density on the sphere. However, horizontally, each latitude ⁇ , may be stretched to a unit length to fit in a rectangular projection, resulting in a horizontal sampling density of 1/cos( ⁇ ). Therefore, to reduce the incidence of over-sampling in equirectangular projection, there may be a need to scale down the horizontal resolution of each tile according to the latitude location of the tile. This re-sampling rate may enable bit-rate reduction and/or maintain uniform spatial sampling across tiles.
- exemplary tiling schemes 5300 and 5350 are depicted, according to certain embodiments.
- the tiling scheme 5300 is a uniform equirectangular tiling scheme, and the tiling scheme is an equirectangular tiling scheme with reduced horizontal sampling at the top and bottom.
- the formula shown in FIGS. 53A and 53B may be used to reduce the width of some of the tiles of the tiling scheme 5300 of FIG. 53A to obtain the reduced horizontal resolution in the tiling scheme 5350 of FIG. 53B .
- the length of pixels in scanline order may be resampled.
- Such resampling may enable the use of a uniform tiling scheme, as in FIG. 53A .
- the system can maintain constant solid angle quality.
- This scheme may leave some of the tiles near the poles blank (for example, the tiles at the corners of FIG. 53A ); these tiles may optionally be skipped while encoding.
- the playback system may need to resample the pixels in scanline order for proper playback, which might incur extra system complexities.
- Numbers 4 and 5 make use of the geometric relationship between different camera views and generate a synthesized reference view using view interpolation and extrapolation.
- Number 6 represents disparity information using meshes and yields higher coding gains with higher quality view interpolation for stereo image compression.
- Other prior techniques also utilize lifting-based wavelet decomposition to encode multi-view data by performing motion-compensated temporal filtering, as in Number 2 above, and disparity-compensated inter-view filtering, as in Number 3 above.
- All inter-view techniques described above have only applied on-camera data with planar projection.
- Numbers 9 and 10 provide a rectangular tiling scheme with multi-spatial resolution layers and enabled pan/tilt/zoom capabilities on high-resolution two-dimensional videos.
- the encoder 3900 may have a compressor 3910 and a decompressor 3920 .
- the encoder 3900 may employ intra-frame prediction 3930 , inter-frame prediction 3940 , inter-vantage prediction 3950 , and/or inter-spatial layer prediction 3960 to compress color information for a given input.
- the input can be a single vantage frame, a tile inside a vantage, and/or a block inside a tile.
- the encoder 3900 may utilize existing techniques, such as any of the list set forth in the previous section, in intra-frame coding to remove redundancies through spatial prediction and/or inter-frame coding to remove temporal redundancies through motion compensation and prediction.
- the encoder 3900 may use projection transform 3970 and inverse projection transform 3975 .
- the encoder 3900 may also have a complexity/latency-aware RDO encoder control 3980 , and may store vantage data in a vantage bank 3990 .
- the inter-vantage prediction carried out by the encoder 3900 may deal with non-planar projection. It may generate meshes by extracting color, texture, and/or depth information from each vantage and rendering a vantage prediction after warping and interpolation.
- Other methods for inter-vantage prediction include, but are not limited to:
- Disocclusions may be filled with special considerations for different cases. According to some examples, disocclusions may be filled with previously mentioned methods such as, without limitation, image inpainting (Patch match), and spatial interpolation.
- the system may also make use of the tiles from one or more lower resolution layers (such as the first layer 3810 and/or the second layer 3820 of FIG. 38 ) to predict tiles on the higher resolution layer (such as the second layer 3820 and/or the third layer 3830 of FIG. 38 ).
- the inter-spatial layer prediction scheme may provide progressive viewing capabilities during downloading and streaming. It may also allow larger storage savings by comparison with storage of each resolution independently.
- the encoding prediction steps mentioned previously may be carried out according to a wide variety of techniques. Some examples will be shown and described in connection with FIGS. 40 through 44 .
- additional reference dependencies may be introduced in the coding scheme. These dependences may introduce higher decoding complexity and longer random access latency in the playback process.
- arrows between frames are used to illustrate dependencies. Where a first frame points to a second frame, data from the first frame will be used to predict the second frame for predictive coding.
- the I-frames are keyframes that can be independently determined.
- the P-frames are predicted frames with a single dependency on a previous frame, and the B-frames are predicted frames with more than one dependency on other frames, which may include future and/or past frames.
- coding complexity may depend on the number of dependencies involved.
- FIG. 44 may have the best coding gain.
- all P-frames and B-frames have both inter-frame and inter-vantage predicted frames as references.
- higher coding complexity and higher coding gain may both be present.
- FIGS. 40 through 44 Many prediction structures may be used for inter-vantage prediction, in addition to or in the alternative to those of FIGS. 40 through 44 .
- every other vantage on the sampling grid may be uniformly selected as a prediction reference.
- Inter-vantage prediction can then synthesize the view between the reference vantages, as shown in FIGS. 45A and 45B .
- V(x,y,z) represents a vantage to be predicted through the use of surrounding vantages and/or a vantage that may be used to predict surrounding vantages.
- Vantages may be distributed throughout a viewing volume in any of a wide variety of arrangements, including but not limited to rectangular/cuboid arrangements, spherical arrangements, hexagonal arrangements, and non-uniform arrangements. The predictive principles set forth in these drawings may be used in conjunction with any such arrangement of vantages within a viewing volume.
- FIG. 45A illustrates the encoding scheme with a low number of intra-coded reference vantages.
- only one reference may need to be encoded.
- relying on a single prediction may not provide the accuracy of multiple predictions, which may result in a drop in quality for the same bandwidth. Accordingly, there may be a tradeoff between bandwidth and the level of detail in the experience.
- there may be tradeoff between decoding complexity and quality with a larger number of dependencies increasing the decoding complexity.
- a large number of dependencies may induce higher latency, which may necessitate buffering future frames.
- the encoding scheme 4550 may also decrease the number of intra-coded reference vantages and have each predicted vantage predict other vantages as well. Therefore, the encoding scheme 4550 may create a chain of dependencies as shown in FIG. 45B . If low decoding complexity and/or random access latency are desired, a single reference scheme, such those of FIGS. 40 through 43 , may be more suitable because they may trade away some of the compression ratio for lower latency and/or complexity.
- a single-reference vantage prediction structure may lay out a three-dimensional sampling grid.
- a reference frame and its prediction dependencies may advantageously be chosen in a rate-distortion optimized manner.
- inter-vantage prediction may be combined with intra-vantage prediction.
- inter-vantage prediction may be combined with inter-temporal and/or inter-spatial layering (not shown). The rate-distortion optimization will be shown and described in greater detail subsequently.
- a full inter-temporal/inter-vantage encoding scheme may be used. Such a scheme may provide optimum encoding efficiency, but may be relatively more difficult to decode.
- a hierarchical coding structure for inter-vantage prediction may provide a scalable solution to vantage compression.
- a set of vantages can be decomposed into hierarchical layers.
- the vantages in the lower layer may be independently encoded and used as references for the upper layers.
- the vantages in a layer may be predicted by either interpolating or extrapolating the vantages views from the lower layers.
- Such a coding scheme may provide scalability to address different rate and/or device constraints.
- Devices with less processing power and/or bandwidth may selectively receive, decode and/or store the lower layers with a smaller viewing volume and/or lower vantage sampling density.
- FIG. 54 a hierarchical coding scheme 5400 is depicted, according to one embodiment.
- the hierarchical scheme 5400 may vary the vantage sampling density and/or viewing volume to support different system constraints of any clients.
- FIG. 54 provides a one-dimensional view of the hierarchical coding scheme 5400 , with exemplary operation of the hierarchical coding scheme 5400 illustrated in two dimensions in FIGS. 55A, 55B, 55C, and 55D .
- FIGS. 55A, 55B, 55C, and 55D a series of views 5500 , 5520 , 5540 , and 5560 , respectively, depict the operation of the hierarchical coding scheme 5400 of FIG. 54 in two dimensions, according to one embodiment.
- all views of layer 1 may be predicted by interpolation of all views from layer 0.
- Layer 2's views may be predicted by extrapolation of layers 0 and 1.
- layer 3 may be predicted by interpolation of layers 0, 1, and 2.
- the hierarchical coding scheme 5400 may extend into three-dimensions. This will be further shown and described in connection with FIGS. 56A, 56B, 56C, and 56D , as follows.
- FIGS. 54 through 55D are merely exemplary; the system disclosed herein supports different layering arrangements in three-dimensional space. Any number of coding layers may be used to obtain the desired viewing volume.
- a series of views 5600 , 5620 , 5640 , and 5660 depict the operation of the hierarchical coding scheme 5400 of FIG. 54 in three dimensions, according to another embodiment.
- all views of layer 1 may be predicted by interpolation of all views from layer 0.
- Layer 2's views may be predicted by extrapolation of layers 0 and 1.
- layer 3 may be predicted by interpolation of layers 0, 1, and 2.
- Such hierarchical coding schemes may also provide enhanced error-resiliency. For example, if the client fails to receive, decode, and/or load the higher layers before the playback deadline, the client can still continue playback by just decoding and processing the lower layers.
- the systems and methods of the present disclosure may utilize a rate-distortion optimized encoder control that addresses different decoding complexity and latency demands from different content types and client playback devices. For example, content with higher resolution or more complex scenery might require higher decoding complexity. Content storage that does not need real-time decoding would exploit the highest compression ratio possible without considering latency.
- the controller may map a client device's capabilities to a set of possible video profiles with different parameter configurations.
- the client device capabilities may include hardware and/or software parameters such as resolution, supported prediction types, frame-rate, prediction structure, number of references, codec support and/or other parameters.
- the encoder control can determine the best possible video profile used for encoding.
- the latency can be reduced by decreasing the intra-frame interval and/or pruning the number of frame dependencies.
- Decoding complexity can be reduced by disabling more complex prediction modes, reducing playback quality, and/or reducing resolution.
- the controller can then apply Lagrangian optimization to select the most optimal prediction structure and encoding parameters, for example, from those set forth previously.
- Exemplary Lagrangian optimization is disclosed in Wiegand, Thomas, and Bernd Girod, “Lagrange multiplier selection in hybrid video coder control.” Image Processing, 2001 . Proceedings. 2001 International Conference on . Vol. 3. IEEE, 2001.
- the encoder control may manage and find the optimal settings for the following encoding parameters on a block or frame level:
- any suitable compression scheme and/or codec can be used for encoding prediction residuals and encoder side information.
- the system may be compatible with image/texture-based and/or video-based encoders, such as BC7, JPEG, H.264/AVC, HEVC, VP8/9 and others.
- Components, such as intra-frame prediction and inter-frame prediction, which exist in other codecs can be reused and integrated with the system and method set forth herein.
- information regarding the depth of objects in a scene may be used to facilitate compression and/or decompression or to otherwise enhance the user experience.
- a depth map which may be a two-dimensional grayscale image with intensity indicative of the depth of objects, may be used.
- depth channel compression or depth map compression may advantageously preserve the mapping of silhouettes in the depth map to their associated color information.
- Image-based and/or video-based lossless compression techniques may advantageously be applied to the depth map data.
- Inter-vantage prediction techniques are applicable to depth map compression as well. Depth values may need to be geometrically re-calculated to another vantage with respect to an origin reference. In a manner similar to that of color, the (x,y) coordinate can be geometrically reprojected to another vantage.
- Each vantage may consist of multi-channel color information, such as RGB, YUV and other color formats, and a single depth channel. Similar techniques can also be performed in connection with other forms of data representation, such as layered depth images, as set forth in Shade, Jonathan, et al. “Layered depth images.” Proceedings of the 25 th annual coference on Computer graphics and interactive techniques . ACM, 1998, epipolar plane image volumes, as set forth in Bolles, Robert C., H. Harlyn Baker, and David H. Marimont. “Epipolar-plane image analysis: An approach to determining structure from motion.” International Journal of Computer Vision 1.1 (1987): 7-55, light field images, three-dimensional point clouds, and meshes.
- Temporal redundancies may be removed by tracking each data sample in the temporal direction for a given representation. Spatial redundancies may be removed by exploiting correlations between neighboring sample points across space and/or layers, depending on the representation. To facilitate spatial random access similar to vantage-based tiling, each sample from the corresponding layers may be grouped together according to their spatial location and/or viewing direction on a two-dimensional, three-dimensional, and/or other multi-dimensional space. Each grouping may be independently encoded such that the viewer only needs to decode samples from a subregion of a viewing volume when facing a given direction with a given field-of-view.
- a series of graphs 5700 , 5720 , 5740 , 5760 depict the projection of depth layers onto planar image from a spherical viewing range from a vantage, according to one embodiment.
- the graph 5700 of FIG. 57A depicts a top-down view of an input field-of-view projected on a simple depth layer map.
- FIGS. 57B, 57C, and 57D depict the projection of the first, second, and third depth layers, respectively, on planar images from the spherical input field-of-view of FIG. 57A .
- Each depth layer may be divided into tiles as shown.
- Such a layering scheme may be used to implement the depth channel compression techniques set forth above.
- This compression scheme may utilize a “Layered depth images” representation. This may be used as an alternative representation to represent a three-dimensional viewing volume, instead of using a vantage-based system.
- each pixel may contain color information about the three-dimensional scene for the corresponding depth.
- each pixel may include extra information to describe how lighting varies between viewing angles.
- the viewpoint may be rendered directly.
- the system may provide six degrees of freedom and/or full parallax in a three-dimensional viewing volume.
- the system may be scalable to support different degrees of immersion. For example, all aforementioned techniques, such as hierarchical vantage prediction, spatial layers, and tiling, may support scalability to different applications.
- Such a scheme may be scaled to support two-dimensional planar video, single viewpoint omnidirectional three-dimensional video, a virtual reality video system with only vertical or horizontal parallax, and/or systems with different degrees of freedom ranging from one degree of freedom to six degrees of freedom.
- a hierarchical vantage scheme may be designed to support different platforms, for example, a base layer that supports one degree of freedom (a single vantage), a secondary layer that supports three degrees of freedom with horizontal parallax (a disk of vantages), and a third layer that supports six degrees of freedom with full parallax (a set of all vantages in a viewing volume).
- Exemplary architecture will be shown and described as follows.
- An input configuration file 4710 may specify parameters such as the number of spatial layers needed and the size and location of the tiles for each spatial layer.
- the tiling and spatial layering scheme may be as depicted in the tile 3800 of FIG. 38 .
- a vantage generator 4720 may generate all of the vantages, each of which may be omnidirectional as described above, and may contain both color and depth information, on a specified three-dimensional sampling grid. The vantages may then be decomposed by a scaler 4730 into multiple spatial resolution layers, as in the tile 3800 .
- the scaler 4730 may advantageously preserve correspondence between the edges of the depth map and the color information to avoid any viewing artifacts.
- a tile generator 4740 may crop the appropriate region and create the specified tile(s).
- Each tile may then be compressed by an encoder 4750 , which may be an encoder as described in any of the previous sections.
- an encoder 4750 may be an encoder as described in any of the previous sections.
- a metafile may be used to describe any additional information, such as the codec used for compression, time segmentation, tile playback dependences and file storage, etc.
- the metadata may thus support playback.
- the tiles and metadata may be stored in storage 4760 .
- Tiles and/or metadata may be retrieved from storage 4810 .
- a tile server 4820 may relay a set of tiles that provide the optimal viewing quality.
- a tile compositor 4840 may combine the fetched tiles together to form the corresponding vantage views needed for rendering.
- the techniques to combine the fetched tiles may include stitching, blending and/or interpolation.
- Tiles can additionally or alternatively be generated by using tiles from another spatial layer and/or neighboring tiles on the same layer.
- the resulting viewpoint video data which may include the combined tiles, may be sent to a player 4850 for playback.
- the playback system may use tiles from other layers and/or other tiles from the same layer for error concealment. Error concealment can be achieved by interpolation, upsampling, downsampling, superresolution, filtering, and/or other predictive techniques.
- pause, fast-forward, and/or rewind functionality may be supported.
- the system may perform spatial-temporal access on the tiles at the same time during fast-forward and rewind, for example, by fast-forwarding or rewinding while the viewer's head is moving.
- the playback tile may continue to stream and/or decode the tiles spatially and temporally while a user is rewinding or fast-forwarding.
- a similar feature may be implemented to facilitate pausing playback.
- FIG. 49 is a diagram 4900 depicting how a vantage view may be composed, according to one embodiment.
- a viewport 4910 illustrates the FoV of the viewer, which may be selected by viewer via motion of his or her head, in the case of a virtual reality experience.
- Tiles 4920 that are at least partially within the central region of the viewport 4910 may be rendered in high resolution. Thus, these tiles may be fetched from a high-resolution layer (for example, the third layer 3830 of FIG. 38 ).
- tiles 4930 outside of the viewport 4910 may be fetched from the lower resolution layers (for example, the first layer 3810 of FIG. 38 ).
- the tile server may also fetch tiles from lower resolution layers in less perceptible regions of the viewport 4910 .
- the vantage may be projected to an equirectangular map.
- the tiles 4940 on top of the viewing area may be fetched from a mid-resolution layer (for example, the second layer 3820 of FIG. 38 ) because the top region of an equirectangular map is often stretched and over-sampled.
- a diagram 5000 depicts the view of a checkerboard pattern from a known virtual reality headset, namely the Oculus rift.
- the user may be unable to perceive a difference between rendering with low-resolution tiles and rendering with high-resolution tiles in those regions.
- tiles 4950 at the bottom region of an equirectangular map may be fetched from a low-resolution layer (for example, the first layer 3810 of FIG. 38 ).
- a particular portion of a scene is not likely to command the viewer's attention, it may be fetched from a lower resolution layer.
- a system and method according to the present disclosure may provide flexibility to optimize perceptual quality when the system is constrained by computing resources such as processing power, storage space, and/or bandwidth. Such flexibility can also support perceptual rendering techniques such as aerial perspective and foveated rendering.
- system 4700 of FIG. 47 and/or the system 4800 of FIG. 48 may be run locally on a client machine, and/or remotely over a network. Additional streaming infrastructure may be required to facilitate tile streaming over a network.
- the system and method may support different modes of content delivery for immersive videos.
- Such content delivery modes may include, for example and without limitation:
- the compressed volumetric video data may be stored on and retrieved from a local physical storage medium and played back in real-time. This may require the presence of sufficient memory bandwidth between the storage medium and the system's CPU and/or GPU.
- the compressed video data may be packaged to support content downloading.
- the compression and packaging may be selected to meet the client device's complexity and storage capabilities. For a less complex device, such as a smartphone, lower resolution video data and/or less complex video data may be downloaded to the client device. In some embodiments, this may be achieved using the scalability techniques described previously.
- the system can remove the decoding complexity constraint and compress the file stream by using the best available compression parameters.
- the client can then decode the package offline and transcode it to another compression format that can be decodable at real-time, usually at the cost of creating a much larger store of compressed volumetric video data.
- a tiling scheme with multiple resolution layers may offer a scalable system that can support any arbitrary viewings from a large number of users inside a video volume at the same time.
- a tiling scheme may help reduce streaming bandwidth, and a spatial layering scheme may help meet different client limitations in bandwidth and decoding complexity.
- a layering scheme may also provide concealment to spatial random access latency and any network packet losses or data corruption.
- a method 5100 is depicted for capturing volumetric video data, encoding the volumetric video data, decoding to obtain viewpoint video data, and displaying the viewpoint video data for a viewer, according to one embodiment.
- the method 5100 may start 5110 with a step 5120 in which the volumetric video data is captured. This may be done, for example, through the use of a tiled camera array such as any of those described above.
- vantages may be distributed throughout the viewing volume.
- the viewing volume may be a designated volume, from within which the captured scene is to be viewable.
- the vantages may be distributed throughout the viewing volume in a regular pattern such as a three-dimensional grid or the like.
- the vantages may instead be distributed in a three-dimensional hexagonal grid, in which each vantage is equidistant from all of its immediate neighbors. Such an arrangement may approximate a sphere.
- Vantages may also be distributed non-uniformly across the three-dimensional viewing volume. For example, regions of the viewing volume that are more likely to be selected as viewpoints, or from which the scene would beneficially be viewed in greater detail, may have comparatively more vantages.
- the volumetric video data may be used to generate video data for each of the vantages.
- the corresponding video data may be usable to generate a view of the scene from a viewpoint located at the vantage.
- a viewpoint within the viewing volume may be designate. This may be done, for example, by a viewer positioning his or her head at a location corresponding to the viewpoint. The orientation of the viewer's head may be used to obtain a view direction along which the view from the viewpoint is to be constructed.
- a subset of the vantages nearest to the viewpoint may be identified.
- the subset may be, for example, the four vantages closest to the viewpoint, which may define a tetrahedral shape containing the viewpoint, as described previously.
- the video data for the subset of vantages may be retrieved.
- the video data from the subset of vantages may be combined together to yield viewpoint video data representing the view of the scene from the viewpoint, from along the view direction.
- the video data may be interpolated if the viewpoint does not lie on or adjacent to one of the vantages.
- various predictive methods may be used, as set forth above, to combine future video data from the viewpoint and/or future video data from proximate the viewpoint. Such predictive methods may be used to generate at least a portion of the viewpoint video data for a future view from any combination of the viewpoint, an additional viewpoint proximate the viewpoint, the view direction, an additional view direction different the view direction.
- the predicted viewpoint video data may be used to streamline the steps needed to display the scene from that viewpoint, along that view direction.
- the playback system may predict one or more viewing trajectories along which the viewer is likely to move his or her head. By predicting the viewing trajectories, the system may pre-fetch the tiles to be decoded and rendered to minimize viewing latencies.
- predictive methods may be used to predict viewpoint video data without having to receive and/or process the underlying video data.
- tighter bandwidth and/or processing power requirements may be met without significantly diminishing the viewing experience.
- the viewpoint video data may be transmitted to the client device.
- this is an optional step, as the steps 5150 , 5160 , 5170 , and 5180 may be optionally performed at the client device. In such an event, there may be no need to transmit the viewpoint video data to the client device.
- the step 5190 may convey the viewpoint video data to the client device.
- the viewpoint video data may be used to display a view of the scene to the viewer, from the viewpoint, with a FoV oriented along the view direction.
- the method 5100 may determine whether the experience is complete. If not, the method 5100 may return to the step 5150 , in which the viewer may provide a new viewpoint and/or a new view direction. The steps 5160 , 5170 , 5180 , 5190 , and 5192 may then be repeated to generate a view of the scene from the new viewpoint and/or along the new view direction. Once the query 5194 is answered in the affirmative, the method 5100 may end 5196 .
- various compression techniques may be used to compress and/or remove data pertaining to the augmented aspects of a virtual reality or augmented reality video stream, such as stereopsis, binocular occlusions, vergence, motion parallax and view-dependent lighting. Examples presented herein show how to remove some or all of the view-dependent lighting from a video stream. Those of skill in the art will recognize that the systems and methods provided herein could also be applied to removal and/or compression of other elements unique to virtual reality or augmented reality video streams.
- a grid-based vantage representation scheme may be used to independently store the color and/or depth information (collectively, “vantage data”) from a fixed viewpoint, or “vantage location,” along the sampling grid. Redundancies between vantages may be removed using inter-vantage prediction that reprojects a vantage (the “base vantage” at a base vantage location) onto another vantage (the “target vantage” at a target vantage location).
- inter-vantage prediction methods may be used, for example, as discussed in connection with FIGS. 40 through 46B , above. Comparison of the actual target vantage with the reprojected target vantage may yield residual data, which may contain elements such as, but not limited to:
- FIG. 58 is a schematic diagram 5800 depicting reprojection error, according to one embodiment.
- Reprojection error describes the geometric error corresponding to the distance deviation between a projected point and a target point (ground truth). This error is usually caused by numerical accuracies, such as inaccurate depth values, inaccurate projection transforms, and/or floating point inaccuracy.
- a computed 3D point 5810 may be located at a series of points 5820 across four exemplary views, as determined by projection lines 5830 . Due to the inaccuracies listed above, reprojection of the computed 3D point 5810 along reprojection lines 5840 may result in reprojected points 5850 that are offset from the points 5820 , resulting in the presence of reprojection error 5860 in each of the exemplary views.
- FIG. 59 is an image 5900 depicting an original vantage, according to one embodiment.
- the image 5900 is from a free virtual reality experience available at https://www.unrealengine.com/en-US/blog/showdown-cinematic-vr-experience-released-for-free.
- FIG. 60 is an image 6000 depicting an inter-vantage view generated by reprojection, according to one embodiment.
- the image 6000 may be generated by reprojecting elements from the image 5900 .
- the image 6000 may contain various inaccuracies that result from reprojection error. Additionally, the image 6000 may have additional information resulting from projection of view-dependent lighting from the image 5900 .
- FIG. 61 is an image 6100 depicting the image 6000 of FIG. 60 , after removal of view-dependent lighting and reprojection error, with only the occluded region shown, according to one embodiment (or in the alternative, the image 6000 of FIG. 60 when reprojected from the image 5900 , excluding reprojection error and view-dependent lighting). As shown, the inaccuracies of the image 6000 of FIG. 60 have generally been removed from the image 6100 of FIG. 61 .
- an image 6200 depicts the residual data from reprojection of a base vantage, i.e., the image 5900 of FIG. 59 , to a target vantage location, according to one embodiment.
- the residual data shown may be obtained by comparing the reprojected target vantage with the actual target vantage.
- the residual data may operate as a roadmap for accurately reconstructing a target vantage, at a target vantage location, from the reprojected view obtained from a base vantage at a base vantage location.
- the residual data may be used in decoding to more accurately obtain the reprojected target vantage data from the base vantage, using inter-vantage prediction.
- the residual data may be stored as part of the compressed video stream, compressing and/or redacting the residual data may improve the compression ratios attainable with the compressed video stream, and may also expedite the decoding process.
- all view-dependent lighting may be removed. In alternative embodiments, only some view-dependent lighting may be removed.
- one or more video-based compression techniques may be applied to the residual data. For example, quantization and/or entropy coding may be applied to the residual data. The result may be removal of small energy elements of the residual data that may not be perceptible to the viewer. For the given scene in the example, a majority of the residual energy is due to reprojection errors and view-dependent lighting. To lower bitrate requirements, view-dependent lighting and reprojection error may be removed from the residual data. Only the occluded region may be compressed, as depicted in FIG. 63 .
- an image 6300 depicts the residual data of FIG. 62 , after removal of all view-dependent lighting and reprojection error, according to one embodiment. Removal of the view-dependent lighting and reprojection error may compress the video stream, while still enabling the compressed video stream to provide a full motion parallax experience to the viewer.
- an occlusion mask can be computed to indicate one or more occluded regions between the vantage references reconstructed from the base vantage and the target vantage to be encoded. Residual energy can then be removed according to the occlusion mask.
- the mask may be provided as part of the compressed data stream. Since the mask may be available to the decoder, all residuals in unoccluded regions may be removed, and only samples in the occluded regions may be encoded in the compressed video stream. In the alternative, the mask may be re-computed at the decoder side.
- the system may adaptively remove only view-dependent information or reprojection-errors that are deemed unlikely to contribute to any perceptual quality differences when rendering the final viewpoint. This is depicted by way of example in FIG. 64 .
- an image 6400 depicts the residual data of FIG. 62 , after removal of non-perceptual view-dependent lighting information, according to one embodiment. Much of the information from the residual data of FIG. 62 has been removed, but the portion of the view-dependent lighting information that is expected to be viewer-perceptible has been retained. Thus, FIG. 64 shows significantly more data than FIG. 63 .
- the residual information can be removed by rate-distortion optimization (RDO) techniques.
- RDO rate-distortion optimization
- the residual information for each vantage can be divided into small rectangular blocks (4 ⁇ 4, 8 ⁇ 8, 4 ⁇ 8, etc.).
- the degree of quantization applied to the residual information can be parameterized. Separate quantization parameters can be chosen for occluded regions and non-occluded regions.
- the degree of quantization may effectively control the degree of view-dependent lighting and reprojection error in the reprojection.
- Quantization parameters can be obtained by optimizing the RDO cost.
- a rate budget can be provided to limit the size of the compressed bitstream. In other words, the system can automatically choose the degree of view-dependent lighting and reprojection error removal depending on the system's rate constraints.
- a wide variety of system configurations may be used to remove all or part of the view-dependent lighting and/or other information from the residual data. Exemplary systems will be described in connection with FIGS. 65 and 66 .
- FIG. 65 is diagram of an inter-vantage based video compression system that provides complete view-dependent lighting removal, or system 6500 , according to one embodiment.
- the system 6500 may iteratively process a video stream to compress and/or decompress a video stream using inter-vantage prediction.
- the system 6500 may have a subtraction module 6510 that receives a reprojected target vantage 6512 from a prior iteration of inter-vantage prediction, and an actual target vantage 6514 from the uncompressed video stream.
- the subtraction module 6510 may compare the reprojected target vantage 6512 with the actual target vantage 6514 to generate residual data 6516 .
- a residual splitter 6520 may receive the residual data 6516 and divide the residual data 6516 into an occluded region 6522 and an unoccluded region (not shown in FIG. 65 ) of the residual data 6516 .
- the unoccluded region may contain only the view-dependent lighting information.
- the occluded region 6522 may include data that cannot be generated from the base vantage data used for inter-vantage prediction.
- the residual splitter 6520 may utilize an occlusion mask 6594 generated as part of the prior iteration, in which the reprojected target vantage 6512 was generated.
- the residual splitter 6520 may use any type of filter that is designed to distinguish between view-dependent lighting, occlusion, and/or reprojection errors.
- the residual data 6516 may then be passed to a residual quantizer 6530 , which may apply quantization to the occluded region 6522 only.
- the resulting quantized data may be in the form of an unsigned n-bit integer 6532 , which may be passed to an encoder 6540 .
- the encoder 6540 may compress the unsigned n-bit integer 6532 and build an encoded bit stream 6542 that can be consumed by a decoder 6550 .
- the encoded bit stream 6542 may also be referred to as a compressed video stream.
- the encoder 6540 may apply entropy encoding and/or other encoding techniques.
- the decoder 6550 may receive the encoded bit stream 6542 and may decompress the encoded bit stream 6542 to generate an unsigned n-bit integer 6552 , which may be similar or even identical to the unsigned n-bit integer 6532 compressed by the encoder 6540 .
- the decoder 6550 may optionally operate by applying the algorithms employed by the encoder 6540 , in reverse.
- the unsigned n-bit integer 6552 may be passed to a residual dequantizer 6560 , which may generate residual data 6562 , which may be similar or identical to the residual data 6516 .
- the residual dequantizer 6560 may optionally operate by applying the algorithms employed by the residual quantizer 6530 , in reverse.
- the residual data 6562 may be passed to an addition module 6570 , which may add the residual data 6562 to the reprojected target vantage 6512 generated in the previous iteration of inter-vantage prediction, to generate a reconstructed frame 6572 for the target vantage.
- the reconstructed frame 6572 may be passed to a vantage references buffer 6580 , and may be used as a base vantage for future inter-vantage prediction.
- the base vantage may be used by an inter-vantage predictor 6590 , which may apply inter-vantage prediction, for example, as set forth in connection with FIGS. 40 through 46B above, to provide the reprojected target vantage 6512 for the next step, as well as projection information 6592 , which may be used to generate an occlusion mask 6594 for the next step.
- the reprojected target vantage 6512 may be passed to the subtraction module 6510 for the next iteration, and the occlusion mask 6594 may be passed to the residual splitter 6520 for the next iteration.
- the system 6500 may continue to iterate until the encoded bit stream 6542 has been generated for the entire virtual reality or augmented reality experience.
- FIG. 66 is a diagram of an inter-vantage based video compression system that provides perceptually-optimized view-dependent lighting removal, or system 6600 , according to another embodiment.
- the system 6600 may have many elements in common with the system 6500 of FIG. 65 , but may have additional functionality designed to identify which portions of the residual data are likely to be viewer-perceptible, and remove only those portions.
- the system 6600 may have a low pass filter 6610 that receives the unoccluded region 6612 of the residual data 6516 after the residual data 6516 has been divided into the occluded region 6522 and the unoccluded region 6612 .
- the residual splitter 6520 may delineate more than one of each of the occluded region 6522 and the unoccluded region 6612 .
- the low pass filter 6610 may be applied to remove high frequency information in the unoccluded region 6612 , which may not contribute to any perceptual quality differences during final viewpoint rendering using the vantages.
- any filter design that removes non-perceptual information may be used.
- This filtering processing may, in other embodiments, be a manual process in which a human editor can manually remove information from the residual frame.
- the degree or threshold for view-dependent information removal can be adjusted by changing the parameters on such a filter. These parameters may allow the system 6600 to adjust the image quality of the encoded vantage and the size of encoded vantage output.
- the system 6600 may also have an addition module 6620 that recombines the unoccluded region 6612 with the occluded region 6522 , with the optional application of smoothing 6622 between the unoccluded region 6612 and the occluded region 6522 .
- the addition module 6620 may apply a Gaussian smoothing kernel to smooth the transitions between the unoccluded region 6612 and the occluded region 6522 .
- the residual data 6516 with smoothing applied, may then be passed to the residual quantizer 6530 for the performance of further steps.
- FIG. 67 Various methods may be used to remove view-dependent lighting, either entirely or in part, from residual data.
- One exemplary method will be shown and described in connection with FIG. 67 , in connection with the system 6500 of FIG. 65 and the system 6600 of FIG. 66 .
- different systems may be used.
- the system 6500 of FIG. 65 and the system 6600 of FIG. 66 may be used in conjunction with methods different from that set forth below.
- a method 6700 is depicted for compressing a video stream, which may be volumetric video data to be used for a virtual reality or augmented reality experience, according to one embodiment.
- the method 6700 may be performed with any of the hardware mentioned previously, such as the post-processing circuitry 3604 , memory 3611 , user input 3615 , and/or other elements of a post-processing system 3700 as described in the above-referenced U.S. Patent Applications.
- the method 6700 may start 6710 with a step 5120 in which the volumetric video data is captured. This may be done, for example, through the use of a tiled camera array such as any of those described above.
- a step 5130 vantages may be distributed throughout the viewing volume.
- the volumetric video data may be used to generate video data for each of the vantages.
- the step 5120 , the step 5130 , and the step 5140 may all be substantially as described above in connection with the method 5100 of FIG. 51 .
- vantage data may be retrieved from the video stream.
- the vantage data may include, for example, base vantage data for the base vantage to be used as a basis for inter-vantage prediction, and target vantage data for the target vantage.
- the vantage data may include the actual target vantage 6514 of FIGS. 65 and 66 .
- the base vantage may be at a base vantage location, and the target vantage may be at a target vantage location.
- the base vantage data may be reprojected to the target vantage location. This may be done, for example, using any of the inter-vantage prediction methods set forth above, in connection with FIGS. 40 through 46B .
- the inter-vantage predictor 6590 of FIGS. 65 and 66 may be used. The result may be generation of reprojected target vantage data.
- the reprojected target vantage data may be compared with the target vantage data retrieved from the video stream in the step 6720 .
- the comparison may involve subtraction, for example, with the subtraction module 6510 of FIGS. 65 and 66 , and may yield residual data 6516 .
- the residual data 6516 may be compressed. This step may involve compression and/or removal of data pertinent to delivery of the virtual reality or augmented reality experience, such as view-dependent lighting information. In some embodiments, the step 6750 may involve removal of all view-dependent lighting information. In other embodiments, the step 6750 may involve removal of only the portion of the view-dependent lighting information deemed to be imperceptible to the viewer. The step 6750 will be shown and described, according to one embodiment, in more detail in connection with FIG. 68 .
- a compressed video stream such as the encoded bit stream 6542 of FIGS. 65 and 66 , may be encoded, for example, with the encoder 6540 of FIGS. 65 and 66 .
- the compressed video stream may be stored, including the compressed residual data.
- the compressed video stream may be decoded, for example, with the decoder 6550 of FIGS. 65 and 66 .
- the step 6770 may also include dequantizing of the unsigned n-bit integer 6552 , for example, with the residual dequantizer 6560 of FIGS. 65 and 66 .
- the resulting dequantized residual data may be combined with the base vantage data, for example, by the addition module 6570 of FIGS. 65 and 66 .
- the resulting reconstructed frame 6572 may then be ready for use in another iteration of the inter-vantage prediction process, which may be carried out by the inter-vantage predictor 6590 .
- a step 6790 the virtual reality or augmented reality experience may be presented for the viewer. This may involve iterative decoding of the compressed video stream.
- the step 6790 may include performance of various other steps, such as the step 5150 , the step 5160 , the step 5170 , the step 5180 , the step 5190 , and the step 5192 of FIG. 51 . These steps may be repeated until, pursuant to the query 5194 of FIG. 51 , a determination is made that the experience is complete.
- the method 6700 may then end 6796 .
- the various steps of the method 6700 of FIG. 67 are merely exemplary. In alternative embodiments, they may be re-ordered or revised to omit one or more steps, replace one or more steps with alternatives, and/or supplement the steps with other steps not specifically shown and described herein.
- the step 6750 may involve a number of sub-steps.
- One manner in which the step 6750 may be carried out will be described in connection with FIG. 68 .
- the step 6750 may start 6810 with a step 6820 in which an occlusion mask, such as the occlusion mask 6594 of FIGS. 65 and 66 , is generated.
- an occlusion mask such as the occlusion mask 6594 of FIGS. 65 and 66
- the residual data may be divided into occluded and unoccluded regions, for example, by the residual splitter 6520 of FIGS. 65 and 66 .
- a low pass filter such as the low pass filter 6610 of FIG. 66
- a low pass filter may be applied to the unoccluded region to filter out viewer-imperceptible information from the residual data.
- the occluded region and the unoccluded region may be blended, for example, with the addition module 6620 of FIG. 66 .
- quantization may be applied to the blended residual data, including the occluded region and the remainder (i.e., the viewer-perceptible portion) of the unoccluded region. This may be done, for example, by the residual quantizer 6530 of FIGS. 65 and 66 .
- the step 6750 may then end 6890 .
- Removal and/or compression of residual data in this manner may have several advantages. For clients with lower data, network bandwidth, and or processing capabilities, viewers can still enjoy a virtual reality or augmented reality experience with full motion parallax experience without view-dependent lighting. Further, the quality of the experience and the degree of compression applied to the video stream may easily be scaled by adjusting parameters such as the degree to which view-dependent lighting is removed. Thus, client devices with a wide range of capabilities may be supported.
- additional compression of the video stream may be carried out, potentially without sacrificing the visual quality of the experience in a viewer-perceptible way. This may particularly be the case where adaptive removal of view-dependent lighting is applied to remove only user-imperceptible aspects of the residual data.
- the video stream may be compressed by a factor of about one thousand by removing view-dependent lighting for a viewing volume about one meter in diameter.
- Some embodiments may include a system or a method for performing the above-described techniques, either singly or in any combination.
- Other embodiments may include a computer program product comprising a non-transitory computer-readable storage medium and computer program code, encoded on the medium, for causing a processor in a computing device or other electronic device to perform the above-described techniques.
- Certain aspects include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of described herein can be embodied in software, firmware and/or hardware, and when embodied in software, can be downloaded to reside on and be operated from different platforms used by a variety of operating systems.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computing device selectively activated or reconfigured by a computer program stored in the computing device.
- a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, flash memory, solid state drives, magnetic or optical cards, application specific integrated circuits (ASICs), and/or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus.
- the computing devices referred to herein may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the techniques described herein can be implemented as software, hardware, and/or other elements for controlling a computer system, computing device, or other electronic device, or any combination or plurality thereof.
- Such an electronic device can include, for example, a processor, an input device (such as a keyboard, mouse, touchpad, trackpad, joystick, trackball, microphone, and/or any combination thereof), an output device (such as a screen, speaker, and/or the like), memory, long-term storage (such as magnetic storage, optical storage, and/or the like), and/or network connectivity, according to techniques that are well known in the art.
- Such an electronic device may be portable or nonportable.
- Examples of electronic devices that may be used for implementing the techniques described herein include: a mobile phone, personal digital assistant, smartphone, kiosk, server computer, enterprise computing device, desktop computer, laptop computer, tablet computer, consumer electronic device, television, set-top box, or the like.
- An electronic device for implementing the techniques described herein may use any operating system such as, for example: Linux; Microsoft Windows, available from Microsoft Corporation of Redmond, Wash.; Mac OS X, available from Apple Inc. of Cupertino, Calif.; iOS, available from Apple Inc. of Cupertino, Calif.; Android, available from Google, Inc. of Mountain View, Calif.; and/or any other operating system that is adapted for use on the device.
- the techniques described herein can be implemented in a distributed processing environment, networked computing environment, or web-based computing environment. Elements can be implemented on client computing devices, servers, routers, and/or other network or non-network components. In some embodiments, the techniques described herein are implemented using a client/server architecture, wherein some components are implemented on one or more client computing devices and other components are implemented on one or more servers. In one embodiment, in the course of implementing the techniques of the present disclosure, client(s) request content from server(s), and server(s) return content in response to the requests.
- a browser may be installed at the client computing device for enabling such requests and responses, and for providing a user interface by which the user can initiate and control such interactions and view the presented content.
- Any or all of the network components for implementing the described technology may, in some embodiments, be communicatively coupled with one another using any suitable electronic network, whether wired or wireless or any combination thereof, and using any suitable protocols for enabling such communication.
- a network is the Internet, although the techniques described herein can be implemented using other networks as well.
Abstract
Description
-
- Active area: the portion of a module that receives light to be provided as image data by the module.
- Array light-field camera: a type of light-field camera that contains an array of objective lenses with overlapping fields-of-view and one or more photosensors, with the viewpoint from each objective lens captured as a separate image.
- Capture surface, or “physical capture surface”: a surface defined by a tiled array of light-field cameras, at which light is received from an environment into the light-field cameras, with exemplary capture surfaces having cylindrical, spherical, cubic, and/or other shapes.
- Capture system: a tiled array of light-field cameras used to fully or sparsely capture a light-field volume.
- Client computing device: a computing device that works in conjunction with a server such that data is exchanged between the client computing device and the server.
- Computing device: any device having a processor.
- Conventional image: an image in which the pixel values are not, collectively or individually, indicative of the angle of incidence at which light is received on the surface of the sensor.
- Data store: a repository of data, which may be at a single location or distributed over multiple locations, and may be provided through the use of any volatile or nonvolatile data storage technologies.
- Depth: a representation of distance between an object and/or corresponding image sample and the entrance pupil of the optics of the capture system.
- Disocclusion: an effect whereby some part of a view from one vantage is not visible from another vantage due to the geometry of objects appearing in the scene.
- Disk: a region in a light-field image that is illuminated by light passing through a single microlens; may be circular or any other suitable shape.
- Disk image: a single image of the aperture stop, viewed through a plenoptic microlens, and captured by a region on the sensor surface.
- Display device: a device such as a video screen that can display images and/or video for a viewer.
- Entrance pupil: the optical image of the physical aperture stop, as “seen” through the front of the lens system, with a geometric size, location, and angular acceptance acting as the camera's window of view into an environment.
- Environment: a real-world scene to be captured for subsequent visualization.
- Fiber optic bundle: a set of aligned optical fibers capable of transmitting light.
- Frame: a single image of a plurality of images or a video stream.
- Free-viewpoint video: video that changes in response to altering the viewpoint of the viewer
- Fully sampled light-field volume: a light-field volume that has been captured in a manner inclusive of ray data from all directions at any location within the light-field volume, enabling the generation of virtual views from any viewpoint, at any orientation, and with any field-of-view.
- Image: a two-dimensional array of pixel values, or pixels, each specifying a color.
- Input device: any device that receives input from a user.
- Layer: a segment of data, which may be stored in conjunction with other layers pertaining to common subject matter such as the video data for a particular vantage.
- Leading end: the end of a fiber optic bundle that receives light.
- Light-field camera: any camera capable of capturing light-field images.
- Light-field coordinate: for a single light-field camera, the four-dimensional coordinate (for example, x, y, u, v) used to index a light-field sample captured by a light-field camera, in which (x, y) may be the spatial coordinate representing the intersection point of a light ray with a microlens array, and (u, v) may be the angular coordinate representing an intersection point of the light ray with an aperture plane.
- Light-field data: data indicative of the angle of incidence at which light is received on the surface of the sensor.
- Light-field image: an image that contains a representation of light-field data captured at the sensor, which may be a four-dimensional sample representing information carried by ray bundles received by a single light-field camera.
- Light-field volume: the combination of all light-field images that represents, either fully or sparsely, light rays entering the physical space defined by the light-field volume.
- Light-field volume coordinate: for a capture system, an extended version of light-field coordinates that may be used for panoramic and/or omnidirectional viewing (for example, rho1, theta1, rho2, theta2), in which (rho1, theta1) represent intersection of a light ray with an inner sphere and (rho2, theta2) represent intersection of the light ray with an outer sphere concentric with the inner sphere.
- Main lens, or “objective lens”: a lens or set of lenses that directs light from a scene toward an image sensor.
- Mask: data to be overlaid over other data, such as an image, indicating the extent to which the underlying data should be used for further processing. A mask may be grayscale (with varying gradations of applicability of the further processing) or binary (indicating that the further processing is or is not to be applied).
- Microlens: a small lens, typically one in an array of similar microlenses.
- Microlens array: an array of microlenses arranged in a predetermined pattern.
- Occlusion region: a region of a field of data, such as an image depicting residual data, affected by disocclusion.
- Omnidirectional stereo video: video in which the user selects a fixed viewpoint from within a viewing volume.
- Packaging: The housing, electronics, and any other components of an image sensor that reside outside the active area.
- Plenoptic light-field camera: a type of light-field camera that employs a microlens-based approach in which a plenoptic microlens array is positioned between the objective lens and the photosensor.
- Plenoptic microlens array: a microlens array in a plenoptic camera that is used to capture directional information for incoming light rays, with each microlens creating an image of the aperture stop of the objective lens on the surface of the image sensor.
- Processor: any processing device capable of processing digital data, which may be a microprocessor, ASIC, FPGA, or other type of processing device.
- Ray bundle, “ray,” or “bundle”: a set of light rays recorded in aggregate by a single pixel in a photosensor.
- Residual data: the data generated by comparing two corresponding sets of data, for example, by subtracting one from the other.
- Ring array: a tiled array of light-field cameras in which the light-field cameras are generally radially symmetrically arranged about an axis to define a cylindrical capture surface of light-field cameras facing outward.
- Scene: some or all of an environment that is to be viewed by a viewer.
- Sectoral portion: a portion of an arcuate or semispherical shape; or in the case of a cylindrical or spherical mapping of video data from a vantage or viewpoint, a portion of the mapping of video data corresponding to a Field-of-View smaller than the mapping.
- Sensor, “photosensor,” or “image sensor”: a light detector in a camera capable of generating images based on light received by the sensor.
- Spherical array: a tiled array of light-field cameras in which the light-field cameras are generally arranged in a spherical pattern to define a spherical capture surface of light-field cameras facing outward.
- Stereo virtual reality: an extended form of virtual reality in which each eye is shown a different view of the virtual world, enabling stereoscopic three-dimensional perception.
- Subset: one or more, but not all, of a group of items.
- Subview: the view or image from an individual view in a light-field camera (a subaperture image in a plenoptic light-field camera, or an image created by a single objective lens in an objective lens array in an array light-field camera).
- Tapered fiber optic bundle, or “taper”: a fiber optic bundle that is larger at one end than at the other.
- Tile: a portion of the view of a scene from a particular viewpoint, pertaining to a particular range of view orientations, i.e., a particular field of view, from that viewpoint.
- Tiled array: an arrangement of light-field cameras in which the light-field cameras are compactly and/or loosely, evenly and/or unevenly distributed about an axis and oriented generally outward to capture an environment surrounding the tiled array, with exemplary tiled arrays including ring-shaped arrays, spherical arrays, cubic arrays, and the like.
- Trailing end: the end of a fiber optic bundle that emits light.
- Vantage: a pre-determined point within a viewing volume, having associated video data that can be used to generate a view from a viewpoint at the vantage.
- Video data: data derived from image or video capture, associated with a particular vantage or viewpoint.
- Vantage location: the location of a vantage in three-dimensional space.
- View direction: a direction along which a scene is to be viewed from a viewpoint; can be conceptualized as a vector extending along the center of a Field-of-View from the viewpoint.
- Viewer-perceptible—a quality of visual information, indicative of the degree to which a viewer would notice its presence or absence in the context of a virtual reality or augmented reality experience.
- Viewpoint: a point from which an environment is to be viewed.
- Viewpoint video data: video data associated with a particular viewpoint that can be used to generate a view from that viewpoint.
- Virtual reality: an immersive viewing experience in which images presented to the viewer are based on the location and/or orientation of the viewer's head and/or eyes.
- Virtual view: a reconstructed view, typically for display in a virtual reality or augmented reality headset, which may be generated by resampling and/or interpolating data from a captured light-field volume.
- Virtual viewpoint: the location, within a coordinate system and/or light-field volume, from which a virtual view is generated.
- Volumetric video: image or video captured in a manner that permits the video to be viewed from multiple viewpoints.
- Volumetric video data: data derived from image or video capture, which can be used to construct a view from multiple viewpoints within a viewing volume.
r_complete=r_surface*sin(surface_half_fov)
f(camera,x,y)→(rho1,theta1,rho2,theta2)
-
- Capture image data with the
inner chart 2610 in place - For each camera in the array of the capture system 2630:
- For each subview:
- Find and register the subview with the global alignment features
- For each pixel:
- Calculate the intersection with the chart as (chi1, y1)
- For each subview:
- Remove the
inner chart 2610 - Capture image data with the
outer chart 2620 in place- For each subview:
- Find and register the subview with the global alignment features
- For each pixel:
- Calculate the intersection with the chart as (chi2, y2)
- For each subview:
- For each pixel:
- Trace the ray defined by (chi1, y, chi2, y2) to intersect with the
inner sphere 510 in the coordinatesystem 500 for the light-field volume to determine (rho1, theta1). - Trace the ray defined by (chi1, y, chi2, y2) to intersect with the
outer sphere 520 in the coordinatesystem 500 for the light-field volume to determine (rho2, theta2).
- Trace the ray defined by (chi1, y, chi2, y2) to intersect with the
- Capture image data with the
view_image = new Image(w, h) | |
view_image.clear( ) | |
for each ray in rays | |
cart_ray = convert_to_cartesian3d(ray) | |
if (intersects(cart_ray, va) && intersects(cart_ray, fs)) | |
point = intersection(cart_ray, fs) | |
norm_point = normalize_point_relative_to(fs) | |
sensor_x = norm_point.x * w | |
sensor_y = norm_point.y * h | |
accumulate(view_image, x, y, ray.color) | |
where: | |
intersects returns true if the supplied ray intersects with the | |
surface | |
intersection returns the location, in Cartesian coordinates, of | |
intersection | |
normalize_point_relative_to normalizes a Cartesian 3D point into a | |
normalized 2D location on the provided surface. Values are in x = | |
[0, 1] and y = [0, 1] | |
accumulate accumulates the color values assigned to the ray into the | |
image. This method may use any sort of interpolation, including | |
nearest neighbor, bilinear, bicubic, or any other method. | |
view_image = new Image(w, h) | |
view_image.clear( ) | |
for each ray in rays | |
cart_ray = convert_to_cartesian3d(ray) | |
if (intersects(cart_ray, vep)) | |
image_ray = trace_ray_through_lens(cart_ray, vl) | |
if (intersects(image_ray, vs)) | |
point = intersection(cart_ray, vs) | |
norm_point = normalize_point_relative_to(vs) | |
sensor_x = norm_point.x * w | |
sensor_y = norm_point.y * h | |
accumulate(view_image, x, y, image_ray.color) | |
Where: | |
intersects returns true if the supplied ray intersects with the | |
surface | |
intersection returns the location, in Cartesian coordinates, of | |
intersection | |
trace_ray_through_lens traces a ray through the virtual lens | |
normalize_point_relative_to normalizes a Cartesian 3D point into a | |
normalized 2D location on the provided surface. Values are in x = | |
[0, 1] and y = [0, 1] | |
accumulate accumulates the color values assigned to the ray into the | |
image. This method may use any sort of interpolation, including | |
nearest neighbor, bilinear, bicubic, or any other method. | |
-
- Reduction of the bandwidth requirement for transmission and playback;
- Provision of fast decoding performance for responsive playback; and/or
- Enablement of low-latency spatial random access for interactive navigation inside the viewing volume.
Spatial Random Access Enabled Volumetric Video—Encoding
- 1. G. Tech, Y. Chen, K. Müller, J.-R. Ohm, A. Vetro, and Y.-K. Wang, “Overview of the Multiview and 3D Extensions of High Efficiency Video Coding”, IEEE Transactions on Circuits and Systems for Video Technology, Vol. 26,
Issue 1, pp. 35-49, September 2015. - 2. Jens-Rainer Ohm, Mihaela van der Schaar, John W. Woods, Interframe wavelet coding—motion picture representation for universal scalability, Signal Processing: Image Communication, Volume 19, Issue 9, October 2004, Pages 877-908, ISSN 0923-5965.
- 3. Chuo-Ling Chang, Xiaoqing Zhu, P. Ramanathan and B. Girod, “Light field compression using disparity-compensated lifting and shape adaptation,” in IEEE Transactions on Image Processing, vol. 15, no. 4, pp. 793-806, April 2006.
- 4. K. Yamamoto et al., “Multiview Video Coding Using View Interpolation and Color Correction,” in IEEE Transactions on Circuits and Systems for Video Technology, vol. 17, no. 11, pp. 1436-1449, November 2007.
- 5. Xiu, Xiaoyu, Derek Pang, and Jie Liang. “Rectification-Based View Interpolation and Extrapolation for Multiview Video Coding.” IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 21.6 (2011): 693.
- 6. Park, Joon Hong, and Hyun Wook Park. “A mesh-based disparity representation method for view interpolation and stereo image compression.” Image Processing, IEEE Transactions on 15.7 (2006): 1751-1762.
- 7. P. Merkle, K. Müller, D. Marpe and T. Wiegand, “Depth Intra Coding for 3D Video Based on Geometric Primitives,” in IEEE Transactions on Circuits and Systems for Video Technology, vol. 26, no. 3, pp. 570-582, March 2016. doi: 10.1109/TCSVT.2015.2407791.
- 8. G. J. Sullivan, J. M. Boyce, Y. Chen, J.-R. Ohm, C. A. Segall, and A. Vetro, “Standardized Extensions of High Efficiency Video Coding”, IEEE Journal on Selected Topics in Signal Processing, Vol. 7, no. 6, pp. 1001-1016, December 2013. http://ieeexploreleee.org/stamp/stamp.jsp?tp=&arnumber=6630053.
- 9. Aditya Mavlankar and Bernd Girod, “Spatial-Random-Access-Enabled Video Coding for Interactive Virtual Pan/Tilt/Zoom Functionality,” IEEE Transactions on Circuits and Systems for Video Technology. vol. 21, no. 5, pp. 577-588, May 2011.
- 10. Mavlankar, P. Agrawal, D. Pang, S. Halawa, N. M. Cheung and B. Girod, “An interactive region-of-interest video streaming system for online lecture viewing,” 2010 18th International Packet Video Workshop, Hong Kong, 2010, pp. 64-71.
- 11. Fraedrich, Roland, Michael Bauer, and Marc Stamminger. “Sequential Data Compression of Very Large Data in Volume Rendering.” VMV. 2007.
- 12. Sohn, Bong-Soo, Chandrajit Bajaj, and Vinay Siddavanahalli. “Feature based volumetric video compression for interactive playback.” Proceedings of the 2002 IEEE symposium on Volume visualization and graphics. IEEE Press, 2002.
-
- Geometric transformation by using depth information and known intrinsic and extrinsic camera parameters;
- Methods mentioned in previous section on virtual view generation from light field data;
- Reprojection techniques described in the above-referenced U.S. patent application Ser. No. 15/590,841 for “Vantage Generation”, and
- Other advanced view synthesis method, such as those set forth in:
- Zitnick, C. Lawrence, et al. “High-quality video view interpolation using a layered representation.” ACM Transactions on Graphics (TOG). Vol. 23. No. 3. ACM, 2004;
- Flynn, John, et al. “DeepStereo: Learning to Predict New Views from the World's Imagery.” arXiv preprint arXiv:1506.06825 (2015); and
- Oh, Kwan-Jung, Sehoon Yea, and Yo-Sung Ho. “Hole filling method using depth based in-painting for view synthesis in free viewpoint television and 3-d video.” Picture Coding Symposium, 2009. PCS 2009. IEEE, 2009.
{circumflex over (m)}=argmin m∈M[D( l,m )+λ·R( l,m )□]
where l denotes the locality of the decision (frame-level or block level), m denotes the parameters or mode decision, D denotes the distortion, R denotes the rate and λ denotes the Lagrangian multiplier.
-
- Codec choice, such as JPEG, H.264/AVC, HEVC, VP8/9;
- Reference selection from vantage bank, which stored all reconstructed frames from past encoded frames, for inter-spatial, inter-vantage and inter-frame predictions;
- Prediction mode and dependencies;
- Bitrates, quantization parameters and/or quality level;
- I-frame interval or Group-of-Picture (GOP) size;
- Resolution (spatial and/or temporal);
- Frame-rate; and/or
- Other codec specific parameters related to complexity and quality, such as motion estimation types, entropy coding types, quantization, post-processing filters, etc.
Compression/Decompression Codecs
-
- Compressed video data storage on physical storage medium;
- Decompressed video data downloaded to client device;
- Compressed video data downloaded to client device with offline decompression; and
- Video data streamed to client device.
Compressed Volumetric Video Data Storage on Physical Storage Medium
-
- 1. View-dependent lighting, such as specular reflection, shading, and shadows;
- 2. Disocclusion between different viewpoints; and
- 3. Reprojection error.
Reprojection Error
Claims (30)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/832,023 US10567464B2 (en) | 2015-04-15 | 2017-12-05 | Video compression with adaptive view-dependent lighting removal |
Applications Claiming Priority (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562148055P | 2015-04-15 | 2015-04-15 | |
US201562148460P | 2015-04-16 | 2015-04-16 | |
US15/084,326 US10085005B2 (en) | 2015-04-15 | 2016-03-29 | Capturing light-field volume image and video data using tiled light-field cameras |
US15/590,877 US10341632B2 (en) | 2015-04-15 | 2017-05-09 | Spatial random access enabled video system with a three-dimensional viewing volume |
US15/590,808 US10440407B2 (en) | 2017-05-09 | 2017-05-09 | Adaptive control for immersive experience delivery |
US15/832,023 US10567464B2 (en) | 2015-04-15 | 2017-12-05 | Video compression with adaptive view-dependent lighting removal |
Related Parent Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/590,877 Continuation-In-Part US10341632B2 (en) | 2015-04-15 | 2017-05-09 | Spatial random access enabled video system with a three-dimensional viewing volume |
US15/590,808 Continuation-In-Part US10440407B2 (en) | 2015-04-15 | 2017-05-09 | Adaptive control for immersive experience delivery |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/590,877 Continuation-In-Part US10341632B2 (en) | 2015-04-15 | 2017-05-09 | Spatial random access enabled video system with a three-dimensional viewing volume |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180097867A1 US20180097867A1 (en) | 2018-04-05 |
US10567464B2 true US10567464B2 (en) | 2020-02-18 |
Family
ID=61757364
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/832,023 Active 2036-04-18 US10567464B2 (en) | 2015-04-15 | 2017-12-05 | Video compression with adaptive view-dependent lighting removal |
Country Status (1)
Country | Link |
---|---|
US (1) | US10567464B2 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10921596B2 (en) * | 2018-07-24 | 2021-02-16 | Disney Enterprises, Inc. | Adaptive luminance/color correction for displays |
US11037365B2 (en) | 2019-03-07 | 2021-06-15 | Alibaba Group Holding Limited | Method, apparatus, medium, terminal, and device for processing multi-angle free-perspective data |
US20220138596A1 (en) * | 2020-11-02 | 2022-05-05 | Adobe Inc. | Increasing efficiency of inferencing digital videos utilizing machine-learning models |
US20220159194A1 (en) * | 2017-08-25 | 2022-05-19 | Canon Kabushiki Kaisha | Image capturing apparatus |
US11357593B2 (en) | 2019-01-10 | 2022-06-14 | Covidien Lp | Endoscopic imaging with augmented parallax |
US11893668B2 (en) | 2021-03-31 | 2024-02-06 | Leica Camera Ag | Imaging system and method for generating a final digital image via applying a profile to image information |
Families Citing this family (44)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US9858649B2 (en) | 2015-09-30 | 2018-01-02 | Lytro, Inc. | Depth-based image blurring |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US9979909B2 (en) | 2015-07-24 | 2018-05-22 | Lytro, Inc. | Automatic lens flare detection and correction for light-field images |
US20170103577A1 (en) * | 2015-10-12 | 2017-04-13 | Cinova Media | Method and apparatus for optimizing video streaming for virtual reality |
US10275892B2 (en) | 2016-06-09 | 2019-04-30 | Google Llc | Multi-view scene segmentation and propagation |
EP3273686A1 (en) * | 2016-07-21 | 2018-01-24 | Thomson Licensing | A method for generating layered depth data of a scene |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US10353946B2 (en) * | 2017-01-18 | 2019-07-16 | Fyusion, Inc. | Client-server communication for live search using multi-view digital media representations |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US11051039B2 (en) | 2017-06-02 | 2021-06-29 | Ostendo Technologies, Inc. | Methods for full parallax light field compression |
EP3416371A1 (en) * | 2017-06-12 | 2018-12-19 | Thomson Licensing | Method for displaying, on a 2d display device, a content derived from light field data |
EP3416381A1 (en) | 2017-06-12 | 2018-12-19 | Thomson Licensing | Method and apparatus for providing information to a user observing a multi view content |
US10545215B2 (en) | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US11012676B2 (en) * | 2017-12-13 | 2021-05-18 | Google Llc | Methods, systems, and media for generating and rendering immersive video content |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
US10713997B2 (en) * | 2018-03-23 | 2020-07-14 | Valve Corporation | Controlling image display via mapping of pixel values to pixels |
KR20200144097A (en) * | 2018-04-12 | 2020-12-28 | 도판 인사츠 가부시키가이샤 | Light field image generation system, image display system, shape information acquisition server, image generation server, display device, light field image generation method and image display method |
US10931956B2 (en) | 2018-04-12 | 2021-02-23 | Ostendo Technologies, Inc. | Methods for MR-DIBR disparity map merging and disparity threshold determination |
US20210250481A1 (en) * | 2018-04-27 | 2021-08-12 | Arizona Board Of Regents On Behalf Of The University Of Arizona | Rotationally Shift Invariant and Multi-Layered Microlens Array |
WO2019215377A1 (en) * | 2018-05-07 | 2019-11-14 | Nokia Technologies Oy | A method and technical equipment for encoding and decoding volumetric video |
US11172222B2 (en) * | 2018-06-26 | 2021-11-09 | Ostendo Technologies, Inc. | Random access in encoded full parallax light field images |
US11082637B2 (en) | 2018-06-28 | 2021-08-03 | Intel Corporation | Video processing in virtual reality environments |
EP3837834A4 (en) * | 2018-08-13 | 2022-05-18 | Nokia Technologies Oy | An apparatus, a method and a computer program for viewing volume signalling for volumetric video |
KR102652943B1 (en) * | 2018-12-03 | 2024-03-29 | 삼성전자주식회사 | Method for outputting a three dimensional image and an electronic device performing the method |
CA3121740A1 (en) * | 2018-12-19 | 2020-06-25 | Bae Systems Plc | Method and system for adjusting luminance profiles in head-mounted displays |
KR102130960B1 (en) * | 2019-05-07 | 2020-07-08 | (주) 솔 | Image sensor package for fine particle counting using virtual grid lines and fine particle counting system and method thereof |
US11381723B2 (en) * | 2019-12-12 | 2022-07-05 | SoliDDD Corp. | Plenoptic integral imaging light field, full-focus, single-element volumetric capture lens and system |
US10949986B1 (en) | 2020-05-12 | 2021-03-16 | Proprio, Inc. | Methods and systems for imaging a scene, such as a medical scene, and tracking objects within the scene |
WO2021237065A1 (en) | 2020-05-21 | 2021-11-25 | Looking Glass Factory, Inc. | System and method for holographic image display |
US11415935B2 (en) | 2020-06-23 | 2022-08-16 | Looking Glass Factory, Inc. | System and method for holographic communication |
US11354866B2 (en) * | 2020-07-17 | 2022-06-07 | Microsoft Technology Licensing, Llc | Dynamic adjustments in mixed-reality environment based on positional assumptions and configurations |
US11388388B2 (en) * | 2020-12-01 | 2022-07-12 | Looking Glass Factory, Inc. | System and method for processing three dimensional images |
CN113613011B (en) * | 2021-07-26 | 2022-09-30 | 北京达佳互联信息技术有限公司 | Light field image compression method and device, electronic equipment and storage medium |
CN115103174A (en) * | 2022-05-27 | 2022-09-23 | 南昌威爱信息科技有限公司 | Method and apparatus for delivering volumetric video content |
Citations (523)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US725567A (en) | 1902-09-25 | 1903-04-14 | Frederic E Ives | Parallax stereogram and process of making same. |
US4383170A (en) | 1979-11-19 | 1983-05-10 | Tokyo Shibaura Denki Kabushiki Kaisha | Image input device |
US4661986A (en) | 1983-06-27 | 1987-04-28 | Rca Corporation | Depth-of-focus imaging process method |
US4694185A (en) | 1986-04-18 | 1987-09-15 | Eastman Kodak Company | Light sensing devices with lenticular pixels |
US4920419A (en) | 1988-05-31 | 1990-04-24 | Eastman Kodak Company | Zoom lens focus control device for film video player |
US5076687A (en) | 1990-08-28 | 1991-12-31 | Massachusetts Institute Of Technology | Optical ranging apparatus |
US5077810A (en) | 1990-07-19 | 1991-12-31 | Eastman Kodak Company | Distributed digital signal processing system using standard resolution processors for a high resolution sensor |
US5157465A (en) | 1990-10-11 | 1992-10-20 | Kronberg James W | Universal fiber-optic C.I.E. colorimeter |
US5251019A (en) | 1991-01-25 | 1993-10-05 | Eastman Kodak Company | Solid state color image sensor using a field-staggered color filter pattern |
US5282045A (en) | 1990-04-27 | 1994-01-25 | Hitachi, Ltd. | Depth-of-field control apparatus and image pickup apparatus having the same therein |
US5499069A (en) | 1994-05-24 | 1996-03-12 | Eastman Kodak Company | Camera system and an optical adapter to reduce image format size |
US5572034A (en) | 1994-08-08 | 1996-11-05 | University Of Massachusetts Medical Center | Fiber optic plates for generating seamless images |
DE19624421A1 (en) | 1995-06-30 | 1997-01-02 | Zeiss Carl Fa | Wave front deformation from object spatially resolved measurement arrangement |
US5610390A (en) | 1994-10-03 | 1997-03-11 | Fuji Photo Optical Co., Ltd. | Solid-state image pickup device having microlenses each with displaced optical axis |
US5729471A (en) | 1995-03-31 | 1998-03-17 | The Regents Of The University Of California | Machine dynamic selection of one video camera/image of a scene from multiple video cameras/images of the scene in accordance with a particular perspective on the scene, an object in the scene, or an event in the scene |
US5748371A (en) | 1995-02-03 | 1998-05-05 | The Regents Of The University Of Colorado | Extended depth of field optical systems |
US5757423A (en) | 1993-10-22 | 1998-05-26 | Canon Kabushiki Kaisha | Image taking apparatus |
US5818525A (en) | 1996-06-17 | 1998-10-06 | Loral Fairchild Corp. | RGB image correction using compressed flat illuminated files and a simple one or two point correction algorithm |
US5835267A (en) | 1997-07-15 | 1998-11-10 | Eastman Kodak Company | Radiometric calibration device and method |
US5907619A (en) | 1996-12-20 | 1999-05-25 | Intel Corporation | Secure compressed imaging |
US5949433A (en) | 1996-04-11 | 1999-09-07 | Discreet Logic, Inc. | Processing image data |
US5974215A (en) | 1998-05-20 | 1999-10-26 | North Carolina State University | Compound image sensor array having staggered array of tapered optical fiber bundles |
US6005936A (en) | 1996-11-28 | 1999-12-21 | Ibm | System for embedding authentication information into an image and an image alteration detecting system |
US6021241A (en) | 1998-07-17 | 2000-02-01 | North Carolina State University | Systems and methods for using diffraction patterns to determine radiation intensity values for areas between and along adjacent sensors of compound sensor arrays |
US6023523A (en) | 1996-02-16 | 2000-02-08 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US6028606A (en) | 1996-08-02 | 2000-02-22 | The Board Of Trustees Of The Leland Stanford Junior University | Camera simulation system |
US6034690A (en) | 1996-08-02 | 2000-03-07 | U.S. Philips Corporation | Post-processing generation of focus/defocus effects for computer graphics images |
US6061083A (en) | 1996-04-22 | 2000-05-09 | Fujitsu Limited | Stereoscopic image display method, multi-viewpoint image capturing method, multi-viewpoint image processing method, stereoscopic image display device, multi-viewpoint image capturing device and multi-viewpoint image processing device |
US6061400A (en) | 1997-11-20 | 2000-05-09 | Hitachi America Ltd. | Methods and apparatus for detecting scene conditions likely to cause prediction errors in reduced resolution video decoders and for using the detected information |
US6069565A (en) | 1992-10-20 | 2000-05-30 | Rosemount Aerospace Inc. | System for detecting ice or snow on surface which specularly reflects light |
US6075889A (en) | 1998-06-12 | 2000-06-13 | Eastman Kodak Company | Computing color specification (luminance and chrominance) values for images |
US6084979A (en) | 1996-06-20 | 2000-07-04 | Carnegie Mellon University | Method for creating virtual reality |
US6091860A (en) | 1997-11-12 | 2000-07-18 | Pagemasters, Inc. | System and method for processing pixels for displaying and storing |
US6097394A (en) | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US6115556A (en) | 1997-04-10 | 2000-09-05 | Reddington; Terrence P. | Digital camera back accessory and methods of manufacture |
US6137100A (en) | 1998-06-08 | 2000-10-24 | Photobit Corporation | CMOS image sensor with different pixel sizes for different colors |
US6169285B1 (en) | 1998-10-23 | 2001-01-02 | Adac Laboratories | Radiation-based imaging system employing virtual light-responsive elements |
US6201899B1 (en) | 1998-10-09 | 2001-03-13 | Sarnoff Corporation | Method and apparatus for extended depth of field imaging |
US6221687B1 (en) | 1999-12-23 | 2001-04-24 | Tower Semiconductor Ltd. | Color image sensor with embedded microlens array |
US6320979B1 (en) | 1998-10-06 | 2001-11-20 | Canon Kabushiki Kaisha | Depth of field enhancement |
US20010048968A1 (en) | 2000-02-16 | 2001-12-06 | Cox W. Royall | Ink-jet printing of gradient-index microlenses |
US20010053202A1 (en) | 1996-02-21 | 2001-12-20 | Mazess Richard B. | Densitometry adapter for compact x-ray fluoroscopy machine |
US20020001395A1 (en) | 2000-01-13 | 2002-01-03 | Davis Bruce L. | Authenticating metadata and embedding metadata in watermarks of media signals |
US20020015048A1 (en) | 2000-06-28 | 2002-02-07 | David Nister | System and method for median fusion of depth maps |
US20020061131A1 (en) | 2000-10-18 | 2002-05-23 | Sawhney Harpreet Singh | Method and apparatus for synthesizing new video and/or still imagery from a collection of real video and/or still imagery |
US6424351B1 (en) | 1999-04-21 | 2002-07-23 | The University Of North Carolina At Chapel Hill | Methods and systems for producing three-dimensional images using relief textures |
US20020109783A1 (en) | 1999-06-02 | 2002-08-15 | Nikon Corporation | Electronic still camera |
US6448544B1 (en) | 1998-06-08 | 2002-09-10 | Brandeis University | Low noise, high resolution image detection system and method |
US6466207B1 (en) | 1998-03-18 | 2002-10-15 | Microsoft Corporation | Real-time image rendering with layered depth images |
US20020159030A1 (en) | 2000-05-08 | 2002-10-31 | Frey Rudolph W. | Apparatus and method for objective measurement of optical systems using wavefront analysis |
US6476805B1 (en) | 1999-12-23 | 2002-11-05 | Microsoft Corporation | Techniques for spatial displacement estimation and multi-resolution operations on light fields |
US6479827B1 (en) | 1999-07-02 | 2002-11-12 | Canon Kabushiki Kaisha | Image sensing apparatus |
US6483535B1 (en) | 1999-12-23 | 2002-11-19 | Welch Allyn, Inc. | Wide angle lens system for electronic imagers having long exit pupil distances |
US20020199106A1 (en) | 2001-02-09 | 2002-12-26 | Canon Kabushiki Kaisha | Information processing apparatus and its control method, computer program, and storage medium |
US6529265B1 (en) | 1997-04-14 | 2003-03-04 | Dicon A/S | Illumination unit and a method for point illumination of a medium |
US20030043270A1 (en) | 2001-08-29 | 2003-03-06 | Rafey Richter A. | Extracting a depth map from known camera and model tracking data |
US20030081145A1 (en) | 2001-10-30 | 2003-05-01 | Seaman Mark D. | Systems and methods for generating digital images having image meta-data combined with the image data |
US20030103670A1 (en) | 2001-11-30 | 2003-06-05 | Bernhard Schoelkopf | Interactive images |
US6577342B1 (en) | 1998-09-25 | 2003-06-10 | Intel Corporation | Image sensor with microlens material structure |
WO2003052465A2 (en) | 2001-12-18 | 2003-06-26 | University Of Rochester | Multifocal aspheric lens obtaining extended field depth |
US20030117511A1 (en) | 2001-12-21 | 2003-06-26 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
US6587147B1 (en) | 1999-02-01 | 2003-07-01 | Intel Corporation | Microlens array |
US20030123700A1 (en) | 2001-12-28 | 2003-07-03 | Canon Kabushiki Kaisha | Image generation apparatus, image file generation method, image verification apparatus and image verification method |
US20030133018A1 (en) | 2002-01-16 | 2003-07-17 | Ted Ziemkowski | System for near-simultaneous capture of multiple camera images |
US6597859B1 (en) | 1999-12-16 | 2003-07-22 | Intel Corporation | Method and apparatus for abstracting video data |
US20030147252A1 (en) | 2002-02-06 | 2003-08-07 | Fioravanti S.R.L. | Front lighting system for a motor vehicle |
US6606099B2 (en) | 2000-06-19 | 2003-08-12 | Alps Electric Co., Ltd. | Display device for creating intermediate gradation levels in pseudo manner and image signal processing method |
US20030156077A1 (en) | 2000-05-19 | 2003-08-21 | Tibor Balogh | Method and apparatus for displaying 3d images |
US20030172131A1 (en) | 2000-03-24 | 2003-09-11 | Yonghui Ao | Method and system for subject video streaming |
US6658168B1 (en) | 1999-05-29 | 2003-12-02 | Lg Electronics Inc. | Method for retrieving image by using multiple features per image subregion |
US20040002179A1 (en) | 2002-06-26 | 2004-01-01 | Barton Eric J. | Glass attachment over micro-lens arrays |
US6674430B1 (en) | 1998-07-16 | 2004-01-06 | The Research Foundation Of State University Of New York | Apparatus and method for real-time volume processing and universal 3D rendering |
US6680976B1 (en) | 1997-07-28 | 2004-01-20 | The Board Of Trustees Of The University Of Illinois | Robust, reliable compression and packetization scheme for transmitting video |
US20040012688A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Large area charge coupled device camera |
US20040012689A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Charge coupled devices in tiled arrays |
US6687419B1 (en) | 1998-12-08 | 2004-02-03 | Synoptics Limited | Automatic image montage system |
US6697062B1 (en) | 1999-08-06 | 2004-02-24 | Microsoft Corporation | Reflection space image based rendering |
US20040101166A1 (en) | 2000-03-22 | 2004-05-27 | Williams David W. | Speed measurement system with onsite digital image capture and processing for use in stop sign enforcement |
US20040114176A1 (en) | 2002-12-17 | 2004-06-17 | International Business Machines Corporation | Editing and browsing images for virtual cameras |
US20040135780A1 (en) | 2002-08-30 | 2004-07-15 | Nims Jerry C. | Multi-dimensional images system for digital image input and output |
US6768980B1 (en) | 1999-09-03 | 2004-07-27 | Thomas W. Meyer | Method of and apparatus for high-bandwidth steganographic embedding of data in a series of digital signals or measurements such as taken from analog data streams or subsampled and/or transformed digital data |
US6785667B2 (en) | 2000-02-14 | 2004-08-31 | Geophoenix, Inc. | Method and apparatus for extracting data objects and locating them in virtual space |
US20040189686A1 (en) | 2002-10-31 | 2004-09-30 | Tanguay Donald O. | Method and system for producing a model from optical images |
US20040212725A1 (en) | 2003-03-19 | 2004-10-28 | Ramesh Raskar | Stylized rendering using a multi-flash camera |
US6833865B1 (en) | 1998-09-01 | 2004-12-21 | Virage, Inc. | Embedded metadata engines in digital capture devices |
US20040257360A1 (en) | 2001-10-22 | 2004-12-23 | Frank Sieckmann | Method and device for producing light-microscopy, three-dimensional images |
US6842297B2 (en) | 2001-08-31 | 2005-01-11 | Cdm Optics, Inc. | Wavefront coding optics |
US20050031203A1 (en) | 2003-08-08 | 2005-02-10 | Hiroaki Fukuda | Image processing apparatus, an image forming apparatus and an image processing method |
US20050049500A1 (en) | 2003-08-28 | 2005-03-03 | Babu Sundar G. | Diagnostic medical ultrasound system having method and apparatus for storing and retrieving 3D and 4D data sets |
US20050052543A1 (en) | 2000-06-28 | 2005-03-10 | Microsoft Corporation | Scene capturing and view rendering based on a longitudinally aligned camera array |
US20050080602A1 (en) | 2003-10-10 | 2005-04-14 | Microsoft Corporation | Systems and methods for all-frequency relighting using spherical harmonics and point light distributions |
US6900841B1 (en) | 1999-01-11 | 2005-05-31 | Olympus Optical Co., Ltd. | Image processing system capable of applying good texture such as blur |
US20050141881A1 (en) | 1995-04-14 | 2005-06-30 | Kabushiki Kaisha Toshiba | Recording medium capable of interactive reproducing and reproduction system for the same |
US20050162540A1 (en) | 2004-01-27 | 2005-07-28 | Fujinon Corporation | Autofocus system |
US6924841B2 (en) | 2001-05-02 | 2005-08-02 | Agilent Technologies, Inc. | System and method for capturing color images that extends the dynamic range of an image sensor using first and second groups of pixels |
US20050212918A1 (en) | 2004-03-25 | 2005-09-29 | Bill Serra | Monitoring system and method |
US20050253728A1 (en) * | 2004-05-13 | 2005-11-17 | Chao-Ho Chen | Method and system for detecting fire in a predetermined area |
US20050276441A1 (en) | 2004-06-12 | 2005-12-15 | University Of Southern California | Performance relighting and reflectance transformation with time-multiplexed illumination |
US20060008265A1 (en) | 2004-07-12 | 2006-01-12 | Kenji Ito | Optical apparatus |
US20060023066A1 (en) | 2004-07-27 | 2006-02-02 | Microsoft Corporation | System and Method for Client Services for Interactive Multi-View Video |
US7003061B2 (en) | 2000-12-21 | 2006-02-21 | Adobe Systems Incorporated | Image extraction from complex scenes in digital video |
US20060050170A1 (en) | 2004-09-09 | 2006-03-09 | Fuji Photo Film Co., Ltd. | Camera system, camera body, and camera head |
US20060056604A1 (en) | 2004-09-15 | 2006-03-16 | Research In Motion Limited | Method for scaling images for usage on a mobile communication device |
US20060056040A1 (en) | 2004-09-02 | 2006-03-16 | Asia Optical Co., Inc. | Image pick-up apparatus with curvature-of-field correction |
US7015954B1 (en) | 1999-08-09 | 2006-03-21 | Fuji Xerox Co., Ltd. | Automatic video system using multiple cameras |
US20060072175A1 (en) | 2004-10-06 | 2006-04-06 | Takahiro Oshino | 3D image printing system |
US7025515B2 (en) | 2003-05-20 | 2006-04-11 | Software 2000 Ltd. | Bit mask generation system |
US20060078052A1 (en) | 2004-10-08 | 2006-04-13 | Dang Philip P | Method and apparatus for parallel processing of in-loop deblocking filter for H.264 video compression standard |
WO2006039486A2 (en) | 2004-10-01 | 2006-04-13 | The Board Of Trustees Of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
US20060082879A1 (en) | 2003-05-29 | 2006-04-20 | Takashi Miyoshi | Stereo optical module and stereo camera |
US7034866B1 (en) | 2000-11-22 | 2006-04-25 | Koninklijke Philips Electronics N.V. | Combined display-camera for an image processing system |
US20060130017A1 (en) | 2002-06-17 | 2006-06-15 | Microsoft Corporation | Combined image views and methods of creating images |
US7079698B2 (en) | 1997-04-01 | 2006-07-18 | Matsushita Electric Industrial Co., Ltd. | Image coding and decoding apparatus, method of image coding and decoding, and recording medium for recording program for image coding and decoding |
US7102666B2 (en) | 2001-02-12 | 2006-09-05 | Carnegie Mellon University | System and method for stabilizing rotational images |
US20060208259A1 (en) | 2003-12-31 | 2006-09-21 | Jeon In G | CMOS image sensors and methods for fabricating the same |
US20060248348A1 (en) | 2003-10-14 | 2006-11-02 | Canon Kabushiki Kaisha | Image data verification |
US20060250322A1 (en) | 2005-05-09 | 2006-11-09 | Optics 1, Inc. | Dynamic vergence and focus control for head-mounted displays |
US20060256226A1 (en) | 2003-01-16 | 2006-11-16 | D-Blur Technologies Ltd. | Camera with image enhancement functions |
US20060274210A1 (en) | 2005-06-04 | 2006-12-07 | Samsung Electronics Co., Ltd. | Method and apparatus for improving quality of composite video signal and method and apparatus for decoding composite video signal |
US20060285741A1 (en) | 2005-06-18 | 2006-12-21 | Muralidhara Subbarao | Direct vision sensor for 3D computer vision, digital imaging, and digital video |
US20070008317A1 (en) | 2005-05-25 | 2007-01-11 | Sectra Ab | Automated medical image visualization using volume rendering with local histograms |
US7164807B2 (en) | 2003-04-24 | 2007-01-16 | Eastman Kodak Company | Method and system for automatically reducing aliasing artifacts |
US20070019883A1 (en) | 2005-07-19 | 2007-01-25 | Wong Earl Q | Method for creating a depth map for auto focus using an all-in-focus picture and two-dimensional scale space matching |
US20070033588A1 (en) | 2005-08-02 | 2007-02-08 | Landsman Richard A | Generic download and upload functionality in a client/server web application architecture |
US20070030357A1 (en) | 2005-08-05 | 2007-02-08 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Techniques for processing images |
US20070052810A1 (en) | 2000-06-14 | 2007-03-08 | Monroe David A | Dual-mode camera |
US20070071316A1 (en) | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US20070081081A1 (en) | 2005-10-07 | 2007-04-12 | Cheng Brett A | Automated multi-frame image capture for panorama stitching using motion sensor |
US7206022B2 (en) | 2002-11-25 | 2007-04-17 | Eastman Kodak Company | Camera system with eye monitoring |
US20070097206A1 (en) | 2005-11-02 | 2007-05-03 | Houvener Robert C | Multi-user stereoscopic 3-D panoramic vision system and method |
US20070103558A1 (en) | 2005-11-04 | 2007-05-10 | Microsoft Corporation | Multi-view video delivery |
US20070113198A1 (en) | 2005-11-16 | 2007-05-17 | Microsoft Corporation | Displaying 2D graphic content using depth wells |
US20070140676A1 (en) | 2005-12-16 | 2007-06-21 | Pentax Corporation | Camera having an autofocus system |
US7239345B1 (en) | 2001-10-12 | 2007-07-03 | Worldscape, Inc. | Camera arrangements with backlighting detection and methods of using same |
WO2007092581A2 (en) | 2006-02-07 | 2007-08-16 | The Board Of Trustees Of The Leland Stanford Junior University | Correction of optical aberrations |
US20070188613A1 (en) | 2005-08-08 | 2007-08-16 | Kunio Nobori | Image synthesizing apparatus and image synthesizing method |
US20070201853A1 (en) | 2006-02-28 | 2007-08-30 | Microsoft Corporation | Adaptive Processing For Images Captured With Flash |
US20070229653A1 (en) | 2006-04-04 | 2007-10-04 | Wojciech Matusik | Method and system for acquiring and displaying 3D light fields |
US20070230944A1 (en) | 2006-04-04 | 2007-10-04 | Georgiev Todor G | Plenoptic camera |
US7286295B1 (en) | 2005-11-30 | 2007-10-23 | Sandia Corporation | Microoptical compound lens |
US20070269108A1 (en) | 2006-05-03 | 2007-11-22 | Fotonation Vision Limited | Foreground / Background Separation in Digital Images |
US20070273795A1 (en) | 2006-04-21 | 2007-11-29 | Mersive Technologies, Inc. | Alignment optimization in image display systems employing multi-camera image acquisition |
US7304670B1 (en) | 1997-03-28 | 2007-12-04 | Hand Held Products, Inc. | Method and apparatus for compensating for fixed pattern noise in an imaging system |
US20080007626A1 (en) | 2006-07-07 | 2008-01-10 | Sony Ericsson Mobile Communications Ab | Active autofocus window |
US20080012988A1 (en) | 2006-07-16 | 2008-01-17 | Ray Baharav | System and method for virtual content placement |
US20080018668A1 (en) | 2004-07-23 | 2008-01-24 | Masaki Yamauchi | Image Processing Device and Image Processing Method |
US20080031537A1 (en) | 2006-08-07 | 2008-02-07 | Dina Gutkowicz-Krusin | Reducing noise in digital images |
US7329856B2 (en) | 2004-08-24 | 2008-02-12 | Micron Technology, Inc. | Image sensor having integrated infrared-filtering optical device and related method |
US7336430B2 (en) | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US20080049113A1 (en) | 2006-07-13 | 2008-02-28 | Canon Kabushiki Kaisha | Image sensing apparatus |
US20080056569A1 (en) | 2006-09-05 | 2008-03-06 | Williams Robert C | Background separated images for print and on-line use |
US20080122940A1 (en) | 2006-11-27 | 2008-05-29 | Sanyo Electric Co., Ltd. | Image shooting apparatus and focus control method |
US20080129728A1 (en) | 2006-12-01 | 2008-06-05 | Fujifilm Corporation | Image file creation device, imaging apparatus and file structure |
US20080144952A1 (en) | 2006-11-30 | 2008-06-19 | Canon Kabushiki Kaisha | Method and Apparatus For Hybrid Image Compression |
US20080152215A1 (en) | 2006-12-26 | 2008-06-26 | Kenichi Horie | Coding method, electronic camera, recording medium storing coded program, and decoding method |
US20080168404A1 (en) | 2007-01-07 | 2008-07-10 | Apple Inc. | List Scrolling and Document Translation, Scaling, and Rotation on a Touch-Screen Display |
CN101226292A (en) | 2007-01-19 | 2008-07-23 | 滨松光子学株式会社 | Phase-modulating apparatus |
US20080180792A1 (en) | 2007-01-25 | 2008-07-31 | Georgiev Todor G | Light Field Microscope With Lenslet Array |
US20080187305A1 (en) | 2007-02-06 | 2008-08-07 | Ramesh Raskar | 4D light field cameras |
US20080193026A1 (en) | 2007-02-09 | 2008-08-14 | Kenichi Horie | Decoding method, decoding apparatus, storage medium in which decoding program is stored, and electronic camera |
US7417670B1 (en) | 2005-01-12 | 2008-08-26 | Ambarella, Inc. | Digital video camera with binning or skipping correction |
US20080205871A1 (en) | 2007-02-27 | 2008-08-28 | Nikon Corporation | Focus detection device for image forming optical system, imaging apparatus, and focus detection method for image forming optical system |
US20080226274A1 (en) | 2005-11-03 | 2008-09-18 | Spielberg Anthony C | Systems For Improved Autofocus in Digital Imaging Systems |
US20080232680A1 (en) | 2007-03-19 | 2008-09-25 | Alexander Berestov | Two dimensional/three dimensional digital information acquisition and display device |
US20080253652A1 (en) | 2007-04-10 | 2008-10-16 | Aricent Inc. | Method of demosaicing a digital mosaiced image |
US20080260291A1 (en) | 2007-04-17 | 2008-10-23 | Nokia Corporation | Image downscaling by binning |
US20080266688A1 (en) | 2004-04-27 | 2008-10-30 | Fico Mirrors, Sa | Folding Mechanism for Exterior Rear-View Mirrors in Automotive Vehicles |
US20080277566A1 (en) | 2005-05-30 | 2008-11-13 | Ken Utagawa | Image Forming State Detection Device |
CN101309359A (en) | 2008-06-20 | 2008-11-19 | 埃派克森微电子(上海)有限公司 | System and method for eliminating fixed mode noise by dummy pixels |
US20080309813A1 (en) | 2007-06-18 | 2008-12-18 | Sony Corporation | Imaging device and method |
US20080316301A1 (en) | 2000-11-29 | 2008-12-25 | Micoy Corporation | System and method for spherical stereoscopic photographing |
US7477304B2 (en) | 2004-08-26 | 2009-01-13 | Micron Technology, Inc. | Two narrow band and one wide band color filter for increasing color image sensor sensitivity |
US20090027542A1 (en) | 2007-07-13 | 2009-01-29 | Sony Corporation | Image pickup apparatus |
US20090041381A1 (en) | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Processing by Demultiplexing in the Frequency Domain |
US20090070710A1 (en) | 2007-09-07 | 2009-03-12 | Canon Kabushiki Kaisha | Content display apparatus and display method thereof |
US20090102956A1 (en) | 2007-10-18 | 2009-04-23 | Georgiev Todor G | Fast Computational Camera Based On Two Arrays of Lenses |
US20090109280A1 (en) | 2007-10-31 | 2009-04-30 | Technion Research And Development Foundation Ltd. | Free viewpoint video |
US20090128658A1 (en) | 2007-11-12 | 2009-05-21 | Sony Corporation | Image pickup apparatus |
US20090135258A1 (en) | 2006-03-31 | 2009-05-28 | Nikon Corporation | Projection Device and Electronic Device |
US20090140131A1 (en) | 2005-06-23 | 2009-06-04 | Nikon Corporation | Image input apparatus, photodetection apparatus, and image synthesis method |
US20090167909A1 (en) | 2006-10-30 | 2009-07-02 | Taro Imagawa | Image generation apparatus and image generation method |
US20090185051A1 (en) | 2008-01-21 | 2009-07-23 | Nikon Corporation | Data processing apparatus, imaging apparatus, and medium storing data processing program |
US20090185801A1 (en) | 2008-01-23 | 2009-07-23 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090190022A1 (en) | 2008-01-28 | 2009-07-30 | Sony Corporation | Image pickup apparatus |
US20090190024A1 (en) | 2008-01-28 | 2009-07-30 | Sony Corporation | Image pickup apparatus |
US20090195689A1 (en) | 2008-02-05 | 2009-08-06 | Samsung Techwin Co., Ltd. | Digital image photographing apparatus, method of controlling the apparatus, and recording medium having program for executing the method |
US20090202235A1 (en) | 2008-02-13 | 2009-08-13 | Qualcomm Incorporated | Auto-focus calibration for image capture device |
US20090204813A1 (en) | 2001-06-07 | 2009-08-13 | John Man Kwong Kwan | System and method for authenticating data using incompatible digest functions |
US20090207233A1 (en) | 2008-02-14 | 2009-08-20 | Mauchly J William | Method and system for videoconference configuration |
US7587109B1 (en) | 2008-09-02 | 2009-09-08 | Spectral Imaging Laboratory | Hybrid fiber coupled artificial compound eye |
US20090273843A1 (en) | 2008-05-01 | 2009-11-05 | Ramesh Raskar | Apparatus and Method for Reducing Glare in Images |
US7623726B1 (en) | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US20090290848A1 (en) | 2007-01-11 | 2009-11-26 | Michael James Brown | Method and System for Generating a Replay Video |
US20090295829A1 (en) | 2008-01-23 | 2009-12-03 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US7633513B2 (en) | 2003-06-27 | 2009-12-15 | Sony Corporation | Signal processing device, signal processing method, program, and recording medium |
US20090309973A1 (en) | 2006-08-02 | 2009-12-17 | Panasonic Corporation | Camera control apparatus and camera control system |
US20090309975A1 (en) | 2008-06-13 | 2009-12-17 | Scott Gordon | Dynamic Multi-Perspective Interactive Event Visualization System and Method |
US20090310885A1 (en) | 2008-06-13 | 2009-12-17 | Fujifilm Corporation | Image processing apparatus, imaging apparatus, image processing method and recording medium |
US20090321861A1 (en) | 2008-06-26 | 2009-12-31 | Micron Technology, Inc. | Microelectronic imagers with stacked lens assemblies and processes for wafer-level packaging of microelectronic imagers |
US20100003024A1 (en) | 2007-12-10 | 2010-01-07 | Amit Kumar Agrawal | Cameras with Varying Spatio-Angular-Temporal Resolutions |
US20100011117A1 (en) | 2008-07-09 | 2010-01-14 | Apple Inc. | Video streaming using multiple channels |
US20100021001A1 (en) | 2007-11-15 | 2010-01-28 | Honsinger Chris W | Method for Making an Assured Image |
JP2010020100A (en) | 2008-07-10 | 2010-01-28 | Olympus Imaging Corp | Image reproducing display device, imaging device, and image reproducing display method |
US20100050120A1 (en) | 2006-02-13 | 2010-02-25 | Google Inc. | User Interface for Selecting Options |
US20100060727A1 (en) | 2006-08-11 | 2010-03-11 | Eran Steinberg | Real-time face tracking with reference images |
US7683951B2 (en) | 2003-04-22 | 2010-03-23 | Fujifilm Corporation | Solid-state imaging apparatus and digital camera for white balance correction |
US7687757B1 (en) | 2009-01-29 | 2010-03-30 | Visera Technologies Company Limited | Design of microlens on pixel array |
US20100097444A1 (en) | 2008-10-16 | 2010-04-22 | Peter Lablans | Camera System for Creating an Image From a Plurality of Images |
US20100107068A1 (en) | 2008-10-23 | 2010-04-29 | Butcher Larry R | User Interface with Parallax Animation |
US20100103311A1 (en) | 2007-06-06 | 2010-04-29 | Sony Corporation | Image processing device, image processing method, and image processing program |
US20100111489A1 (en) | 2007-04-13 | 2010-05-06 | Presler Ari M | Digital Camera System for Recording, Editing and Visualizing Images |
US20100123784A1 (en) | 2008-11-19 | 2010-05-20 | Yuanyuan Ding | Catadioptric Projectors |
US7723662B2 (en) | 2005-10-07 | 2010-05-25 | The Board Of Trustees Of The Leland Stanford Junior University | Microscopy arrangements and approaches |
US7724952B2 (en) | 2006-05-15 | 2010-05-25 | Microsoft Corporation | Object matting using flash and no-flash images |
US20100141780A1 (en) | 2008-12-09 | 2010-06-10 | Kar-Han Tan | View Projection Matrix Based High Performance Low Latency Display Pipeline |
US20100142839A1 (en) | 2008-11-19 | 2010-06-10 | Canon Kabushiki Kaisha | Dvc as generic file format for plenoptic camera |
US7748022B1 (en) | 2006-02-21 | 2010-06-29 | L-3 Communications Sonoma Eo, Inc. | Real-time data characterization with token generation for fast data retrieval |
US20100201789A1 (en) | 2009-01-05 | 2010-08-12 | Fujifilm Corporation | Three-dimensional display device and digital zoom correction method |
US20100253782A1 (en) | 2009-04-07 | 2010-10-07 | Latent Image Technology Ltd. | Device and method for automated verification of polarization-variant images |
US20100265385A1 (en) | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US20100277629A1 (en) | 2009-05-01 | 2010-11-04 | Samsung Electronics Co., Ltd. | Photo detecting device and image pickup device and method thereon |
US20100277617A1 (en) | 2009-05-02 | 2010-11-04 | Hollinger Steven J | Ball with camera and trajectory control for reconnaissance or recreation |
US20100303288A1 (en) | 2002-09-30 | 2010-12-02 | Myport Technologies, Inc. | Method for multi-media recognition, data conversion, creation of metatags, storage and search retrieval |
US7847825B2 (en) | 2003-05-29 | 2010-12-07 | Panasonic Corporation | Image capturing apparatus |
US20100328485A1 (en) | 2008-03-31 | 2010-12-30 | Panasonic Corporation | Imaging device, imaging module, electronic still camera, and electronic movie camera |
US20110001858A1 (en) | 2008-02-22 | 2011-01-06 | Dai Shintani | Imaging apparatus |
US20110018903A1 (en) | 2004-08-03 | 2011-01-27 | Silverbrook Research Pty Ltd | Augmented reality device for presenting virtual imagery registered to a viewed surface |
US20110019056A1 (en) | 2009-07-26 | 2011-01-27 | Massachusetts Institute Of Technology | Bi-Directional Screen |
WO2011010234A1 (en) | 2009-07-23 | 2011-01-27 | Philips Lumileds Lighting Company, Llc | Led with molded reflective sidewall coating |
US20110025827A1 (en) | 2009-07-30 | 2011-02-03 | Primesense Ltd. | Depth Mapping Based on Pattern Matching and Stereoscopic Information |
US20110032338A1 (en) | 2009-08-06 | 2011-02-10 | Qualcomm Incorporated | Encapsulating three-dimensional video data in accordance with transport protocols |
US20110050864A1 (en) | 2009-09-01 | 2011-03-03 | Prime Focus Vfx Services Ii Inc. | System and process for transforming two-dimensional images into three-dimensional images |
US20110050909A1 (en) | 2009-09-01 | 2011-03-03 | Geovector Corporation | Photographer's guidance systems |
US20110063414A1 (en) * | 2009-09-16 | 2011-03-17 | Xuemin Chen | Method and system for frame buffer compression and memory resource reduction for 3d video |
WO2011029209A2 (en) | 2009-09-10 | 2011-03-17 | Liberovision Ag | Method and apparatus for generating and processing depth-enhanced images |
US20110069175A1 (en) | 2009-08-10 | 2011-03-24 | Charles Mistretta | Vision system and method for motion adaptive integration of image frames |
US20110075729A1 (en) | 2006-12-28 | 2011-03-31 | Gokce Dane | method and apparatus for automatic visual artifact analysis and artifact reduction |
US20110090255A1 (en) | 2009-10-16 | 2011-04-21 | Wilson Diego A | Content boundary signaling techniques |
US20110091192A1 (en) | 2008-06-30 | 2011-04-21 | Nikon Corporation | Focus detecting apparatus and imaging apparatus |
US7936377B2 (en) | 2007-04-30 | 2011-05-03 | Tandent Vision Science, Inc. | Method and system for optimizing an image for improved analysis of material and illumination image features |
US7941634B2 (en) | 2006-12-01 | 2011-05-10 | Thomson Licensing | Array of processing elements with local registers |
US7945653B2 (en) | 2006-10-11 | 2011-05-17 | Facebook, Inc. | Tagging digital media |
US7949252B1 (en) | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US20110123183A1 (en) | 2008-04-04 | 2011-05-26 | Eth Zurich | Spatially adaptive photographic flash unit |
US20110129120A1 (en) | 2009-12-02 | 2011-06-02 | Canon Kabushiki Kaisha | Processing captured images having geolocations |
US20110129165A1 (en) | 2009-11-27 | 2011-06-02 | Samsung Electronics Co., Ltd. | Image processing apparatus and method |
US20110133649A1 (en) | 2009-12-07 | 2011-06-09 | At&T Intellectual Property I, L.P. | Mechanisms for light management |
US20110148764A1 (en) | 2009-12-18 | 2011-06-23 | Avago Technologies Ecbu Ip (Singapore) Pte. Ltd. | Optical navigation system and method for performing self-calibration on the system using a calibration cover |
US20110149074A1 (en) | 2009-12-18 | 2011-06-23 | Electronics And Telecommunications Research Institute | Portable multi-view image acquisition system and multi-view image preprocessing method |
WO2011081187A1 (en) | 2009-12-28 | 2011-07-07 | 株式会社ニコン | Image capture element, and image capture device |
JP2011135170A (en) | 2009-12-22 | 2011-07-07 | Samsung Electronics Co Ltd | Imaging apparatus and imaging method |
US20110169994A1 (en) | 2009-10-19 | 2011-07-14 | Pixar | Super light-field lens |
US7982776B2 (en) | 2007-07-13 | 2011-07-19 | Ethicon Endo-Surgery, Inc. | SBI motion artifact removal apparatus and method |
US20110194617A1 (en) | 2010-02-11 | 2011-08-11 | Nokia Corporation | Method and Apparatus for Providing Multi-Threaded Video Decoding |
US20110205384A1 (en) | 2010-02-24 | 2011-08-25 | Panavision Imaging, Llc | Variable active image area image sensor |
US20110221947A1 (en) | 2009-11-20 | 2011-09-15 | Kouhei Awazu | Solid-state imaging device |
US20110242334A1 (en) | 2010-04-02 | 2011-10-06 | Microsoft Corporation | Time Interleaved Exposures And Multiplexed Illumination |
US20110242352A1 (en) | 2010-03-30 | 2011-10-06 | Nikon Corporation | Image processing method, computer-readable storage medium, image processing apparatus, and imaging apparatus |
US20110249341A1 (en) | 2009-10-19 | 2011-10-13 | Pixar | Super light-field lens with doublet lenslet array element |
US20110261164A1 (en) | 2008-12-05 | 2011-10-27 | Unisensor A/S | Optical sectioning of a sample and detection of particles in a sample |
US20110261205A1 (en) | 2010-04-23 | 2011-10-27 | Hon Hai Precision Industry Co., Ltd. | Method for coordinating camera array |
US20110267263A1 (en) | 2000-07-17 | 2011-11-03 | Microsoft Corporation | Changing input tolerances based on device movement |
US20110267348A1 (en) | 2010-04-29 | 2011-11-03 | Dennis Lin | Systems and methods for generating a virtual camera viewpoint for an image |
US20110273466A1 (en) | 2010-05-10 | 2011-11-10 | Canon Kabushiki Kaisha | View-dependent rendering system with intuitive mixed reality |
US20110279479A1 (en) | 2009-03-03 | 2011-11-17 | Rodriguez Tony F | Narrowcasting From Public Displays, and Related Methods |
US20110292258A1 (en) | 2010-05-28 | 2011-12-01 | C2Cure, Inc. | Two sensor imaging systems |
US20110293179A1 (en) | 2010-05-31 | 2011-12-01 | Mert Dikmen | Systems and methods for illumination correction of an image |
US20110304745A1 (en) | 2010-06-10 | 2011-12-15 | Microsoft Corporation | Light transport reconstruction from sparsely captured images |
US20110311046A1 (en) | 2010-06-21 | 2011-12-22 | Kyocera Mita Corporation | Image Forming System, Image Forming Apparatus, and Method in which an Application is Added |
US8085391B2 (en) | 2007-08-02 | 2011-12-27 | Aptina Imaging Corporation | Integrated optical characteristic measurements in a CMOS image sensor |
US20110316968A1 (en) | 2010-06-29 | 2011-12-29 | Yuichi Taguchi | Digital Refocusing for Wide-Angle Images Using Axial-Cone Cameras |
US20120014837A1 (en) | 2010-02-19 | 2012-01-19 | Pacific Biosciences Of California, Inc. | Illumination of integrated analytical systems |
US8106856B2 (en) | 2006-09-06 | 2012-01-31 | Apple Inc. | Portable electronic device for photo management |
US8115814B2 (en) | 2004-09-14 | 2012-02-14 | Canon Kabushiki Kaisha | Mobile tracking system, camera and photographing method |
US20120044330A1 (en) | 2010-04-21 | 2012-02-23 | Tatsumi Watanabe | Stereoscopic video display apparatus and stereoscopic video display method |
US20120050562A1 (en) | 2009-04-22 | 2012-03-01 | Raytrix Gmbh | Digital imaging system, plenoptic optical device and image data processing method |
US20120056889A1 (en) | 2010-09-07 | 2012-03-08 | Microsoft Corporation | Alternate source for controlling an animation |
US20120056982A1 (en) | 2010-09-08 | 2012-03-08 | Microsoft Corporation | Depth camera based on structured light and stereo vision |
US20120057040A1 (en) | 2010-05-11 | 2012-03-08 | Byung Kwan Park | Apparatus and method for processing light field data using a mask with an attenuation pattern |
US20120057806A1 (en) | 2010-05-31 | 2012-03-08 | Erik Johan Vendel Backlund | User interface with three dimensional user input |
US20120062755A1 (en) | 2010-03-31 | 2012-03-15 | Sony Corporation | Camera system, signal delay amount adjusting method and program |
US8155478B2 (en) | 2006-10-26 | 2012-04-10 | Broadcom Corporation | Image creation with software controllable depth of field |
US8155456B2 (en) | 2008-04-29 | 2012-04-10 | Adobe Systems Incorporated | Method and apparatus for block-based compression of light-field images |
US20120120240A1 (en) | 2009-10-21 | 2012-05-17 | Panasonic Corporation | Video image conversion device and image capture device |
US8189089B1 (en) | 2009-01-20 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for reducing plenoptic camera artifacts |
US20120132803A1 (en) | 2009-08-10 | 2012-05-31 | Hitachi High-Technologies Corporation | Charged particle beam device and image display method |
US20120133746A1 (en) | 2010-11-29 | 2012-05-31 | DigitalOptics Corporation Europe Limited | Portrait Image Synthesis from Multiple Images Captured on a Handheld Device |
US20120147205A1 (en) | 2010-12-14 | 2012-06-14 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using super-resolution processes |
US20120176481A1 (en) | 2008-02-29 | 2012-07-12 | Disney Enterprises, Inc. | Processing image data from multiple cameras for motion pictures |
US20120183055A1 (en) | 2011-01-18 | 2012-07-19 | Vidyo, Inc. | Temporal Prediction Structure Aware Temporal Filter |
US8228417B1 (en) | 2009-07-15 | 2012-07-24 | Adobe Systems Incorporated | Focused plenoptic camera employing different apertures or filtering at different microlenses |
US20120188344A1 (en) | 2011-01-20 | 2012-07-26 | Canon Kabushiki Kaisha | Systems and methods for collaborative image capturing |
US20120201475A1 (en) | 2009-10-05 | 2012-08-09 | I.C.V.T. Ltd. | Method and system for processing an image |
US20120206574A1 (en) | 2011-02-15 | 2012-08-16 | Nintendo Co., Ltd. | Computer-readable storage medium having stored therein display control program, display control apparatus, display control system, and display control method |
US20120218463A1 (en) | 2007-10-12 | 2012-08-30 | Microsoft Corporation | Multi-spectral imaging |
US8259198B2 (en) | 2009-10-20 | 2012-09-04 | Apple Inc. | System and method for detecting and correcting defective pixels in an image sensor |
US20120224787A1 (en) | 2011-03-02 | 2012-09-06 | Canon Kabushiki Kaisha | Systems and methods for image capturing |
US8264546B2 (en) | 2008-11-28 | 2012-09-11 | Sony Corporation | Image processing system for estimating camera parameters |
US20120229691A1 (en) | 2011-03-10 | 2012-09-13 | Canon Kabushiki Kaisha | Image pickup apparatus having lens array and image pickup optical system |
US20120237222A9 (en) | 2007-02-01 | 2012-09-20 | Alliance Fiber Optic Products, Inc. | Micro Free-Space WDM Device |
US8279325B2 (en) | 2008-11-25 | 2012-10-02 | Lytro, Inc. | System and method for acquiring, editing, generating and outputting video data |
US20120249529A1 (en) | 2011-03-31 | 2012-10-04 | Fujifilm Corporation | 3d image displaying apparatus, 3d image displaying method, and 3d image displaying program |
US20120251131A1 (en) | 2011-03-31 | 2012-10-04 | Henderson Thomas A | Compensating for periodic nonuniformity in electrophotographic printer |
US20120249819A1 (en) | 2011-03-28 | 2012-10-04 | Canon Kabushiki Kaisha | Multi-modal image capture |
US20120249550A1 (en) | 2009-04-18 | 2012-10-04 | Lytro, Inc. | Selective Transmission of Image Data Based on Device Attributes |
US20120257795A1 (en) | 2011-04-08 | 2012-10-11 | Lg Electronics Inc. | Mobile terminal and image depth control method thereof |
US20120257065A1 (en) | 2011-04-08 | 2012-10-11 | Qualcomm Incorporated | Systems and methods to calibrate a multi camera device |
US8290358B1 (en) | 2007-06-25 | 2012-10-16 | Adobe Systems Incorporated | Methods and apparatus for light-field imaging |
US8289440B2 (en) | 2008-12-08 | 2012-10-16 | Lytro, Inc. | Light field data acquisition devices, and methods of using and manufacturing same |
US20120272271A1 (en) | 2009-10-30 | 2012-10-25 | Sony Computer Entertainment Inc. | Information Processing Apparatus, Tuner, And Information Processing Method |
US20120268367A1 (en) | 2003-03-21 | 2012-10-25 | Roel Vertegaal | Method and Apparatus for Communication Between Humans and Devices |
US20120269274A1 (en) | 2009-10-01 | 2012-10-25 | Sk Telecom Co., Ltd. | Method and apparatus for encoding/decoding video using split layer |
US20120271115A1 (en) | 2011-04-21 | 2012-10-25 | Andre Buerk | Light-conducting device for an endoscope |
US8310554B2 (en) | 2005-09-20 | 2012-11-13 | Sri International | Method and apparatus for performing coordinated multi-PTZ camera tracking |
US20120287246A1 (en) | 2011-05-11 | 2012-11-15 | Canon Kabushiki Kaisha | Image processing apparatus capable of displaying image indicative of face area, method of controlling the image processing apparatus, and storage medium |
US20120287296A1 (en) | 2011-05-10 | 2012-11-15 | Canon Kabushiki Kaisha | Imaging apparatus, method of controlling the same, and program |
US20120287329A1 (en) | 2011-05-09 | 2012-11-15 | Canon Kabushiki Kaisha | Image processing apparatus and method thereof |
US8315476B1 (en) | 2009-01-20 | 2012-11-20 | Adobe Systems Incorporated | Super-resolution with the focused plenoptic camera |
US20120293075A1 (en) | 2010-01-29 | 2012-11-22 | Koninklijke Philips Electronics, N.V. | Interactive lighting control system and method |
US20120300091A1 (en) | 2011-05-23 | 2012-11-29 | Shroff Sapna A | Focusing and Focus Metrics for a Plenoptic Imaging System |
US20120321172A1 (en) | 2010-02-26 | 2012-12-20 | Jachalsky Joern | Confidence map, method for generating the same and method for refining a disparity map |
US8345144B1 (en) | 2009-07-15 | 2013-01-01 | Adobe Systems Incorporated | Methods and apparatus for rich image capture with focused plenoptic cameras |
US20130002902A1 (en) | 2011-06-30 | 2013-01-03 | Nikon Corporation | Flare determination apparatus, image processing apparatus, and storage medium storing flare determination program |
US20130002936A1 (en) | 2011-06-30 | 2013-01-03 | Nikon Corporation | Image pickup apparatus, image processing apparatus, and storage medium storing image processing program |
US20130021486A1 (en) | 2011-07-22 | 2013-01-24 | Naturalpoint, Inc. | Hosted camera remote control |
US20130041215A1 (en) | 2011-08-12 | 2013-02-14 | Ian McDowall | Feature differentiation image capture unit and method in a surgical instrument |
US20130038696A1 (en) | 2011-08-10 | 2013-02-14 | Yuanyuan Ding | Ray Image Modeling for Fast Catadioptric Light Field Rendering |
US20130044290A1 (en) | 2010-04-21 | 2013-02-21 | Panasonic Corporation | Visual function testing device |
US20130050546A1 (en) | 2011-08-30 | 2013-02-28 | Canon Kabushiki Kaisha | Image processing apparatus and method |
US20130064453A1 (en) | 2011-09-08 | 2013-03-14 | Casio Computer Co., Ltd. | Interpolation image generation apparatus, reconstructed image generation apparatus, method of generating interpolation image, and computer-readable recording medium storing program |
US20130064532A1 (en) | 2011-09-13 | 2013-03-14 | J. Brian Caldwell | Optical attachment for reducing the focal length of an objective lens |
US8400555B1 (en) | 2009-12-01 | 2013-03-19 | Adobe Systems Incorporated | Focused plenoptic camera employing microlenses with different focal lengths |
US8400533B1 (en) | 2010-02-23 | 2013-03-19 | Xilinx, Inc. | Methods of reducing aberrations in a digital image |
US20130070060A1 (en) | 2011-09-19 | 2013-03-21 | Pelican Imaging Corporation | Systems and methods for determining depth from multiple views of a scene that include aliasing using hypothesized fusion |
US20130070059A1 (en) | 2011-03-31 | 2013-03-21 | Sony Corporation | Image processing device, image processing method and image processing computer program product |
US20130077880A1 (en) | 2011-09-28 | 2013-03-28 | Pelican Imaging Corporation | Systems and methods for encoding light field image files |
US8411948B2 (en) | 2010-03-05 | 2013-04-02 | Microsoft Corporation | Up-sampling binary images for segmentation |
US20130082905A1 (en) | 2011-01-18 | 2013-04-04 | Disney Enterprises, Inc. | Multi-layer plenoptic displays that combine multiple emissive and light modulating planes |
US20130088616A1 (en) | 2011-10-10 | 2013-04-11 | Apple Inc. | Image Metadata Control Based on Privacy Rules |
US20130093859A1 (en) | 2010-04-28 | 2013-04-18 | Fujifilm Corporation | Stereoscopic image reproduction device and method, stereoscopic image capturing device, and stereoscopic display device |
US20130094101A1 (en) | 2010-06-29 | 2013-04-18 | Kowa Company Ltd. | Telephoto lens unit |
US20130093844A1 (en) | 2011-10-14 | 2013-04-18 | Kabushiki Kaisha Toshiba | Electronic apparatus and display control method |
US8427548B2 (en) | 2008-06-18 | 2013-04-23 | Samsung Electronics Co., Ltd. | Apparatus and method for capturing digital images |
US20130107085A1 (en) | 2006-02-07 | 2013-05-02 | The Board Of Trustees Of The Leland Stanford Junior University | Correction of Optical Aberrations |
US20130113981A1 (en) | 2006-12-01 | 2013-05-09 | Lytro, Inc. | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
US8442397B2 (en) | 2009-09-22 | 2013-05-14 | Samsung Electronics Co., Ltd. | Modulator, apparatus for obtaining light field data using modulator, and apparatus and method for processing light field data using modulator |
US20130120605A1 (en) | 2010-03-03 | 2013-05-16 | Todor G. Georgiev | Methods, Apparatus, and Computer-Readable Storage Media for Blended Rendering of Focused Plenoptic Camera Data |
US20130120636A1 (en) | 2011-11-10 | 2013-05-16 | Apple Inc. | Illumination system |
US20130121577A1 (en) | 2009-10-30 | 2013-05-16 | Jue Wang | Methods and Apparatus for Chatter Reduction in Video Object Segmentation Using Optical Flow Assisted Gaussholding |
US20130129213A1 (en) | 2011-08-31 | 2013-05-23 | Elya Shechtman | Non-Rigid Dense Correspondence |
US20130127901A1 (en) | 2010-08-27 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Calibrating Focused Plenoptic Camera Data |
US20130128087A1 (en) | 2010-08-27 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Super-Resolution in Integral Photography |
US20130128052A1 (en) | 2009-11-17 | 2013-05-23 | Telefonaktiebolaget L M Ericsson (Publ) | Synchronization of Cameras for Multi-View Session Capturing |
US20130135448A1 (en) | 2011-11-28 | 2013-05-30 | Sony Corporation | Image processing device and method, recording medium, and program |
US20130176481A1 (en) | 2012-01-09 | 2013-07-11 | Lifetouch Inc. | Video Photography System |
US8494304B2 (en) | 2007-05-11 | 2013-07-23 | Xerox Corporation | Punched hole detection and removal |
US20130188068A1 (en) | 2010-10-06 | 2013-07-25 | Hewlett-Packard Development Company, L.P. | Systems and methods for acquiring and processing image data produced by camera arrays |
US20130215226A1 (en) | 2010-09-22 | 2013-08-22 | Laurent Chauvier | Enriched Digital Photographs |
US20130215108A1 (en) | 2012-02-21 | 2013-08-22 | Pelican Imaging Corporation | Systems and Methods for the Manipulation of Captured Light Field Image Data |
US20130222656A1 (en) | 2012-02-28 | 2013-08-29 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
US20130234935A1 (en) | 2010-10-26 | 2013-09-12 | Bae Systems Plc | Display assembly |
US20130243391A1 (en) | 2010-11-23 | 2013-09-19 | Samsung Electronics Co., Ltd. | Method and apparatus for creating a media file for multilayer images in a multimedia system, and media-file-reproducing apparatus using same |
US20130242137A1 (en) | 2010-11-25 | 2013-09-19 | Lester Kirkland | Imaging robot |
US20130262511A1 (en) | 2012-04-02 | 2013-10-03 | Google Inc. | Determining 3D Model Information From Stored Images |
US20130258451A1 (en) | 2012-03-27 | 2013-10-03 | Ostendo Technologies, Inc. | Spatio-Temporal Directional Light Modulator |
US8559705B2 (en) | 2006-12-01 | 2013-10-15 | Lytro, Inc. | Interactive refocusing of electronic images |
US20130286236A1 (en) | 2012-04-27 | 2013-10-31 | Research In Motion Limited | System and method of adjusting camera image data |
US8581998B2 (en) | 2010-12-17 | 2013-11-12 | Canon Kabushiki Kaisha | Image sensing apparatus and method of controlling the image sensing apparatus |
US8589374B2 (en) | 2009-03-16 | 2013-11-19 | Apple Inc. | Multifunction device with integrated search and application selection |
US8593564B2 (en) | 2011-09-22 | 2013-11-26 | Apple Inc. | Digital camera including refocusable imaging mode adaptor |
US20130321581A1 (en) | 2012-06-01 | 2013-12-05 | Ostendo Technologies, Inc. | Spatio-Temporal Light Field Cameras |
US20130321574A1 (en) | 2012-06-04 | 2013-12-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US20130321677A1 (en) | 2012-05-31 | 2013-12-05 | Apple Inc. | Systems and methods for raw image processing |
US8605199B2 (en) | 2011-06-28 | 2013-12-10 | Canon Kabushiki Kaisha | Adjustment of imaging properties for an imaging assembly having light-field optics |
US20130329132A1 (en) | 2012-06-06 | 2013-12-12 | Apple Inc. | Flare Detection and Mitigation in Panoramic Images |
US20130329107A1 (en) | 2012-06-11 | 2013-12-12 | Disney Enterprises, Inc. | Streaming Light Propagation |
US20130335596A1 (en) | 2012-06-15 | 2013-12-19 | Microsoft Corporation | Combining multiple images in bracketed photography |
US20130342700A1 (en) | 2012-06-26 | 2013-12-26 | Aharon Kass | System and method for using pattern matching to determine the presence of designated objects in digital images |
US8619082B1 (en) | 2012-08-21 | 2013-12-31 | Pelican Imaging Corporation | Systems and methods for parallax detection and correction in images captured using array cameras that contain occlusions using subsets of images to perform depth estimation |
US20140003719A1 (en) | 2012-06-29 | 2014-01-02 | Xue Bai | Adaptive Trimap Propagation for Video Matting |
US20140002502A1 (en) | 2012-06-27 | 2014-01-02 | Samsung Electronics Co., Ltd. | Method and apparatus for outputting graphics to a display |
US20140002699A1 (en) | 2012-06-27 | 2014-01-02 | Honeywell International Inc. doing business as (d.b.a) Honeywell Scanning and Mobility | Imaging apparatus having imaging lens |
US8629930B2 (en) | 2009-10-14 | 2014-01-14 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Device, image processing device and method for optical imaging |
US20140035959A1 (en) | 2012-08-04 | 2014-02-06 | Paul Lapstun | Light Field Display Device and Method |
US20140037280A1 (en) | 2009-02-10 | 2014-02-06 | Canon Kabushiki Kaisha | Imaging apparatus, flash device, and control method thereof |
US20140059462A1 (en) | 2011-05-04 | 2014-02-27 | Sony Ericsson Mobile Communications Ab | Method, graphical user interface, and computer program product for processing of a light field image |
US8665440B1 (en) | 2011-02-10 | 2014-03-04 | Physical Optics Corporation | Pseudo-apposition eye spectral imaging system |
US8675073B2 (en) | 2001-11-08 | 2014-03-18 | Kenneth Joseph Aagaard | Video system and methods for operating a video system |
US20140085282A1 (en) | 2012-09-21 | 2014-03-27 | Nvidia Corporation | See-through optical image processing |
US20140092424A1 (en) | 2012-09-28 | 2014-04-03 | Interactive Memories, Inc. | Methods for Real Time Discovery, Selection, and Engagement of Most Economically Feasible Printing Service Vendors among Multiple Known Vendors |
US20140098191A1 (en) | 2012-10-05 | 2014-04-10 | Vidinoti Sa | Annotation method and apparatus |
US20140133749A1 (en) | 2012-05-31 | 2014-05-15 | Apple Inc. | Systems And Methods For Statistics Collection Using Pixel Mask |
US20140139538A1 (en) | 2012-11-19 | 2014-05-22 | Datacolor Holding Ag | Method and apparatus for optimizing image quality based on measurement of image processing artifacts |
US8736710B2 (en) | 2012-05-24 | 2014-05-27 | International Business Machines Corporation | Automatic exposure control for flash photography |
US8736751B2 (en) | 2008-08-26 | 2014-05-27 | Empire Technology Development Llc | Digital presenter for displaying image captured by camera with illumination system |
US8749620B1 (en) | 2010-02-20 | 2014-06-10 | Lytro, Inc. | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same |
US8750509B2 (en) | 2004-09-23 | 2014-06-10 | Smartvue Corporation | Wireless surveillance system releasably mountable to track lighting |
US20140167196A1 (en) | 2012-11-02 | 2014-06-19 | Heptagon Micro Optics Pte. Ltd. | Optical modules including focal length adjustment and fabrication of the optical modules |
US20140168484A1 (en) | 2012-09-11 | 2014-06-19 | Satoshi Suzuki | Image processing apparatus, image processing method, and program, and image pickup apparatus with image processing apparatus |
US20140176710A1 (en) | 2009-01-05 | 2014-06-26 | Duke University | Multiscale telescopic imaging system |
US20140177905A1 (en) | 2012-12-20 | 2014-06-26 | United Video Properties, Inc. | Methods and systems for customizing a plenoptic media asset |
US20140176540A1 (en) | 2012-12-20 | 2014-06-26 | Ricoh Co., Ltd. | Occlusion-Aware Reconstruction of Three-Dimensional Scenes from Light Field Images |
US20140176592A1 (en) | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
US8768102B1 (en) | 2011-02-09 | 2014-07-01 | Lytro, Inc. | Downsampling light field images |
US20140184885A1 (en) | 2012-12-28 | 2014-07-03 | Canon Kabushiki Kaisha | Image capture apparatus and method for controlling the same |
US20140195921A1 (en) | 2012-09-28 | 2014-07-10 | Interactive Memories, Inc. | Methods and systems for background uploading of media files for improved user experience in production of media-based products |
US20140192208A1 (en) | 2013-01-08 | 2014-07-10 | Peripheral Vision, Inc. | Lighting System Characterization |
US20140193047A1 (en) | 2012-09-28 | 2014-07-10 | Interactive Memories, Inc. | Systems and methods for generating autoflow of content based on image and user analysis as well as use case data for a media-based printable product |
US20140204111A1 (en) | 2013-01-18 | 2014-07-24 | Karthik Vaidyanathan | Layered light field reconstruction for defocus blur |
US8797321B1 (en) | 2009-04-01 | 2014-08-05 | Microsoft Corporation | Augmented lighting environments |
US20140218540A1 (en) | 2013-02-05 | 2014-08-07 | Google Inc. | Noise Models for Image Processing |
US20140226038A1 (en) | 2013-02-12 | 2014-08-14 | Canon Kabushiki Kaisha | Image processing apparatus, image capturing apparatus, control method, and recording medium |
US8811769B1 (en) | 2012-02-28 | 2014-08-19 | Lytro, Inc. | Extended depth of field and variable center of perspective in light-field processing |
US20140245367A1 (en) | 2012-08-10 | 2014-08-28 | Panasonic Corporation | Method for providing a video, transmitting device, and receiving device |
US20140240578A1 (en) | 2013-02-22 | 2014-08-28 | Lytro, Inc. | Light-field based autofocus |
US8831377B2 (en) | 2012-02-28 | 2014-09-09 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
US20140267639A1 (en) | 2013-03-13 | 2014-09-18 | Kabushiki Kaisha Toshiba | Image display apparatus |
US20140267243A1 (en) | 2013-03-13 | 2014-09-18 | Pelican Imaging Corporation | Systems and Methods for Synthesizing Images from Image Data Captured by an Array Camera Using Restricted Depth of Field Depth Maps in which Depth Estimation Precision Varies |
US8848970B2 (en) | 2011-04-26 | 2014-09-30 | Digimarc Corporation | Salient point-based arrangements |
US20140300753A1 (en) | 2013-04-04 | 2014-10-09 | Apple Inc. | Imaging pipeline for spectro-colorimeters |
US8860856B2 (en) | 2009-01-19 | 2014-10-14 | Dolby Laboratories Licensing Corporation | Multiplexed imaging |
US20140313350A1 (en) | 2013-04-19 | 2014-10-23 | Aptina Imaging Corporation | Imaging systems with reference pixels for image flare mitigation |
US20140333787A1 (en) | 2008-05-20 | 2014-11-13 | Pelican Imaging Corporation | Systems and Methods for Parallax Measurement Using Camera Arrays Incorporating 3 x 3 Camera Configurations |
US20140340390A1 (en) | 2013-05-17 | 2014-11-20 | Nvidia Corporation | System, method, and computer program product to produce images for a near-eye light field display having a defect |
US20140347540A1 (en) | 2013-05-23 | 2014-11-27 | Samsung Electronics Co., Ltd | Image display method, image display apparatus, and recording medium |
US8903232B1 (en) | 2013-08-05 | 2014-12-02 | Caldwell Photographic, Inc. | Optical attachment for reducing the focal length of an objective lens |
US20140354863A1 (en) | 2013-05-31 | 2014-12-04 | Samsung Electronics Co., Ltd. | Image Sensors and Imaging Devices Including the Same |
US8908058B2 (en) | 2009-04-18 | 2014-12-09 | Lytro, Inc. | Storage and transmission of pictures including multiple frames |
US20140368494A1 (en) | 2013-06-18 | 2014-12-18 | Nvidia Corporation | Method and system for rendering simulated depth-of-field visual effect |
US20140368640A1 (en) | 2012-02-29 | 2014-12-18 | Flir Systems Ab | Method and system for performing alignment of a projection image to detected infrared (ir) radiation information |
US8948545B2 (en) | 2012-02-28 | 2015-02-03 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US8953882B2 (en) | 2012-05-31 | 2015-02-10 | Apple Inc. | Systems and methods for determining noise statistics of image data |
US20150062386A1 (en) | 2012-05-10 | 2015-03-05 | Fujifilm Corporation | Imaging device and signal correcting method |
US20150062178A1 (en) | 2013-09-05 | 2015-03-05 | Facebook, Inc. | Tilting to scroll |
US8988317B1 (en) | 2014-06-12 | 2015-03-24 | Lytro, Inc. | Depth determination for light field images |
US8995785B2 (en) | 2012-02-28 | 2015-03-31 | Lytro, Inc. | Light-field processing and analysis, camera control, and user interfaces and interaction on light-field capture devices |
US8997021B2 (en) | 2012-11-06 | 2015-03-31 | Lytro, Inc. | Parallax and/or three-dimensional effects for thumbnail image displays |
US20150092071A1 (en) | 2013-09-28 | 2015-04-02 | Ricoh Co., Ltd. | Color filter modules for plenoptic xyz imaging systems |
US9001226B1 (en) | 2012-12-04 | 2015-04-07 | Lytro, Inc. | Capturing and relighting images using multiple devices |
US20150104101A1 (en) | 2013-10-14 | 2015-04-16 | Apple Inc. | Method and ui for z depth image segmentation |
US9013611B1 (en) | 2013-09-06 | 2015-04-21 | Xilinx, Inc. | Method and device for generating a digital image based upon a selected set of chrominance groups |
US20150130986A1 (en) | 2012-04-25 | 2015-05-14 | Nikon Corporation | Focus detection device, focus adjustment device and camera |
US20150161798A1 (en) | 2013-03-15 | 2015-06-11 | Pelican Imaging Corporation | Array Cameras Including an Array Camera Module Augmented with a Separate Camera |
US20150193937A1 (en) | 2013-12-12 | 2015-07-09 | Qualcomm Incorporated | Method and apparatus for generating plenoptic depth maps |
US20150207990A1 (en) | 2012-08-20 | 2015-07-23 | The Regents Of The University Of California | Monocentric lens designs and associated imaging systems having wide field of view and high resolution |
US20150206340A1 (en) | 2014-01-17 | 2015-07-23 | Carl J. Munkberg | Layered Reconstruction for Defocus and Motion Blur |
US9106914B2 (en) | 2006-05-25 | 2015-08-11 | Thomson Licensing | Method and system for weighted encoding |
US20150223731A1 (en) | 2013-10-09 | 2015-08-13 | Nedim T. SAHIN | Systems, environment and methods for identification and analysis of recurring transitory physiological states and events using a wearable data collection device |
US20150237273A1 (en) | 2012-12-05 | 2015-08-20 | Fujifilm Corporation | Image capture device, anomalous oblique incident light detection method, and recording medium |
US20150264337A1 (en) | 2013-03-15 | 2015-09-17 | Pelican Imaging Corporation | Autofocus System for a Conventional Camera That Uses Depth Information from an Array Camera |
US20150288867A1 (en) | 2014-04-02 | 2015-10-08 | Canon Kabushiki Kaisha | Image processing apparatus, image capturing apparatus, and control method thereof |
US20150304544A1 (en) | 2014-04-16 | 2015-10-22 | Canon Kabushiki Kaisha | Image pickup apparatus, image processing method, and recording medium |
US20150304667A1 (en) | 2013-01-04 | 2015-10-22 | GE Video Compression, LLC. | Efficient scalable coding concept |
US20150310592A1 (en) | 2014-04-25 | 2015-10-29 | Canon Kabushiki Kaisha | Image processing apparatus that performs image restoration processing and image processing method |
US20150312593A1 (en) | 2014-04-24 | 2015-10-29 | Lytro, Inc. | Compression of light field images |
US9184199B2 (en) | 2011-08-01 | 2015-11-10 | Lytro, Inc. | Optical assembly including plenoptic microlens array |
US20150334420A1 (en) | 2014-05-13 | 2015-11-19 | Alcatel Lucent | Method and apparatus for encoding and decoding video |
US9201193B1 (en) | 2013-02-18 | 2015-12-01 | Exelis, Inc. | Textured fiber optic coupled image intensified camera |
US9201142B2 (en) | 2013-03-14 | 2015-12-01 | Navico Holding As | Sonar and radar display |
US20150346832A1 (en) | 2014-05-29 | 2015-12-03 | Nextvr Inc. | Methods and apparatus for delivering content and/or playing back content |
US9210391B1 (en) | 2014-07-31 | 2015-12-08 | Apple Inc. | Sensor data rescaler with chroma reduction |
US9214013B2 (en) | 2012-09-14 | 2015-12-15 | Pelican Imaging Corporation | Systems and methods for correcting user identified artifacts in light field images |
US20150370011A1 (en) | 2014-06-18 | 2015-12-24 | Canon Kabushiki Kaisha | Image pickup apparatus |
US20150373279A1 (en) | 2014-06-20 | 2015-12-24 | Qualcomm Incorporated | Wide field of view array camera for hemispheric and spherical imaging |
US20150370012A1 (en) | 2014-06-18 | 2015-12-24 | Canon Kabushiki Kaisha | Imaging apparatus |
US20160029017A1 (en) | 2012-02-28 | 2016-01-28 | Lytro, Inc. | Calibration of light-field camera geometry via robust fitting |
US20160029002A1 (en) | 2014-07-26 | 2016-01-28 | Soeren Balko | Platform-agnostic Video Player For Mobile Computing Devices And Desktop Computers |
US20160037178A1 (en) | 2013-04-05 | 2016-02-04 | Samsung Electronics Co., Ltd. | Video encoding method and apparatus thereof and a video decoding method and apparatus thereof |
US9262067B1 (en) | 2012-12-10 | 2016-02-16 | Amazon Technologies, Inc. | Approaches for displaying alternate views of information |
US20160065931A1 (en) | 2013-05-14 | 2016-03-03 | Huawei Technologies Co., Ltd. | Method and Apparatus for Computing a Synthesized Picture |
US20160065947A1 (en) | 2014-09-03 | 2016-03-03 | Nextvr Inc. | Methods and apparatus for receiving and/or playing back content |
US9294662B2 (en) | 2013-10-16 | 2016-03-22 | Broadcom Corporation | Depth map generation and post-capture focusing |
US9300932B2 (en) | 2012-05-09 | 2016-03-29 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US9305375B2 (en) | 2014-03-25 | 2016-04-05 | Lytro, Inc. | High-quality post-rendering depth blur |
US20160142615A1 (en) | 2014-11-13 | 2016-05-19 | Lytro, Inc. | Robust layered light-field rendering |
US20160155215A1 (en) | 2014-11-27 | 2016-06-02 | Samsung Display Co., Ltd. | Image processing device, and an image processing method |
US20160165206A1 (en) | 2014-12-03 | 2016-06-09 | National Tsing Hua University | Digital refocusing method |
US20160182893A1 (en) | 2014-12-22 | 2016-06-23 | Canon Kabushiki Kaisha | Multiscale depth estimation using depth from defocus |
US9392153B2 (en) | 2013-12-24 | 2016-07-12 | Lytro, Inc. | Plenoptic camera resolution |
US20160227244A1 (en) | 2013-09-13 | 2016-08-04 | Canon Kabushiki Kaisha | Method, apparatus and system for encoding and decoding video data |
US20160247324A1 (en) | 2015-02-25 | 2016-08-25 | Brian Mullins | Augmented reality content creation |
US20160253837A1 (en) | 2015-02-26 | 2016-09-01 | Lytro, Inc. | Parallax bounce |
US20160269620A1 (en) | 2013-04-22 | 2016-09-15 | Lytro, Inc. | Phase detection autofocus using subaperture images |
US20160307368A1 (en) | 2015-04-17 | 2016-10-20 | Lytro, Inc. | Compression and interactive playback of light field pictures |
US20160309065A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Light guided image plane tiled arrays with dense fiber optic bundles for light-field and high resolution image acquisition |
US20160307372A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Capturing light-field volume image and video data using tiled light-field cameras |
US9497380B1 (en) | 2013-02-15 | 2016-11-15 | Red.Com, Inc. | Dense field imaging |
US20160337635A1 (en) | 2015-05-15 | 2016-11-17 | Semyon Nisenzon | Generarting 3d images using multi-resolution camera set |
US20160353026A1 (en) | 2015-05-29 | 2016-12-01 | Thomson Licensing | Method and apparatus for displaying a light field based image on a user's device, and corresponding computer program product |
US20160353006A1 (en) | 2015-05-29 | 2016-12-01 | Phase One A/S | Adaptive autofocusing system |
US20160381348A1 (en) | 2013-09-11 | 2016-12-29 | Sony Corporation | Image processing device and method |
US20170031146A1 (en) | 2015-07-27 | 2017-02-02 | University Of Connecticut | Imaging Assemblies With Rapid Sample Auto-Focusing |
US20170059305A1 (en) | 2015-08-25 | 2017-03-02 | Lytro, Inc. | Active illumination for enhanced depth map generation |
US20170067832A1 (en) | 2015-09-08 | 2017-03-09 | Xerox Corporation | Methods and devices for improved accuracy of test results |
US20170078578A1 (en) | 2015-09-10 | 2017-03-16 | Canon Kabushiki Kaisha | Image capture apparatus and method of controlling the same |
US9607424B2 (en) | 2012-06-26 | 2017-03-28 | Lytro, Inc. | Depth-assigned content for depth-enhanced pictures |
US20170094906A1 (en) | 2015-10-06 | 2017-04-06 | Deere & Company | System and Method For Clearing A Feeder House and Belt Pickup |
US9635332B2 (en) | 2014-09-08 | 2017-04-25 | Lytro, Inc. | Saturated pixel recovery in light-field images |
US9639945B2 (en) | 2015-08-27 | 2017-05-02 | Lytro, Inc. | Depth-based application of image effects |
US9647150B2 (en) | 2013-05-21 | 2017-05-09 | Jorge Vicente Blasco Claret | Monolithic integration of plenoptic lenses on photosensor substrates |
US20170134639A1 (en) | 2006-12-01 | 2017-05-11 | Lytro, Inc. | Video Refocusing |
US20170221226A1 (en) | 2014-11-04 | 2017-08-03 | SZ DJI Technology Co., Ltd. | Camera calibration |
US20170237971A1 (en) | 2015-04-15 | 2017-08-17 | Lytro, Inc. | Image capture for virtual reality displays |
US20170243373A1 (en) | 2015-04-15 | 2017-08-24 | Lytro, Inc. | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US20170256036A1 (en) | 2016-03-03 | 2017-09-07 | Lytro, Inc. | Automatic microlens array artifact correction for light-field images |
US20170263012A1 (en) | 2016-03-14 | 2017-09-14 | Thomson Licensing | Method and device for processing lightfield data |
US20170302903A1 (en) | 2012-06-26 | 2017-10-19 | Lytro, Inc. | Depth-assigned content for depth-enhanced virtual reality images |
US20170316602A1 (en) | 2014-10-31 | 2017-11-02 | Nokia Technologies Oy | Method for alignment of low-quality noisy depth map to the high-resolution colour image |
US20170358092A1 (en) | 2016-06-09 | 2017-12-14 | Lytro, Inc. | Multi-view scene segmentation and propagation |
US20170365068A1 (en) | 2015-04-15 | 2017-12-21 | Lytro, Inc. | Combining light-field data with active depth data for depth map generation |
US20170374411A1 (en) | 2016-01-17 | 2017-12-28 | Bitmovin Gmbh | Adaptive streaming of an immersive video scene |
US9858649B2 (en) | 2015-09-30 | 2018-01-02 | Lytro, Inc. | Depth-based image blurring |
US20180007253A1 (en) | 2016-06-17 | 2018-01-04 | Canon Kabushiki Kaisha | Focus detection apparatus, focus control apparatus, image capturing apparatus, focus detection method, and storage medium |
US20180012397A1 (en) | 2016-07-08 | 2018-01-11 | Lytro, Inc. | Immersive content framing |
US20180024753A1 (en) | 2016-07-21 | 2018-01-25 | HGST Netherlands B.V. | Internally preconditioning solid state drives for various workloads |
US20180033209A1 (en) | 2015-04-15 | 2018-02-01 | Lytro, Inc. | Stereo image generation and interactive playback |
US9900510B1 (en) | 2016-12-08 | 2018-02-20 | Lytro, Inc. | Motion blur for light-field images |
US20180124371A1 (en) | 2016-10-31 | 2018-05-03 | Verizon Patent And Licensing Inc. | Methods and Systems for Generating Depth Data by Converging Independently-Captured Depth Maps |
US20180139436A1 (en) | 2016-11-11 | 2018-05-17 | Disney Enterprises, Inc. | Object reconstruction from dense light fields via depth from gradients |
US9979909B2 (en) | 2015-07-24 | 2018-05-22 | Lytro, Inc. | Automatic lens flare detection and correction for light-field images |
US20180158198A1 (en) | 2016-12-05 | 2018-06-07 | Lytro, Inc. | Multi-view rotoscope contour propagation |
US20180199039A1 (en) * | 2017-01-11 | 2018-07-12 | Microsoft Technology Licensing, Llc | Reprojecting Holographic Video to Enhance Streaming Bandwidth/Quality |
US10244266B1 (en) * | 2016-02-11 | 2019-03-26 | Amazon Technologies, Inc. | Noisy media content encoding |
-
2017
- 2017-12-05 US US15/832,023 patent/US10567464B2/en active Active
Patent Citations (573)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US725567A (en) | 1902-09-25 | 1903-04-14 | Frederic E Ives | Parallax stereogram and process of making same. |
US4383170A (en) | 1979-11-19 | 1983-05-10 | Tokyo Shibaura Denki Kabushiki Kaisha | Image input device |
US4661986A (en) | 1983-06-27 | 1987-04-28 | Rca Corporation | Depth-of-focus imaging process method |
US4694185A (en) | 1986-04-18 | 1987-09-15 | Eastman Kodak Company | Light sensing devices with lenticular pixels |
US4920419A (en) | 1988-05-31 | 1990-04-24 | Eastman Kodak Company | Zoom lens focus control device for film video player |
US5282045A (en) | 1990-04-27 | 1994-01-25 | Hitachi, Ltd. | Depth-of-field control apparatus and image pickup apparatus having the same therein |
US5077810A (en) | 1990-07-19 | 1991-12-31 | Eastman Kodak Company | Distributed digital signal processing system using standard resolution processors for a high resolution sensor |
US5076687A (en) | 1990-08-28 | 1991-12-31 | Massachusetts Institute Of Technology | Optical ranging apparatus |
US5157465A (en) | 1990-10-11 | 1992-10-20 | Kronberg James W | Universal fiber-optic C.I.E. colorimeter |
US5251019A (en) | 1991-01-25 | 1993-10-05 | Eastman Kodak Company | Solid state color image sensor using a field-staggered color filter pattern |
US6069565A (en) | 1992-10-20 | 2000-05-30 | Rosemount Aerospace Inc. | System for detecting ice or snow on surface which specularly reflects light |
US5757423A (en) | 1993-10-22 | 1998-05-26 | Canon Kabushiki Kaisha | Image taking apparatus |
US5499069A (en) | 1994-05-24 | 1996-03-12 | Eastman Kodak Company | Camera system and an optical adapter to reduce image format size |
US5572034A (en) | 1994-08-08 | 1996-11-05 | University Of Massachusetts Medical Center | Fiber optic plates for generating seamless images |
US5610390A (en) | 1994-10-03 | 1997-03-11 | Fuji Photo Optical Co., Ltd. | Solid-state image pickup device having microlenses each with displaced optical axis |
US5748371A (en) | 1995-02-03 | 1998-05-05 | The Regents Of The University Of Colorado | Extended depth of field optical systems |
US5729471A (en) | 1995-03-31 | 1998-03-17 | The Regents Of The University Of California | Machine dynamic selection of one video camera/image of a scene from multiple video cameras/images of the scene in accordance with a particular perspective on the scene, an object in the scene, or an event in the scene |
US20050141881A1 (en) | 1995-04-14 | 2005-06-30 | Kabushiki Kaisha Toshiba | Recording medium capable of interactive reproducing and reproduction system for the same |
DE19624421A1 (en) | 1995-06-30 | 1997-01-02 | Zeiss Carl Fa | Wave front deformation from object spatially resolved measurement arrangement |
US6023523A (en) | 1996-02-16 | 2000-02-08 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US20010053202A1 (en) | 1996-02-21 | 2001-12-20 | Mazess Richard B. | Densitometry adapter for compact x-ray fluoroscopy machine |
US5949433A (en) | 1996-04-11 | 1999-09-07 | Discreet Logic, Inc. | Processing image data |
US6061083A (en) | 1996-04-22 | 2000-05-09 | Fujitsu Limited | Stereoscopic image display method, multi-viewpoint image capturing method, multi-viewpoint image processing method, stereoscopic image display device, multi-viewpoint image capturing device and multi-viewpoint image processing device |
US5818525A (en) | 1996-06-17 | 1998-10-06 | Loral Fairchild Corp. | RGB image correction using compressed flat illuminated files and a simple one or two point correction algorithm |
US6084979A (en) | 1996-06-20 | 2000-07-04 | Carnegie Mellon University | Method for creating virtual reality |
US6028606A (en) | 1996-08-02 | 2000-02-22 | The Board Of Trustees Of The Leland Stanford Junior University | Camera simulation system |
US6034690A (en) | 1996-08-02 | 2000-03-07 | U.S. Philips Corporation | Post-processing generation of focus/defocus effects for computer graphics images |
US6005936A (en) | 1996-11-28 | 1999-12-21 | Ibm | System for embedding authentication information into an image and an image alteration detecting system |
US5907619A (en) | 1996-12-20 | 1999-05-25 | Intel Corporation | Secure compressed imaging |
US7304670B1 (en) | 1997-03-28 | 2007-12-04 | Hand Held Products, Inc. | Method and apparatus for compensating for fixed pattern noise in an imaging system |
US7079698B2 (en) | 1997-04-01 | 2006-07-18 | Matsushita Electric Industrial Co., Ltd. | Image coding and decoding apparatus, method of image coding and decoding, and recording medium for recording program for image coding and decoding |
US6115556A (en) | 1997-04-10 | 2000-09-05 | Reddington; Terrence P. | Digital camera back accessory and methods of manufacture |
US6529265B1 (en) | 1997-04-14 | 2003-03-04 | Dicon A/S | Illumination unit and a method for point illumination of a medium |
US6097394A (en) | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US5835267A (en) | 1997-07-15 | 1998-11-10 | Eastman Kodak Company | Radiometric calibration device and method |
US6680976B1 (en) | 1997-07-28 | 2004-01-20 | The Board Of Trustees Of The University Of Illinois | Robust, reliable compression and packetization scheme for transmitting video |
US6091860A (en) | 1997-11-12 | 2000-07-18 | Pagemasters, Inc. | System and method for processing pixels for displaying and storing |
US6061400A (en) | 1997-11-20 | 2000-05-09 | Hitachi America Ltd. | Methods and apparatus for detecting scene conditions likely to cause prediction errors in reduced resolution video decoders and for using the detected information |
US6466207B1 (en) | 1998-03-18 | 2002-10-15 | Microsoft Corporation | Real-time image rendering with layered depth images |
US5974215A (en) | 1998-05-20 | 1999-10-26 | North Carolina State University | Compound image sensor array having staggered array of tapered optical fiber bundles |
US6137100A (en) | 1998-06-08 | 2000-10-24 | Photobit Corporation | CMOS image sensor with different pixel sizes for different colors |
US6448544B1 (en) | 1998-06-08 | 2002-09-10 | Brandeis University | Low noise, high resolution image detection system and method |
US6075889A (en) | 1998-06-12 | 2000-06-13 | Eastman Kodak Company | Computing color specification (luminance and chrominance) values for images |
US6674430B1 (en) | 1998-07-16 | 2004-01-06 | The Research Foundation Of State University Of New York | Apparatus and method for real-time volume processing and universal 3D rendering |
US6021241A (en) | 1998-07-17 | 2000-02-01 | North Carolina State University | Systems and methods for using diffraction patterns to determine radiation intensity values for areas between and along adjacent sensors of compound sensor arrays |
US6833865B1 (en) | 1998-09-01 | 2004-12-21 | Virage, Inc. | Embedded metadata engines in digital capture devices |
US6577342B1 (en) | 1998-09-25 | 2003-06-10 | Intel Corporation | Image sensor with microlens material structure |
US6320979B1 (en) | 1998-10-06 | 2001-11-20 | Canon Kabushiki Kaisha | Depth of field enhancement |
US6201899B1 (en) | 1998-10-09 | 2001-03-13 | Sarnoff Corporation | Method and apparatus for extended depth of field imaging |
US6169285B1 (en) | 1998-10-23 | 2001-01-02 | Adac Laboratories | Radiation-based imaging system employing virtual light-responsive elements |
US6687419B1 (en) | 1998-12-08 | 2004-02-03 | Synoptics Limited | Automatic image montage system |
US6900841B1 (en) | 1999-01-11 | 2005-05-31 | Olympus Optical Co., Ltd. | Image processing system capable of applying good texture such as blur |
US6587147B1 (en) | 1999-02-01 | 2003-07-01 | Intel Corporation | Microlens array |
US6424351B1 (en) | 1999-04-21 | 2002-07-23 | The University Of North Carolina At Chapel Hill | Methods and systems for producing three-dimensional images using relief textures |
US6658168B1 (en) | 1999-05-29 | 2003-12-02 | Lg Electronics Inc. | Method for retrieving image by using multiple features per image subregion |
US20020109783A1 (en) | 1999-06-02 | 2002-08-15 | Nikon Corporation | Electronic still camera |
US6479827B1 (en) | 1999-07-02 | 2002-11-12 | Canon Kabushiki Kaisha | Image sensing apparatus |
US6697062B1 (en) | 1999-08-06 | 2004-02-24 | Microsoft Corporation | Reflection space image based rendering |
US7015954B1 (en) | 1999-08-09 | 2006-03-21 | Fuji Xerox Co., Ltd. | Automatic video system using multiple cameras |
US6768980B1 (en) | 1999-09-03 | 2004-07-27 | Thomas W. Meyer | Method of and apparatus for high-bandwidth steganographic embedding of data in a series of digital signals or measurements such as taken from analog data streams or subsampled and/or transformed digital data |
US6597859B1 (en) | 1999-12-16 | 2003-07-22 | Intel Corporation | Method and apparatus for abstracting video data |
US6476805B1 (en) | 1999-12-23 | 2002-11-05 | Microsoft Corporation | Techniques for spatial displacement estimation and multi-resolution operations on light fields |
US6483535B1 (en) | 1999-12-23 | 2002-11-19 | Welch Allyn, Inc. | Wide angle lens system for electronic imagers having long exit pupil distances |
US6221687B1 (en) | 1999-12-23 | 2001-04-24 | Tower Semiconductor Ltd. | Color image sensor with embedded microlens array |
US20020001395A1 (en) | 2000-01-13 | 2002-01-03 | Davis Bruce L. | Authenticating metadata and embedding metadata in watermarks of media signals |
US6785667B2 (en) | 2000-02-14 | 2004-08-31 | Geophoenix, Inc. | Method and apparatus for extracting data objects and locating them in virtual space |
US20010048968A1 (en) | 2000-02-16 | 2001-12-06 | Cox W. Royall | Ink-jet printing of gradient-index microlenses |
US20040101166A1 (en) | 2000-03-22 | 2004-05-27 | Williams David W. | Speed measurement system with onsite digital image capture and processing for use in stop sign enforcement |
US20030172131A1 (en) | 2000-03-24 | 2003-09-11 | Yonghui Ao | Method and system for subject video streaming |
US20020159030A1 (en) | 2000-05-08 | 2002-10-31 | Frey Rudolph W. | Apparatus and method for objective measurement of optical systems using wavefront analysis |
US20030156077A1 (en) | 2000-05-19 | 2003-08-21 | Tibor Balogh | Method and apparatus for displaying 3d images |
US20070052810A1 (en) | 2000-06-14 | 2007-03-08 | Monroe David A | Dual-mode camera |
US6606099B2 (en) | 2000-06-19 | 2003-08-12 | Alps Electric Co., Ltd. | Display device for creating intermediate gradation levels in pseudo manner and image signal processing method |
US20020015048A1 (en) | 2000-06-28 | 2002-02-07 | David Nister | System and method for median fusion of depth maps |
US20050052543A1 (en) | 2000-06-28 | 2005-03-10 | Microsoft Corporation | Scene capturing and view rendering based on a longitudinally aligned camera array |
US20110267263A1 (en) | 2000-07-17 | 2011-11-03 | Microsoft Corporation | Changing input tolerances based on device movement |
US20020061131A1 (en) | 2000-10-18 | 2002-05-23 | Sawhney Harpreet Singh | Method and apparatus for synthesizing new video and/or still imagery from a collection of real video and/or still imagery |
US7034866B1 (en) | 2000-11-22 | 2006-04-25 | Koninklijke Philips Electronics N.V. | Combined display-camera for an image processing system |
US20080316301A1 (en) | 2000-11-29 | 2008-12-25 | Micoy Corporation | System and method for spherical stereoscopic photographing |
US7003061B2 (en) | 2000-12-21 | 2006-02-21 | Adobe Systems Incorporated | Image extraction from complex scenes in digital video |
US20020199106A1 (en) | 2001-02-09 | 2002-12-26 | Canon Kabushiki Kaisha | Information processing apparatus and its control method, computer program, and storage medium |
US7102666B2 (en) | 2001-02-12 | 2006-09-05 | Carnegie Mellon University | System and method for stabilizing rotational images |
US6924841B2 (en) | 2001-05-02 | 2005-08-02 | Agilent Technologies, Inc. | System and method for capturing color images that extends the dynamic range of an image sensor using first and second groups of pixels |
US20090204813A1 (en) | 2001-06-07 | 2009-08-13 | John Man Kwong Kwan | System and method for authenticating data using incompatible digest functions |
US20030043270A1 (en) | 2001-08-29 | 2003-03-06 | Rafey Richter A. | Extracting a depth map from known camera and model tracking data |
US6842297B2 (en) | 2001-08-31 | 2005-01-11 | Cdm Optics, Inc. | Wavefront coding optics |
US7239345B1 (en) | 2001-10-12 | 2007-07-03 | Worldscape, Inc. | Camera arrangements with backlighting detection and methods of using same |
US20040257360A1 (en) | 2001-10-22 | 2004-12-23 | Frank Sieckmann | Method and device for producing light-microscopy, three-dimensional images |
US20030081145A1 (en) | 2001-10-30 | 2003-05-01 | Seaman Mark D. | Systems and methods for generating digital images having image meta-data combined with the image data |
US8675073B2 (en) | 2001-11-08 | 2014-03-18 | Kenneth Joseph Aagaard | Video system and methods for operating a video system |
US20140132741A1 (en) | 2001-11-08 | 2014-05-15 | Kenneth Joseph Aagaard | Video system and methods for operating a video system |
US20030103670A1 (en) | 2001-11-30 | 2003-06-05 | Bernhard Schoelkopf | Interactive images |
WO2003052465A2 (en) | 2001-12-18 | 2003-06-26 | University Of Rochester | Multifocal aspheric lens obtaining extended field depth |
US6927922B2 (en) | 2001-12-18 | 2005-08-09 | The University Of Rochester | Imaging using a multifocal aspheric lens to obtain extended depth of field |
US20030117511A1 (en) | 2001-12-21 | 2003-06-26 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
US20030123700A1 (en) | 2001-12-28 | 2003-07-03 | Canon Kabushiki Kaisha | Image generation apparatus, image file generation method, image verification apparatus and image verification method |
US20030133018A1 (en) | 2002-01-16 | 2003-07-17 | Ted Ziemkowski | System for near-simultaneous capture of multiple camera images |
US20030147252A1 (en) | 2002-02-06 | 2003-08-07 | Fioravanti S.R.L. | Front lighting system for a motor vehicle |
US20060130017A1 (en) | 2002-06-17 | 2006-06-15 | Microsoft Corporation | Combined image views and methods of creating images |
US20040002179A1 (en) | 2002-06-26 | 2004-01-01 | Barton Eric J. | Glass attachment over micro-lens arrays |
US20040012689A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Charge coupled devices in tiled arrays |
US20040012688A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Large area charge coupled device camera |
US20040135780A1 (en) | 2002-08-30 | 2004-07-15 | Nims Jerry C. | Multi-dimensional images system for digital image input and output |
US20100303288A1 (en) | 2002-09-30 | 2010-12-02 | Myport Technologies, Inc. | Method for multi-media recognition, data conversion, creation of metatags, storage and search retrieval |
US20040189686A1 (en) | 2002-10-31 | 2004-09-30 | Tanguay Donald O. | Method and system for producing a model from optical images |
US7206022B2 (en) | 2002-11-25 | 2007-04-17 | Eastman Kodak Company | Camera system with eye monitoring |
US20040114176A1 (en) | 2002-12-17 | 2004-06-17 | International Business Machines Corporation | Editing and browsing images for virtual cameras |
US20060256226A1 (en) | 2003-01-16 | 2006-11-16 | D-Blur Technologies Ltd. | Camera with image enhancement functions |
US20040212725A1 (en) | 2003-03-19 | 2004-10-28 | Ramesh Raskar | Stylized rendering using a multi-flash camera |
US20120268367A1 (en) | 2003-03-21 | 2012-10-25 | Roel Vertegaal | Method and Apparatus for Communication Between Humans and Devices |
US7683951B2 (en) | 2003-04-22 | 2010-03-23 | Fujifilm Corporation | Solid-state imaging apparatus and digital camera for white balance correction |
US7164807B2 (en) | 2003-04-24 | 2007-01-16 | Eastman Kodak Company | Method and system for automatically reducing aliasing artifacts |
US7025515B2 (en) | 2003-05-20 | 2006-04-11 | Software 2000 Ltd. | Bit mask generation system |
US20060082879A1 (en) | 2003-05-29 | 2006-04-20 | Takashi Miyoshi | Stereo optical module and stereo camera |
US7847825B2 (en) | 2003-05-29 | 2010-12-07 | Panasonic Corporation | Image capturing apparatus |
US7633513B2 (en) | 2003-06-27 | 2009-12-15 | Sony Corporation | Signal processing device, signal processing method, program, and recording medium |
US20050031203A1 (en) | 2003-08-08 | 2005-02-10 | Hiroaki Fukuda | Image processing apparatus, an image forming apparatus and an image processing method |
US20050049500A1 (en) | 2003-08-28 | 2005-03-03 | Babu Sundar G. | Diagnostic medical ultrasound system having method and apparatus for storing and retrieving 3D and 4D data sets |
US20050080602A1 (en) | 2003-10-10 | 2005-04-14 | Microsoft Corporation | Systems and methods for all-frequency relighting using spherical harmonics and point light distributions |
US20060248348A1 (en) | 2003-10-14 | 2006-11-02 | Canon Kabushiki Kaisha | Image data verification |
US20060208259A1 (en) | 2003-12-31 | 2006-09-21 | Jeon In G | CMOS image sensors and methods for fabricating the same |
US20050162540A1 (en) | 2004-01-27 | 2005-07-28 | Fujinon Corporation | Autofocus system |
US20050212918A1 (en) | 2004-03-25 | 2005-09-29 | Bill Serra | Monitoring system and method |
US20080266688A1 (en) | 2004-04-27 | 2008-10-30 | Fico Mirrors, Sa | Folding Mechanism for Exterior Rear-View Mirrors in Automotive Vehicles |
US20050253728A1 (en) * | 2004-05-13 | 2005-11-17 | Chao-Ho Chen | Method and system for detecting fire in a predetermined area |
US20050276441A1 (en) | 2004-06-12 | 2005-12-15 | University Of Southern California | Performance relighting and reflectance transformation with time-multiplexed illumination |
US20060008265A1 (en) | 2004-07-12 | 2006-01-12 | Kenji Ito | Optical apparatus |
US20080018668A1 (en) | 2004-07-23 | 2008-01-24 | Masaki Yamauchi | Image Processing Device and Image Processing Method |
US20060023066A1 (en) | 2004-07-27 | 2006-02-02 | Microsoft Corporation | System and Method for Client Services for Interactive Multi-View Video |
US20110018903A1 (en) | 2004-08-03 | 2011-01-27 | Silverbrook Research Pty Ltd | Augmented reality device for presenting virtual imagery registered to a viewed surface |
US7329856B2 (en) | 2004-08-24 | 2008-02-12 | Micron Technology, Inc. | Image sensor having integrated infrared-filtering optical device and related method |
US7477304B2 (en) | 2004-08-26 | 2009-01-13 | Micron Technology, Inc. | Two narrow band and one wide band color filter for increasing color image sensor sensitivity |
US20060056040A1 (en) | 2004-09-02 | 2006-03-16 | Asia Optical Co., Inc. | Image pick-up apparatus with curvature-of-field correction |
US7336430B2 (en) | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US20060050170A1 (en) | 2004-09-09 | 2006-03-09 | Fuji Photo Film Co., Ltd. | Camera system, camera body, and camera head |
US8115814B2 (en) | 2004-09-14 | 2012-02-14 | Canon Kabushiki Kaisha | Mobile tracking system, camera and photographing method |
US20060056604A1 (en) | 2004-09-15 | 2006-03-16 | Research In Motion Limited | Method for scaling images for usage on a mobile communication device |
US8750509B2 (en) | 2004-09-23 | 2014-06-10 | Smartvue Corporation | Wireless surveillance system releasably mountable to track lighting |
US20140049663A1 (en) | 2004-10-01 | 2014-02-20 | The Board Of Trustees Of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
US7936392B2 (en) | 2004-10-01 | 2011-05-03 | The Board Of Trustees Of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
WO2006039486A2 (en) | 2004-10-01 | 2006-04-13 | The Board Of Trustees Of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
US20060072175A1 (en) | 2004-10-06 | 2006-04-06 | Takahiro Oshino | 3D image printing system |
US20060078052A1 (en) | 2004-10-08 | 2006-04-13 | Dang Philip P | Method and apparatus for parallel processing of in-loop deblocking filter for H.264 video compression standard |
US7417670B1 (en) | 2005-01-12 | 2008-08-26 | Ambarella, Inc. | Digital video camera with binning or skipping correction |
US20060250322A1 (en) | 2005-05-09 | 2006-11-09 | Optics 1, Inc. | Dynamic vergence and focus control for head-mounted displays |
US20070008317A1 (en) | 2005-05-25 | 2007-01-11 | Sectra Ab | Automated medical image visualization using volume rendering with local histograms |
US20080277566A1 (en) | 2005-05-30 | 2008-11-13 | Ken Utagawa | Image Forming State Detection Device |
US20060274210A1 (en) | 2005-06-04 | 2006-12-07 | Samsung Electronics Co., Ltd. | Method and apparatus for improving quality of composite video signal and method and apparatus for decoding composite video signal |
US20060285741A1 (en) | 2005-06-18 | 2006-12-21 | Muralidhara Subbarao | Direct vision sensor for 3D computer vision, digital imaging, and digital video |
US20090140131A1 (en) | 2005-06-23 | 2009-06-04 | Nikon Corporation | Image input apparatus, photodetection apparatus, and image synthesis method |
US20070019883A1 (en) | 2005-07-19 | 2007-01-25 | Wong Earl Q | Method for creating a depth map for auto focus using an all-in-focus picture and two-dimensional scale space matching |
US20070033588A1 (en) | 2005-08-02 | 2007-02-08 | Landsman Richard A | Generic download and upload functionality in a client/server web application architecture |
US20070030357A1 (en) | 2005-08-05 | 2007-02-08 | Searete Llc, A Limited Liability Corporation Of The State Of Delaware | Techniques for processing images |
US20070188613A1 (en) | 2005-08-08 | 2007-08-16 | Kunio Nobori | Image synthesizing apparatus and image synthesizing method |
US8310554B2 (en) | 2005-09-20 | 2012-11-13 | Sri International | Method and apparatus for performing coordinated multi-PTZ camera tracking |
US20070071316A1 (en) | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US20070081081A1 (en) | 2005-10-07 | 2007-04-12 | Cheng Brett A | Automated multi-frame image capture for panorama stitching using motion sensor |
US7723662B2 (en) | 2005-10-07 | 2010-05-25 | The Board Of Trustees Of The Leland Stanford Junior University | Microscopy arrangements and approaches |
US20070097206A1 (en) | 2005-11-02 | 2007-05-03 | Houvener Robert C | Multi-user stereoscopic 3-D panoramic vision system and method |
US20080226274A1 (en) | 2005-11-03 | 2008-09-18 | Spielberg Anthony C | Systems For Improved Autofocus in Digital Imaging Systems |
US20070103558A1 (en) | 2005-11-04 | 2007-05-10 | Microsoft Corporation | Multi-view video delivery |
US20070113198A1 (en) | 2005-11-16 | 2007-05-17 | Microsoft Corporation | Displaying 2D graphic content using depth wells |
US7623726B1 (en) | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US7286295B1 (en) | 2005-11-30 | 2007-10-23 | Sandia Corporation | Microoptical compound lens |
US20070140676A1 (en) | 2005-12-16 | 2007-06-21 | Pentax Corporation | Camera having an autofocus system |
WO2007092545A2 (en) | 2006-02-07 | 2007-08-16 | The Board Of Trustees Of The Leland Stanford Junior University | Variable imaging arrangements and methods therefor |
WO2007092581A2 (en) | 2006-02-07 | 2007-08-16 | The Board Of Trustees Of The Leland Stanford Junior University | Correction of optical aberrations |
US8248515B2 (en) | 2006-02-07 | 2012-08-21 | The Board Of Trustees Of The Leland Stanford Junior University | Variable imaging arrangements and methods therefor |
US20130107085A1 (en) | 2006-02-07 | 2013-05-02 | The Board Of Trustees Of The Leland Stanford Junior University | Correction of Optical Aberrations |
US20100026852A1 (en) | 2006-02-07 | 2010-02-04 | Yi-Ren Ng | Variable imaging arrangements and methods therefor |
US20090128669A1 (en) | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US20100050120A1 (en) | 2006-02-13 | 2010-02-25 | Google Inc. | User Interface for Selecting Options |
US7748022B1 (en) | 2006-02-21 | 2010-06-29 | L-3 Communications Sonoma Eo, Inc. | Real-time data characterization with token generation for fast data retrieval |
US20070201853A1 (en) | 2006-02-28 | 2007-08-30 | Microsoft Corporation | Adaptive Processing For Images Captured With Flash |
US20090135258A1 (en) | 2006-03-31 | 2009-05-28 | Nikon Corporation | Projection Device and Electronic Device |
US20070230944A1 (en) | 2006-04-04 | 2007-10-04 | Georgiev Todor G | Plenoptic camera |
US7620309B2 (en) | 2006-04-04 | 2009-11-17 | Adobe Systems, Incorporated | Plenoptic camera |
US20070229653A1 (en) | 2006-04-04 | 2007-10-04 | Wojciech Matusik | Method and system for acquiring and displaying 3D light fields |
US20070273795A1 (en) | 2006-04-21 | 2007-11-29 | Mersive Technologies, Inc. | Alignment optimization in image display systems employing multi-camera image acquisition |
US20070269108A1 (en) | 2006-05-03 | 2007-11-22 | Fotonation Vision Limited | Foreground / Background Separation in Digital Images |
US7724952B2 (en) | 2006-05-15 | 2010-05-25 | Microsoft Corporation | Object matting using flash and no-flash images |
US9106914B2 (en) | 2006-05-25 | 2015-08-11 | Thomson Licensing | Method and system for weighted encoding |
US20080007626A1 (en) | 2006-07-07 | 2008-01-10 | Sony Ericsson Mobile Communications Ab | Active autofocus window |
US20080049113A1 (en) | 2006-07-13 | 2008-02-28 | Canon Kabushiki Kaisha | Image sensing apparatus |
US20080012988A1 (en) | 2006-07-16 | 2008-01-17 | Ray Baharav | System and method for virtual content placement |
US20090309973A1 (en) | 2006-08-02 | 2009-12-17 | Panasonic Corporation | Camera control apparatus and camera control system |
US20080031537A1 (en) | 2006-08-07 | 2008-02-07 | Dina Gutkowicz-Krusin | Reducing noise in digital images |
US20100060727A1 (en) | 2006-08-11 | 2010-03-11 | Eran Steinberg | Real-time face tracking with reference images |
US20080056569A1 (en) | 2006-09-05 | 2008-03-06 | Williams Robert C | Background separated images for print and on-line use |
US8106856B2 (en) | 2006-09-06 | 2012-01-31 | Apple Inc. | Portable electronic device for photo management |
US7945653B2 (en) | 2006-10-11 | 2011-05-17 | Facebook, Inc. | Tagging digital media |
US8155478B2 (en) | 2006-10-26 | 2012-04-10 | Broadcom Corporation | Image creation with software controllable depth of field |
US20090167909A1 (en) | 2006-10-30 | 2009-07-02 | Taro Imagawa | Image generation apparatus and image generation method |
US20080122940A1 (en) | 2006-11-27 | 2008-05-29 | Sanyo Electric Co., Ltd. | Image shooting apparatus and focus control method |
US20080144952A1 (en) | 2006-11-30 | 2008-06-19 | Canon Kabushiki Kaisha | Method and Apparatus For Hybrid Image Compression |
US20170134639A1 (en) | 2006-12-01 | 2017-05-11 | Lytro, Inc. | Video Refocusing |
US8559705B2 (en) | 2006-12-01 | 2013-10-15 | Lytro, Inc. | Interactive refocusing of electronic images |
US20140013273A1 (en) | 2006-12-01 | 2014-01-09 | Lytro, Inc. | Interactive refocusing of electronic images |
US20130113981A1 (en) | 2006-12-01 | 2013-05-09 | Lytro, Inc. | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
US20080129728A1 (en) | 2006-12-01 | 2008-06-05 | Fujifilm Corporation | Image file creation device, imaging apparatus and file structure |
US7941634B2 (en) | 2006-12-01 | 2011-05-10 | Thomson Licensing | Array of processing elements with local registers |
US20080152215A1 (en) | 2006-12-26 | 2008-06-26 | Kenichi Horie | Coding method, electronic camera, recording medium storing coded program, and decoding method |
US20110075729A1 (en) | 2006-12-28 | 2011-03-31 | Gokce Dane | method and apparatus for automatic visual artifact analysis and artifact reduction |
US7469381B2 (en) | 2007-01-07 | 2008-12-23 | Apple Inc. | List scrolling and document translation, scaling, and rotation on a touch-screen display |
US20080168404A1 (en) | 2007-01-07 | 2008-07-10 | Apple Inc. | List Scrolling and Document Translation, Scaling, and Rotation on a Touch-Screen Display |
US20090290848A1 (en) | 2007-01-11 | 2009-11-26 | Michael James Brown | Method and System for Generating a Replay Video |
CN101226292A (en) | 2007-01-19 | 2008-07-23 | 滨松光子学株式会社 | Phase-modulating apparatus |
US20080180792A1 (en) | 2007-01-25 | 2008-07-31 | Georgiev Todor G | Light Field Microscope With Lenslet Array |
US20120237222A9 (en) | 2007-02-01 | 2012-09-20 | Alliance Fiber Optic Products, Inc. | Micro Free-Space WDM Device |
US20080187305A1 (en) | 2007-02-06 | 2008-08-07 | Ramesh Raskar | 4D light field cameras |
US20080193026A1 (en) | 2007-02-09 | 2008-08-14 | Kenichi Horie | Decoding method, decoding apparatus, storage medium in which decoding program is stored, and electronic camera |
US20080205871A1 (en) | 2007-02-27 | 2008-08-28 | Nikon Corporation | Focus detection device for image forming optical system, imaging apparatus, and focus detection method for image forming optical system |
US20080232680A1 (en) | 2007-03-19 | 2008-09-25 | Alexander Berestov | Two dimensional/three dimensional digital information acquisition and display device |
US20080253652A1 (en) | 2007-04-10 | 2008-10-16 | Aricent Inc. | Method of demosaicing a digital mosaiced image |
US20100111489A1 (en) | 2007-04-13 | 2010-05-06 | Presler Ari M | Digital Camera System for Recording, Editing and Visualizing Images |
US20080260291A1 (en) | 2007-04-17 | 2008-10-23 | Nokia Corporation | Image downscaling by binning |
US7936377B2 (en) | 2007-04-30 | 2011-05-03 | Tandent Vision Science, Inc. | Method and system for optimizing an image for improved analysis of material and illumination image features |
US8494304B2 (en) | 2007-05-11 | 2013-07-23 | Xerox Corporation | Punched hole detection and removal |
US20100103311A1 (en) | 2007-06-06 | 2010-04-29 | Sony Corporation | Image processing device, image processing method, and image processing program |
US20080309813A1 (en) | 2007-06-18 | 2008-12-18 | Sony Corporation | Imaging device and method |
US8290358B1 (en) | 2007-06-25 | 2012-10-16 | Adobe Systems Incorporated | Methods and apparatus for light-field imaging |
US7982776B2 (en) | 2007-07-13 | 2011-07-19 | Ethicon Endo-Surgery, Inc. | SBI motion artifact removal apparatus and method |
US20090027542A1 (en) | 2007-07-13 | 2009-01-29 | Sony Corporation | Image pickup apparatus |
US8085391B2 (en) | 2007-08-02 | 2011-12-27 | Aptina Imaging Corporation | Integrated optical characteristic measurements in a CMOS image sensor |
US20090041381A1 (en) | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Processing by Demultiplexing in the Frequency Domain |
US20090041448A1 (en) | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Capture by Multiplexing in the Frequency Domain |
US20090070710A1 (en) | 2007-09-07 | 2009-03-12 | Canon Kabushiki Kaisha | Content display apparatus and display method thereof |
US20120218463A1 (en) | 2007-10-12 | 2012-08-30 | Microsoft Corporation | Multi-spectral imaging |
US20090102956A1 (en) | 2007-10-18 | 2009-04-23 | Georgiev Todor G | Fast Computational Camera Based On Two Arrays of Lenses |
US20090109280A1 (en) | 2007-10-31 | 2009-04-30 | Technion Research And Development Foundation Ltd. | Free viewpoint video |
US20090128658A1 (en) | 2007-11-12 | 2009-05-21 | Sony Corporation | Image pickup apparatus |
US20100021001A1 (en) | 2007-11-15 | 2010-01-28 | Honsinger Chris W | Method for Making an Assured Image |
US20100003024A1 (en) | 2007-12-10 | 2010-01-07 | Amit Kumar Agrawal | Cameras with Varying Spatio-Angular-Temporal Resolutions |
US20090185051A1 (en) | 2008-01-21 | 2009-07-23 | Nikon Corporation | Data processing apparatus, imaging apparatus, and medium storing data processing program |
US20090295829A1 (en) | 2008-01-23 | 2009-12-03 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090185801A1 (en) | 2008-01-23 | 2009-07-23 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090190024A1 (en) | 2008-01-28 | 2009-07-30 | Sony Corporation | Image pickup apparatus |
US20090190022A1 (en) | 2008-01-28 | 2009-07-30 | Sony Corporation | Image pickup apparatus |
US20090195689A1 (en) | 2008-02-05 | 2009-08-06 | Samsung Techwin Co., Ltd. | Digital image photographing apparatus, method of controlling the apparatus, and recording medium having program for executing the method |
US20090202235A1 (en) | 2008-02-13 | 2009-08-13 | Qualcomm Incorporated | Auto-focus calibration for image capture device |
US8577216B2 (en) | 2008-02-13 | 2013-11-05 | Qualcomm Incorporated | Auto-focus calibration for image capture device |
US20090207233A1 (en) | 2008-02-14 | 2009-08-20 | Mauchly J William | Method and system for videoconference configuration |
US20110001858A1 (en) | 2008-02-22 | 2011-01-06 | Dai Shintani | Imaging apparatus |
US20120176481A1 (en) | 2008-02-29 | 2012-07-12 | Disney Enterprises, Inc. | Processing image data from multiple cameras for motion pictures |
US20100328485A1 (en) | 2008-03-31 | 2010-12-30 | Panasonic Corporation | Imaging device, imaging module, electronic still camera, and electronic movie camera |
US20110123183A1 (en) | 2008-04-04 | 2011-05-26 | Eth Zurich | Spatially adaptive photographic flash unit |
US8155456B2 (en) | 2008-04-29 | 2012-04-10 | Adobe Systems Incorporated | Method and apparatus for block-based compression of light-field images |
US20090273843A1 (en) | 2008-05-01 | 2009-11-05 | Ramesh Raskar | Apparatus and Method for Reducing Glare in Images |
US20140333787A1 (en) | 2008-05-20 | 2014-11-13 | Pelican Imaging Corporation | Systems and Methods for Parallax Measurement Using Camera Arrays Incorporating 3 x 3 Camera Configurations |
US20090309975A1 (en) | 2008-06-13 | 2009-12-17 | Scott Gordon | Dynamic Multi-Perspective Interactive Event Visualization System and Method |
US20090310885A1 (en) | 2008-06-13 | 2009-12-17 | Fujifilm Corporation | Image processing apparatus, imaging apparatus, image processing method and recording medium |
US8427548B2 (en) | 2008-06-18 | 2013-04-23 | Samsung Electronics Co., Ltd. | Apparatus and method for capturing digital images |
CN101309359A (en) | 2008-06-20 | 2008-11-19 | 埃派克森微电子(上海)有限公司 | System and method for eliminating fixed mode noise by dummy pixels |
US20090321861A1 (en) | 2008-06-26 | 2009-12-31 | Micron Technology, Inc. | Microelectronic imagers with stacked lens assemblies and processes for wafer-level packaging of microelectronic imagers |
US20110091192A1 (en) | 2008-06-30 | 2011-04-21 | Nikon Corporation | Focus detecting apparatus and imaging apparatus |
US20100011117A1 (en) | 2008-07-09 | 2010-01-14 | Apple Inc. | Video streaming using multiple channels |
JP2010020100A (en) | 2008-07-10 | 2010-01-28 | Olympus Imaging Corp | Image reproducing display device, imaging device, and image reproducing display method |
US8736751B2 (en) | 2008-08-26 | 2014-05-27 | Empire Technology Development Llc | Digital presenter for displaying image captured by camera with illumination system |
US7587109B1 (en) | 2008-09-02 | 2009-09-08 | Spectral Imaging Laboratory | Hybrid fiber coupled artificial compound eye |
US20100097444A1 (en) | 2008-10-16 | 2010-04-22 | Peter Lablans | Camera System for Creating an Image From a Plurality of Images |
US20100107068A1 (en) | 2008-10-23 | 2010-04-29 | Butcher Larry R | User Interface with Parallax Animation |
US20100123784A1 (en) | 2008-11-19 | 2010-05-20 | Yuanyuan Ding | Catadioptric Projectors |
US20100142839A1 (en) | 2008-11-19 | 2010-06-10 | Canon Kabushiki Kaisha | Dvc as generic file format for plenoptic camera |
US8279325B2 (en) | 2008-11-25 | 2012-10-02 | Lytro, Inc. | System and method for acquiring, editing, generating and outputting video data |
US8614764B2 (en) | 2008-11-25 | 2013-12-24 | Lytro, Inc. | Acquiring, editing, generating and outputting video data |
US8760566B2 (en) | 2008-11-25 | 2014-06-24 | Lytro, Inc. | Video refocusing |
US8570426B2 (en) | 2008-11-25 | 2013-10-29 | Lytro, Inc. | System of and method for video refocusing |
US20140240463A1 (en) | 2008-11-25 | 2014-08-28 | Lytro, Inc. | Video Refocusing |
US8446516B2 (en) | 2008-11-25 | 2013-05-21 | Lytro, Inc. | Generating and outputting video data from refocusable light field video data |
US8264546B2 (en) | 2008-11-28 | 2012-09-11 | Sony Corporation | Image processing system for estimating camera parameters |
US20110261164A1 (en) | 2008-12-05 | 2011-10-27 | Unisensor A/S | Optical sectioning of a sample and detection of particles in a sample |
US8289440B2 (en) | 2008-12-08 | 2012-10-16 | Lytro, Inc. | Light field data acquisition devices, and methods of using and manufacturing same |
US20140211077A1 (en) | 2008-12-08 | 2014-07-31 | Lytro, Inc. | Light field data acquisition |
US9467607B2 (en) | 2008-12-08 | 2016-10-11 | Lytro, Inc. | Light field data acquisition |
US8976288B2 (en) | 2008-12-08 | 2015-03-10 | Lytro, Inc. | Light field data acquisition |
US8724014B2 (en) | 2008-12-08 | 2014-05-13 | Lytro, Inc. | Light field data acquisition |
US8013904B2 (en) | 2008-12-09 | 2011-09-06 | Seiko Epson Corporation | View projection matrix based high performance low latency display pipeline |
US20110298960A1 (en) | 2008-12-09 | 2011-12-08 | Seiko Epson Corporation | View Projection Matrix Based High Performance Low Latency Display Pipeline |
US20100141780A1 (en) | 2008-12-09 | 2010-06-10 | Kar-Han Tan | View Projection Matrix Based High Performance Low Latency Display Pipeline |
US7949252B1 (en) | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US20100201789A1 (en) | 2009-01-05 | 2010-08-12 | Fujifilm Corporation | Three-dimensional display device and digital zoom correction method |
US20140176710A1 (en) | 2009-01-05 | 2014-06-26 | Duke University | Multiscale telescopic imaging system |
US8860856B2 (en) | 2009-01-19 | 2014-10-14 | Dolby Laboratories Licensing Corporation | Multiplexed imaging |
US8315476B1 (en) | 2009-01-20 | 2012-11-20 | Adobe Systems Incorporated | Super-resolution with the focused plenoptic camera |
US8189089B1 (en) | 2009-01-20 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for reducing plenoptic camera artifacts |
US20130128081A1 (en) | 2009-01-20 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Reducing Plenoptic Camera Artifacts |
US7687757B1 (en) | 2009-01-29 | 2010-03-30 | Visera Technologies Company Limited | Design of microlens on pixel array |
US20140037280A1 (en) | 2009-02-10 | 2014-02-06 | Canon Kabushiki Kaisha | Imaging apparatus, flash device, and control method thereof |
US20110279479A1 (en) | 2009-03-03 | 2011-11-17 | Rodriguez Tony F | Narrowcasting From Public Displays, and Related Methods |
US8589374B2 (en) | 2009-03-16 | 2013-11-19 | Apple Inc. | Multifunction device with integrated search and application selection |
US8797321B1 (en) | 2009-04-01 | 2014-08-05 | Microsoft Corporation | Augmented lighting environments |
US20100253782A1 (en) | 2009-04-07 | 2010-10-07 | Latent Image Technology Ltd. | Device and method for automated verification of polarization-variant images |
US20100265385A1 (en) | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US20120249550A1 (en) | 2009-04-18 | 2012-10-04 | Lytro, Inc. | Selective Transmission of Image Data Based on Device Attributes |
US8908058B2 (en) | 2009-04-18 | 2014-12-09 | Lytro, Inc. | Storage and transmission of pictures including multiple frames |
US20120050562A1 (en) | 2009-04-22 | 2012-03-01 | Raytrix Gmbh | Digital imaging system, plenoptic optical device and image data processing method |
US20100277629A1 (en) | 2009-05-01 | 2010-11-04 | Samsung Electronics Co., Ltd. | Photo detecting device and image pickup device and method thereon |
US20100277617A1 (en) | 2009-05-02 | 2010-11-04 | Hollinger Steven J | Ball with camera and trajectory control for reconnaissance or recreation |
US8345144B1 (en) | 2009-07-15 | 2013-01-01 | Adobe Systems Incorporated | Methods and apparatus for rich image capture with focused plenoptic cameras |
US8228417B1 (en) | 2009-07-15 | 2012-07-24 | Adobe Systems Incorporated | Focused plenoptic camera employing different apertures or filtering at different microlenses |
WO2011010234A1 (en) | 2009-07-23 | 2011-01-27 | Philips Lumileds Lighting Company, Llc | Led with molded reflective sidewall coating |
US20110019056A1 (en) | 2009-07-26 | 2011-01-27 | Massachusetts Institute Of Technology | Bi-Directional Screen |
US20110025827A1 (en) | 2009-07-30 | 2011-02-03 | Primesense Ltd. | Depth Mapping Based on Pattern Matching and Stereoscopic Information |
US20110032338A1 (en) | 2009-08-06 | 2011-02-10 | Qualcomm Incorporated | Encapsulating three-dimensional video data in accordance with transport protocols |
US20120132803A1 (en) | 2009-08-10 | 2012-05-31 | Hitachi High-Technologies Corporation | Charged particle beam device and image display method |
US20110069175A1 (en) | 2009-08-10 | 2011-03-24 | Charles Mistretta | Vision system and method for motion adaptive integration of image frames |
US20110050909A1 (en) | 2009-09-01 | 2011-03-03 | Geovector Corporation | Photographer's guidance systems |
US20110050864A1 (en) | 2009-09-01 | 2011-03-03 | Prime Focus Vfx Services Ii Inc. | System and process for transforming two-dimensional images into three-dimensional images |
WO2011029209A2 (en) | 2009-09-10 | 2011-03-17 | Liberovision Ag | Method and apparatus for generating and processing depth-enhanced images |
US20110063414A1 (en) * | 2009-09-16 | 2011-03-17 | Xuemin Chen | Method and system for frame buffer compression and memory resource reduction for 3d video |
US8442397B2 (en) | 2009-09-22 | 2013-05-14 | Samsung Electronics Co., Ltd. | Modulator, apparatus for obtaining light field data using modulator, and apparatus and method for processing light field data using modulator |
US20120269274A1 (en) | 2009-10-01 | 2012-10-25 | Sk Telecom Co., Ltd. | Method and apparatus for encoding/decoding video using split layer |
US20120201475A1 (en) | 2009-10-05 | 2012-08-09 | I.C.V.T. Ltd. | Method and system for processing an image |
US8629930B2 (en) | 2009-10-14 | 2014-01-14 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Device, image processing device and method for optical imaging |
US20110090255A1 (en) | 2009-10-16 | 2011-04-21 | Wilson Diego A | Content boundary signaling techniques |
US20110249341A1 (en) | 2009-10-19 | 2011-10-13 | Pixar | Super light-field lens with doublet lenslet array element |
US20110169994A1 (en) | 2009-10-19 | 2011-07-14 | Pixar | Super light-field lens |
US8259198B2 (en) | 2009-10-20 | 2012-09-04 | Apple Inc. | System and method for detecting and correcting defective pixels in an image sensor |
US20120120240A1 (en) | 2009-10-21 | 2012-05-17 | Panasonic Corporation | Video image conversion device and image capture device |
US20120272271A1 (en) | 2009-10-30 | 2012-10-25 | Sony Computer Entertainment Inc. | Information Processing Apparatus, Tuner, And Information Processing Method |
US20130121577A1 (en) | 2009-10-30 | 2013-05-16 | Jue Wang | Methods and Apparatus for Chatter Reduction in Video Object Segmentation Using Optical Flow Assisted Gaussholding |
US20130128052A1 (en) | 2009-11-17 | 2013-05-23 | Telefonaktiebolaget L M Ericsson (Publ) | Synchronization of Cameras for Multi-View Session Capturing |
US20110221947A1 (en) | 2009-11-20 | 2011-09-15 | Kouhei Awazu | Solid-state imaging device |
US20110129165A1 (en) | 2009-11-27 | 2011-06-02 | Samsung Electronics Co., Ltd. | Image processing apparatus and method |
US8400555B1 (en) | 2009-12-01 | 2013-03-19 | Adobe Systems Incorporated | Focused plenoptic camera employing microlenses with different focal lengths |
US20110129120A1 (en) | 2009-12-02 | 2011-06-02 | Canon Kabushiki Kaisha | Processing captured images having geolocations |
US20110133649A1 (en) | 2009-12-07 | 2011-06-09 | At&T Intellectual Property I, L.P. | Mechanisms for light management |
US20110149074A1 (en) | 2009-12-18 | 2011-06-23 | Electronics And Telecommunications Research Institute | Portable multi-view image acquisition system and multi-view image preprocessing method |
US20110148764A1 (en) | 2009-12-18 | 2011-06-23 | Avago Technologies Ecbu Ip (Singapore) Pte. Ltd. | Optical navigation system and method for performing self-calibration on the system using a calibration cover |
JP2011135170A (en) | 2009-12-22 | 2011-07-07 | Samsung Electronics Co Ltd | Imaging apparatus and imaging method |
WO2011081187A1 (en) | 2009-12-28 | 2011-07-07 | 株式会社ニコン | Image capture element, and image capture device |
US20120293075A1 (en) | 2010-01-29 | 2012-11-22 | Koninklijke Philips Electronics, N.V. | Interactive lighting control system and method |
US20110194617A1 (en) | 2010-02-11 | 2011-08-11 | Nokia Corporation | Method and Apparatus for Providing Multi-Threaded Video Decoding |
US20120014837A1 (en) | 2010-02-19 | 2012-01-19 | Pacific Biosciences Of California, Inc. | Illumination of integrated analytical systems |
US8749620B1 (en) | 2010-02-20 | 2014-06-10 | Lytro, Inc. | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same |
US8400533B1 (en) | 2010-02-23 | 2013-03-19 | Xilinx, Inc. | Methods of reducing aberrations in a digital image |
US20110205384A1 (en) | 2010-02-24 | 2011-08-25 | Panavision Imaging, Llc | Variable active image area image sensor |
US20120321172A1 (en) | 2010-02-26 | 2012-12-20 | Jachalsky Joern | Confidence map, method for generating the same and method for refining a disparity map |
US20130120605A1 (en) | 2010-03-03 | 2013-05-16 | Todor G. Georgiev | Methods, Apparatus, and Computer-Readable Storage Media for Blended Rendering of Focused Plenoptic Camera Data |
US20130120356A1 (en) | 2010-03-03 | 2013-05-16 | Todor G. Georgiev | Methods, Apparatus, and Computer-Readable Storage Media for Depth-Based Rendering of Focused Plenoptic Camera Data |
US8411948B2 (en) | 2010-03-05 | 2013-04-02 | Microsoft Corporation | Up-sampling binary images for segmentation |
US20110242352A1 (en) | 2010-03-30 | 2011-10-06 | Nikon Corporation | Image processing method, computer-readable storage medium, image processing apparatus, and imaging apparatus |
US20120062755A1 (en) | 2010-03-31 | 2012-03-15 | Sony Corporation | Camera system, signal delay amount adjusting method and program |
US20110242334A1 (en) | 2010-04-02 | 2011-10-06 | Microsoft Corporation | Time Interleaved Exposures And Multiplexed Illumination |
US20120044330A1 (en) | 2010-04-21 | 2012-02-23 | Tatsumi Watanabe | Stereoscopic video display apparatus and stereoscopic video display method |
US20130044290A1 (en) | 2010-04-21 | 2013-02-21 | Panasonic Corporation | Visual function testing device |
US20110261205A1 (en) | 2010-04-23 | 2011-10-27 | Hon Hai Precision Industry Co., Ltd. | Method for coordinating camera array |
US20130093859A1 (en) | 2010-04-28 | 2013-04-18 | Fujifilm Corporation | Stereoscopic image reproduction device and method, stereoscopic image capturing device, and stereoscopic display device |
US20110267348A1 (en) | 2010-04-29 | 2011-11-03 | Dennis Lin | Systems and methods for generating a virtual camera viewpoint for an image |
US20110273466A1 (en) | 2010-05-10 | 2011-11-10 | Canon Kabushiki Kaisha | View-dependent rendering system with intuitive mixed reality |
US20120057040A1 (en) | 2010-05-11 | 2012-03-08 | Byung Kwan Park | Apparatus and method for processing light field data using a mask with an attenuation pattern |
US20110292258A1 (en) | 2010-05-28 | 2011-12-01 | C2Cure, Inc. | Two sensor imaging systems |
US20110293179A1 (en) | 2010-05-31 | 2011-12-01 | Mert Dikmen | Systems and methods for illumination correction of an image |
US20120057806A1 (en) | 2010-05-31 | 2012-03-08 | Erik Johan Vendel Backlund | User interface with three dimensional user input |
US20110304745A1 (en) | 2010-06-10 | 2011-12-15 | Microsoft Corporation | Light transport reconstruction from sparsely captured images |
US20110311046A1 (en) | 2010-06-21 | 2011-12-22 | Kyocera Mita Corporation | Image Forming System, Image Forming Apparatus, and Method in which an Application is Added |
US20130094101A1 (en) | 2010-06-29 | 2013-04-18 | Kowa Company Ltd. | Telephoto lens unit |
US20110316968A1 (en) | 2010-06-29 | 2011-12-29 | Yuichi Taguchi | Digital Refocusing for Wide-Angle Images Using Axial-Cone Cameras |
US20130127901A1 (en) | 2010-08-27 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Calibrating Focused Plenoptic Camera Data |
US20130128087A1 (en) | 2010-08-27 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Super-Resolution in Integral Photography |
US20120056889A1 (en) | 2010-09-07 | 2012-03-08 | Microsoft Corporation | Alternate source for controlling an animation |
US20120056982A1 (en) | 2010-09-08 | 2012-03-08 | Microsoft Corporation | Depth camera based on structured light and stereo vision |
US20130215226A1 (en) | 2010-09-22 | 2013-08-22 | Laurent Chauvier | Enriched Digital Photographs |
US20130188068A1 (en) | 2010-10-06 | 2013-07-25 | Hewlett-Packard Development Company, L.P. | Systems and methods for acquiring and processing image data produced by camera arrays |
US20130234935A1 (en) | 2010-10-26 | 2013-09-12 | Bae Systems Plc | Display assembly |
US20130243391A1 (en) | 2010-11-23 | 2013-09-19 | Samsung Electronics Co., Ltd. | Method and apparatus for creating a media file for multilayer images in a multimedia system, and media-file-reproducing apparatus using same |
US20130242137A1 (en) | 2010-11-25 | 2013-09-19 | Lester Kirkland | Imaging robot |
US20120133746A1 (en) | 2010-11-29 | 2012-05-31 | DigitalOptics Corporation Europe Limited | Portrait Image Synthesis from Multiple Images Captured on a Handheld Device |
US20120147205A1 (en) | 2010-12-14 | 2012-06-14 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using super-resolution processes |
US8581998B2 (en) | 2010-12-17 | 2013-11-12 | Canon Kabushiki Kaisha | Image sensing apparatus and method of controlling the image sensing apparatus |
US20130082905A1 (en) | 2011-01-18 | 2013-04-04 | Disney Enterprises, Inc. | Multi-layer plenoptic displays that combine multiple emissive and light modulating planes |
US20120183055A1 (en) | 2011-01-18 | 2012-07-19 | Vidyo, Inc. | Temporal Prediction Structure Aware Temporal Filter |
US20120188344A1 (en) | 2011-01-20 | 2012-07-26 | Canon Kabushiki Kaisha | Systems and methods for collaborative image capturing |
US8768102B1 (en) | 2011-02-09 | 2014-07-01 | Lytro, Inc. | Downsampling light field images |
US8665440B1 (en) | 2011-02-10 | 2014-03-04 | Physical Optics Corporation | Pseudo-apposition eye spectral imaging system |
US20120206574A1 (en) | 2011-02-15 | 2012-08-16 | Nintendo Co., Ltd. | Computer-readable storage medium having stored therein display control program, display control apparatus, display control system, and display control method |
US20140176592A1 (en) | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
US20120224787A1 (en) | 2011-03-02 | 2012-09-06 | Canon Kabushiki Kaisha | Systems and methods for image capturing |
US20120229691A1 (en) | 2011-03-10 | 2012-09-13 | Canon Kabushiki Kaisha | Image pickup apparatus having lens array and image pickup optical system |
US20120249819A1 (en) | 2011-03-28 | 2012-10-04 | Canon Kabushiki Kaisha | Multi-modal image capture |
US20120249529A1 (en) | 2011-03-31 | 2012-10-04 | Fujifilm Corporation | 3d image displaying apparatus, 3d image displaying method, and 3d image displaying program |
US20120251131A1 (en) | 2011-03-31 | 2012-10-04 | Henderson Thomas A | Compensating for periodic nonuniformity in electrophotographic printer |
US20130070059A1 (en) | 2011-03-31 | 2013-03-21 | Sony Corporation | Image processing device, image processing method and image processing computer program product |
US20120257065A1 (en) | 2011-04-08 | 2012-10-11 | Qualcomm Incorporated | Systems and methods to calibrate a multi camera device |
US20120257795A1 (en) | 2011-04-08 | 2012-10-11 | Lg Electronics Inc. | Mobile terminal and image depth control method thereof |
US20120271115A1 (en) | 2011-04-21 | 2012-10-25 | Andre Buerk | Light-conducting device for an endoscope |
US8848970B2 (en) | 2011-04-26 | 2014-09-30 | Digimarc Corporation | Salient point-based arrangements |
US20140059462A1 (en) | 2011-05-04 | 2014-02-27 | Sony Ericsson Mobile Communications Ab | Method, graphical user interface, and computer program product for processing of a light field image |
US20120287329A1 (en) | 2011-05-09 | 2012-11-15 | Canon Kabushiki Kaisha | Image processing apparatus and method thereof |
US20120287296A1 (en) | 2011-05-10 | 2012-11-15 | Canon Kabushiki Kaisha | Imaging apparatus, method of controlling the same, and program |
US20120287246A1 (en) | 2011-05-11 | 2012-11-15 | Canon Kabushiki Kaisha | Image processing apparatus capable of displaying image indicative of face area, method of controlling the image processing apparatus, and storage medium |
US20120300091A1 (en) | 2011-05-23 | 2012-11-29 | Shroff Sapna A | Focusing and Focus Metrics for a Plenoptic Imaging System |
US8531581B2 (en) | 2011-05-23 | 2013-09-10 | Ricoh Co., Ltd. | Focusing and focus metrics for a plenoptic imaging system |
US8605199B2 (en) | 2011-06-28 | 2013-12-10 | Canon Kabushiki Kaisha | Adjustment of imaging properties for an imaging assembly having light-field optics |
US20130002902A1 (en) | 2011-06-30 | 2013-01-03 | Nikon Corporation | Flare determination apparatus, image processing apparatus, and storage medium storing flare determination program |
US20130002936A1 (en) | 2011-06-30 | 2013-01-03 | Nikon Corporation | Image pickup apparatus, image processing apparatus, and storage medium storing image processing program |
US20130021486A1 (en) | 2011-07-22 | 2013-01-24 | Naturalpoint, Inc. | Hosted camera remote control |
US9419049B2 (en) | 2011-08-01 | 2016-08-16 | Lytro, Inc. | Optical assembly including plenoptic microlens array |
US9184199B2 (en) | 2011-08-01 | 2015-11-10 | Lytro, Inc. | Optical assembly including plenoptic microlens array |
US9305956B2 (en) | 2011-08-01 | 2016-04-05 | Lytro, Inc. | Optical assembly including plenoptic microlens array |
US20130038696A1 (en) | 2011-08-10 | 2013-02-14 | Yuanyuan Ding | Ray Image Modeling for Fast Catadioptric Light Field Rendering |
US20130041215A1 (en) | 2011-08-12 | 2013-02-14 | Ian McDowall | Feature differentiation image capture unit and method in a surgical instrument |
US20130050546A1 (en) | 2011-08-30 | 2013-02-28 | Canon Kabushiki Kaisha | Image processing apparatus and method |
US20130129213A1 (en) | 2011-08-31 | 2013-05-23 | Elya Shechtman | Non-Rigid Dense Correspondence |
US20130064453A1 (en) | 2011-09-08 | 2013-03-14 | Casio Computer Co., Ltd. | Interpolation image generation apparatus, reconstructed image generation apparatus, method of generating interpolation image, and computer-readable recording medium storing program |
US8879901B2 (en) | 2011-09-13 | 2014-11-04 | Caldwell Photographic, Inc. | Optical attachment for reducing the focal length of an objective lens |
US20130064532A1 (en) | 2011-09-13 | 2013-03-14 | J. Brian Caldwell | Optical attachment for reducing the focal length of an objective lens |
US20130070060A1 (en) | 2011-09-19 | 2013-03-21 | Pelican Imaging Corporation | Systems and methods for determining depth from multiple views of a scene that include aliasing using hypothesized fusion |
US8593564B2 (en) | 2011-09-22 | 2013-11-26 | Apple Inc. | Digital camera including refocusable imaging mode adaptor |
US20130077880A1 (en) | 2011-09-28 | 2013-03-28 | Pelican Imaging Corporation | Systems and methods for encoding light field image files |
US8542933B2 (en) | 2011-09-28 | 2013-09-24 | Pelican Imaging Corporation | Systems and methods for decoding light field image files |
US20130088616A1 (en) | 2011-10-10 | 2013-04-11 | Apple Inc. | Image Metadata Control Based on Privacy Rules |
US20130093844A1 (en) | 2011-10-14 | 2013-04-18 | Kabushiki Kaisha Toshiba | Electronic apparatus and display control method |
US20130120636A1 (en) | 2011-11-10 | 2013-05-16 | Apple Inc. | Illumination system |
US20130135448A1 (en) | 2011-11-28 | 2013-05-30 | Sony Corporation | Image processing device and method, recording medium, and program |
US20130176481A1 (en) | 2012-01-09 | 2013-07-11 | Lifetouch Inc. | Video Photography System |
US20130215108A1 (en) | 2012-02-21 | 2013-08-22 | Pelican Imaging Corporation | Systems and Methods for the Manipulation of Captured Light Field Image Data |
US20160029017A1 (en) | 2012-02-28 | 2016-01-28 | Lytro, Inc. | Calibration of light-field camera geometry via robust fitting |
US9386288B2 (en) | 2012-02-28 | 2016-07-05 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US20150097985A1 (en) | 2012-02-28 | 2015-04-09 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US8995785B2 (en) | 2012-02-28 | 2015-03-31 | Lytro, Inc. | Light-field processing and analysis, camera control, and user interfaces and interaction on light-field capture devices |
US8811769B1 (en) | 2012-02-28 | 2014-08-19 | Lytro, Inc. | Extended depth of field and variable center of perspective in light-field processing |
US8831377B2 (en) | 2012-02-28 | 2014-09-09 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
US9172853B2 (en) | 2012-02-28 | 2015-10-27 | Lytro, Inc. | Microlens array architecture for avoiding ghosting in projected images |
US20130222656A1 (en) | 2012-02-28 | 2013-08-29 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
US8971625B2 (en) | 2012-02-28 | 2015-03-03 | Lytro, Inc. | Generating dolly zoom effect using light field image data |
US8948545B2 (en) | 2012-02-28 | 2015-02-03 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US20140368640A1 (en) | 2012-02-29 | 2014-12-18 | Flir Systems Ab | Method and system for performing alignment of a projection image to detected infrared (ir) radiation information |
US20130258451A1 (en) | 2012-03-27 | 2013-10-03 | Ostendo Technologies, Inc. | Spatio-Temporal Directional Light Modulator |
US20130262511A1 (en) | 2012-04-02 | 2013-10-03 | Google Inc. | Determining 3D Model Information From Stored Images |
US20150130986A1 (en) | 2012-04-25 | 2015-05-14 | Nikon Corporation | Focus detection device, focus adjustment device and camera |
US20130286236A1 (en) | 2012-04-27 | 2013-10-31 | Research In Motion Limited | System and method of adjusting camera image data |
US20160173844A1 (en) | 2012-05-09 | 2016-06-16 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US9866810B2 (en) | 2012-05-09 | 2018-01-09 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US20180070067A1 (en) | 2012-05-09 | 2018-03-08 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US20180070066A1 (en) | 2012-05-09 | 2018-03-08 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US9300932B2 (en) | 2012-05-09 | 2016-03-29 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US20150062386A1 (en) | 2012-05-10 | 2015-03-05 | Fujifilm Corporation | Imaging device and signal correcting method |
US8736710B2 (en) | 2012-05-24 | 2014-05-27 | International Business Machines Corporation | Automatic exposure control for flash photography |
US20140133749A1 (en) | 2012-05-31 | 2014-05-15 | Apple Inc. | Systems And Methods For Statistics Collection Using Pixel Mask |
US8953882B2 (en) | 2012-05-31 | 2015-02-10 | Apple Inc. | Systems and methods for determining noise statistics of image data |
US20130321677A1 (en) | 2012-05-31 | 2013-12-05 | Apple Inc. | Systems and methods for raw image processing |
US20160191823A1 (en) | 2012-06-01 | 2016-06-30 | Ostendo Technologies, Inc. | Spatio-Temporal Light Field Cameras |
US9774800B2 (en) | 2012-06-01 | 2017-09-26 | Ostendo Technologies, Inc. | Spatio-temporal light field cameras |
US20130321581A1 (en) | 2012-06-01 | 2013-12-05 | Ostendo Technologies, Inc. | Spatio-Temporal Light Field Cameras |
US9681069B2 (en) | 2012-06-01 | 2017-06-13 | Ostendo Technologies, Inc. | Spatio-temporal light field cameras |
US20130321574A1 (en) | 2012-06-04 | 2013-12-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US20130329132A1 (en) | 2012-06-06 | 2013-12-12 | Apple Inc. | Flare Detection and Mitigation in Panoramic Images |
US20130329107A1 (en) | 2012-06-11 | 2013-12-12 | Disney Enterprises, Inc. | Streaming Light Propagation |
US20130335596A1 (en) | 2012-06-15 | 2013-12-19 | Microsoft Corporation | Combining multiple images in bracketed photography |
US9607424B2 (en) | 2012-06-26 | 2017-03-28 | Lytro, Inc. | Depth-assigned content for depth-enhanced pictures |
US20130342700A1 (en) | 2012-06-26 | 2013-12-26 | Aharon Kass | System and method for using pattern matching to determine the presence of designated objects in digital images |
US20180082405A1 (en) | 2012-06-26 | 2018-03-22 | Lytro, Inc. | Depth-based image blurring |
US20170302903A1 (en) | 2012-06-26 | 2017-10-19 | Lytro, Inc. | Depth-assigned content for depth-enhanced virtual reality images |
US20140002502A1 (en) | 2012-06-27 | 2014-01-02 | Samsung Electronics Co., Ltd. | Method and apparatus for outputting graphics to a display |
US20140002699A1 (en) | 2012-06-27 | 2014-01-02 | Honeywell International Inc. doing business as (d.b.a) Honeywell Scanning and Mobility | Imaging apparatus having imaging lens |
US20140003719A1 (en) | 2012-06-29 | 2014-01-02 | Xue Bai | Adaptive Trimap Propagation for Video Matting |
US20140035959A1 (en) | 2012-08-04 | 2014-02-06 | Paul Lapstun | Light Field Display Device and Method |
US8754829B2 (en) | 2012-08-04 | 2014-06-17 | Paul Lapstun | Scanning light field camera and display |
US20140245367A1 (en) | 2012-08-10 | 2014-08-28 | Panasonic Corporation | Method for providing a video, transmitting device, and receiving device |
US20150207990A1 (en) | 2012-08-20 | 2015-07-23 | The Regents Of The University Of California | Monocentric lens designs and associated imaging systems having wide field of view and high resolution |
US8619082B1 (en) | 2012-08-21 | 2013-12-31 | Pelican Imaging Corporation | Systems and methods for parallax detection and correction in images captured using array cameras that contain occlusions using subsets of images to perform depth estimation |
US20150042767A1 (en) | 2012-08-21 | 2015-02-12 | Pelican Imaging Corporation | Systems and Methods for Measuring Depth Based Upon Occlusion Patterns in Images |
US20150049915A1 (en) | 2012-08-21 | 2015-02-19 | Pelican Imaging Corporation | Systems and Methods for Generating Depth Maps and Corresponding Confidence Maps Indicating Depth Estimation Reliability |
US20140168484A1 (en) | 2012-09-11 | 2014-06-19 | Satoshi Suzuki | Image processing apparatus, image processing method, and program, and image pickup apparatus with image processing apparatus |
US9214013B2 (en) | 2012-09-14 | 2015-12-15 | Pelican Imaging Corporation | Systems and methods for correcting user identified artifacts in light field images |
US20140085282A1 (en) | 2012-09-21 | 2014-03-27 | Nvidia Corporation | See-through optical image processing |
US20140092424A1 (en) | 2012-09-28 | 2014-04-03 | Interactive Memories, Inc. | Methods for Real Time Discovery, Selection, and Engagement of Most Economically Feasible Printing Service Vendors among Multiple Known Vendors |
US20140193047A1 (en) | 2012-09-28 | 2014-07-10 | Interactive Memories, Inc. | Systems and methods for generating autoflow of content based on image and user analysis as well as use case data for a media-based printable product |
US20140195921A1 (en) | 2012-09-28 | 2014-07-10 | Interactive Memories, Inc. | Methods and systems for background uploading of media files for improved user experience in production of media-based products |
US20140098191A1 (en) | 2012-10-05 | 2014-04-10 | Vidinoti Sa | Annotation method and apparatus |
US20140167196A1 (en) | 2012-11-02 | 2014-06-19 | Heptagon Micro Optics Pte. Ltd. | Optical modules including focal length adjustment and fabrication of the optical modules |
US8997021B2 (en) | 2012-11-06 | 2015-03-31 | Lytro, Inc. | Parallax and/or three-dimensional effects for thumbnail image displays |
US20140139538A1 (en) | 2012-11-19 | 2014-05-22 | Datacolor Holding Ag | Method and apparatus for optimizing image quality based on measurement of image processing artifacts |
US20150312553A1 (en) | 2012-12-04 | 2015-10-29 | Lytro, Inc. | Capturing and relighting images using multiple devices |
US9001226B1 (en) | 2012-12-04 | 2015-04-07 | Lytro, Inc. | Capturing and relighting images using multiple devices |
US20150237273A1 (en) | 2012-12-05 | 2015-08-20 | Fujifilm Corporation | Image capture device, anomalous oblique incident light detection method, and recording medium |
US9262067B1 (en) | 2012-12-10 | 2016-02-16 | Amazon Technologies, Inc. | Approaches for displaying alternate views of information |
US20140176540A1 (en) | 2012-12-20 | 2014-06-26 | Ricoh Co., Ltd. | Occlusion-Aware Reconstruction of Three-Dimensional Scenes from Light Field Images |
US20140177905A1 (en) | 2012-12-20 | 2014-06-26 | United Video Properties, Inc. | Methods and systems for customizing a plenoptic media asset |
US20140184885A1 (en) | 2012-12-28 | 2014-07-03 | Canon Kabushiki Kaisha | Image capture apparatus and method for controlling the same |
US20150304667A1 (en) | 2013-01-04 | 2015-10-22 | GE Video Compression, LLC. | Efficient scalable coding concept |
US20140192208A1 (en) | 2013-01-08 | 2014-07-10 | Peripheral Vision, Inc. | Lighting System Characterization |
US20140204111A1 (en) | 2013-01-18 | 2014-07-24 | Karthik Vaidyanathan | Layered light field reconstruction for defocus blur |
US20140218540A1 (en) | 2013-02-05 | 2014-08-07 | Google Inc. | Noise Models for Image Processing |
US20140226038A1 (en) | 2013-02-12 | 2014-08-14 | Canon Kabushiki Kaisha | Image processing apparatus, image capturing apparatus, control method, and recording medium |
US9497380B1 (en) | 2013-02-15 | 2016-11-15 | Red.Com, Inc. | Dense field imaging |
US9201193B1 (en) | 2013-02-18 | 2015-12-01 | Exelis, Inc. | Textured fiber optic coupled image intensified camera |
US20140240578A1 (en) | 2013-02-22 | 2014-08-28 | Lytro, Inc. | Light-field based autofocus |
US20140267639A1 (en) | 2013-03-13 | 2014-09-18 | Kabushiki Kaisha Toshiba | Image display apparatus |
US20140267243A1 (en) | 2013-03-13 | 2014-09-18 | Pelican Imaging Corporation | Systems and Methods for Synthesizing Images from Image Data Captured by an Array Camera Using Restricted Depth of Field Depth Maps in which Depth Estimation Precision Varies |
US9201142B2 (en) | 2013-03-14 | 2015-12-01 | Navico Holding As | Sonar and radar display |
US20150161798A1 (en) | 2013-03-15 | 2015-06-11 | Pelican Imaging Corporation | Array Cameras Including an Array Camera Module Augmented with a Separate Camera |
US20150264337A1 (en) | 2013-03-15 | 2015-09-17 | Pelican Imaging Corporation | Autofocus System for a Conventional Camera That Uses Depth Information from an Array Camera |
US20140300753A1 (en) | 2013-04-04 | 2014-10-09 | Apple Inc. | Imaging pipeline for spectro-colorimeters |
US20160037178A1 (en) | 2013-04-05 | 2016-02-04 | Samsung Electronics Co., Ltd. | Video encoding method and apparatus thereof and a video decoding method and apparatus thereof |
US20140313350A1 (en) | 2013-04-19 | 2014-10-23 | Aptina Imaging Corporation | Imaging systems with reference pixels for image flare mitigation |
US20140313375A1 (en) | 2013-04-19 | 2014-10-23 | Aptina Imaging Corporation | Systems and methods for mitigating image sensor pixel value clipping |
US20160269620A1 (en) | 2013-04-22 | 2016-09-15 | Lytro, Inc. | Phase detection autofocus using subaperture images |
US20160065931A1 (en) | 2013-05-14 | 2016-03-03 | Huawei Technologies Co., Ltd. | Method and Apparatus for Computing a Synthesized Picture |
US20140340390A1 (en) | 2013-05-17 | 2014-11-20 | Nvidia Corporation | System, method, and computer program product to produce images for a near-eye light field display having a defect |
US9647150B2 (en) | 2013-05-21 | 2017-05-09 | Jorge Vicente Blasco Claret | Monolithic integration of plenoptic lenses on photosensor substrates |
US20140347540A1 (en) | 2013-05-23 | 2014-11-27 | Samsung Electronics Co., Ltd | Image display method, image display apparatus, and recording medium |
US20140354863A1 (en) | 2013-05-31 | 2014-12-04 | Samsung Electronics Co., Ltd. | Image Sensors and Imaging Devices Including the Same |
US20140368494A1 (en) | 2013-06-18 | 2014-12-18 | Nvidia Corporation | Method and system for rendering simulated depth-of-field visual effect |
US8903232B1 (en) | 2013-08-05 | 2014-12-02 | Caldwell Photographic, Inc. | Optical attachment for reducing the focal length of an objective lens |
US20150062178A1 (en) | 2013-09-05 | 2015-03-05 | Facebook, Inc. | Tilting to scroll |
US9013611B1 (en) | 2013-09-06 | 2015-04-21 | Xilinx, Inc. | Method and device for generating a digital image based upon a selected set of chrominance groups |
US20160381348A1 (en) | 2013-09-11 | 2016-12-29 | Sony Corporation | Image processing device and method |
US20160227244A1 (en) | 2013-09-13 | 2016-08-04 | Canon Kabushiki Kaisha | Method, apparatus and system for encoding and decoding video data |
US20150092071A1 (en) | 2013-09-28 | 2015-04-02 | Ricoh Co., Ltd. | Color filter modules for plenoptic xyz imaging systems |
US20150223731A1 (en) | 2013-10-09 | 2015-08-13 | Nedim T. SAHIN | Systems, environment and methods for identification and analysis of recurring transitory physiological states and events using a wearable data collection device |
US20150104101A1 (en) | 2013-10-14 | 2015-04-16 | Apple Inc. | Method and ui for z depth image segmentation |
US9294662B2 (en) | 2013-10-16 | 2016-03-22 | Broadcom Corporation | Depth map generation and post-capture focusing |
US20150193937A1 (en) | 2013-12-12 | 2015-07-09 | Qualcomm Incorporated | Method and apparatus for generating plenoptic depth maps |
US9628684B2 (en) | 2013-12-24 | 2017-04-18 | Lytro, Inc. | Light-field aberration correction |
US9392153B2 (en) | 2013-12-24 | 2016-07-12 | Lytro, Inc. | Plenoptic camera resolution |
US20150206340A1 (en) | 2014-01-17 | 2015-07-23 | Carl J. Munkberg | Layered Reconstruction for Defocus and Motion Blur |
US9305375B2 (en) | 2014-03-25 | 2016-04-05 | Lytro, Inc. | High-quality post-rendering depth blur |
US20150288867A1 (en) | 2014-04-02 | 2015-10-08 | Canon Kabushiki Kaisha | Image processing apparatus, image capturing apparatus, and control method thereof |
US20150304544A1 (en) | 2014-04-16 | 2015-10-22 | Canon Kabushiki Kaisha | Image pickup apparatus, image processing method, and recording medium |
US20150312593A1 (en) | 2014-04-24 | 2015-10-29 | Lytro, Inc. | Compression of light field images |
US20150310592A1 (en) | 2014-04-25 | 2015-10-29 | Canon Kabushiki Kaisha | Image processing apparatus that performs image restoration processing and image processing method |
US20150334420A1 (en) | 2014-05-13 | 2015-11-19 | Alcatel Lucent | Method and apparatus for encoding and decoding video |
US20150346832A1 (en) | 2014-05-29 | 2015-12-03 | Nextvr Inc. | Methods and apparatus for delivering content and/or playing back content |
US8988317B1 (en) | 2014-06-12 | 2015-03-24 | Lytro, Inc. | Depth determination for light field images |
US20150370011A1 (en) | 2014-06-18 | 2015-12-24 | Canon Kabushiki Kaisha | Image pickup apparatus |
US20150370012A1 (en) | 2014-06-18 | 2015-12-24 | Canon Kabushiki Kaisha | Imaging apparatus |
US20150373279A1 (en) | 2014-06-20 | 2015-12-24 | Qualcomm Incorporated | Wide field of view array camera for hemispheric and spherical imaging |
US20160029002A1 (en) | 2014-07-26 | 2016-01-28 | Soeren Balko | Platform-agnostic Video Player For Mobile Computing Devices And Desktop Computers |
US9210391B1 (en) | 2014-07-31 | 2015-12-08 | Apple Inc. | Sensor data rescaler with chroma reduction |
US20160065947A1 (en) | 2014-09-03 | 2016-03-03 | Nextvr Inc. | Methods and apparatus for receiving and/or playing back content |
US9635332B2 (en) | 2014-09-08 | 2017-04-25 | Lytro, Inc. | Saturated pixel recovery in light-field images |
US20170316602A1 (en) | 2014-10-31 | 2017-11-02 | Nokia Technologies Oy | Method for alignment of low-quality noisy depth map to the high-resolution colour image |
US20170221226A1 (en) | 2014-11-04 | 2017-08-03 | SZ DJI Technology Co., Ltd. | Camera calibration |
US20160142615A1 (en) | 2014-11-13 | 2016-05-19 | Lytro, Inc. | Robust layered light-field rendering |
US20160155215A1 (en) | 2014-11-27 | 2016-06-02 | Samsung Display Co., Ltd. | Image processing device, and an image processing method |
US20160165206A1 (en) | 2014-12-03 | 2016-06-09 | National Tsing Hua University | Digital refocusing method |
US20160182893A1 (en) | 2014-12-22 | 2016-06-23 | Canon Kabushiki Kaisha | Multiscale depth estimation using depth from defocus |
US20160247324A1 (en) | 2015-02-25 | 2016-08-25 | Brian Mullins | Augmented reality content creation |
US20160253837A1 (en) | 2015-02-26 | 2016-09-01 | Lytro, Inc. | Parallax bounce |
US20160307372A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Capturing light-field volume image and video data using tiled light-field cameras |
US20170139131A1 (en) | 2015-04-15 | 2017-05-18 | Lytro, Inc. | Coherent fiber array with dense fiber optic bundles for light-field and high resolution image acquisition |
US20160309065A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Light guided image plane tiled arrays with dense fiber optic bundles for light-field and high resolution image acquisition |
US20170237971A1 (en) | 2015-04-15 | 2017-08-17 | Lytro, Inc. | Image capture for virtual reality displays |
US20170243373A1 (en) | 2015-04-15 | 2017-08-24 | Lytro, Inc. | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US20170365068A1 (en) | 2015-04-15 | 2017-12-21 | Lytro, Inc. | Combining light-field data with active depth data for depth map generation |
US20180033209A1 (en) | 2015-04-15 | 2018-02-01 | Lytro, Inc. | Stereo image generation and interactive playback |
US20160307368A1 (en) | 2015-04-17 | 2016-10-20 | Lytro, Inc. | Compression and interactive playback of light field pictures |
US20160337635A1 (en) | 2015-05-15 | 2016-11-17 | Semyon Nisenzon | Generarting 3d images using multi-resolution camera set |
US20160353006A1 (en) | 2015-05-29 | 2016-12-01 | Phase One A/S | Adaptive autofocusing system |
US20160353026A1 (en) | 2015-05-29 | 2016-12-01 | Thomson Licensing | Method and apparatus for displaying a light field based image on a user's device, and corresponding computer program product |
US9979909B2 (en) | 2015-07-24 | 2018-05-22 | Lytro, Inc. | Automatic lens flare detection and correction for light-field images |
US20170031146A1 (en) | 2015-07-27 | 2017-02-02 | University Of Connecticut | Imaging Assemblies With Rapid Sample Auto-Focusing |
US20170059305A1 (en) | 2015-08-25 | 2017-03-02 | Lytro, Inc. | Active illumination for enhanced depth map generation |
US9639945B2 (en) | 2015-08-27 | 2017-05-02 | Lytro, Inc. | Depth-based application of image effects |
US20170067832A1 (en) | 2015-09-08 | 2017-03-09 | Xerox Corporation | Methods and devices for improved accuracy of test results |
US20170078578A1 (en) | 2015-09-10 | 2017-03-16 | Canon Kabushiki Kaisha | Image capture apparatus and method of controlling the same |
US9858649B2 (en) | 2015-09-30 | 2018-01-02 | Lytro, Inc. | Depth-based image blurring |
US20170094906A1 (en) | 2015-10-06 | 2017-04-06 | Deere & Company | System and Method For Clearing A Feeder House and Belt Pickup |
US20170374411A1 (en) | 2016-01-17 | 2017-12-28 | Bitmovin Gmbh | Adaptive streaming of an immersive video scene |
US10244266B1 (en) * | 2016-02-11 | 2019-03-26 | Amazon Technologies, Inc. | Noisy media content encoding |
US20170256036A1 (en) | 2016-03-03 | 2017-09-07 | Lytro, Inc. | Automatic microlens array artifact correction for light-field images |
US20170263012A1 (en) | 2016-03-14 | 2017-09-14 | Thomson Licensing | Method and device for processing lightfield data |
US20170358092A1 (en) | 2016-06-09 | 2017-12-14 | Lytro, Inc. | Multi-view scene segmentation and propagation |
US20180007253A1 (en) | 2016-06-17 | 2018-01-04 | Canon Kabushiki Kaisha | Focus detection apparatus, focus control apparatus, image capturing apparatus, focus detection method, and storage medium |
US20180012397A1 (en) | 2016-07-08 | 2018-01-11 | Lytro, Inc. | Immersive content framing |
US20180024753A1 (en) | 2016-07-21 | 2018-01-25 | HGST Netherlands B.V. | Internally preconditioning solid state drives for various workloads |
US20180124371A1 (en) | 2016-10-31 | 2018-05-03 | Verizon Patent And Licensing Inc. | Methods and Systems for Generating Depth Data by Converging Independently-Captured Depth Maps |
US20180139436A1 (en) | 2016-11-11 | 2018-05-17 | Disney Enterprises, Inc. | Object reconstruction from dense light fields via depth from gradients |
US20180158198A1 (en) | 2016-12-05 | 2018-06-07 | Lytro, Inc. | Multi-view rotoscope contour propagation |
US9900510B1 (en) | 2016-12-08 | 2018-02-20 | Lytro, Inc. | Motion blur for light-field images |
US20180199039A1 (en) * | 2017-01-11 | 2018-07-12 | Microsoft Technology Licensing, Llc | Reprojecting Holographic Video to Enhance Streaming Bandwidth/Quality |
Non-Patent Citations (192)
Title |
---|
Adelsberger, R. et al., "Spatially Adaptive Photographic Flash," ETH Zurich, Department of Computer Science, Technical Report 612, 2008, pp. 1-12. |
Adelson et al., "Single Lens Stereo with a Plenoptic Camera" IEEE Translation on Pattern Analysis and Machine Intelligence, Feb. 1992. vol. 14, No. 2, pp. 99-106. |
Adelson, E. H., and Bergen, J. R. 1991. The plenoptic function and the elements of early vision. In Computational Models of Visual Processing, edited by Michael S. Landy and J. Anthony Movshon. Cambridge, Mass.: mit Press. |
Adobe Systems Inc, "XMP Specification", Sep. 2005. |
Adobe, "Photoshop CS6 / in depth: Digital Negative (DNG)", http://www.adobe.com/products/photoshop/extend.displayTab2html. Retrieved Jan. 2013. |
Agarwala, A., et al., "Interactive Digital Photomontage," ACM Transactions on Graphics, Proceedings of SIGGRAPH 2004, vol. 32, No. 3, 2004. |
Andreas Observatory, Spectrograph Manual: IV. Flat-Field Correction, Jul. 2006. |
Apple, "Apple iPad: Photo Features on the iPad", Retrieved Jan. 2013. |
Bae, S., et al., "Defocus Magnification", Computer Graphics Forum, vol. 26, Issue 3 (Proc. Of Eurographics 2007), pp. 1-9. |
Belhumeur, Peter et al., "The Bas-Relief Ambiguity", International Journal of Computer Vision, 1997, pp. 1060-1066. |
Belhumeur, Peter, et al., "The Bas-Relief Ambiguity", International Journal of Computer Vision, 1999, pp. 33-44, revised version. |
Bhat, P. et al. "GradientShop: A Gradient-Domain Optimization Framework for Image and Video Filtering," SIGGRAPH 2010; 14 pages. |
Bolles, R., et al., "Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion", International Journal of Computer Vision, 1, 7-55 (1987). |
Bourke, Paul, "Image filtering in the Frequency Domain," pp. 1-9, Jun. 1998. |
Canon, Canon Speedlite wireless flash system, User manual for Model 550EX, Sep. 1998. |
Chai, Jin-Xang et al., "Plenoptic Sampling", ACM SIGGRAPH 2000, Annual Conference Series, 2000, pp. 307-318. |
Chen, S. et al., "A CMOS Image Sensor with On-Chip Image Compression Based on Predictive Boundary Adaptation and Memoryless QTD Algorithm," Very Large Scalee Integration (VLSI) Systems, IEEE Transactions, vol. 19, Issue 4; Apr. 2011. |
Chen, W., et al., "Light Field mapping: Efficient representation and hardware rendering of surface light fields", ACM Transactions on Graphics 21, 3, 447-456, 2002. |
Cohen, Noy et al., "Enhancing the performance of the light field microscope using wavefront coding," Optics Express, vol. 22, issue 20; 2014. |
Daly, D., "Microlens Arrays" Retrieved Jan. 2013. |
Debevec, et al, "A Lighting Reproduction Approach to Live-Action Compoisting" Proceedings SIGGRAPH 2002. |
Debevec, P., et al., "Acquiring the reflectance field of a human face", SIGGRAPH 2000. |
Debevec, P., et al., "Recovering high dynamic radiance maps from photographs", SIGGRAPH 1997, 369-378. |
Design of the xBox menu. Retrieved Jan. 2013. |
Digital Photography Review, "Sony Announce new RGBE CCD," Jul. 2003. |
Dorsey, J., et al., "Design and simulation of opera light and projection effects", in Computer Graphics (Proceedings of SIGGRAPH 91), vol. 25, 41-50, 1991. |
Dorsey, J., et al., "Interactive design of complex time dependent lighting", IEEE Computer Graphics and Applications 15, 2 (Mar. 1995), 26-36. |
Dowski et al., "Wavefront coding: a modern method of achieving high performance and/or low cost imaging systems" SPIE Proceedings, vol. 3779, Jul. 1999, pp. 137-145. |
Dowski, Jr. "Extended Depth of Field Through Wave-Front Coding," Applied Optics, vol. 34, No. 11, Apr. 10, 1995; pp. 1859-1866. |
Duparre, J. et al., "Micro-Optical Artificial Compound Eyes," Institute of Physics Publishing, Apr. 2006. |
Eisemann, Elmar, et al., "Flash Photography Enhancement via Intrinsic Relighting", SIGGRAPH 2004. |
Fattal, Raanan, et al., "Multiscale Shape and Detail Enhancement from Multi-light Image Collections", SIGGRAPH 2007. |
Fernando, Randima, "Depth of Field-A Survey of Techniques," GPU Gems. Boston, MA; Addison-Wesley, 2004. |
Fernando, Randima, "Depth of Field—A Survey of Techniques," GPU Gems. Boston, MA; Addison-Wesley, 2004. |
Fitzpatrick, Brad, "Camlistore", Feb. 1, 2011. |
Fujifilm, Super CCD EXR Sensor by Fujifilm, brochure reference No. EB-807E, 2008. |
Georgiev, T. et al., "Reducing Plenoptic Camera Artifacts," Computer Graphics Forum, vol. 29, No. 6, pp. 1955-1968; 2010. |
Georgiev, T., et al., "Spatio-Angular Resolution Tradeoff in Integral Photography," Proceedings of Eurographics Symposium on Rendering, 2006. |
Georgiev, T., et al., "Suppersolution with Plenoptic 2.0 Cameras," Optical Society of America 2009; pp. 1-3. |
Georgiev, T., et al., "Unified Frequency Domain Analysis of Lightfield Cameras" (2008). |
Georgiev, T., et al., Plenoptic Camera 2.0 (2008). |
Girod, B., "Mobile Visual Search", IEEE Signal Processing Magazine, Jul. 2011. |
Gortler et al., "The lumigraph" SIGGRAPH 96, pp. 43-54, 1996. |
Groen et al., "A Comparison of Different Focus Functions for Use in Autofocus Algorithms," Cytometry 6:81-91, 1985. |
Haeberli, Paul "A Multifocus Method for Controlling Depth of Field" GRAPHICA Obscura, 1994, pp. 1-3. |
Heide, F. et al., "High-Quality Computational Imaging Through Simple Lenses," ACM Transactions on Graphics, SIGGRAPH 2013; pp. 1-7. |
Heidelberg Collaboratory for Image Processing, "Consistent Depth Estimation in a 4D Light Field," May 2013. |
Hirigoyen, F., et al., "1.1 um Backside Imager vs. Frontside Image: an optics-dedicated FDTD approach", IEEE 2009 International Image Sensor Workshop. |
Huang, Fu-Chung et al., "Eyeglasses-free Display: Towards Correcting Visual Aberrations with Computational Light Field Displays," ACM Transaction on Graphics, Aug. 2014, pp. 1-12. |
Isaksen, A., et al., "Dynamically Reparameterized Light Fields," SIGGRAPH 2000, pp. 297-306. |
Ives H., "Optical properties of a Lippman lenticulated sheet," J. Opt. Soc. Am. 21, 171 (1931). |
Ives, H. "Parallax Panoramagrams Made with a Large Diameter Lens", Journal of the Optical Society of America; 1930. |
Jackson et al., "Selection of a Convolution Function for Fourier Inversion Using Gridding" IEEE Transactions on Medical Imaging, Sep. 1991, vol. 10, No. 3, pp. 473-478. |
Kautz, J., et al., "Fast arbitrary BRDF shading for low-frequency lighting using spherical harmonics", in Eurographic Rendering Workshop 2002, 291-296. |
Koltun, et al., "Virtual Occluders: An Efficient Interediate PVS Representation", Rendering Techniques 2000: Proc. 11th Eurographics Workshop Rendering, pp. 59-70, Jun. 2000. |
Kopf, J., et al., Deep Photo: Model-Based Photograph Enhancement and Viewing, SIGGRAPH Asia 2008. |
Lehtinen, J., et al. "Matrix radiance transfer", in Symposium on Interactive 3D Graphics, 59-64, 2003. |
Lesser, Michael, "Back-Side Illumination", 2009. |
Levin, A., et al., "Image and Depth from a Conventional Camera with a Coded Aperture", SIGGRAPH 2007, pp. 1-9. |
Levoy et al.,"Light Field Rendering" SIGGRAPH 96 Proceeding, 1996. pp. 31-42. |
Levoy, "Light Fields and Computational Imaging" IEEE Computer Society, Aug. 2006, pp. 46-55. |
Levoy, M. "Light Field Photography and Videography," Oct. 18, 2005. |
Levoy, M. "Stanford Light Field Microscope Project," 2008; http://graphics.stanford.edu/projects/lfmicroscope/, 4 pages. |
Levoy, M., "Autofocus: Contrast Detection", http://graphics.stanford.edu/courses/cs178/applets/autofocusPD.html, pp. 1-3, 2010. |
Levoy, M., "Autofocus: Phase Detection", http://graphics.stanford.edu/courses/cs178/applets/autofocusPD.html, pp. 1-3, 2010. |
Levoy, M., et al., "Light Field Microscopy," ACM Transactions on Graphics, vol. 25, No. 3, Proceedings SIGGRAPH 2006. |
Liang, Chia-Kai, et al., "Programmable Aperture Photography: Multiplexed Light Field Acquisition", ACM SIGGRAPH, 2008. |
Lippmann, "Reversible Prints", Communication at the French Society of Physics, Journal of Physics, 7 , 4, Mar. 1908, pp. 821-825. |
Lumsdaine et al., "Full Resolution Lightfield Rendering" Adobe Technical Report Jan. 2008, pp. 1-12. |
Maeda, Y. et al., "A CMOS Image Sensor with Pseudorandom Pixel Placement for Clear Imaging," 2009 International Symposium on Intelligent Signal Processing and Communication Systems, Dec. 2009. |
Magnor, M. et al., "Model-Aided Coding of Multi-Viewpoint Image Data," Proceedings IEEE Conference on Image Processing, ICIP-2000, Vancouver, Canada, Sep. 2000. https://graphics.tu-bs.de/static/people/magnor/publications/icip00.pdf. |
Mallat, Stephane, "A Wavelet Tour of Signal Processing", Academic Press 1998. |
Malzbender, et al., "Polynomial Texture Maps", Proceedings SIGGRAPH 2001. |
Marshall, Richard J. et al., "Improving Depth Estimation from a Plenoptic Camera by Patterned Illumination," Proc. Of SPIE, vol. 9528, 2015, pp. 1-6. |
Masselus, Vincent, et al., "Relighting with 4D Incident Light Fields", SIGGRAPH 2003. |
Meng, J. et al., "An Approach on Hardware Design for Computational Photography Applications Based on Light Field Refocusing Algorithm," Nov. 18, 2007, 12 pages. |
Meynants, G., et al., "Pixel Binning in CMOS Image Sensors," Frontiers in Electronic Imaging Conference, 2009. |
Moreno-Noguer, F. et al., "Active Refocusing of Images and Videos," ACM Transactions on Graphics, Aug. 2007; pp. 1-9. |
Munkberg, J. et al., "Layered Reconstruction for Defocus and Motion Blur" EGSR 2014, pp. 1-12. |
Naemura et al., "3-D Computer Graphics based on Integral Photography" Optics Express, Feb. 12, 2001. vol. 8, No. 2, pp. 255-262. |
Nakamura, J., "Image Sensors and Signal Processing for Digital Still Cameras" (Optical Science and Engineering), 2005. |
National Instruments, "Anatomy of a Camera," pp. 1-5, Sep. 6, 2006. |
Nayar, Shree, et al., "Shape from Focus", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, No. 8, pp. 824-831, Aug. 1994. |
Ng, R., et al. "Light Field Photography with a Hand-held Plenoptic Camera," Stanford Technical Report, CSTR 2005-2, 2005. |
Ng, R., et al., "All-Frequency Shadows Using Non-linear Wavelet Lighting Approximation. ACM Transactions on Graphics," ACM Transactions on Graphics; Proceedings of SIGGRAPH 2003. |
Ng, R., et al., "Triple Product Wavelet Integrals for All-Frequency Relighting", ACM Transactions on Graphics (Proceedings of SIGGRAPH 2004). |
Ng, Yi-Ren, "Digital Light Field Photography," Doctoral Thesis, Standford University, Jun. 2006; 203 pages. |
Ng., R., "Fourier Slice Photography," ACM Transactions on Graphics, Proceedings of SIGGRAPH 2005, vol. 24, No. 3, 2005, pp. 735-744. |
Nguyen, Hubert. "Practical Post-Process Depth of Field." GPU Gems 3. Upper Saddle River, NJ: Addison-Wesley, 2008. |
Nimeroff, J., et al., "Efficient rendering of naturally illuminatied environments" in Fifth Eurographics Workshop on Rendering, 359-373, 1994. |
Nokia, "City Lens", May 2012. |
Ogden, J., "Pyramid-Based Computer Graphics", 1985. |
Okano et al., "Three-dimensional video system based on integral photography" Optical Engineering, Jun. 1999. vol. 38, No. 6, pp. 1072-1077. |
Orzan, Alexandrina, et al., "Diffusion Curves: A Vector Representation for Smooth-Shaded Images," ACM Transactions on Graphics-Proceedings of SIGGRAPH 2008; vol. 27; 2008. |
Orzan, Alexandrina, et al., "Diffusion Curves: A Vector Representation for Smooth-Shaded Images," ACM Transactions on Graphics—Proceedings of SIGGRAPH 2008; vol. 27; 2008. |
Pain, B., "Back-Side Illumination Technology for SOI-CMOS Image Sensors", 2009. |
Perez, Patrick et al., "Poisson Image Editing," ACM Transactions on Graphics-Proceedings of ACM SIGGRAPH 2003; vol. 22, Issue 3; Jul. 2003; pp. 313-318. |
Perez, Patrick et al., "Poisson Image Editing," ACM Transactions on Graphics—Proceedings of ACM SIGGRAPH 2003; vol. 22, Issue 3; Jul. 2003; pp. 313-318. |
Petschnigg, George, et al., "Digial Photography with Flash and No-Flash Image Pairs", SIGGRAPH 2004. |
Primesense, "The Primesense 3D Awareness Sensor", 2007. |
Ramamoorthi, R., et al, "Frequency space environment map rendering" ACM Transactions on Graphics (SIGGRAPH 2002 proceedings) 21, 3, 517-526. |
Ramamoorthi, R., et al., "An efficient representation for irradiance environment maps", in Proceedings of SIGGRAPH 2001, 497-500. |
Raskar, Ramesh et al., "Glare Aware Photography: 4D Ray Sampling for Reducing Glare Effects of Camera Lenses," ACM Transactions on Graphics-Proceedings of ACM SIGGRAPH, Aug. 2008; vol. 27, Issue 3; pp. 1-10. |
Raskar, Ramesh et al., "Non-photorealistic Camera: Depth Edge Detection and Stylized Rendering using Multi-Flash Imaging", SIGGRAPH 2004. |
Raskar, Ramesh et al., "Glare Aware Photography: 4D Ray Sampling for Reducing Glare Effects of Camera Lenses," ACM Transactions on Graphics—Proceedings of ACM SIGGRAPH, Aug. 2008; vol. 27, Issue 3; pp. 1-10. |
Raytrix, "Raytrix Lightfield Camera," Raytrix GmbH, Germany 2012, pp. 1-35. |
Roper Scientific, Germany "Fiber Optics," 2012. |
Scharstein, Daniel, et al., "High-Accuracy Stereo Depth Maps Using Structured Light," CVPR'03 Proceedings of the 2003 IEEE Computer Society, pp. 195-202. |
Schirmacher, H. et al., "High-Quality Interactive Lumigraph Rendering Through Warping," May 2000, Graphics Interface 2000. |
Shade, Jonathan, et al., "Layered Depth Images", SIGGRAPH 98, pp. 1-2, 1998. |
Shreiner, OpenGL Programming Guide, 7th edition, Chapter 8, 2010. |
Simpleviewer, "Tiltview", http://simpleviewer.net/tiltviewer. Retrieved Jan. 2013. |
Skodras, A. et al., "The JPEG 2000 Still Image Compression Standard," Sep. 2001, IEEE Signal Processing Magazine, pp. 36-58. |
Sloan, P., et al., "Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments", ACM Transactions on Graphics 21, 3, 527-536, 2002. |
Snavely, Noah, et al., "Photo-tourism: Exploring Photo collections in 3D", ACM Transactions on Graphics (SIGGRAPH Proceedings), 2006. |
Sokolov, "Autostereoscopy and Integral Photography by Professor Lippmann's Method" , 1911, pp. 23-29. |
Sony Corp, "Interchangeable Lens Digital Camera Handbook", 2011. |
Sony, Sony's First Curved Sensor Photo: http://www.engadget.com; Jul. 2014. |
Stensvold, M., "Hybrid AF: A New Approach to Autofocus Is Emerging for both Still and Video", Digital Photo Magazine, Nov. 13, 2012. |
Story, D., "The Future of Photography", Optics Electronics, Oct. 2008. |
Sun, Jian, et al., "Stereo Matching Using Belief Propagation", 2002. |
Tagging photos on Flickr, Facebook and other online photo sharing sites (see, for example, http://support.gnip.com/customer/portal/articles/809309-flickr-geo-photos-tag-search). Retrieved Jan. 2013. |
Takahashi, Keita, et al., "All in-focus View Synthesis from Under-Sampled Light Fields", ICAT 2003, Tokyo, Japan. |
Tanida et al., "Thin observation module by bound optics (TOMBO): concept and experimental verification" Applied Optics 40, 11 (Apr. 10, 2001), pp. 1806-1813. |
Tao, Michael, et al., "Depth from Combining Defocus and Correspondence Using Light-Field Cameras", Dec. 2013. |
Techcrunch, "Coolinis", Retrieved Jan. 2013. |
Teo, P., et al., "Efficient linear rendering for interactive light design", Tech. Rep. STAN-CS-TN-97-60, 1998, Stanford University. |
Teranishi, N. "Evolution of Optical Structure in Images Sensors," Electron Devices Meeting (IEDM) 2012 IEEE International; Dec. 10-13, 2012. |
U.S. Appl. No. 15/590,808, filed May 9, 2017 listing Alex Song et al. as inventors, entitled "Adaptive Control for Immersive Experience Delivery". |
U.S. Appl. No. 15/590,841, filed May 9, 2017 listing Kurt Akeley et al. as inventors, entitled "Vantage Generation and Inteactive Playback". |
U.S. Appl. No. 15/590,951, filed May 9, 2017 listing Alex Song et al. as inventors, entitled "Wedge-Based Light-Field Video Capture". |
U.S. Appl. No. 15/605,037, filed May 25, 2017 listing Zejing Wang et al. as inventors, entitled "Multi-View Back-Projection to a Light-Field". |
U.S. Appl. No. 15/666,298, filed Aug. 1, 2017 listing Yonggang Ha et al. as inventors, entitled "Focal Reducer With Controlled Optical Properties for Interchangeable Lens Light-Field Camera". |
U.S. Appl. No. 15/703,553, filed Sep. 13, 2017 listing Jon Karafin et al. as inventors, entitled "4D Camera Tracking and Optical Stabilization". |
U.S. Appl. No. 15/864,938, filed Jan. 8, 2018 listing Jon Karafin et al. as inventors, entitled "Motion Blur for Light-Field Images". |
U.S. Appl. No. 15/874,723, filed Jan. 18, 2018 listing Mark Weir et al. as inventors, entitled "Multi-Camera Navigation Interface". |
U.S. Appl. No. 15/897,836, filed Feb. 15, 2018 listing Francois Bleibel et al. as inventors, entitled "Multi-View Contour Tracking". |
U.S. Appl. No. 15/897,942, filed Feb. 15, 2018 listing Francois Bleibel et al. as inventors, entitled "Multi-View Contour Tracking With Grabcut". |
U.S. Appl. No. 15/897,994, filed Feb. 15, 2018 listing Trevor Carothers et al. as inventors, entitled "Generation of Virtual Reality With 6 Degrees of Freesom From Limited Viewer Data". |
U.S. Appl. No. 15/944,551, filed Apr. 3, 2018 listing Zejing Wang et al. as inventors, entitled "Generating Dolly Zoom Effect Using Light Field Image Data". |
U.S. Appl. No. 15/967,076, filed Apr. 30, 2018 listing Jiantao Kuang et al. as inventors, entitled "Automatic Lens Flare Detection and Correction for Light-Field Images". |
Vaish et al., "Using plane + parallax for calibrating dense camera arrays", In Proceedings CVPR 2004, pp. 2-9. |
Vaish, V., et al., "Synthetic Aperture Focusing Using a Shear-Warp Factorization of the Viewing Transform," Workshop on Advanced 3D Imaging for Safety and Security (in conjunction with CVPR 2005), 2005. |
VR Playhouse, "The Surrogate," http://www.vrplayhouse.com/the-surrogate, 2016. |
Wanner, S. et al., "Globally Consistent Depth Labeling of 4D Light Fields," IEEE Conference on Computer Vision and Pattern Recognition, 2012. |
Wanner, S. et al., "Variational Light Field Analysis for Disparity Estimation and Super-Resolution," IEEE Transacations on Pattern Analysis and Machine Intellegence, 2013. |
Wenger, et al, "Performance Relighting and Reflectance Transformation with Time-Multiplexed Illumination", Institute for Creative Technologies, SIGGRAPH 2005. |
Wetzstein, Gordon, et al., "Sensor Saturation in Fourier Multiplexed Imaging", IEEE Conference on Computer Vision and Pattern Recognition (2010). |
Wikipedia-Adaptive Optics: http://en.wikipedia.orgiwiki/adaptive_optics. Retrieved Feb. 2014. |
Wikipedia—Adaptive Optics: http://en.wikipedia.orgiwiki/adaptive_optics. Retrieved Feb. 2014. |
Wikipedia-Autofocus systems and methods: http://en.wikipedia.org/wiki/Autofocus. Retrieved Jan. 2013. |
Wikipedia—Autofocus systems and methods: http://en.wikipedia.org/wiki/Autofocus. Retrieved Jan. 2013. |
Wikipedia-Bayer Filter: http:/en.wikipedia.org/wiki/Bayer_filter. Retrieved Jun. 20, 2013. |
Wikipedia—Bayer Filter: http:/en.wikipedia.org/wiki/Bayer_filter. Retrieved Jun. 20, 2013. |
Wikipedia-Color Image Pipeline: http://en.wikipedia.org/wiki/color_image_pipeline. Retrieved Jan. 15, 2014. |
Wikipedia—Color Image Pipeline: http://en.wikipedia.org/wiki/color_image_pipeline. Retrieved Jan. 15, 2014. |
Wikipedia-Compression standard JPEG XR: http://en.wikipedia.org/wiki/JPEG_XR. Retrieved Jan. 2013. |
Wikipedia—Compression standard JPEG XR: http://en.wikipedia.org/wiki/JPEG_XR. Retrieved Jan. 2013. |
Wikipedia-CYGM Filter: http://en.wikipedia.org/wiki/CYGM_filter. Retrieved Jun. 20, 2013. |
Wikipedia—CYGM Filter: http://en.wikipedia.org/wiki/CYGM_filter. Retrieved Jun. 20, 2013. |
Wikipedia-Data overlay techniques for real-time visual feed. For example, heads-up displays: http://en.wikipedia.org/wiki/Head-up_display. Retrieved Jan. 2013. |
Wikipedia—Data overlay techniques for real-time visual feed. For example, heads-up displays: http://en.wikipedia.org/wiki/Head-up_display. Retrieved Jan. 2013. |
Wikipedia-Exchangeable image file format: http://en.wikipedia.org/wiki/Exchangeable_image_file_format. Retrieved Jan. 2013. |
Wikipedia—Exchangeable image file format: http://en.wikipedia.org/wiki/Exchangeable_image_file_format. Retrieved Jan. 2013. |
Wikipedia-Expeed: http://en.wikipedia.org/wiki/EXPEED. Retrieved Jan. 15, 2014. |
Wikipedia—Expeed: http://en.wikipedia.org/wiki/EXPEED. Retrieved Jan. 15, 2014. |
Wikipedia-Extensible Metadata Platform: http://en.wikipedia.org/wiki/Extensible_Metadata_Platform. Retrieved Jan. 2013. |
Wikipedia—Extensible Metadata Platform: http://en.wikipedia.org/wiki/Extensible_Metadata_Platform. Retrieved Jan. 2013. |
Wikipedia-Key framing for video animation: http://en.wikipedia.org/wiki/Key_frame. Retrieved Jan. 2013. |
Wikipedia—Key framing for video animation: http://en.wikipedia.org/wiki/Key_frame. Retrieved Jan. 2013. |
Wikipedia-Lazy loading of image data: http://en.wikipedia.org/wiki/Lazy_loading. Retrieved Jan. 2013. |
Wikipedia—Lazy loading of image data: http://en.wikipedia.org/wiki/Lazy_loading. Retrieved Jan. 2013. |
Wikipedia-Methods of Variable Bitrate Encoding: http://en.wikipedia.org/wiki/Variable_bitrate#Methods_of_VBR_encoding. Retrieved Jan. 2013. |
Wikipedia—Methods of Variable Bitrate Encoding: http://en.wikipedia.org/wiki/Variable_bitrate#Methods_of_VBR_encoding. Retrieved Jan. 2013. |
Wikipedia-Portable Network Graphics format: http://en.wikipedia.org/wiki/Portable_Network_Graphics. Retrieved Jan. 2013. |
Wikipedia—Portable Network Graphics format: http://en.wikipedia.org/wiki/Portable_Network_Graphics. Retrieved Jan. 2013. |
Wikipedia-Unsharp Mask Technique: https://en.wikipedia.org/wiki/Unsharp_masking. Retrieved May 3, 2016. |
Wikipedia—Unsharp Mask Technique: https://en.wikipedia.org/wiki/Unsharp_masking. Retrieved May 3, 2016. |
Wilburn et al., "High Performance Imaging using Large Camera Arrays", ACM Transactions on Graphics (TOG), vol. 24, Issue 3 (Jul. 2005), Proceedings of ACM SIGGRAPH 2005, pp. 765-776. |
Wilburn, Bennett, et al., "High Speed Video Using A Dense Camera Array", 2004. |
Wilburn, Bennett, et al., "The Light Field Video Camera", Proceedings of Media Processors 2002. |
Williams, L. "Pyramidal Parametrics," Computer Graphic (1983). |
Winnemoller, H., et al., "Light Waving: Estimating Light Positions From Photographs Alone", Eurographics 2005. |
Wippermann, F. "Chirped Refractive Microlens Array," Dissertation 2007. |
Wuu, S., et al., "A Manufacturable Back-Side Illumination Technology Using Bulk Si Substrate for Advanced CMOS Image Sensors", 2009 International Image Sensor Workshop, Bergen, Norway. |
Wuu, S., et al., "BSI Technology with Bulk Si Wafer", 2009 International Image Sensor Workshop, Bergen, Norway. |
Xiao, Z. et al., "Aliasing Detection and Reduction in Plenoptic Imaging," IEEE Conference on Computer Vision and Pattern Recognition; 2014. |
Xu, Xin et al., "Robust Automatic Focus Algorithm for Low Contrast Images Using a New Contrast Measure," Sensors 2011; 14 pages. |
Zheng, C. et al., "Parallax Photography: Creating 3D Cinematic Effects from Stills", Proceedings of Graphic Interface, 2009. |
Zitnick, L. et al., "High-Quality Video View Interpolation Using a Layered Representation," Aug. 2004; ACM Transactions on Graphics (TOG), Proceedings of ACM SIGGRAPH 2004; vol. 23, Issue 3; pp. 600-608. |
Zoberbier, M., et al., "Wafer Cameras-Novel Fabrication and Packaging Technologies", 2009 International Image Senor Workshop, Bergen, Norway, 5 pages. |
Zoberbier, M., et al., "Wafer Cameras—Novel Fabrication and Packaging Technologies", 2009 International Image Senor Workshop, Bergen, Norway, 5 pages. |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220159194A1 (en) * | 2017-08-25 | 2022-05-19 | Canon Kabushiki Kaisha | Image capturing apparatus |
US11653106B2 (en) * | 2017-08-25 | 2023-05-16 | Canon Kabushiki Kaisha | Image capturing apparatus with circular imaging portion guide and separating wall |
US10921596B2 (en) * | 2018-07-24 | 2021-02-16 | Disney Enterprises, Inc. | Adaptive luminance/color correction for displays |
US11357593B2 (en) | 2019-01-10 | 2022-06-14 | Covidien Lp | Endoscopic imaging with augmented parallax |
US11793390B2 (en) | 2019-01-10 | 2023-10-24 | Covidien Lp | Endoscopic imaging with augmented parallax |
US11037365B2 (en) | 2019-03-07 | 2021-06-15 | Alibaba Group Holding Limited | Method, apparatus, medium, terminal, and device for processing multi-angle free-perspective data |
US11055901B2 (en) | 2019-03-07 | 2021-07-06 | Alibaba Group Holding Limited | Method, apparatus, medium, and server for generating multi-angle free-perspective video data |
US11257283B2 (en) | 2019-03-07 | 2022-02-22 | Alibaba Group Holding Limited | Image reconstruction method, system, device and computer-readable storage medium |
US11341715B2 (en) | 2019-03-07 | 2022-05-24 | Alibaba Group Holding Limited | Video reconstruction method, system, device, and computer readable storage medium |
US11521347B2 (en) | 2019-03-07 | 2022-12-06 | Alibaba Group Holding Limited | Method, apparatus, medium, and device for generating multi-angle free-respective image data |
US20220138596A1 (en) * | 2020-11-02 | 2022-05-05 | Adobe Inc. | Increasing efficiency of inferencing digital videos utilizing machine-learning models |
US11893668B2 (en) | 2021-03-31 | 2024-02-06 | Leica Camera Ag | Imaging system and method for generating a final digital image via applying a profile to image information |
Also Published As
Publication number | Publication date |
---|---|
US20180097867A1 (en) | 2018-04-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10567464B2 (en) | Video compression with adaptive view-dependent lighting removal | |
US10546424B2 (en) | Layered content delivery for virtual and augmented reality experiences | |
US10341632B2 (en) | Spatial random access enabled video system with a three-dimensional viewing volume | |
US10469873B2 (en) | Encoding and decoding virtual reality video | |
US10419737B2 (en) | Data structures and delivery methods for expediting virtual reality playback | |
US11405643B2 (en) | Sequential encoding and decoding of volumetric video | |
US11599968B2 (en) | Apparatus, a method and a computer program for volumetric video | |
US10757423B2 (en) | Apparatus and methods for compressing video content using adaptive projection selection | |
KR102594003B1 (en) | Method, apparatus and stream for encoding/decoding volumetric video | |
CN107439010B (en) | Streaming spherical video | |
JP6410918B2 (en) | System and method for use in playback of panoramic video content | |
US10389994B2 (en) | Decoder-centric UV codec for free-viewpoint video streaming | |
US11202086B2 (en) | Apparatus, a method and a computer program for volumetric video | |
US11430156B2 (en) | Apparatus, a method and a computer program for volumetric video | |
EP3434021B1 (en) | Method, apparatus and stream of formatting an immersive video for legacy and immersive rendering devices | |
WO2019229293A1 (en) | An apparatus, a method and a computer program for volumetric video | |
WO2019115867A1 (en) | An apparatus, a method and a computer program for volumetric video | |
WO2019008233A1 (en) | A method and apparatus for encoding media content | |
CN116097652A (en) | Dual stream dynamic GOP access based on viewport changes |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: LYTRO, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PANG, DEREK;PITTS, COLVIN;AKELEY, KURT;SIGNING DATES FROM 20171115 TO 20171128;REEL/FRAME:044314/0196 |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO SMALL (ORIGINAL EVENT CODE: SMAL); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LYTRO, INC.;REEL/FRAME:048764/0079Effective date: 20180325 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |