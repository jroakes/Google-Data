US20120328005A1 - Construction of combined list using temporal distance - Google Patents
Construction of combined list using temporal distance Download PDFInfo
- Publication number
- US20120328005A1 US20120328005A1 US13/530,428 US201213530428A US2012328005A1 US 20120328005 A1 US20120328005 A1 US 20120328005A1 US 201213530428 A US201213530428 A US 201213530428A US 2012328005 A1 US2012328005 A1 US 2012328005A1
- Authority
- US
- United States
- Prior art keywords
- current picture
- pictures
- reference pictures
- picture
- combined list
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/137—Motion inside a coding unit, e.g. average field, frame or block difference
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/105—Selection of the reference unit for prediction within a chosen coding or prediction mode, e.g. adaptive choice of position and number of pixels used for prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/14—Coding unit complexity, e.g. amount of activity or edge presence estimation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/172—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a picture, frame or field
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
- H04N19/61—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding in combination with predictive coding
Definitions
- Video compression systems employ block processing for most of the compression operations.
- a block is a group of neighboring pixels and may be treated as one coding unit in terms of the compression operations. Theoretically, a larger coding unit is preferred to take advantage of correlation among immediate neighboring pixels.
- Various video compression standards e.g., Motion Picture Expert Group (MPEG)-1, MPEG-2, and MPEG-4, use block sizes of 4 ⁇ 4, 8 ⁇ 8, and 16 ⁇ 16 (referred to as a macroblock (MB)).
- MPEG Motion Picture Expert Group
- MPEG-4 use block sizes of 4 ⁇ 4, 8 ⁇ 8, and 16 ⁇ 16 (referred to as a macroblock (MB)).
- High efficiency video coding is also a block-based hybrid spatial and temporal predictive coding scheme.
- HEVC partitions an input picture into square blocks referred to as largest coding units (LCUs) as shown in FIG. 1 .
- LCU largest coding units
- the LCU can be as large as 128 ⁇ 128 pixels.
- Each LCU can be partitioned into smaller square blocks called coding units (CUs).
- FIG. 2 shows an example of an LCU partition of CUs.
- An LCU 100 is first partitioned into four CUs 102 .
- Each CU 102 may also be further split into four smaller CUs 102 that are a quarter of the size of the CU 102 .
- CUs 102 - 1 , 102 - 3 , and 102 - 4 are a quarter of the size of LCU 100 . Further, a CU 102 - 2 has been split into four CUs 102 - 5 , 102 - 6 , 102 - 7 , and 102 - 8 .
- Each CU 102 may include one or more prediction units (PUs).
- FIG. 3 shows an example of a CU partition of PUs.
- the PUs may be used to perform spatial prediction or temporal prediction.
- a CU can be either spatially or temporally predictive coded. If a CU is coded in intra mode, each PU of the CU can have its own spatial prediction direction. If a CU is coded in inter mode, each PU of the CU can have its own motion vector(s) and associated reference picture(s).
- HEVC Similar to other video coding standards, HEVC supports intra picture, such as I picture, and inter picture, such as B picture. Intra picture is coded without referring to other pictures. Hence, only spatial prediction is allowed for a CU/PU inside an intra picture. Intra picture provides a possible point where decoding can begin. On the other hand, inter picture aims at high compression. Inter picture supports both intra and inter prediction. A CU/PU in inter picture is either spatially or temporally predictive coded. Temporal references are the previously coded intra or inter pictures.
- Pred_BI bi-directional prediction
- Pred_LC uni-directional prediction
- a syntax element, inter_pred_flag at the PU level indicates which mode is used. If inter_pred_flag is set to 1, bi-directional prediction with two reference lists (Pred_BI) is used. If inter_pred_flag is set to 0, uni-directional prediction (Pred_LC) with a single combined list is used.
- the combined list is constructed by interleaving the entries of list 0 and list 1 in ascending order of the indices, beginning with the smallest index of list 0 . Indices pointing to the reference pictures that have already been included in the combined list will be skipped.
- a method receives a current picture of video content. The method then determines a set of reference pictures for the current picture and a temporal distance from the current picture for each of the set of reference pictures. A combined list of reference pictures in the set of reference pictures is determined where an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture. The method then uses the combined list to perform temporal prediction for the current picture.
- an apparatus comprising: one or more computer processors; and a computer-readable storage medium comprising instructions, that when executed, control the one or more computer processors to be configured for: receiving a current picture of video content; determining a set of reference pictures for the current picture; determining a temporal distance from the current picture for each of the set of reference pictures; generating a combined list of reference pictures in the set of reference pictures, wherein an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture; and using the combined list to perform temporal prediction for the current picture.
- a non-transitory computer-readable storage medium if provided comprising instructions, that when executed, control the one or more computer processors to be configured for: receiving a current picture of video content; determining a set of reference pictures for the current picture; determining a temporal distance from the current picture for each of the set of reference pictures; generating a combined list of reference pictures in the set of reference pictures, wherein an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture; and using the combined list to perform temporal prediction for the current picture
- FIG. 1 shows partitions of an input picture into square blocks referred to as largest coding units (LCUs).
- LCUs largest coding units
- FIG. 2 shows an example of an LCU partition of CUs.
- FIG. 3 shows an example of a CU partition of PUs.
- FIG. 4 depicts an example of a system for encoding and decoding video content according to one embodiment.
- FIG. 5 shows an example of a sequence of pictures from video content that is displayed in a display order from left to right according to one embodiment.
- FIG. 6 depicts a simplified flowchart of a method for determining a combined list according to one embodiment.
- FIG. 7 depicts an example of a sequence of pictures according to one embodiment.
- FIG. 8 depicts a sequence of pictures according to one embodiment.
- FIG. 9A depicts an example of encoder according to one embodiment.
- FIG. 9B depicts an example of decoder according to one embodiment.
- FIG. 4 depicts an example of a system for encoding and decoding video content according to one embodiment.
- the system includes an encoder 400 and a decoder 401 , both of which will be described in more detail below.
- a temporal prediction block 406 in either encoder 400 or decoder 401 is used to perform temporal prediction.
- reference pictures from the combined list may be selected as reference pictures for a current picture when performing uni-directional prediction motion estimation.
- Reference pictures that are at lower indices in the combined list may be more similar to the current picture and may take less bits to encode the current picture if the reference picture at lower indices are used.
- the combined list may be used when two or more (but not necessary two or more, can be one) consecutive non-reference (but not necessary non-reference, it also can be reference picture for other picture) pictures are found in a video sequence.
- the combined list may be generated from entries that would be found in two lists, list 0 and list 1 .
- the combined list is generated by determining a reference picture with a shortest temporal distance from a current picture. This reference picture is assigned the lowest reference index in the combined list, such as reference index 0 .
- the second reference picture with a second shortest temporal distance from the current picture is then determined and assigned the next reference index, reference index 1 .
- This process continues as reference pictures with shorter temporal distances from the current picture are assigned smaller reference indices.
- This index assignment is performed because it is likely that blocks in the current picture that are closer than more distant reference pictures will be more similar to the those reference pictures. Using similar pictures in motion estimation may increase coding efficiency because a similar block is more likely to be found in pictures that are temporally closer.
- different algorithms may be used to select one of the reference pictures. For example, the reference picture that is in the past is assigned a smaller reference index.
- FIG. 5 shows an example of a sequence of pictures from video content that is displayed in a display order from left to right according to one embodiment.
- the left side of current pictures 502 - 0 and 502 - 1 represents pictures that are displayed before pictures to the right side of current pictures 502 - 0 and 502 - 1 .
- a current picture is a picture currently being coded.
- current pictures 502 - 0 and 502 - 1 are referred to as “current” pictures, it will be understood that only one picture may currently be coded. Thus, both current pictures 502 - 0 and 502 - 1 may be coded at different times.
- Reference pictures 504 - 0 - 504 - 3 are reference pictures for current pictures 502 - 0 and 502 - 1 . Reference pictures are used in motion estimation operations. For example, a block in reference pictures 504 that is similar to a block in current picture 502 is searched for during motion estimation. Additional pictures 506 are also shown. Pictures 506 may be non-reference pictures.
- Current pictures 502 - 0 and 502 - 1 have four reference pictures, reference pictures 504 - 0 , 504 - 1 , 504 - 2 , and 504 - 3 . If two reference lists are used, reference indices for list 0 and list 1 are the same for current pictures 502 - 0 and 502 - 1 . For example, the two separate reference lists for both current pictures 502 - 0 and 502 - 1 are:
- current picture 502 - 0 and current picture 502 - 1 will have different combined lists because the combined list is constructed according to the temporal distances between a current picture and its reference pictures 504 .
- reference picture 504 - 0 is assigned index 0 in the combined list because reference picture 504 - 0 is temporally closest to current picture 502 - 0 .
- Reference picture 504 - 2 is assigned index 1 because reference picture 504 - 2 is the second temporally closest picture to current picture 502 - 0 .
- Reference pictures 504 - 1 and 504 - 3 are then assigned indices 2 and 3 , respectively, because reference picture 504 - 1 is temporally closer to current picture 502 - 0 than reference picture 504 - 3 .
- the combined list for current picture 502 - 0 is as follows:
- reference picture 504 - 2 is assigned index 0 because reference picture 504 - 2 is temporally closest to current picture 502 - 1 .
- Reference picture 504 - 0 is assigned index 1 because reference picture 504 - 0 is the second temporally closest picture to current picture 502 - 1 .
- Reference pictures 504 - 3 and 504 - 1 are assigned indices 2 and 3 , respectively, because reference picture 504 - 3 is temporally closer to current picture 502 - 1 than reference picture 504 - 1 .
- the combined list for current picture 502 - 1 is as follows:
- FIG. 6 depicts a simplified flowchart 600 of a method for determining a combined list according to one embodiment.
- combined list manager 404 receives a current picture 502 for video content.
- combined list manager 404 determines a set of reference pictures.
- the set of reference pictures may include a first set of reference pictures that are in the past as compared to current picture 502 - 0 and a second set of reference pictures 504 that are in the future as compared to current picture 502 .
- combined list manager 404 determines a temporal distance from current picture 502 to each of the reference pictures 504 .
- the temporal distance may be computed and the list of reference pictures 504 may be sorted.
- the sorted list may resolve ties where reference pictures 504 from the past are put before reference pictures 504 in the future in the list.
- combined list manager 404 generates a combined list of reference pictures 504 .
- Combined list manager 404 inserts pictures into the combined list according to the sorted list.
- temporal prediction block 406 may perform temporal prediction using the combined list for current picture 502 .
- combined list manager 404 generates the combined list based on the temporal distances to current picture 502 and may also take into account quantization parameters for reference pictures 504 .
- quantization parameters for reference pictures 504 may be taken into account.
- the quantization parameter regulates how much spatial detail is saved. When a quantization parameter is very small, almost all of the detail in a picture is retained. As the quantization parameter is increased, some of the detail in the bitstream is aggregated so that the bit rate drops, which results in an increase in distortion and some loss of quality.
- the different quantization parameters may be based on different coding levels.
- the coding level may be the amount of redundancy that is included in each reference picture 504 .
- the reference picture 504 with the smallest quantization parameter is inserted in the combined list at a lower index than the other reference picture 504 . This is done because the quality may be better for a reference picture 504 that has a smaller quantization parameter.
- reference picture 504 with a shorter temporal distance from current picture 502 is assigned a smaller reference index. If two reference pictures 504 have the same temporal distance from current picture 502 , then the reference picture 504 with a smaller quantization parameter (e.g., average quantization parameter) is assigned a smaller reference index. Also, if two reference pictures 504 have the same temporal distance from current picture 502 and the same average quantization parameters (the average quantization parameters may be within a certain range or threshold to be the same), the reference picture 504 in the past is assigned a smaller reference index in the combined list.
- a quantization parameter e.g., average quantization parameter
- FIG. 7 depicts an example of a sequence of pictures according to one embodiment.
- a current picture 502 - 3 is currently being coded.
- Reference pictures 504 - 4 and 504 - 5 are both reference pictures for current picture 502 - 3 .
- the temporal distance between reference picture 504 - 4 and current picture 502 - 3 is the same as the temporal distance between reference picture 504 - 5 and current picture 502 - 3 .
- an average quantization parameter for reference picture 504 - 5 is smaller than that of reference picture 504 - 4 , and thus reference picture 504 - 5 is assigned a smaller reference index in the combined list than reference picture 504 - 4 .
- reference picture 504 - 5 is assigned index 0 and reference picture 504 - 4 is assigned index 1 .
- reference pictures 504 - 6 and 504 - 7 are reference pictures for current picture 502 - 3 .
- the temporal distances from reference picture 504 - 6 to current picture 502 - 3 is the same as the temporal distance from reference picture 504 - 7 to current picture 502 - 3 .
- the quantization parameter for reference picture 504 - 6 is smaller than that of reference picture 504 - 7 , and thus reference picture 504 - 6 is assigned a smaller reference index than reference picture 504 - 7 in the combined reference list.
- index 2 may be assigned to reference picture 504 - 6 and index 3 is assigned to reference picture 504 - 7 .
- the quantization parameters may be used without taking into account temporal distance.
- the smaller quantization parameters for reference pictures 504 have lower indices in the combined list.
- Combined list manager 404 uses a picture order count (POC) to determine the combined list according to one embodiment.
- POC picture order count
- a delta POC may be used to determine the combined list.
- the POC specifies the picture order count according to display order.
- Delta POC is the difference between the POC of current picture 502 and reference picture 504 .
- the absolute value of the delta POC is used because some POCs will be negative.
- the POC or delta POC can be used to represent the temporal distance between a current picture 502 and a reference picture 504 in list 0 or list 1 .
- the lowest absolute value of the delta POCs may represent the closest reference pictures 504 to current picture 502 .
- the combined reference list may be formed by reviewing the absolute value of the delta POCs of reference pictures 504 .
- the reference pictures 504 with the smaller absolute value of the delta POCs are added to the list with lower index numbers. If two absolute values of the delta POCs have the same absolute value in the two lists, then, in one embodiment, reference picture 504 in the past is assigned a smaller reference index.
- reference pictures 504 with shorter temporal distances from current pictures 502 are assigned smaller reference indices.
- FIG. 8 depicts a sequence of pictures according to one embodiment.
- a current picture 502 - 4 is being encoded or decoded.
- a list 0 for current picture 502 - 4 includes reference pictures in the order of POC of reference picture 504 - 8 , reference picture 504 - 9 , and reference picture 504 - 10 .
- a list 1 for current picture 502 - 4 is reference picture 504 - 10 , reference picture 504 - 11 , and reference picture 504 - 9 .
- the conventional method may generate the combined list of reference picture 504 - 8 , reference picture 504 - 10 and reference picture 504 - 9 . This is due to the sequential selecting of reference pictures from both lists.
- particular embodiments use the absolute value of the delta POC and QP to select the combined list to be reference picture 504 - 10 , reference picture 504 - 8 , and reference picture 504 - 9 .
- the absolute value of the delta POC between reference picture 502 - 4 and reference picture 504 - 10 is same as the absolute value of the delta POC between reference picture 502 - 4 and reference picture 504 - 9 .
- the QP may be taken into account then.
- the QP for reference picture 504 - 10 is smaller than the QP for reference picture 504 - 9 .
- reference picture 504 - 10 is put as the first entry of combined list.
- reference picture 504 - 8 and reference picture 504 - 9 because the QP for reference picture 504 - 8 (I picture) is much smaller than reference picture 504 - 9 (B picture), reference picture 504 - 8 is put before reference picture 504 - 9 .
- FIG. 9A depicts an example of encoder 400 according to one embodiment.
- a general operation of encoder 400 will now be described; however, it will be understood that variations on the encoding process described will be appreciated by a person skilled in the art based on the disclosure and teachings herein.
- a spatial prediction block 904 may include different spatial prediction directions per PU, such as horizontal, vertical, 45-degree diagonal, 135-degree diagonal, DC (flat averaging), and planar.
- Temporal prediction block 406 performs temporal prediction through a motion estimation operation.
- the motion estimation operation searches for a best match prediction for the current PU over reference pictures.
- the best match prediction is described by a motion vector (MV) and associated reference picture (refldx).
- MV motion vector
- reffldx reference picture
- the motion vector and associated reference picture are included in the coded bit stream.
- Transform block 907 performs a transform operation with the residual PU, e. Transform block 907 outputs the residual PU in a transform domain, E.
- a quantizer 908 then quantizes the transform coefficients of the residual PU, E.
- Quantizer 908 converts the transform coefficients into a finite number of possible values.
- Entropy coding block 910 entropy encodes the quantized coefficients, which results in final compression bits to be transmitted. Different entropy coding methods may be used, such as context-adaptive variable length coding (CAVLC) or context-adaptive binary arithmetic coding (CABAC).
- CAVLC context-adaptive variable length coding
- CABAC context-adaptive binary arithmetic coding
- a de-quantizer 912 de-quantizes the quantized transform coefficients of the residual PU.
- De-quantizer 912 then outputs the de-quantized transform coefficients of the residual PU, E′.
- An inverse transform block 914 receives the de-quantized transform coefficients, which are then inverse transformed resulting in a reconstructed residual PU, e′.
- the reconstructed PU, e′ is then added to the corresponding prediction, x′, either spatial or temporal, to form the new reconstructed PU, x′′.
- a loop filter 916 performs de-blocking on the reconstructed PU, x′′, to reduce blocking artifacts.
- loop filter 916 may perform a sample adaptive offset process after the completion of the de-blocking filter process for the decoded picture, which compensates for a pixel value offset between reconstructed pixels and original pixels. Also, loop filter 916 may perform adaptive loop filtering over the reconstructed PU, which minimizes coding distortion between the input and output pictures. Additionally, if the reconstructed pictures are reference pictures, the reference pictures are stored in a reference buffer 918 for future temporal prediction.
- FIG. 9B depicts an example of decoder 401 according to one embodiment.
- Decoder 401 receives input bits from encoder 400 for encoded video content.
- An entropy decoding block 930 performs entropy decoding on the input bitstream to generate quantized transform coefficients of a residual PU.
- a de-quantizer 932 de-quantizes the quantized transform coefficients of the residual PU.
- De-quantizer 932 then outputs the de-quantized transform coefficients of the residual PU, E′.
- An inverse transform block 934 receives the de-quantized transform coefficients, which are then inverse transformed resulting in a reconstructed residual PU, e′.
- the reconstructed PU, e′ is then added to the corresponding prediction, x′, either spatial or temporal, to form the new reconstructed PU, x′′.
- a loop filter 936 performs de-blocking on the reconstructed PU, x′′, to reduce blocking artifacts. Additionally, loop filter 936 may perform a sample adaptive offset process after the completion of the de-blocking filter process for the decoded picture, which compensates for a pixel value offset between reconstructed pixels and original pixels. Also, loop filter 936 may perform adaptive loop filtering over the reconstructed PU, which minimizes coding distortion between the input and output pictures. Additionally, if the reconstructed pictures are reference pictures, the reference pictures are stored in a reference buffer 938 for future temporal prediction.
- the prediction PU, x′ is obtained through either spatial prediction or temporal prediction.
- a spatial prediction block 940 may receive decoded spatial prediction directions per PU, such as horizontal, vertical, 45-degree diagonal, 135-degree diagonal, DC (flat averaging), and planar. The spatial prediction directions are used to determine the prediction PU, x′.
- a temporal prediction block 406 performs temporal prediction through a motion estimation operation.
- a decoded motion vector is used to determine the prediction PU, x′. Interpolation may be used in the motion estimation operation.
- Particular embodiments may be implemented in a non-transitory computer-readable storage medium for use by or in connection with the instruction execution system, apparatus, system, or machine.
- the computer-readable storage medium contains instructions for controlling a computer system to perform a method described by particular embodiments.
- the instructions when executed by one or more computer processors, may be operable to perform that which is described in particular embodiments.
Abstract
Description
- The present application claims priority to:
- U.S. Provisional App. No. 61/500,008 for “The Construction of Combined List for HEVC” filed Jun. 22, 2011;
- U.S. Provisional App. No. 61/507,391 for “Reference Picture Indexing for Combined Reference List for HEVC” filed Jul. 13, 2011;
- U.S. Provisional App. No. 61/564,470 for “The Reference Picture Construction of Combined List for HEVC” filed Nov. 29, 2011; and
- U.S. Provisional App. No. 61/557,880 for “The Reference Picture Construction of Combined List for HEVC” filed Nov. 9, 2011, the contents of all of which are incorporated herein by reference in their entirety.
- Video compression systems employ block processing for most of the compression operations. A block is a group of neighboring pixels and may be treated as one coding unit in terms of the compression operations. Theoretically, a larger coding unit is preferred to take advantage of correlation among immediate neighboring pixels. Various video compression standards, e.g., Motion Picture Expert Group (MPEG)-1, MPEG-2, and MPEG-4, use block sizes of 4×4, 8×8, and 16×16 (referred to as a macroblock (MB)).
- High efficiency video coding (HEVC) is also a block-based hybrid spatial and temporal predictive coding scheme. HEVC partitions an input picture into square blocks referred to as largest coding units (LCUs) as shown in
FIG. 1 . Unlike prior coding standards, the LCU can be as large as 128×128 pixels. Each LCU can be partitioned into smaller square blocks called coding units (CUs).FIG. 2 shows an example of an LCU partition of CUs. An LCU 100 is first partitioned into fourCUs 102. Each CU 102 may also be further split into foursmaller CUs 102 that are a quarter of the size of the CU 102. This partitioning process can be repeated based on certain criteria, such as limits to the number of times a CU can be partitioned may be imposed. As shown, CUs 102-1, 102-3, and 102-4 are a quarter of the size ofLCU 100. Further, a CU 102-2 has been split into four CUs 102-5, 102-6, 102-7, and 102-8. - Each
CU 102 may include one or more prediction units (PUs).FIG. 3 shows an example of a CU partition of PUs. The PUs may be used to perform spatial prediction or temporal prediction. A CU can be either spatially or temporally predictive coded. If a CU is coded in intra mode, each PU of the CU can have its own spatial prediction direction. If a CU is coded in inter mode, each PU of the CU can have its own motion vector(s) and associated reference picture(s). - Similar to other video coding standards, HEVC supports intra picture, such as I picture, and inter picture, such as B picture. Intra picture is coded without referring to other pictures. Hence, only spatial prediction is allowed for a CU/PU inside an intra picture. Intra picture provides a possible point where decoding can begin. On the other hand, inter picture aims at high compression. Inter picture supports both intra and inter prediction. A CU/PU in inter picture is either spatially or temporally predictive coded. Temporal references are the previously coded intra or inter pictures.
- In the HEVC, there are two possible prediction modes for inter partition in the generalized B slices. One is bi-directional prediction, Pred_BI, with two reference lists of
list 0 andlist 1 and the other is uni-directional prediction, Pred_LC, with a single combined reference list. A syntax element, inter_pred_flag at the PU level, indicates which mode is used. If inter_pred_flag is set to 1, bi-directional prediction with two reference lists (Pred_BI) is used. If inter_pred_flag is set to 0, uni-directional prediction (Pred_LC) with a single combined list is used. - The combined list is constructed by interleaving the entries of
list 0 andlist 1 in ascending order of the indices, beginning with the smallest index oflist 0. Indices pointing to the reference pictures that have already been included in the combined list will be skipped. - In one embodiment, a method receives a current picture of video content. The method then determines a set of reference pictures for the current picture and a temporal distance from the current picture for each of the set of reference pictures. A combined list of reference pictures in the set of reference pictures is determined where an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture. The method then uses the combined list to perform temporal prediction for the current picture.
- In one embodiment, an apparatus is provided comprising: one or more computer processors; and a computer-readable storage medium comprising instructions, that when executed, control the one or more computer processors to be configured for: receiving a current picture of video content; determining a set of reference pictures for the current picture; determining a temporal distance from the current picture for each of the set of reference pictures; generating a combined list of reference pictures in the set of reference pictures, wherein an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture; and using the combined list to perform temporal prediction for the current picture.
- In one embodiment, a non-transitory computer-readable storage medium if provided comprising instructions, that when executed, control the one or more computer processors to be configured for: receiving a current picture of video content; determining a set of reference pictures for the current picture; determining a temporal distance from the current picture for each of the set of reference pictures; generating a combined list of reference pictures in the set of reference pictures, wherein an order of pictures in the combined list is based on the temporal distance for each of the set of reference pictures to the current picture; and using the combined list to perform temporal prediction for the current picture
- The following detailed description and accompanying drawings provide a better understanding of the nature and advantages of particular embodiments.
-
FIG. 1 shows partitions of an input picture into square blocks referred to as largest coding units (LCUs). -
FIG. 2 shows an example of an LCU partition of CUs. -
FIG. 3 shows an example of a CU partition of PUs. -
FIG. 4 depicts an example of a system for encoding and decoding video content according to one embodiment. -
FIG. 5 shows an example of a sequence of pictures from video content that is displayed in a display order from left to right according to one embodiment. -
FIG. 6 depicts a simplified flowchart of a method for determining a combined list according to one embodiment. -
FIG. 7 depicts an example of a sequence of pictures according to one embodiment. -
FIG. 8 depicts a sequence of pictures according to one embodiment. -
FIG. 9A depicts an example of encoder according to one embodiment. -
FIG. 9B depicts an example of decoder according to one embodiment. - Described herein are techniques for a video compression system. In the following description, for purposes of explanation, numerous examples and specific details are set forth in order to provide a thorough understanding of particular embodiments. Particular embodiments as defined by the claims may include some or all of the features in these examples alone or in combination with other features described below, and may further include modifications and equivalents of the features and concepts described herein.
-
FIG. 4 depicts an example of a system for encoding and decoding video content according to one embodiment. The system includes anencoder 400 and adecoder 401, both of which will be described in more detail below. - A
temporal prediction block 406 in eitherencoder 400 ordecoder 401 is used to perform temporal prediction. In temporal prediction, reference pictures from the combined list may be selected as reference pictures for a current picture when performing uni-directional prediction motion estimation. Reference pictures that are at lower indices in the combined list may be more similar to the current picture and may take less bits to encode the current picture if the reference picture at lower indices are used. - Particular embodiments use a combined
list manager 404 in eitherencoder 400 ordecoder 401 to construct a combined list to improve coding efficiency. In one embodiment, the combined list may be used when two or more (but not necessary two or more, can be one) consecutive non-reference (but not necessary non-reference, it also can be reference picture for other picture) pictures are found in a video sequence. As described above, the combined list may be generated from entries that would be found in two lists,list 0 andlist 1. In one embodiment, the combined list is generated by determining a reference picture with a shortest temporal distance from a current picture. This reference picture is assigned the lowest reference index in the combined list, such asreference index 0. The second reference picture with a second shortest temporal distance from the current picture is then determined and assigned the next reference index,reference index 1. This process continues as reference pictures with shorter temporal distances from the current picture are assigned smaller reference indices. This index assignment is performed because it is likely that blocks in the current picture that are closer than more distant reference pictures will be more similar to the those reference pictures. Using similar pictures in motion estimation may increase coding efficiency because a similar block is more likely to be found in pictures that are temporally closer. In one embodiment, if two reference pictures have the same temporal distance from the current picture, different algorithms may be used to select one of the reference pictures. For example, the reference picture that is in the past is assigned a smaller reference index. -
FIG. 5 shows an example of a sequence of pictures from video content that is displayed in a display order from left to right according to one embodiment. The left side of current pictures 502-0 and 502-1 represents pictures that are displayed before pictures to the right side of current pictures 502-0 and 502-1. A current picture is a picture currently being coded. Although current pictures 502-0 and 502-1 are referred to as “current” pictures, it will be understood that only one picture may currently be coded. Thus, both current pictures 502-0 and 502-1 may be coded at different times. Reference pictures 504-0-504-3 are reference pictures for current pictures 502-0 and 502-1. Reference pictures are used in motion estimation operations. For example, a block in reference pictures 504 that is similar to a block in current picture 502 is searched for during motion estimation.Additional pictures 506 are also shown.Pictures 506 may be non-reference pictures. - Current pictures 502-0 and 502-1 have four reference pictures, reference pictures 504-0, 504-1, 504-2, and 504-3. If two reference lists are used, reference indices for
list 0 andlist 1 are the same for current pictures 502-0 and 502-1. For example, the two separate reference lists for both current pictures 502-0 and 502-1 are: -
-
- Reference indices for Ref0, Ref1, Ref2 and Ref3 in
list 0.
- Reference indices for Ref0, Ref1, Ref2 and Ref3 in
-
-
- Reference indices for Ref0, Ref1, Ref2 and Ref3 in
list 1.
- Reference indices for Ref0, Ref1, Ref2 and Ref3 in
- However, if a combined list is used, current picture 502-0 and current picture 502-1 will have different combined lists because the combined list is constructed according to the temporal distances between a current picture and its reference pictures 504. For current picture 502-0, reference picture 504-0 is assigned
index 0 in the combined list because reference picture 504-0 is temporally closest to current picture 502-0. Reference picture 504-2 is assignedindex 1 because reference picture 504-2 is the second temporally closest picture to current picture 502-0. Reference pictures 504-1 and 504-3 are then assignedindices -
- For current picture 502-1, reference picture 504-2 is assigned
index 0 because reference picture 504-2 is temporally closest to current picture 502-1. Reference picture 504-0 is assignedindex 1 because reference picture 504-0 is the second temporally closest picture to current picture 502-1. Reference pictures 504-3 and 504-1 are assignedindices -
-
FIG. 6 depicts asimplified flowchart 600 of a method for determining a combined list according to one embodiment. At 602, combinedlist manager 404 receives a current picture 502 for video content. At 604, combinedlist manager 404 determines a set of reference pictures. For example, the set of reference pictures may include a first set of reference pictures that are in the past as compared to current picture 502-0 and a second set of reference pictures 504 that are in the future as compared to current picture 502. - At 606, combined
list manager 404 determines a temporal distance from current picture 502 to each of the reference pictures 504. For example, the temporal distance may be computed and the list of reference pictures 504 may be sorted. The sorted list may resolve ties where reference pictures 504 from the past are put before reference pictures 504 in the future in the list. - At 608, combined
list manager 404 generates a combined list of reference pictures 504. Combinedlist manager 404 inserts pictures into the combined list according to the sorted list. Once the combined list is generated,temporal prediction block 406 may perform temporal prediction using the combined list for current picture 502. - In one embodiment, combined
list manager 404 generates the combined list based on the temporal distances to current picture 502 and may also take into account quantization parameters for reference pictures 504. For example, average quantization parameters for reference pictures 504 may be taken into account. The quantization parameter regulates how much spatial detail is saved. When a quantization parameter is very small, almost all of the detail in a picture is retained. As the quantization parameter is increased, some of the detail in the bitstream is aggregated so that the bit rate drops, which results in an increase in distortion and some loss of quality. The different quantization parameters may be based on different coding levels. The coding level may be the amount of redundancy that is included in each reference picture 504. - When two reference pictures 504 have the same temporal distances, then the reference picture 504 with the smallest quantization parameter is inserted in the combined list at a lower index than the other reference picture 504. This is done because the quality may be better for a reference picture 504 that has a smaller quantization parameter.
- In one embodiment, as described above, given a current picture 502, if two reference pictures 504 have different temporal distances from a current picture 502, reference picture 504 with a shorter temporal distance from current picture 502 is assigned a smaller reference index. If two reference pictures 504 have the same temporal distance from current picture 502, then the reference picture 504 with a smaller quantization parameter (e.g., average quantization parameter) is assigned a smaller reference index. Also, if two reference pictures 504 have the same temporal distance from current picture 502 and the same average quantization parameters (the average quantization parameters may be within a certain range or threshold to be the same), the reference picture 504 in the past is assigned a smaller reference index in the combined list.
-
FIG. 7 depicts an example of a sequence of pictures according to one embodiment. A current picture 502-3 is currently being coded. Reference pictures 504-4 and 504-5 are both reference pictures for current picture 502-3. The temporal distance between reference picture 504-4 and current picture 502-3 is the same as the temporal distance between reference picture 504-5 and current picture 502-3. However, an average quantization parameter for reference picture 504-5 is smaller than that of reference picture 504-4, and thus reference picture 504-5 is assigned a smaller reference index in the combined list than reference picture 504-4. For example, reference picture 504-5 is assignedindex 0 and reference picture 504-4 is assignedindex 1. - Additionally, reference pictures 504-6 and 504-7 are reference pictures for current picture 502-3. The temporal distances from reference picture 504-6 to current picture 502-3 is the same as the temporal distance from reference picture 504-7 to current picture 502-3. The quantization parameter for reference picture 504-6 is smaller than that of reference picture 504-7, and thus reference picture 504-6 is assigned a smaller reference index than reference picture 504-7 in the combined reference list. For example,
index 2 may be assigned to reference picture 504-6 andindex 3 is assigned to reference picture 504-7. - In another example, the quantization parameters may be used without taking into account temporal distance. In this case, the smaller quantization parameters for reference pictures 504 have lower indices in the combined list.
- Combined
list manager 404 uses a picture order count (POC) to determine the combined list according to one embodiment. Also, a delta POC may be used to determine the combined list. The POC specifies the picture order count according to display order. Delta POC is the difference between the POC of current picture 502 and reference picture 504. In one example, because the difference is taken, the absolute value of the delta POC is used because some POCs will be negative. The POC or delta POC can be used to represent the temporal distance between a current picture 502 and a reference picture 504 inlist 0 orlist 1. For example, the lowest absolute value of the delta POCs may represent the closest reference pictures 504 to current picture 502. - Given a current picture 502 with two reference lists, the combined reference list may be formed by reviewing the absolute value of the delta POCs of reference pictures 504. The reference pictures 504 with the smaller absolute value of the delta POCs are added to the list with lower index numbers. If two absolute values of the delta POCs have the same absolute value in the two lists, then, in one embodiment, reference picture 504 in the past is assigned a smaller reference index. By using the absolute value of the delta POCs, reference pictures 504 with shorter temporal distances from current pictures 502 are assigned smaller reference indices.
-
FIG. 8 depicts a sequence of pictures according to one embodiment. A current picture 502-4 is being encoded or decoded. Alist 0 for current picture 502-4 includes reference pictures in the order of POC of reference picture 504-8, reference picture 504-9, and reference picture 504-10. Alist 1 for current picture 502-4 is reference picture 504-10, reference picture 504-11, and reference picture 504-9. The conventional method may generate the combined list of reference picture 504-8, reference picture 504-10 and reference picture 504-9. This is due to the sequential selecting of reference pictures from both lists. However, particular embodiments use the absolute value of the delta POC and QP to select the combined list to be reference picture 504-10, reference picture 504-8, and reference picture 504-9. In this case, the absolute value of the delta POC between reference picture 502-4 and reference picture 504-10 is same as the absolute value of the delta POC between reference picture 502-4 and reference picture 504-9. The QP may be taken into account then. The QP for reference picture 504-10 is smaller than the QP for reference picture 504-9. As a result, reference picture 504-10 is put as the first entry of combined list. Regarding reference picture 504-8 and reference picture 504-9, because the QP for reference picture 504-8 (I picture) is much smaller than reference picture 504-9 (B picture), reference picture 504-8 is put before reference picture 504-9. -
FIG. 9A depicts an example ofencoder 400 according to one embodiment. A general operation ofencoder 400 will now be described; however, it will be understood that variations on the encoding process described will be appreciated by a person skilled in the art based on the disclosure and teachings herein. - For a current PU, x, a prediction PU, x′, is obtained through either spatial prediction or temporal prediction. The prediction PU is then subtracted from the current PU, resulting in a residual PU, e. A
spatial prediction block 904 may include different spatial prediction directions per PU, such as horizontal, vertical, 45-degree diagonal, 135-degree diagonal, DC (flat averaging), and planar. -
Temporal prediction block 406 performs temporal prediction through a motion estimation operation. The motion estimation operation searches for a best match prediction for the current PU over reference pictures. The best match prediction is described by a motion vector (MV) and associated reference picture (refldx). The motion vector and associated reference picture are included in the coded bit stream. -
Transform block 907 performs a transform operation with the residual PU, e.Transform block 907 outputs the residual PU in a transform domain, E. - A
quantizer 908 then quantizes the transform coefficients of the residual PU,E. Quantizer 908 converts the transform coefficients into a finite number of possible values.Entropy coding block 910 entropy encodes the quantized coefficients, which results in final compression bits to be transmitted. Different entropy coding methods may be used, such as context-adaptive variable length coding (CAVLC) or context-adaptive binary arithmetic coding (CABAC). - Also, in a decoding process within
encoder 400, a de-quantizer 912 de-quantizes the quantized transform coefficients of the residual PU.De-quantizer 912 then outputs the de-quantized transform coefficients of the residual PU, E′. Aninverse transform block 914 receives the de-quantized transform coefficients, which are then inverse transformed resulting in a reconstructed residual PU, e′. The reconstructed PU, e′, is then added to the corresponding prediction, x′, either spatial or temporal, to form the new reconstructed PU, x″. Aloop filter 916 performs de-blocking on the reconstructed PU, x″, to reduce blocking artifacts. Additionally,loop filter 916 may perform a sample adaptive offset process after the completion of the de-blocking filter process for the decoded picture, which compensates for a pixel value offset between reconstructed pixels and original pixels. Also,loop filter 916 may perform adaptive loop filtering over the reconstructed PU, which minimizes coding distortion between the input and output pictures. Additionally, if the reconstructed pictures are reference pictures, the reference pictures are stored in areference buffer 918 for future temporal prediction. -
FIG. 9B depicts an example ofdecoder 401 according to one embodiment. A general operation ofdecoder 401 will now be described; however, it will be understood that variations on the decoding process described will be appreciated by a person skilled in the art based on the disclosure and teachings herein.Decoder 401 receives input bits fromencoder 400 for encoded video content. - An
entropy decoding block 930 performs entropy decoding on the input bitstream to generate quantized transform coefficients of a residual PU. A de-quantizer 932 de-quantizes the quantized transform coefficients of the residual PU.De-quantizer 932 then outputs the de-quantized transform coefficients of the residual PU, E′. Aninverse transform block 934 receives the de-quantized transform coefficients, which are then inverse transformed resulting in a reconstructed residual PU, e′. - The reconstructed PU, e′, is then added to the corresponding prediction, x′, either spatial or temporal, to form the new reconstructed PU, x″. A
loop filter 936 performs de-blocking on the reconstructed PU, x″, to reduce blocking artifacts. Additionally,loop filter 936 may perform a sample adaptive offset process after the completion of the de-blocking filter process for the decoded picture, which compensates for a pixel value offset between reconstructed pixels and original pixels. Also,loop filter 936 may perform adaptive loop filtering over the reconstructed PU, which minimizes coding distortion between the input and output pictures. Additionally, if the reconstructed pictures are reference pictures, the reference pictures are stored in areference buffer 938 for future temporal prediction. - The prediction PU, x′, is obtained through either spatial prediction or temporal prediction. A
spatial prediction block 940 may receive decoded spatial prediction directions per PU, such as horizontal, vertical, 45-degree diagonal, 135-degree diagonal, DC (flat averaging), and planar. The spatial prediction directions are used to determine the prediction PU, x′. - A
temporal prediction block 406 performs temporal prediction through a motion estimation operation. A decoded motion vector is used to determine the prediction PU, x′. Interpolation may be used in the motion estimation operation. - Particular embodiments may be implemented in a non-transitory computer-readable storage medium for use by or in connection with the instruction execution system, apparatus, system, or machine. The computer-readable storage medium contains instructions for controlling a computer system to perform a method described by particular embodiments. The instructions, when executed by one or more computer processors, may be operable to perform that which is described in particular embodiments.
- As used in the description herein and throughout the claims that follow, “a”, “an”, and “the” includes plural references unless the context clearly dictates otherwise. Also, as used in the description herein and throughout the claims that follow, the meaning of “in” includes “in” and “on” unless the context clearly dictates otherwise.
- The above description illustrates various embodiments along with examples of how aspects of particular embodiments may be implemented. The above examples and embodiments should not be deemed to be the only embodiments, and are presented to illustrate the flexibility and advantages of particular embodiments as defined by the following claims. Based on the above disclosure and the following claims, other arrangements, embodiments, implementations and equivalents may be employed without departing from the scope hereof as defined by the claims.
Claims (20)
Priority Applications (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/530,428 US20120328005A1 (en) | 2011-06-22 | 2012-06-22 | Construction of combined list using temporal distance |
EP12735681.4A EP2724537A1 (en) | 2011-06-22 | 2012-06-22 | Construction of combined list using temporal distance |
PCT/US2012/043748 WO2012178008A1 (en) | 2011-06-22 | 2012-06-22 | Construction of combined list using temporal distance |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161500008P | 2011-06-22 | 2011-06-22 | |
US201161507391P | 2011-07-13 | 2011-07-13 | |
US201161557880P | 2011-11-09 | 2011-11-09 | |
US201161564470P | 2011-11-29 | 2011-11-29 | |
US13/530,428 US20120328005A1 (en) | 2011-06-22 | 2012-06-22 | Construction of combined list using temporal distance |
Publications (1)
Publication Number | Publication Date |
---|---|
US20120328005A1 true US20120328005A1 (en) | 2012-12-27 |
Family
ID=47361835
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/530,428 Abandoned US20120328005A1 (en) | 2011-06-22 | 2012-06-22 | Construction of combined list using temporal distance |
Country Status (3)
Country | Link |
---|---|
US (1) | US20120328005A1 (en) |
EP (1) | EP2724537A1 (en) |
WO (1) | WO2012178008A1 (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130089134A1 (en) * | 2011-10-10 | 2013-04-11 | Qualcomm Incorporated | Efficient signaling of reference picture sets |
US20130259122A1 (en) * | 2012-03-30 | 2013-10-03 | Panasonic Corporation | Image coding method and image decoding method |
US20140023140A1 (en) * | 2011-02-08 | 2014-01-23 | Viktor Wahadaniah | Methods and apparatuses for encoding and decoding video using multiple reference pictures |
US9374596B2 (en) | 2008-09-11 | 2016-06-21 | Google Inc. | System and method for video encoding using constructed reference frame |
US9392280B1 (en) | 2011-04-07 | 2016-07-12 | Google Inc. | Apparatus and method for using an alternate reference frame to decode a video frame |
US9426459B2 (en) | 2012-04-23 | 2016-08-23 | Google Inc. | Managing multi-reference picture buffers and identifiers to facilitate video data coding |
US9609341B1 (en) | 2012-04-23 | 2017-03-28 | Google Inc. | Video data encoding and decoding using reference picture lists |
US9756331B1 (en) | 2013-06-17 | 2017-09-05 | Google Inc. | Advance coded reference prediction |
US10027957B2 (en) | 2011-01-12 | 2018-07-17 | Sun Patent Trust | Methods and apparatuses for encoding and decoding video using multiple reference pictures |
US20190306526A1 (en) * | 2018-04-03 | 2019-10-03 | Electronics And Telecommunications Research Institute | Inter-prediction method and apparatus using reference frame generated based on deep learning |
US10666970B2 (en) * | 2015-11-18 | 2020-05-26 | Canon Kabushiki Kaisha | Encoding apparatus, encoding method, and storage medium |
US10887619B2 (en) * | 2016-02-03 | 2021-01-05 | Sharp Kabushiki Kaisha | Moving image decoding device, moving image coding device, and prediction image generation device |
Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040037357A1 (en) * | 2002-06-11 | 2004-02-26 | Stmicroelectronics S.R.I. | Method and apparatus for variable bit-rate control in video encoding systems and computer program product therefor |
US20050147167A1 (en) * | 2003-12-24 | 2005-07-07 | Adriana Dumitras | Method and system for video encoding using a variable number of B frames |
US20050259736A1 (en) * | 2004-05-21 | 2005-11-24 | Christopher Payson | Video decoding for motion compensation with weighted prediction |
US20060159174A1 (en) * | 2003-12-22 | 2006-07-20 | Nec Corporation | Method and device for encoding moving picture |
US20070019730A1 (en) * | 2005-07-19 | 2007-01-25 | Samsung Electronics Co., Ltd. | Video encoding/decoding method and apparatus in temporal direct mode in hierarchical structure |
US20080273599A1 (en) * | 2007-05-02 | 2008-11-06 | Samsung Electronics Co., Ltd. | Method and apparatus for encoding and decoding multi-view video data |
US20090147856A1 (en) * | 2007-12-05 | 2009-06-11 | Samsung Electronics Co., Ltd. | Variable color format based video encoding and decoding methods and apparatuses |
US20090148058A1 (en) * | 2007-12-10 | 2009-06-11 | Qualcomm Incorporated | Reference selection for video interpolation or extrapolation |
US20090154563A1 (en) * | 2007-12-18 | 2009-06-18 | Edward Hong | Video codec with shared intra-prediction module and method for use therewith |
US20090175330A1 (en) * | 2006-07-17 | 2009-07-09 | Zhi Bo Chen | Method and apparatus for adapting a default encoding of a digital video signal during a scene change period |
US20110090960A1 (en) * | 2008-06-16 | 2011-04-21 | Dolby Laboratories Licensing Corporation | Rate Control Model Adaptation Based on Slice Dependencies for Video Coding |
US20120092452A1 (en) * | 2009-07-04 | 2012-04-19 | Dolby Laboratories Licensing Corporation | Encoding and Decoding Architectures for Format Compatible 3D Video Delivery |
US20120189058A1 (en) * | 2011-01-24 | 2012-07-26 | Qualcomm Incorporated | Single reference picture list construction for video coding |
US8284846B2 (en) * | 2004-08-10 | 2012-10-09 | Thales | Method for shaping frames of a video sequence |
-
2012
- 2012-06-22 WO PCT/US2012/043748 patent/WO2012178008A1/en active Application Filing
- 2012-06-22 EP EP12735681.4A patent/EP2724537A1/en not_active Withdrawn
- 2012-06-22 US US13/530,428 patent/US20120328005A1/en not_active Abandoned
Patent Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040037357A1 (en) * | 2002-06-11 | 2004-02-26 | Stmicroelectronics S.R.I. | Method and apparatus for variable bit-rate control in video encoding systems and computer program product therefor |
US20060159174A1 (en) * | 2003-12-22 | 2006-07-20 | Nec Corporation | Method and device for encoding moving picture |
US20050147167A1 (en) * | 2003-12-24 | 2005-07-07 | Adriana Dumitras | Method and system for video encoding using a variable number of B frames |
US20050259736A1 (en) * | 2004-05-21 | 2005-11-24 | Christopher Payson | Video decoding for motion compensation with weighted prediction |
US8284846B2 (en) * | 2004-08-10 | 2012-10-09 | Thales | Method for shaping frames of a video sequence |
US20070019730A1 (en) * | 2005-07-19 | 2007-01-25 | Samsung Electronics Co., Ltd. | Video encoding/decoding method and apparatus in temporal direct mode in hierarchical structure |
US20090175330A1 (en) * | 2006-07-17 | 2009-07-09 | Zhi Bo Chen | Method and apparatus for adapting a default encoding of a digital video signal during a scene change period |
US20080273599A1 (en) * | 2007-05-02 | 2008-11-06 | Samsung Electronics Co., Ltd. | Method and apparatus for encoding and decoding multi-view video data |
US20090147856A1 (en) * | 2007-12-05 | 2009-06-11 | Samsung Electronics Co., Ltd. | Variable color format based video encoding and decoding methods and apparatuses |
US20090148058A1 (en) * | 2007-12-10 | 2009-06-11 | Qualcomm Incorporated | Reference selection for video interpolation or extrapolation |
US20090154563A1 (en) * | 2007-12-18 | 2009-06-18 | Edward Hong | Video codec with shared intra-prediction module and method for use therewith |
US20110090960A1 (en) * | 2008-06-16 | 2011-04-21 | Dolby Laboratories Licensing Corporation | Rate Control Model Adaptation Based on Slice Dependencies for Video Coding |
US20120092452A1 (en) * | 2009-07-04 | 2012-04-19 | Dolby Laboratories Licensing Corporation | Encoding and Decoding Architectures for Format Compatible 3D Video Delivery |
US20120189058A1 (en) * | 2011-01-24 | 2012-07-26 | Qualcomm Incorporated | Single reference picture list construction for video coding |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9374596B2 (en) | 2008-09-11 | 2016-06-21 | Google Inc. | System and method for video encoding using constructed reference frame |
US11375240B2 (en) | 2008-09-11 | 2022-06-28 | Google Llc | Video coding using constructed reference frames |
US10027957B2 (en) | 2011-01-12 | 2018-07-17 | Sun Patent Trust | Methods and apparatuses for encoding and decoding video using multiple reference pictures |
US10841573B2 (en) * | 2011-02-08 | 2020-11-17 | Sun Patent Trust | Methods and apparatuses for encoding and decoding video using multiple reference pictures |
US20140023140A1 (en) * | 2011-02-08 | 2014-01-23 | Viktor Wahadaniah | Methods and apparatuses for encoding and decoding video using multiple reference pictures |
US9392280B1 (en) | 2011-04-07 | 2016-07-12 | Google Inc. | Apparatus and method for using an alternate reference frame to decode a video frame |
US9451284B2 (en) * | 2011-10-10 | 2016-09-20 | Qualcomm Incorporated | Efficient signaling of reference picture sets |
US20130089134A1 (en) * | 2011-10-10 | 2013-04-11 | Qualcomm Incorporated | Efficient signaling of reference picture sets |
US20130259122A1 (en) * | 2012-03-30 | 2013-10-03 | Panasonic Corporation | Image coding method and image decoding method |
US10390041B2 (en) * | 2012-03-30 | 2019-08-20 | Sun Patent Trust | Predictive image coding and decoding using two reference pictures |
US9426459B2 (en) | 2012-04-23 | 2016-08-23 | Google Inc. | Managing multi-reference picture buffers and identifiers to facilitate video data coding |
US9609341B1 (en) | 2012-04-23 | 2017-03-28 | Google Inc. | Video data encoding and decoding using reference picture lists |
US9756331B1 (en) | 2013-06-17 | 2017-09-05 | Google Inc. | Advance coded reference prediction |
US10666970B2 (en) * | 2015-11-18 | 2020-05-26 | Canon Kabushiki Kaisha | Encoding apparatus, encoding method, and storage medium |
US10887619B2 (en) * | 2016-02-03 | 2021-01-05 | Sharp Kabushiki Kaisha | Moving image decoding device, moving image coding device, and prediction image generation device |
US11234019B2 (en) * | 2016-02-03 | 2022-01-25 | Sharp Kabushiki Kaisha | Prediction image generating method, method for decoding a moving image, and method for coding a moving image |
US11019355B2 (en) * | 2018-04-03 | 2021-05-25 | Electronics And Telecommunications Research Institute | Inter-prediction method and apparatus using reference frame generated based on deep learning |
US20190306526A1 (en) * | 2018-04-03 | 2019-10-03 | Electronics And Telecommunications Research Institute | Inter-prediction method and apparatus using reference frame generated based on deep learning |
Also Published As
Publication number | Publication date |
---|---|
EP2724537A1 (en) | 2014-04-30 |
WO2012178008A1 (en) | 2012-12-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20120328005A1 (en) | Construction of combined list using temporal distance | |
US9066104B2 (en) | Spatial block merge mode | |
US8737480B2 (en) | Joint spatial and temporal block merge mode for HEVC | |
US9100649B2 (en) | Method and apparatus for processing a video signal | |
US9210442B2 (en) | Efficient transform unit representation | |
EP3058739B1 (en) | Features of intra block copy prediction mode for video and image coding and decoding | |
KR101316060B1 (en) | Decoding method of inter coded moving picture | |
US9210425B2 (en) | Signaling of temporal motion vector predictor (MVP) flag for temporal prediction | |
EP2756675B1 (en) | Deriving reference mode values and encoding and decoding information representing prediction modes | |
US9094689B2 (en) | Motion vector prediction design simplification | |
US9549177B2 (en) | Evaluation of signaling of collocated reference picture for temporal prediction | |
US9300959B2 (en) | Implicit determination of collocated picture for temporal prediction | |
US9380319B2 (en) | Implicit transform unit representation | |
US9319681B2 (en) | Signaling of temporal motion vector predictor (MVP) enable flag | |
US20120320984A1 (en) | Inter-Prediction Candidate Index Coding Independent of Inter-Prediction Candidate List Construction in Video Coding | |
US20130039417A1 (en) | Residual tree structure of transform unit partitioning | |
US20130022117A1 (en) | Temporal block merge mode | |
US20140086311A1 (en) | Signaling of scaling list | |
US20140092956A1 (en) | Adaptive transform options for scalable extension | |
US20130083845A1 (en) | Methods and devices for data compression using a non-uniform reconstruction space | |
US20130022108A1 (en) | Quantization parameter derivation from qp predictor | |
KR101895389B1 (en) | Method and Apparatus for image encoding | |
WO2014028631A1 (en) | Signaling of temporal motion vector predictor (mvp) enable flag |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GENERAL INSTRUMENT CORPORATION, PENNSYLVANIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:YU, YUE;FANG, XUE;WANG, LIMIN;AND OTHERS;REEL/FRAME:028448/0036Effective date: 20120621 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT HOLDINGS, INC.;REEL/FRAME:030866/0113Effective date: 20130528Owner name: GENERAL INSTRUMENT HOLDINGS, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT CORPORATION;REEL/FRAME:030764/0575Effective date: 20130415 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034280/0001Effective date: 20141028 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |