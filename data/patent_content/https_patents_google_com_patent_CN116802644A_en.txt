CN116802644A - Machine learning ranking and predictive calibration - Google Patents
Machine learning ranking and predictive calibration Download PDFInfo
- Publication number
- CN116802644A CN116802644A CN202280011998.8A CN202280011998A CN116802644A CN 116802644 A CN116802644 A CN 116802644A CN 202280011998 A CN202280011998 A CN 202280011998A CN 116802644 A CN116802644 A CN 116802644A
- Authority
- CN
- China
- Prior art keywords
- machine learning
- learning model
- digital
- digital components
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 166
- 238000000034 method Methods 0.000 claims abstract description 98
- 238000012549 training Methods 0.000 claims abstract description 54
- 238000003860 storage Methods 0.000 claims abstract description 21
- 230000008569 process Effects 0.000 claims description 37
- 238000012545 processing Methods 0.000 claims description 26
- 238000013528 artificial neural network Methods 0.000 claims description 20
- 230000001902 propagating effect Effects 0.000 claims description 3
- 238000004590 computer program Methods 0.000 abstract description 12
- 239000013598 vector Substances 0.000 description 24
- 230000003993 interaction Effects 0.000 description 16
- 238000011156 evaluation Methods 0.000 description 11
- 238000004891 communication Methods 0.000 description 7
- 238000004519 manufacturing process Methods 0.000 description 7
- 238000012546 transfer Methods 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 230000000694 effects Effects 0.000 description 6
- 230000006870 function Effects 0.000 description 6
- 230000000644 propagated effect Effects 0.000 description 6
- 230000004044 response Effects 0.000 description 6
- 238000013459 approach Methods 0.000 description 5
- 238000009877 rendering Methods 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 3
- 239000013589 supplement Substances 0.000 description 3
- 238000007477 logistic regression Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 239000000047 product Substances 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000026676 system process Effects 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000007670 refining Methods 0.000 description 1
- 238000010079 rubber tapping Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000000153 supplemental effect Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0241—Advertisements
- G06Q30/0251—Targeted advertisements
- G06Q30/0269—Targeted advertisements based on user profile or attribute
- G06Q30/0271—Personalized advertisement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0282—Rating or review of business operators or products
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/06—Buying, selling or leasing transactions
- G06Q30/0601—Electronic shopping [e-shopping]
- G06Q30/0631—Item recommendations
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training and using a Machine Learning (ML) model. In one aspect, a method includes receiving a digital component request. The first ML model may output a score indicating a likelihood of a positive result of the digital component. The input data may be provided to the second ML model and may include feature values for a subset of the digital components selected based on the output scores. The second ML model may be trained to output participation predictions and/or rankings for the digital components based at least in part on the feature values of the digital components to be provided together as the recommendation, and may generate a second output comprising the rankings and participation predictions for the digital components in the subset of digital components. At least one digital component may be provided based on the second output.
Description
Technical Field
The present description relates to machine learning ranking and predictive calibration for recommendation systems.
Background
Recommendation or User Interaction Rate (UIR) predictive models are typically trained using logistic regression, minimizing the logarithmic penalty of positive outcome of the prediction or probability of clicking. Such techniques may be used to predict the probability of a target variable, e.g., recommending that there be a positive result.
Disclosure of Invention
In general, this specification describes systems, methods, and techniques for capturing the impact on the label results of an example caused by other examples that are to be recommended with the example using machine learning calibration in a recommended machine learning model. These techniques may be used for Content Selection Process Based Recommendation (CSPBR) systems and other types of recommendation systems, particularly but not limited to those in which there are two or more goals to meet, improve, or optimize in the recommendation.
Improving the accuracy of ranking is important in recommendation systems. The recommendation system may predict a rating or other score for each item in a set of items based on one or more metrics related to the item, its intended use, past performance in the use, its user, and/or other information, and use these ratings/scores to provide a recommendation. For example, given appropriate data regarding the efficiency of components used together in a manufacturing process, a properly trained recommendation system may provide a set of ranked components that can be used to complete the process. Note that in this case, the selection of components to use may be interdependent, i.e., the selection of one component may affect the efficiency of other selected components.
Training the recommended machine learning model using logistic regression may not be optimal. The actual model may be incorrectly specified and credit attributes between features that are optimized to predict personal engagement rates may negatively impact ranking. Thus, the impact of features that are inaccessible to the model designer is marginalized, resulting in predictions that are averaged over a training population, even if the population is heterogeneous.
Measuring additional ranking losses for paired or list-wise relationships between examples (e.g., items) recommended in response to the same request may partially address this effect, specifically pushing only ranking-related gradient updates to model features that distinguish between different examples in a common recommendation set (i.e., examples recommended together), rather than features that are common to all examples recommended together. This technique balances the error setting (miss-specification) by improving the ranking rather than improving the accuracy of each individual example prediction.
However, this approach still does not solve the main error setting problem: model features may not capture the impact of one item on another. In addition, because of the lack of knowledge about which items are co-recommended on the same request at the time of inference, this approach still does not help to generate predictions that better model the behavior of a given item in the presence of other items presented with the given item. Knowledge of recommended items is often counter-facts after the set of items has been predicted.
Ranking and predictive calibration techniques described in this document may be used to improve recommendations related to interactive content. For example, when determining which items (e.g., digital components or search results) to recommend in connection with search results or other digital content, and where, the ranking of candidate items may be more important (or equally important) than the actual score generated by the machine learning model used to predict the performance (e.g., interaction or engagement rate) of the items. The highest ranked item may be recommended for the most prominent position and the sequentially lower ranked items may be recommended for the less prominent positions. In addition, the ranking of items may be affected by other recommended items.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The techniques described in this document may be used in a recommendation system to improve ranking and scoring item accuracy by allowing calibration to capture signals that were not captured by initial inferences before knowing the commonly recommended items, resulting in a more efficient recommendation. Furthermore, the techniques described in this document may be used not only to improve ranking, but also to improve the predictive accuracy of interaction or engagement probabilities for a group of components that are recommended together. In addition, when determining a ranking between items initially selected by a first more resource-intensive machine learning model, the technique may be used where computing resources are limited by using a second less resource-intensive calibration model.
In addition, the calibration techniques described in this specification reduce the distribution of content that does not result in co-display of user participation, thereby reducing wasted resources. The described techniques reduce the amount of resources spent distributing such content and provide content more efficiently across networks. In other words, computing resources (such as network bandwidth, processor cycles, and/or allocated memory) are not wasted by using these resources to distribute content that should not be distributed or that would not be consumed by the entity to which the content is distributed.
Furthermore, the techniques described in this specification reduce the resources required to generate predictions. In particular, the prediction of the primary model is improved using a second calibration model that requires less computational resources (e.g., less processor cycles and/or less memory to store the model and/or intermediate values calculated using the model) than the primary model. The calibration model may be performed faster and use less memory than running the primary model twice. The primary model may select a set of items to be recommended without ranking the items, and the second lighter weight calibration model may run only on the selected items and, by using knowledge of which items to select, produce more accurate predictions, including ranking of the items, than can be produced before knowing the set of selected items. A more accurate ranking may increase the likelihood that the user will engage in the selected items and/or enable the system to select a subset of the selected items with which the user is more likely to interact if provided.
One aspect features receiving a digital component request. First input data including feature values for features of each digital component in the set of digital components may be provided as input to a first machine learning model. The first input data may include a feature value for a feature of each digital component in the set of digital components. The first machine learning model may be trained to output, for each digital component, a score indicating a likelihood of a positive result of the digital component, where the positive result may indicate that a user interacted with or is likely to interact with the digital component when displayed on the device. The first input data may be processed using a first machine learning model. A first output of the first machine learning model may be received and the first output may include respective scores of digital components in the set of digital components. The second input data may include a feature value for a feature of each digital component in the subset of digital components selected based on a respective score of the digital components in the set of digital components. The second input data may be provided as input to a second machine learning model. The second machine learning model may be trained to output a ranking of the digital components based at least in part on feature values of features of the digital components to be provided together as a recommendation. The recommendation may be a recommendation of a digital component to be displayed on the device. The second input data may be processed using a second machine learning model. A second output of the second machine learning model may be received and the second output of the second machine learning model includes a ranking of the digital components in the subset of digital components. At least one digital component in the subset of digital components may be provided based on the second ranking.
One or more of the following features may be included. The second machine learning model may be the same machine learning model as the first machine learning model. The second machine learning model may be a different machine learning model than the first machine learning model. The second machine learning model may be trained differently than the first machine learning model. When processing the same input as the first machine learning model, the second machine learning model may execute fewer instructions to process the same input than the first machine learning model. The second machine learning model may be trained on training examples that include features of a commonly recommended set of digital components that have been provided together as a recommendation.
The first plurality of training examples may be selected from training examples including commonly recommended digital components. One or more features of the first plurality of training examples may be modified, and modifying features of the one or more features may include removing information about the co-recommended items. The first plurality of training examples may be added to the training examples.
The first machine learning model may generate a gradient, and the gradient may be propagated to the plurality of digital component embeddings. Digital component embedding may represent features of commonly recommended digital components. The first machine learning model may process an input including an marginalized embedding representing a marginal contribution of the first feature relative to the contribution of the second feature, and the second machine learning model processes an input including a plurality of digital component embeddings. The first machine learning model may be a neural network and the output of at least one layer of the neural network may be used to train the second machine learning model. The second machine learning model may be a neural network that may include a partially or fully hidden layer configured to generate or use a third score associated with the first hidden digital component as an input based on an input associated with the at least one second digital component. The third score may be a direct loss, a ranking loss, or a similarity score, and may be used as an input to generate a predictive score for the second model.
The details of one or more implementations of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example environment in which a recommendation system provides recommendations using ranking and predictive calibration.
FIG. 2 illustrates a process for providing recommendations using ranking and predictive calibration.
FIG. 3 illustrates a training diagram and deployment for a two-stage inference model.
FIG. 4 shows ranking calibration with top-level embedding.
FIG. 5 illustrates a ranking calibration with top-level embedded similarity scores.
FIG. 6 illustrates ranking calibration with separate top-level embedding.
FIG. 7 illustrates ranking calibration with separate top-level embedded similarity scores.
FIG. 8 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The present specification describes the use of a machine learning calibration technique that captures the impact on the label results (i.e., results produced by a machine learning model) for a given item that are caused by other items that will be recommended with the given item. For example, if N items are to be recommended, the first item (R 1 ) The recommendation is subject to other items (R 2 To R N ) And thus R 1 Should be relative to R 2 To R N Calibration is performed. Furthermore, the calibration can improve the term (R 1 To R N ) Predictive ranking between and participation rate. A first machine learning model may be used to select recommended items (i.e., R 1 To R N ) And the second machine learning model may be used to calibrate the results of the first machine learning model. In some cases, a single machine learning model may be used as both the first machine learning model and the second machine learning model, although the first machine learning model and the second machine learning model may be different machine learning models.
The methods described in this specification allow for calibration to capture signals that cannot be captured by training the various examples without directly training interactions between the examples. Those captured signals are applied to increase prediction accuracy and improve predictions of participation and ranking in examples recommended together by the recommendation system.
The method may be used in a CSPBR system configured to operate with multiple items recommended and the presence of one item may affect participation with another item, including when there is a lack of knowledge during deployment that needs to be resolved which items are to be co-recommended with the item for which the prediction is to be provided. The techniques described in this specification include two-stage methods. In the first phase, recommended items are individually selected, and in the second phase, predictions of engagement rate and rank are adjusted to account for commonly recommended items. One example method is a two-stage complete inference method that regenerates predictions that include interactions with the complete model in the second stage. Another example uses a smaller component calibration model in the second phase to adjust or refine the predictions in the first phase to consider predictions of the impact of co-recommended items on the engagement rate of any particular item. In some implementations, the first stage selects only one set of items without ranking the items, and the second stage ranks them.
The following description is primarily directed to UIR prediction systems, such as click through rates or content selection systems, that predict the likelihood that a user or users will interact with content (e.g., digital components). However, as noted above, the methods described in this document may be used to improve other types of recommendation and/or user participation systems.
Some UIR systems typically have two or more targets. The first goal may be used to generate accurate UIR predictions for a particular item (e.g., a particular digital component). Such predictions may be used to determine values associated with the digital components selected in the content selection processing mechanism (e.g., values of the second-ranked digital components). Another goal may be to produce an accurate ranking between items, which may allow for more accurate selection of a subset of items to be recommended, and for more accurate ordering within the subset, such that overall value is improved. For example, using accurate predictions enables the UIR system to place certain digital components in preferred locations, e.g., more prominent locations within a user interface, e.g., a web page or resources of a native application that includes multiple slots for displaying digital components to a user.
FIG. 1 illustrates an example environment 100 in which a recommendation system 109 provides recommendations using ranking and predictive calibration. Recommendation system 109 may include a request receiver engine 110, an input data generation engine 120, a machine learning model evaluation engine 130, a digital component selection engine 140, a calibration input data generation engine 150, a calibration machine learning model evaluation engine 160, and a recommendation provider engine 170.
The recommendation system 109 may interact with one or more devices 105. In some implementations, the device 105 includes a client device, such as a laptop computer, desktop computer, server, mobile phone, tablet computer, smart speaker, game console, video streaming device, and the like. In some implementations, the equipment 105 includes manufacturing equipment, robots, machines, process controllers, inventory controllers, and the like. The device 105 may provide a request for one or more recommended items, such as digital components or other items, such as parts or components used in manufacturing, processes for diagnosing and/or correcting problems in a manufacturing facility, resources that will provide search results in response to the request, and the like. While the request 107 may be received from various types of devices and may be for various different types of items, for brevity, the components of the recommendation system 109 are described in terms of digital components.
As used throughout this document, the phrase "digital component" refers to a discrete unit of digital content or digital information (e.g., a video clip, an audio clip, a multimedia clip, an image, text, or another unit of content). The digital components may be electronically stored in the physical memory of the device as a single file or collection of files, and the digital components may take the form of video files, audio files, multimedia files, image files, or text files, and may include advertising information, video recommendations, and the like. For example, the digital component may be content intended to supplement a web page, application content (e.g., application pages), or other resources displayed by an application. More specifically, the digital components may include digital content related to the resource content, e.g., the digital components may relate to the same theme as the web page content, or related themes. Thus, the provision of digital components may supplement and generally enhance web pages or application content.
The request receiver engine 110 may receive the request 107 from the device 105. The request may be encoded in various formats. For example, the request may be included as a parameter in a hypertext transfer protocol (HTTP) or Hypertext Transfer Protocol Secure (HTTPs) request. The request may be included as a parameter in the message, e.g., encoded as Simple Object Access Protocol (SOAP) or representational state transfer (REST). The request 107 may include data related to the request and data describing the context of the request, e.g., information about the device 105, such as device type, language, location, display size, display resolution, etc. In some implementations, the request 107 can also include an identifier of the device 105 that sent the request. The data related to the request may be represented as keywords, full text descriptions, boolean search phrases, and the like.
The input data generation engine 120 may create the input data using, for example, information included in the request 107 and/or additional information stored by the recommendation system 109. In addition, the input data generation engine may supplement the data included in the request 107, for example, by using the device identifier included in the request 107, to obtain descriptive information about the device 105 from which the request 107 was received. The input data generation engine 120 may generate input that is an embedded representation of the information included in the request 107 and supplemental information obtained, for example, from an external data store.
The machine learning model evaluation engine 130 may accept the input data generated by the input data generation engine 120 and process the input data using a first machine learning model trained to generate an output comprising, for each of a plurality of digital components in a component list: (i) a first score indicating a likelihood of a positive result; and (ii) a first ranking of one or more components in the list of digital components. A positive result may indicate that the user is interacting with the item or is likely to interact with the item. Examples of user interactions are a user selecting a digital component by clicking, tapping, providing voice input, etc. The likelihood of a positive result may help determine if resources would be wasted if a particular digital component were to be recommended for display but then not interacted with or somehow utilized. For example, if digital components are to be distributed and displayed but not interacted with by a user, network bandwidth, local processor computing, and associated device and infrastructure resources (such as memory allocation and centralized digital component publishing resources) would be wasted.
The machine learning model may be a neural network that has been trained on examples. Each example may include a set of feature values and a result. The result may be a 1 indicating that a positive result occurred for this example, or the result may be a 0 indicating that no positive result occurred for this example. Other values may be used to represent positive and negative results.
Features may include keywords in example 107 that indicate items of interest (such as digital content), a context in which the request was generated (such as information displayed when the request was sent), a geographic location in which the request was sent, a type of device that sent the request, a language used on the device that sent the request, and so forth. Additionally, features may include features of items that can be recommended, such as features of movies or digital component creatives. For example, a movie may have categories (e.g., action, comedy, etc.), a lead actor, a language, etc., and these may all be features of the movie. Further, the features may include possible renderings of the recommendation, such as a location in a list of recommended items.
The digital component selection engine 140 may use the output from the machine learning model evaluation engine 130 to generate a set of selected items. The digital component selection engine 140 may select, for example, the digital component with the highest rank, as further described with reference to fig. 2. The set of selected digital components may include each selected digital component, a feature value associated with the digital component, and an output generated by the machine learning model evaluation engine 130.
The calibration input data generation engine 150 may create calibration input data for the digital component selected by the digital component selection engine 150 using the information included in the request 107 and the output generated by the machine learning model evaluation engine 130. The calibration input data generation engine 150 may generate an input that is an embedded representation of this information, which may be a numerical vector of feature values representing the feature.
The calibration machine learning model evaluation engine 160 may accept the input data generated by the calibration input data generation engine 150 and process the input data using a calibration machine learning model that is trained to generate an output comprising, for each of the plurality of digital components in the digital component list selected by the digital component selection engine 140: (i) A calibration score indicating a likelihood of a positive result in the presence of a co-recommended item; and (ii) in the presence of all other commonly recommended items, the calibration rank of one or more components in the list of components and their predicted values. As described above, a positive result may indicate that the user interacted with the item, for example, by selecting a digital component.
The calibration machine learning model 160 may be a neural network that has been trained on examples that, for each example, includes a set of feature values associated with a set of features and results. In some embodiments, the calibration model 160 is an adjunct to the machine learning model described above, and is not itself a complete model. The result may be a 1 indicating that a positive result occurred for this example, or the result may be a 0 indicating that no positive result occurred for this example. Other values may be used to represent positive and negative results. The calibration machine learning model is described in more detail below.
In some implementations, the calibrated machine learning model used by the calibrated machine learning model evaluation engine 160 may be the same machine learning model as the machine learning model used by the machine learning model evaluation engine 130. In some implementations, the calibrated machine learning model may be a different and much simpler machine learning model, such as a machine learning model configured to be completed using fewer computing resources, e.g., it may execute fewer instructions to process the same input than the machine learning model used by the machine learning model evaluation engine 130. The calibrated machine learning model evaluation engine 160 may generate a model output that may include, for each digital component, a ranking of the digital components and a likelihood of a predicted positive result.
The recommendation provider engine 170 may provide the result data 195 to the device. The result data may include one or more recommended items (e.g., digital content items) and may include an indication of how the recommended items are ranked. For example, items may be listed in the result data in a ranked order, or they may have an associated ranking in the result data.
The recommendation provider engine 170 may provide the result data 195 using any suitable communication method, such as providing the result data 195 through HTTP, HTTPS, TCP/IP or the like.
Fig. 2 illustrates a process 200 for ranking and predictive calibration. For convenience, process 200 will be described as being performed by a system for ranking and predictive calibration (e.g., recommendation system 109 of fig. 1) that is suitably programmed to perform the process. The operations of process 200 may also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and execution of the instructions by one or more data processing apparatus may cause the one or more data processing apparatus to perform the operations of process 200. One or more other components described herein may perform the operations of process 200.
Although the example process 200 is described in terms of using a machine learning model to select digital components as recommended items, the process 200 may also be used to generate and provide recommendations regarding other items, such as parts or components used in manufacturing, processes for diagnosing and/or correcting problems in a manufacturing facility, resources that will provide search results in response to a request, and so forth, as described above.
In operation 202, the system receives a request. The system may receive the request using any suitable technique, including using a network protocol (e.g., hypertext transfer protocol (HTTP), secure hypertext transfer protocol (HTTPs), etc.), through an Application Programming Interface (API), etc. The request may be received from any entity that interacts with the system, such as a user or another system.
In operation 204, the system creates input data for the first machine learning model and provides the data to the first machine learning model. In some implementations, the system can create at least some of the input data by parsing the request to identify feature values for the relevant features. Some features may be determined from the request, such as keywords, rough geographic locations of the requesting user, meta-grammars, unique identifiers of the requesting user, and so forth. Some features may be associated with candidates such as movie categories, topics for digital components, formats for digital components, and so forth.
In some implementations, the model includes multiple phases, and in intermediate phases, the candidate recommendation set may be retrieved from a storage system (e.g., a relational database). Candidate recommendations may be scored by a machine learning model using the features described above.
The system may create an input from the feature, for example, by mapping the feature values of the feature to the constituent embedded numerical vectors using a lookup table. The mapped values may be initialized to zero or random values and learned as part of a training process for training the machine learning model.
In operation 206, the system processes the first input data using a first machine learning model, the first machine learning model trained to generate a first output, the first output comprising, for a set of digital components in the search component list: (i) A first score indicating a likelihood that a component satisfies a request for one or more components; and optionally, (ii) a first ranking of one or more components in the list of components. For example, for a digital component, the likelihood that the component satisfies the request may be the likelihood that the user will select the digital component. For physical components such as tools, the likelihood may be that the user uses the tool.
In addition to the features included in the input, the machine learning model may evaluate the input describing the features of the digital component and together describe the features of the digital component and the request.
The machine learning model may be any type of machine learning model configured to generate recommendations based on inputs. For example, the machine learning model may be a neural network trained to generate recommendations based on request features, digital component features, features of the digital component along with the request, and features of a rendering system (e.g., a client device that will render recommended items). Features of the rendering system may include, for example, the display location(s) of the recommended item(s) on the screen, the type of user interface in which the recommended item(s) are to be presented, the format in which the recommended item(s) are to be presented (e.g., image, video, audio, text, etc.), and so forth. When the machine learning model is a neural network, the output of the machine learning model, or at least the output of one layer of the neural network, may be used to train a second machine learning model, as described further below.
The machine learning model may be trained using examples that include, for each example, feature values associated with the digital components (i.e., feature values discussed with reference to operation 204). The output value in each training example may be an indication of whether a digital component is selected.
In operation 207, the system receives a first score, and optionally a first ranking. The system may use various methods to receive the scores and rankings. For example, the system may retrieve scores and rankings by calling an API provided by a server running the machine learning model. In some implementations, the system and the machine learning model execute on the same computing device, and the system may receive the score and rank by directly accessing memory, for example, by accessing variables defined on the system or by accessing storage locations known to hold the score and rank.
In operation 208, the system provides a second input. The second input may include a feature value for a feature of each digital component in the subset of digital components. The system may select the subset based on the respective scores of the digital components. For example, the system may select the plurality of digital components with the highest first scores generated in operation 206. In some embodiments, the number of digital components selected is a preconfigured value used by the system. In some implementations, all digital components having a score above a configuration threshold are selected for inclusion in the digital component candidate set.
In operation 210, the system may process the second input using a second machine learning model trained to output a ranking of the digital components based at least in part on feature values of features of the digital components provided together as a recommendation.
The second machine learning model may also output one or more scores. The scores may include, for example, scores indicating the likelihood that the user will participate in the digital component and scores indicating a loss (e.g., a direct loss or a ranking loss) or a similarity score. The second machine learning model may be a neural network, such as a deep neural network. The deep neural network may include a partially or fully hidden layer that generates a score associated with a first hidden component of the neural network based on inputs associated with at least one second component of the neural network. The score may be, for example, a direct loss, a ranking loss, or a similarity score as described above.
In operation 210, the system processes a second input, which may be identical to the first input, using a second machine learning model (or "model" for brevity) that is trained to output a ranking of the digital components based at least in part on feature values of features of the digital components to be provided as recommendations.
In some implementations, the second input may also include additional features not used in the first stage. Such features may be relevant to, for example, a rendering system, such as a final recommended rendering format, location, UI, etc. The second input may include the first input and a characteristic value associated with the digital component in the candidate set of recommendations generated in operation 208. As described below, the second machine learning model may be the first machine learning model or a different machine learning model. The feature values associated with the digital components in the recommended candidate set may include attributes such as size, shape, theme, color, producer, etc. of the components.
The model may evaluate several categories of features, including: (i) Request-only features, which are common to all examples recommended for a given request; (ii) Only digital component features, which distinguish between different examples, and no information about the request that resulted in the recommendation of the digital component; (iii) Digital components and request features (written as "digital component x request" for brevity) that carry joint information about requests and digital component recommendations; and (iv) calibration features that can affect predictions based on other signals, such as recommended locations of digital components within the user interface, digital component formats, and so forth. The model may evaluate features in all such categories or a subset of the categories.
A wide range of features may be used in each feature class, and this specification lists only a subset as an example. The request-only features may include keywords in the request, the location where the request was made, features of the requester (such as request history), and the device used to make the request. Digital component-only features may include physical characteristics of the digital component, such as size, shape, and color, the manufacturer of the component, the description of the component, and so forth. Digital component x request features may include features of the digital component considered in conjunction with the requested features. For example, keywords included in the request may be considered with the category associated with the digital component and may be considered in conjunction with the category associated with the digital item; the category associated with the request may consider the keywords associated with the request in conjunction with the keywords (associated with the request and the digital components), and so on. The calibration features may include the type of user interface, the format of the digital component (such as large or small, static or multimedia, images or text, color or black and white, the location where the digital component is to be displayed, etc.).
While such a model may be trained to use features that capture interactions between a first digital component and a second digital component that is recommended with the first digital component to produce a ranking and prediction, utilizing the signal in performing the inference may be complex. Complexity arises because during the content selection process, knowledge of which digital components will be recommended with a given digital component is generally not available, so this information cannot affect its predictive score, but such a score is required to determine which digital components are recommended or in which order items are recommended. Thus, training typically begins on a conventional example where the log contains the necessary information, allowing such signals to be used for training. However, training should be applied so that it is useful in inferring UIR with the model. For this reason, it is advantageous to execute a second machine learning model (which may be the same as or different from the first machine learning model), wherein the first model selects items without knowing which items are co-selected, and the second model refines predictions and rankings within the selection with knowledge of which items are co-selected together and with one or more features of the items available. Thus, the second model may be trained using a training example, which may include features of a set of commonly recommended digital components that have been provided together as a recommendation.
The accuracy of the training model (e.g., using cross-entropy log-loss or other loss) may assign credits to request features, digital component features, and digital component x request features. In addition to accuracy loss (which may be accomplished by training on score differences in addition to cross entropy), training on models for ranking (predictive or score difference) loss may assign ranking credits to digital component and digital component x request features, and may avoid assigning credits to request-only features, achieving better attribution of credits between digital component (and digital component x request) related features and request features. The weights assigned to the digital components by the two mechanisms may capture the impact of marginalization, i.e., determination of marginal contribution of features, on other digital components (for digital component features) and on specific requests for other digital components (for digital component x request features). Note that for a particular digital component with respect to a particular request, the model weights learned for a cross digital component x request may capture the average impact of all other digital components on that digital component with that request. For example, if all similar queries always show the same set of digital components, the result may be sufficient. However, if the digital component x request feature in the model is still marginalized on a different population of choices of other digital components in the request, the result may not be accurate enough. For example, where a particular digital component may be shown to have two or more different sets of digital components in response to the same request, the characteristics of the digital component and the characteristics of the digital component x request will give an marginalized representation of all other digital components that are co-recommended with the digital component and of all other digital components that are co-recommended with the digital component for the request, respectively. However, with knowledge of other digital components, marginalization can be resolved, resulting in better predictions. Specifically, the system may select a training example from training examples including the common recommendation component, and for each selected training example, the system may add the same training example as the training example except that the newly added training example omits information about the common recommendation item. From these examples, the model learns the marginalization of the omitted information, and in particular, it learns the marginal contribution of the first feature relative to the second feature. For examples where this information is not omitted, the system learns the effect of the specified co-recommended items. Removing this information about the random or pseudo-random subsets allows the model to generate predictions of marginalized co-recommended items that can be used in the first stage when the items are unknown. Additionally, the first machine learning model may output a gradient, and the gradient may be propagated to some digital components in the embedding. The digital component embedding may represent features of the co-recommended component.
To clarify the concept, consider an example in which: item a has a positive result in 1000 of the 1000 examples when recommended with item B, and item a has 0 positive result in 1000 examples when recommended with item C. Overall, item a has a positive result of 50% on average, so without additional information about co-recommended items, item a would have a predicted positive result rate of 50%. If 50% meets the recommendation threshold, item A may be recommended, but with less confidence. However, if the system can determine that item A will be recommended along with item B, the predicted positive result rate will be 100% and item A should be recommended. Conversely, if the system can determine that item A will be recommended with item C, the predicted positive result rate will be 0%, so item A should not be recommended.
Even when a model is able to predict which other digital components may be co-recommended with a single digital component, the ability to generate such predictions means that if different digital components are co-recommended with the model, the model will not be able to express different effects on its recommendation. Thus, such a solution may not be sufficient to provide sufficiently accurate predictions for recommendations.
Instead, calibration is used in process 200 between predictions of a given digital component by a first machine learning model through the digital components that are recommended in common with the given digital component, i.e., a second machine learning model is performed, and the process may be repeated for each recommended digital component. Assuming that the first machine learning model first selects which digital components to recommend without knowing which other digital components are recommended, an initial prediction may be generated that uses the marginalized information in the form of features from the request, the digital components, and the digital component x request for that digital component. The initial prediction may be calibrated by the relationship of the digital component to other digital components that are candidates for joint recommendation with the digital component, which are now available to the system after outputting the results of the first machine learning model.
The system can learn the representation (which may also be referred to as embedding) through direct loss (cross entropy or otherwise) and ranking loss of the digital components. The representation may be learned as an embedding, for example, near the output of a deep neural network that is trained to make predictions about the project. The system may use the representation in a calibration model. In some implementations, as described below, the calibration model may be used during a second pass over the complete model if inferred resources allow. In some implementations, the system may apply a calibration model to the predictions of the original passes. As described above, the reasons for calibration may include: at retrieval, it is not known which other digital components will be shown in the request, but this information becomes available before the final prediction must be provided. This information may be provided to the model in the second phase, but if the second phase is resource limited, the system may use this information in a calibration model that is applied in addition to the original predictions.
In this method, a model for predicting a tag of a digital component of interest, denoted DC1, includes inputs representing other digital components shown with DC 1. Such input may include, for example, embedding in an embedding vector representing characteristic values of other digital components, which may be a complete hidden layer of a depth network used to predict other digital components. In some implementations, the input may include an embedded sum of other digital components shown with DC 1. For example, the sum of the embeddings may be represented by an embedding vector. In some implementations, the input can include a vector of similarity scores between DC1 and each other digital component shown with DC 1. For example, if there are seven other digital components that have been shown with DC1, the vector will include seven similarity scores, one for each of the seven other digital components. The similarity score of the digital component represents a measure of similarity between the features of DC1 and the digital component. The similarity score may be a correlation or cosine similarity, which is essentially the scalar product of the embedded vector of DC1 and the embedded vector of the digital component. Other similarity functions may also be used.
Training of DC1 under the influence of other digital components may begin by applying an update to the embeddings of the other digital components based on predicting the loss of labels of DC1, or without applying such an update, wherein the embeddings of the other digital components are updated only if predictions of these other digital components are lost. The training may then be repeated for all digital components in the set of digital components.
In some implementations, if the same model is used for both passes, the model may learn to marginalize on all such digital components for the first inference phase (search and other initial prediction phases) in addition to learning interactions with other digital components. Training may randomly discard scores generated from a small portion of the example other digital components and replace them with features of DC1 that represent marginalization of scores on all other commonly displayed digital components. By using the marginalized embedding vector for a small portion of the training examples, rather than for examples whose embedding is randomly selected, the discarded score may be replaced in the embedding of inputs from other digital components. The technique enables the model to learn (i) inferences when other examples are known and (ii) inferences when other components are unknown. Case (ii) can be used in the first stage of the model process and case (i) is used in the second stage once the other digital components are known. The embedding of case (ii) may be stored as a feature of DC1, representing marginalization of all recommendations co-recommended with DC 1. The embedded vector is trained with the model. The first inference stage uses the marginalized embedded vector as input to determine an initial user interaction probability for the digital component. The second inference may use embedding (or based on an embedded score) of other digital components that are co-recommended with DC 1. This is shown in fig. 3.
After selecting the set of qualified digital components to be shown with the digital components, another stage of forward propagation is applied to the model, where forward propagation uses inputs representing all other digital components in the set. Examples of two-stage inference or calibration processes are further described with reference to fig. 3-7. The prediction generated in this pass is the final prediction of DC 1.
In operation 211, the system receives the digital component ranking from the second machine learning model and optionally receives the score of the digital component. As described above, the system may use various methods to receive scores and rankings. For example, the system may retrieve scores and rankings by calling an API provided by a server running the machine learning model. In some implementations, the system and the machine learning model execute on the same computing device, and the system may receive the score and rank by directly accessing memory, for example, by accessing variables defined on the system or by accessing memory locations known to hold the score and rank.
In operation 212, the system provides a recommended digital component. Recommendations may be provided using conventional data transfer techniques, such as sending messages over a network protocol (e.g., HTTP or HTTPs) or as a return value for API calls.
An example of a two-stage inference system is shown in fig. 3, which shows a training diagram and deployment for a two-pass calibration (primary) model. Model 300 may generate a prediction of the probability of a positive label that represents the likelihood that a digital component will be interacted with (e.g., selected by a user based on accuracy and/or ranking loss).
Training of the model may use features that are randomly or pseudo-randomly selected between the marginalized feature and the feature representing the other digital component (back propagation may be configured to allow or stop gradient updates from representing the embedding of the other digital component when the other digital component is used). Inference can include two passes, one using the marginalized features of the other digital components, and one having the actual features of the other digital components and/or embedding to set content selection parameters. In this case, the same model is used for both phases. More generally, the inference can be performed twice using a mainframe learning model, or using a mainframe learning model and a calibration machine learning model, the master model followed by a calibration model, as described below.
Two-pass inference requires the full complexity of the complete model to be applied twice, first generating predictions of pre-selected digital components, and then refining the predictions with knowledge of which components were pre-selected. In embodiments where two-pass inference is impractical, for example, where the prediction of the first retrieval (digital component eligibility) stage is expensive, one pass inference (of the primary model) may be used followed by one pass inference using a less resource intensive calibration model. The master model may generate an initial prediction of DC1 independent of (i.e., implicitly marginalized on) other digital components selected with DC 1. The training of the primary model may be independent of other digital components, i.e., digital components that are to be co-selected to appear with DC1, and the inference may produce predictions that are used to determine which digital components are shown for the request in an initial (eligibility) stage.
Next, the calibration model takes as input the predicted score of DC1 along with conventional calibration features and information about other digital components shown with DC 1. In updating, the gradient from the calibration model may optionally be applied to parameters of the primary model, and a stop gradient from the calibration model to the primary model that produced the first prediction prevents updating from the calibration model to the primary model. Parameters in the calibration model (including other calibration features) may be updated as part of the calibration training phase. As described below, there are multiple implementations, each using a different signal to calibrate the commonly recommended components.
Inference of the deployment model may require multiple steps. The predictions of the primary model may be used to select which digital components to initially select, and the predictions may be supplemented with other calibration features. Knowledge of other digital components may be utilized to calculate the calibration in the predicted inference of DC 1. Specifically, if the system first selects all digital components to illustrate, the system may use features representing other digital components in evaluating the calibration model of DC 1. If the system sequentially selects digital components (by location), first the top digital component to be shown, then the next digital component, until the final digital component is selected, the system can evaluate the features from the previously selected digital components to calibrate the predictions for DC 1. As described above, the marginalized features learned by the model may also be used to represent digital components that have not been selected. However, in some cases, this approach may impair the result of model resolution marginalization on commonly displayed digital components.
The method described in this specification can train the embedded vector to represent each other digital component and use it for calibration of DC 1. The embedded vector may be trained using a variety of methods. First, the embedding may be the top layer (or another layer, or a component of that other layer) of the direct inferred path of other digital components (i.e., the top hidden layer of DC2 and all other digital components are used to calibrate DC 1). This method is illustrated in fig. 4 and 5, as follows. The calibrated gradient update on DC1 may optionally be propagated to the master predictive model of DC1 and/or DC 2. The embedding of DC1 and DC2 may be provided directly into the calibration model of DC1, the calibration model of DC1 may employ embedding of all other digital components (DC 3, etc.) cascaded with the embedding of DC2 (if there are a fixed number of digital components shown), or averaged or summed into one embedded vector input, as shown in fig. 4. Note that averaging and summing involves less computational resources than cascading, but unlike cascading, information may be lost. Instead of providing the calibration model with an embedding of all digital components, the model may receive a similarity score (such as a simple dot product or cosine similarity) between the embedding of DC1 and the embedding of DC2, where such score may be entered separately for each of the commonly displayed digital components, or averaged over all commonly displayed digital components, as shown in fig. 5. The hidden vector, separate from the normal training loss, may be used to generate an embedding for ranking, as shown in fig. 6 and 7. The vector may be generated using individual ranking losses and may be fed directly into the calibration model (as shown in fig. 6) or through a similarity score (as shown in fig. 7). Gradients may also be propagated from the calibration model, but such propagation is not required.
In addition to other losses including ranking losses, a primary model with regular cross entropy losses can be trained for DC 1. The calibration model, while it may include ranking losses, may be trained on cross entropy losses only, optimizing UIR predictions as a function of representations of other commonly displayed digital components, or at least as a function of relationships between digital components trained for DC1 and their commonly displayed digital components.
Fig. 4-7 illustrate various configuration alternatives. In fig. 4, the system uses the top layer of both DC1 and DC2, both trained for cross entropy and ranking, as a calibration embedding provided to the calibration model, with or without propagating gradients into the primary model. The embedding of DC1 may optionally be propagated into the calibration model (it may not be necessary to propagate them given that the calibration model already knows the predictions of DC 1). The calibration model may obtain the prediction score (log) of DC1 and use information from the calibration model to adjust the predictions. The calibration model will learn the interactions between the representation of DC2 and the representation of DC1 and generate a calibration prediction that utilizes such interactions. Interactions can be illustrated using an example containing two items a and B, where a has a much greater marginal participation rate than B. However, when a is recommended with B, the participation rate is opposite, i.e., B has a much greater marginal participation rate than a. In this case, B should be recommended to be ranked higher than a because they are all recommended together. The feature of B in the model of a may be an embedded vector representing B. The vector may absorb interaction information and may result in a predicted decrease in the engagement rate of item a. Similarly, an embedded vector representing a in the model of B will result in an increased prediction of the participation rank of B in the presence of a. The calibration model learns from the training data to make such changes to its predictions. Furthermore, if a different vector is inserted in the model of a, for some example C (which has no effect on the participation of a), the model will learn this information in training, and the embedded vector representing C will have no effect on the prediction of a, but for the embedding of B, the model will have an effect.
Fig. 5 shows a similar approach, but the input from the hidden embedding layer takes the form of a similarity score between the embedding of DC1 and the embedding of the co-displayed digital components. The method shown in fig. 6 is similar to that shown in fig. 4, except that the embedding is trained with the rank loss separately from the cross entropy loss at the top of the primary model, and a separate embedding is used to calibrate the model. This approach enhances the impact of the relationship DC2 (and other digital components) imposed on DC1 in the calibration prediction of DC 1. Fig. 7 is similar to fig. 6 except that the similarity score between DC1 and the digital components with which it is displayed can be used as an input to the calibration model instead of being actually embedded.
FIG. 4 shows ranking calibration with top-level embedding. The two (base) networks use cross entropy and ranking penalty to generate predictions for DC1 and DC 2. The top hidden layer is used as an embedded input to the calibration model, which generates the final prediction of DC 1. The prediction of DC1 also uses other commonly recommended digital components DC3, etc. in a similar manner as it uses DC 2. Gradients may optionally flow from the calibration model to the underlying network. The calibration model may optionally include hidden layer embedding of DC 1. The calibration operation may use both the original base prediction and the calibration signal to produce a final prediction. Similar additional inputs to the calibration model for DC1 may be applied from additional commonly displayed digital components (DC 3, DC4, etc.).
Fig. 5 illustrates ranking calibration using top-level embedded similarity scores. This calibration configuration for generating predictions of DC1 is similar to that shown in FIG. 4, except that the input to the calibration network may be factorization of the embedding of DC1 with the embedding of DC2, or a similarity score between the two embedding vectors. Similar additional inputs to the calibration model for DC1 may be applied from additional commonly displayed digital components (DC 3, DC4, etc.).
FIG. 6 illustrates ranking calibration with separate top-level embedding. This calibration configuration for generating predictions for DC1 is similar to that shown in FIG. 4, except that a separate embedding, which trains the losses by ranking, is used as an input to the calibration model. In the figure, "S"620 in the node represents Sigmoid (logical) function transformation, and "e"610 represents a component of the embedded vector of DC 1. Similar additional inputs may be applied to the calibration model of DC1 from additional commonly displayed digital components (DC 3, DC4, etc.).
FIG. 7 illustrates ranking calibration with separate top-level embedded similarity scores. This calibration configuration for generating predictions for DC1 is similar to that shown in FIG. 6, except that the input to the calibration network may be factorization of DC1 embedding with DC2 embedding, or a similarity score between the two embedding vectors. Similar additional inputs to the calibration model of DC1 may be applied from additional commonly displayed digital components (DC 3, DC4, etc.).
FIG. 8 is a block diagram of an example computer system 800 that may be used to perform the operations described above. System 800 includes a processor 810, a memory 820, a storage device 830, and an input/output device 840. Each of the components 810, 820, 830, and 840 may be interconnected, for example, using a system bus 850. Processor 810 is capable of processing instructions for execution within system 800. In some implementations, the processor 810 is a single-threaded processor. In another implementation, the processor 810 is a multi-threaded processor. The processor 810 is capable of processing instructions stored in the memory 820 or on the storage device 830.
Memory 820 stores information within system 800. In one implementation, the memory 820 is a computer-readable medium. In some implementations, the memory 820 is a volatile memory unit. In another embodiment, memory 820 is a non-volatile memory unit.
Storage device 830 is capable of providing mass storage for system 800. In some implementations, the storage device 830 is a computer-readable medium. In various different implementations, storage device 830 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
Input/output device 840 provides input/output operations for system 800. In some implementations, the input/output device 840 may include one or more of a network interface device (e.g., an ethernet card), a serial communication device (e.g., an RS-232 port), and/or a wireless interface device (e.g., an 802.11 card). In another implementation, the input/output devices may include a driver device configured to receive input data and transmit output data to external devices 860, such as keyboards, printers, and display devices. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and the like.
Although an example processing system has been described in FIG. 8, implementations of the subject matter and functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
The term "configuration" is used in this specification in connection with systems and computer program components. For a system of one or more computers configured to perform a particular operation or action, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a storage medium, which can be a tangible, non-transitory storage medium, for execution by, or to control the operation of, data processing by an apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all kinds of apparatus, devices and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, app, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combination of, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor or both, or any other type of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD-ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example by sending a web page to a web browser on the user device in response to a request received from the web browser. Further, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smart phone running a messaging application) and receiving response messages from the user in return.
The data processing means for implementing the machine learning model may also comprise, for example, dedicated hardware accelerator units for handling public and computationally intensive parts of machine learning training or production, i.e. inference, workload.
The machine learning model can be implemented and deployed using a machine learning framework (e.g., a TensorFlow framework, microsoft Cognitive Toolkit framework, apache Singa framework, or Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include Local Area Networks (LANs) and Wide Area Networks (WANs), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data (e.g., HTML pages) to a user of the device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
In addition to the embodiments described above, the following embodiments are also innovative and are disclosed in the form of numbered clauses:
clause 1 is a method comprising:
receiving a digital component request;
providing first input data as input to a first machine learning model, the first input data comprising a feature value of a feature of each digital component in a set of digital components, wherein the first machine learning model is trained to output, for each digital component, a score indicative of a likelihood of a positive result for the digital component;
processing the first input data using a first machine learning model;
receiving respective scores of digital components in the set of digital components as a first output of a first machine learning model;
providing second input data as input to a second machine learning model, the second input data including feature values of features of each digital component in the subset of digital components selected based on respective scores of the digital components in the set of digital components, wherein the second machine learning model is trained to output a ranking of the digital components based at least in part on the feature values of features of the digital components to be provided together as a recommendation;
processing the second input data using a second machine learning model;
Receiving a ranking of the digital components in the subset of digital components as a second output of a second machine learning model; and
at least one digital component in the subset of digital components is provided based on the second ranking.
Clause 2 is the method of clause 1, wherein the second machine learning model is the same machine learning model as the first machine learning model.
Clause 3 is the method of clauses 1-2, wherein the second machine learning model is a different machine learning model than the first machine learning model, and wherein the second machine learning model has been trained differently than the first machine learning model.
Clause 4 is the method of clause 3, wherein when processing the same input as the first machine learning model, the second machine learning model executes fewer instructions to process the same input than the first machine learning model.
Clause 5 is the method of clauses 1-4, wherein the second machine learning model is trained on training examples including features of a set of commonly recommended digital components that have been provided together as a recommendation.
Clause 6 is the method of clause 5, further comprising:
Selecting a first plurality of training examples from training examples including commonly recommended digital components;
modifying one or more features of the first plurality of training examples, wherein modifying features of the one or more features includes removing information about commonly recommended items; and
a first plurality of training examples is added to the training examples.
Clause 7 is the method of clause 6, wherein training the first machine learning model produces a gradient, the method further comprising propagating the gradient to a plurality of digital component embeddings, wherein the digital component embeddings represent features of the co-recommended digital components.
Clause 8 is the method of clauses 1-7, wherein the first machine learning model process comprises marginalized embedded input representing marginal contribution of the first feature relative to the contribution of the second feature, and the second machine learning model process comprises a plurality of digital component embedded input.
Clause 9 is the method of clause 8, wherein the first machine learning model is a neural network, and the output of at least one layer of the neural network is used to train the second machine learning model.
Clause 10 is the method of clauses 1-9, wherein the second machine learning model is a neural network comprising a partially hidden layer or a fully hidden layer configured to generate a third score associated with the first hidden digital component based on an input associated with the at least one second digital component.
Clause 11 is the method of clause 10, wherein the third score is a direct loss, a ranking loss, or a similarity score, and is used as an input to generate a predictive score for the second model.
While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in a particular order in the claims, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (15)
1. A computer-implemented method, comprising:
receiving a digital component request;
providing first input data as input to a first machine learning model, the first input data comprising a feature value of a feature of each digital component in a set of digital components, wherein the first machine learning model is trained to output, for each digital component, a score indicative of a likelihood of a positive result for the digital component;
processing the first input data using a first machine learning model;
receiving respective scores of digital components in the set of digital components as a first output of a first machine learning model;
providing second input data as input to a second machine learning model, the second input data including feature values of features of each digital component in a subset of digital components selected based on respective scores of the digital components in the set of digital components, wherein the second machine learning model is trained to output a ranking of digital components based at least in part on the feature values of features of the digital components to be provided together as a recommendation;
Processing the second input data using a second machine learning model;
receiving a ranking of the digital components in the subset of digital components as a second output of a second machine learning model; and
at least one digital component in the subset of digital components is provided based on a second ranking.
2. The computer-implemented method of claim 1, wherein the second machine learning model is the same machine learning model as the first machine learning model.
3. The computer-implemented method of claim 1, wherein the second machine learning model is a different machine learning model than the first machine learning model, and wherein the second machine learning model has been trained differently than the first machine learning model.
4. The method of claim 3, wherein when processing the same input as the first machine learning model, the second machine learning model executes fewer instructions to process the same input than the first machine learning model.
5. The method of any preceding claim, wherein the second machine learning model is trained on training examples comprising features of a set of commonly recommended digital components that have been provided together as recommendations.
6. The method of claim 5, further comprising:
selecting a first plurality of training examples from training examples including commonly recommended digital components;
modifying one or more features of the first plurality of training examples, wherein modifying features of the one or more features includes removing information about the co-recommended item; and
a first plurality of training examples is added to the training examples.
7. The method of claim 6, wherein training the first machine learning model produces a gradient, the method further comprising propagating the gradient to a plurality of digital component embeddings, wherein the digital component embeddings represent features of commonly recommended digital components.
8. The method of any preceding claim, wherein the first machine learning model process comprises an marginalized embedded input, the marginalized embedded input representing a marginal contribution of the first feature relative to the contribution of the second feature, and the second machine learning model process comprises a plurality of digital component embedded inputs.
9. The method of claim 8, wherein the first machine learning model is a neural network and an output of at least one layer of the neural network is used to train the second machine learning model.
10. The method of any preceding claim, wherein the second machine learning model is a neural network comprising a partially or fully hidden layer configured to generate a third score associated with the first hidden digital component based on an input associated with the at least one second digital component.
11. The method of claim 10, wherein the third score is a direct loss, a ranking loss, or a similarity score and is used as an input to generate a predictive score for the second model.
12. A method according to any preceding claim, wherein a positive result indicates that a user is interacting with or is likely to interact with the digital component when displayed on the device.
13. The method of any preceding claim, wherein the recommendation is a recommendation of a digital component to be displayed on the device.
14. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising the method of any one of claims 1-13.
15. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising the method of any one of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IL288917A IL288917A (en) | 2021-12-12 | 2021-12-12 | Machine learning rank and prediction calibration |
IL288917 | 2021-12-12 | ||
PCT/US2022/044482 WO2023107180A1 (en) | 2021-12-12 | 2022-09-23 | Machine learning rank and prediction calibration |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116802644A true CN116802644A (en) | 2023-09-22 |
Family
ID=83899659
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280011998.8A Pending CN116802644A (en) | 2021-12-12 | 2022-09-23 | Machine learning ranking and predictive calibration |
Country Status (4)
Country | Link |
---|---|
EP (1) | EP4268137A1 (en) |
CN (1) | CN116802644A (en) |
IL (1) | IL288917A (en) |
WO (1) | WO2023107180A1 (en) |
-
2021
- 2021-12-12 IL IL288917A patent/IL288917A/en unknown
-
2022
- 2022-09-23 CN CN202280011998.8A patent/CN116802644A/en active Pending
- 2022-09-23 WO PCT/US2022/044482 patent/WO2023107180A1/en active Application Filing
- 2022-09-23 EP EP22792983.3A patent/EP4268137A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
IL288917A (en) | 2023-07-01 |
WO2023107180A1 (en) | 2023-06-15 |
EP4268137A1 (en) | 2023-11-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210326674A1 (en) | Content recommendation method and apparatus, device, and storage medium | |
KR102240662B1 (en) | Wide and deep machine learning models | |
US20220043810A1 (en) | Reinforcement learning techniques to improve searching and/or to conserve computational and network resources | |
AU2014201827B2 (en) | Scoring concept terms using a deep network | |
RU2725659C2 (en) | Method and system for evaluating data on user-element interactions | |
US11790233B2 (en) | Generating larger neural networks | |
JP2022031624A (en) | Content recommendation method, apparatus, electronic device, storage medium, and computer program | |
CN112313697A (en) | System and method for generating interpretable description-based recommendations describing angle augmentation | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
CN111652378B (en) | Learning to select vocabulary for category features | |
WO2019101836A1 (en) | Population based training of neural networks | |
US11875116B2 (en) | Machine learning models with improved semantic awareness | |
CN115841366A (en) | Article recommendation model training method and device, electronic equipment and storage medium | |
CN112269943B (en) | Information recommendation system and method | |
WO2024051707A1 (en) | Recommendation model training method and apparatus, and resource recommendation method and apparatus | |
JP2023533723A (en) | Evaluate interpretation of search queries | |
CN112989174A (en) | Information recommendation method and device, medium and equipment | |
US11887155B2 (en) | Method and a system for selecting a targeted message to be included within a web resource | |
CN116802644A (en) | Machine learning ranking and predictive calibration | |
CN111340605B (en) | Method and device for training user behavior prediction model and user behavior prediction | |
CN114580652A (en) | Method and system for item recommendation applied to automatic artificial intelligence | |
CN115244941A (en) | User interface for improved video packaging | |
CN116720003B (en) | Ordering processing method, ordering processing device, computer equipment and storage medium | |
JP7223164B2 (en) | Data integrity optimization | |
US20240028935A1 (en) | Context-aware prediction and recommendation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |