JP2023165769A - Video timed anchor - Google Patents
Video timed anchor Download PDFInfo
- Publication number
- JP2023165769A JP2023165769A JP2023148643A JP2023148643A JP2023165769A JP 2023165769 A JP2023165769 A JP 2023165769A JP 2023148643 A JP2023148643 A JP 2023148643A JP 2023148643 A JP2023148643 A JP 2023148643A JP 2023165769 A JP2023165769 A JP 2023165769A
- Authority
- JP
- Japan
- Prior art keywords
- video
- anchor
- appropriate subset
- frame
- videos
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 45
- 238000012545 processing Methods 0.000 claims description 32
- 238000011524 similarity measure Methods 0.000 claims description 22
- 230000000007 visual effect Effects 0.000 claims description 20
- 230000004044 response Effects 0.000 claims description 16
- 230000009471 action Effects 0.000 claims description 14
- 238000004891 communication Methods 0.000 claims description 7
- 238000004590 computer program Methods 0.000 abstract description 13
- 230000008569 process Effects 0.000 description 25
- 238000010586 diagram Methods 0.000 description 9
- 230000008859 change Effects 0.000 description 7
- 239000004615 ingredient Substances 0.000 description 7
- 230000003993 interaction Effects 0.000 description 4
- 238000010801 machine learning Methods 0.000 description 4
- 239000000843 powder Substances 0.000 description 4
- 230000002123 temporal effect Effects 0.000 description 4
- 238000013528 artificial neural network Methods 0.000 description 3
- 239000000203 mixture Substances 0.000 description 3
- 239000000463 material Substances 0.000 description 2
- 238000012015 optical character recognition Methods 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 239000012254 powdered material Substances 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 238000005201 scrubbing Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/34—Indicating arrangements
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/70—Labelling scene content, e.g. deriving syntactic or semantic representations
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/102—Programmed access in sequence to addressed parts of tracks of operating record carriers
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
- G11B27/32—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording on separate auxiliary tracks of the same or an auxiliary record carrier
Abstract
Description
本明細書は、ビデオ処理に関する。 TECHNICAL FIELD This specification relates to video processing.
ビデオは、ウェブドキュメントと同様にしてざっと目を通すことができず、ユーザがビデオにおいて何か特定のものを探しているとき、ビデオを視聴すること、またはビデオを手動でスクラブすることが、ビデオ中のキーモーメント(key moment)をユーザが見つける結果にならないことがよくある。 Videos cannot be skimmed in the same way as web documents, and when a user is looking for something specific in a video, watching the video or manually scrubbing the video This often results in users not finding the key moments inside.
本開示は、ビデオ用のビデオアンカーの作成および配布を容易にするコンピュータ実装方法およびシステムに関する。 The present disclosure relates to computer-implemented methods and systems that facilitate the creation and distribution of video anchors for videos.
一般に、本明細書で説明する主題の1つの新規の態様が、ビデオに対して、複数のキーモーメント識別子を取得するアクションであって、各キーモーメント識別子が、ビデオにおける再生時間を指定する時間インデックス値を含み、ビデオ内の顕著なトピックを定義する1つまたは複数の関心基準(interest criteria)を満たすと決定されたビデオの主題を示す、取得するアクションと、各キーモーメント識別子に対して、時間インデックス値によって指定された再生時間に始まるビデオの適切なサブセットを選択するアクションであって、ビデオの適切なサブセットが、時間インデックス値によって指定される再生時間に始まり、別のキーモーメント識別子の別の時間インデックス値によって指定される次の最も近い再生時間に終わるビデオセグメントの長さよりも短い、選択するアクションと、ビデオの適切なサブセットについて、キーモーメント識別子のテキストのラベルを決定するアクションと、ビデオの適切なサブセットからビデオフレームを選択するかどうかを決定するためにビデオの適切なサブセットの各ビデオフレームを処理するアクションと、各キーモーメント識別子に対して、ビデオアンカーを生成するアクションであって、ビデオアンカーが、キーモーメント識別子用のテキストのラベルと、ビデオの適切なサブセットのビデオフレームを選択する決定に応じて、ビデオフレームから生成された画像と、ユーザデバイス上のビデオプレーヤに、キーモーメント識別子の時間インデックス値によって指定された再生時間のビデオの再生を開始させる命令とを含む、生成するアクションと、ユーザデバイスにデータを提供するアクションであって、データがユーザデバイスのビデオプレーヤ環境でユーザデバイスに、ビデオアンカーの各々と、各ビデオアンカーに対して、ビデオプレーヤのプログレスバー内の時間インジケータであって、時間インデックス値によって指定された再生時間に対応する時間インジケータと、各ビデオアンカーに対して、対応する時間インジケータからビデオアンカーへのビジュアルリンクとをレンダリングさせる、提供するアクションとを含み、各ビデオアンカーが、ユーザによって選択可能であり、ビデオアンカーが選択されると、ビデオアンカーの命令が、ユーザデバイス上のビデオプレーヤに、時間インデックス値によって指定された再生時間のビデオの再生を開始させる、方法において具体化され得る。この態様の他の実施形態は、対応するシステム、装置、および、方法のアクションを行うように構成され、コンピュータストレージデバイス上に符号化されたコンピュータプログラムを含む。 In general, one novel aspect of the subject matter described herein is an action for obtaining a plurality of key moment identifiers for a video, wherein each key moment identifier is a temporal index specifying a playing time in the video. For each key moment identifier, the action to retrieve and the time An action that selects an appropriate subset of videos starting at a playback time specified by an index value, the appropriate subset of videos starting at a playback time specified by a time index value, and selecting another of the videos with a different key moment identifier. an action to select a video segment that is shorter than the length of the video segment ending at the next closest playback time specified by the time index value; an action that processes each video frame of the appropriate subset of the video to determine whether to select the video frame from the appropriate subset; and an action that generates a video anchor for each key moment identifier, the video Depending on the anchor's decision to select the video frames of the appropriate subset of the video with a textual label for the key moment identifier, the image generated from the video frame and the video player on the user device is provided with the key moment identifier. an instruction to start playing a video for a playback time specified by a time index value; , and for each video anchor, a time indicator in the progress bar of the video player that corresponds to the playback time specified by the time index value; a visual link from the corresponding time indicator to the video anchor; and an action to provide that causes the video anchor to be rendered, each video anchor being selectable by the user, and when the video anchor is selected, the instructions for the video anchor are The method may be embodied in a method of causing a video player on a device to begin playing a video for a playback time specified by a time index value. Other embodiments of this aspect include computer programs encoded on a computer storage device and configured to perform the actions of the corresponding systems, apparatus, and methods.
本明細書で説明される主題の特定の実施形態は、以下の利点のうちの1つまたは複数を実現するために実装され得る。「ビデオアンカー」と呼ばれるビデオ時間調節アンカー(video timed anchor)は、再生環境の作用の仕方を変える。詳細には、ビデオアンカーは、ユーザがビデオ中のキーモーメントを迅速に確認することを可能にし、ビデオ自体のより優れた認識をユーザに与える。ビデオ時間調節アンカーはまた、ユーザがビデオ中のあるポイントまで直ちにスキップすることを可能にし、ユーザの時間を節約する。 Particular embodiments of the subject matter described herein may be implemented to achieve one or more of the following advantages. Video timed anchors, referred to as "video anchors," change the way the playback environment behaves. In particular, video anchors allow users to quickly see key moments in a video, giving them a better awareness of the video itself. Video timing anchors also allow the user to immediately skip to a point in the video, saving the user time.
処理システムは、ビデオ内の定義された顕著なトピックにかなうと決定されたビデオの主題を示す関心基準を使用する。様々な関心基準は、様々なビデオタイプに調整され得る。たとえば、スポーツビデオは、得点、ブロック、およびファウルに対して示されたイベントをシステムが認識することを可能にする特定の関心基準に従って処理されてもよく、講義ビデオは、システムが主題またはトピック変更を決定することを可能にする言語基準に従って処理されてもよく、「リスト」を含み、その上リストに含まれる要素を説明するビデオは、システムが示されたリストの要素を認識し、次いでリストに記載された要素の1つからリストに記載された別の要素へ主題が変わる、ビデオ中の瞬間を識別することを可能にするリスト基準に従って処理されてもよい。言い換えれば、システムは、異なるタイプの関心基準を組み込むことによって、多くの異なるタイプのビデオを処理し、ビデオ内の複数の顕著なトピックにビデオアンカーを生成する柔軟性を可能にする。 The processing system uses interest criteria to indicate the subject matter of the video that is determined to fit a defined salient topic within the video. Different interest criteria may be adjusted to different video types. For example, sports videos may be processed according to specific interest criteria that allow the system to recognize events indicated for scores, blocks, and fouls, and lecture videos may be processed according to specific interest criteria that allow the system to recognize events indicated for points, blocks, and fouls, and lecture videos may be A video containing a "list" and also explaining the elements contained in the list may be processed according to linguistic criteria that allow the system to recognize the elements of the indicated list and then may be processed according to a list criterion that makes it possible to identify moments in the video where the subject changes from one of the elements described in the list to another element described in the list. In other words, the system allows the flexibility to process many different types of videos and generate video anchors for multiple salient topics within the video by incorporating different types of interest criteria.
システムは、1つまたは複数のビデオフレーム含有基準に基づいて、ビデオアンカーにビデオフレームを含むかどうかを決定することができる。各ビデオアンカーは、画面のスペースが限られているので、ビデオアンカーにビデオフレームを含むかどうかの決定は、各ビデオアンカーに対して表示されるデータが他の各ビデオアンカーとは区別を生じることを確実にする。言い換えれば、アンカーが対応する顕著なトピックの情報を与えないビデオフレームは、ビデオアンカーに含まれない。たとえば、ビデオが講義のビデオである場合、各ビデオアンカーに対する話者の画像は情報を与えない。したがって、ビデオフレームを使用しないことによって、より記述的なテキストのラベルが使用されてもよく、各テキストのラベルは、話者が論じている主題を説明する。 The system can determine whether to include a video frame in the video anchor based on one or more video frame inclusion criteria. Because each video anchor has limited space on the screen, the decision to include video frames in a video anchor is important because the data displayed for each video anchor is distinct from each other. ensure that In other words, video frames that do not give information of the salient topic to which the anchor corresponds are not included in the video anchor. For example, if the video is a lecture video, the image of the speaker for each video anchor provides no information. Therefore, by not using video frames, more descriptive text labels may be used, each text label explaining the subject matter being discussed by the speaker.
ビデオアンカーはビデオの顕著なトピックを示すので、ユーザは、ビデオ全体を流す代わりに、ビデオ中のいくつかのポイントで再生を開始するためにビデオアンカーを選択する可能性が高い。これは、ネットワーク帯域幅ストリーミング使用量を減らし、ネットワークリソースを節約する。さらに、クライアント側では、デコーディングおよびレンダリングなどの使用ビデオ処理計算リソースは、同様に縮小される。 Because the video anchor indicates the salient topic of the video, the user is likely to select the video anchor to start playing at some point in the video instead of playing the entire video. This reduces network bandwidth streaming usage and saves network resources. Additionally, on the client side, the video processing computational resources used, such as decoding and rendering, are similarly reduced.
本明細書で説明される主題の1つまたは複数の実施形態の詳細は、添付の図面および以下の説明に記載される。主題の他の特徴、態様、および利点は、説明、図面、および特許請求の範囲から明らかとなろう。 The details of one or more embodiments of the subject matter described herein are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will be apparent from the description, drawings, and claims.
様々な図面における同じ参照番号および名称は、同じ要素を示す。 The same reference numbers and designations in the various drawings indicate the same elements.
本出願の主題は、ビデオ時間調節アンカーの使用によって、ビデオの異なる部分を見えるようにする。ビデオアンカーに対応するビデオの各部分が、「キーモーメント」に始まる。ビデオアンカーは、ユーザがビデオ中の重要なポイントを素早く確認することを可能にし、ビデオ自体のより優れた理解をユーザに与え、またユーザがビデオ中のあるポイントまで直ちにスキップすることを可能にし、ユーザの時間を節約する。 The subject matter of this application makes different parts of a video visible through the use of video timing anchors. Each portion of the video that corresponds to a video anchor begins with a "key moment." Video anchors allow the user to quickly see important points in the video, give the user a better understanding of the video itself, and also allow the user to immediately skip to a point in the video, Save user time.
ビデオ時間調節アンカー処理システムが、ビデオの各々にビデオアンカーを生成するようにビデオを処理する。動作時、システムは、ビデオに対して、複数のキーモーメント識別子を取得する。キーモーメント識別子は、訓練済みニューラルネットワークによるなど、アルゴリズム的に決定されてもよく、または人間のキュレータによって提供されてもよい。各キーモーメント識別子は、ビデオにおける再生時間を指定する時間インデックス値を含み、ビデオ内の顕著なトピックを定義する1つまたは複数の関心基準を満たすと決定されたビデオの主題を示している。 A video timing anchor processing system processes the videos to generate video anchors for each of the videos. In operation, the system obtains multiple key moment identifiers for the video. Key moment identifiers may be determined algorithmically, such as by a trained neural network, or may be provided by a human curator. Each key moment identifier includes a time index value that specifies a playback time in the video and is indicative of the subject matter of the video that is determined to meet one or more criteria of interest that define salient topics within the video.
各キーモーメント識別子に対して、システムは、時間インデックス値によって指定された再生時間に始まるビデオの適切なサブセットを選択する。ビデオの適切なサブセットは、時間インデックス値によって指定された再生時間に始まり、別のキーモーメント識別子の別の時間インデックス値によって指定された次の最も近い再生時間に終わる、ビデオセグメントの長さよりも短いビデオの一部分である。たとえば、最初のキーモーメント識別子が1:00の再生時間を示し、次のキーモーメント識別子が2:30の再生時間を示す場合、ビデオの適切なサブセットは、1:00に始まり、2:30前に終わる。 For each key moment identifier, the system selects the appropriate subset of the video starting at the playback time specified by the temporal index value. A suitable subset of the video is shorter than the length of the video segment, starting at the playback time specified by the time index value and ending at the next closest playback time specified by another time index value for another key moment identifier. It's part of the video. For example, if the first key moment identifier indicates a playback time of 1:00 and the second key moment identifier indicates a playback time of 2:30, the appropriate subset of the video would start at 1:00 and before 2:30. It ends in
システムは、ビデオの適切なサブセットについて、キーモーメント識別子用のテキストのラベルを決定する。テキストのラベルは、テキストの信号、視覚信号、および手作業のキュレーションのうちの1つまたは複数によって決定され得る。テキストの信号は、光学文字認識、キャプションデータ、およびビデオメタデータを含む。視覚信号は、埋込み、オーディオ、および画像ラベルの生成を含む。手作業のキュレーションは、手作業で生成されたアノテーションを含む。 The system determines textual labels for key moment identifiers for the appropriate subset of videos. Text labels may be determined by one or more of textual signals, visual signals, and manual curation. The text signal includes optical character recognition, caption data, and video metadata. Visual signals include embedding, audio, and image label generation. Manual curation includes manually generated annotations.
システムはまた、ビデオの適切なサブセットからビデオフレームを選択するかどうかを決定するために、ビデオの適切なサブセットの各ビデオフレームを処理し、次いで、各キーモーメント識別子に対して、ビデオアンカーを生成する。各ビデオアンカーは、キーモーメント識別子用のテキストのラベルを含み、ビデオフレームが選択された場合は、ビデオフレームを含む。各ビデオアンカーはまた、ユーザデバイス上のビデオプレーヤに、キーモーメント識別子の時間インデックス値によって指定された再生時間のビデオの再生を開始させる命令を含む。 The system also processes each video frame of the appropriate subset of the video to determine whether to select the video frame from the appropriate subset of the video, and then generates a video anchor for each key moment identifier. do. Each video anchor includes a textual label for a key moment identifier and, if a video frame is selected, a video frame. Each video anchor also includes instructions that cause a video player on the user device to begin playing the video for a playback time specified by the key moment identifier's time index value.
ビデオアンカーを定義するデータは、次いでインデックスに記憶され、データが対応するビデオに関連付けられる。データはユーザデバイスに、ユーザデバイスのビデオプレーヤ環境において、ビデオアンカーの各々をレンダリングさせる。データは次いで、ビデオを要求するユーザデバイスに、ビデオ自体と一緒に供給され得る。システムはユーザデバイスに、ビデオ要求に応じてデータを提供することができる。各ビデオアンカーに対して、ユーザデバイスは、ビデオプレーヤのプログレスバー内の対応する時間インジケータ、および対応する時間インジケータからビジュアルアンカーへのビジュアルリンクを表示する。表示される各ビデオアンカーは、ユーザによって選択可能であり、ビデオアンカーが選択されると、ビデオアンカーの命令が、ユーザデバイス上のビデオプレーヤに、時間インデックス値によって指定された再生時間のビデオの再生を開始させる。 The data defining the video anchor is then stored in an index so that the data is associated with the corresponding video. The data causes the user device to render each of the video anchors in the user device's video player environment. The data may then be provided along with the video itself to a user device requesting the video. The system can provide data to user devices in response to video requests. For each video anchor, the user device displays a corresponding time indicator in a progress bar of the video player and a visual link from the corresponding time indicator to the visual anchor. Each displayed video anchor is selectable by the user, and when a video anchor is selected, the video anchor's instructions cause the video player on the user device to play the video for the playback time specified by the time index value. start.
これらの特徴および追加の特徴について、以下でより詳細に説明する。 These and additional features are described in more detail below.
図1は、ビデオアンカー120、130、および140が表示された、第1のビデオ表示環境100の図である。例示的な環境100は、スマートフォン、タブレット、またはパーソナルコンピュータ上に実装されてもよい。スマートテレビジョンなど、他のコンピュータ実装デバイスもまた、表示環境100を実装するのに使用されてもよい。 FIG. 1 is a diagram of a first video display environment 100 in which video anchors 120, 130, and 140 are displayed. Exemplary environment 100 may be implemented on a smartphone, tablet, or personal computer. Other computer-implemented devices may also be used to implement display environment 100, such as smart televisions.
図1の例示的な環境100では、検索クエリ[Buy a smartphone(スマートフォンを買う)]が、検索入力フィールド102の使用によって検索エンジンに提供された。結果のビデオが、結果環境104に表示される。トップランクの結果ビデオが、ビデオプレーヤウィンドウ110に表示される。ビデオの第1のフレームが表示され、プログレスバー112が、ビデオの時間の長さを示す。 In the example environment 100 of FIG. 1, the search query "Buy a smartphone" was provided to the search engine through the use of the search input field 102. The resulting video is displayed in the results environment 104. The top-ranked result videos are displayed in video player window 110. The first frame of the video is displayed and a progress bar 112 indicates the length of the video.
ビデオプレーヤウィンドウ110の下に、3つのビデオアンカー120、130、および140がある。各ビデオアンカー120、130、および140は、ビデオプレーヤのプログレスバー112に、対応する時間インジケータ122、132、および142を有する。各時間インジケータは、ビデオアンカーに対して時間インデックス値によって指定された再生時間に対応する。さらに、各ビデオアンカー120、130、および140は、対応する時間インジケータ122、132、および142からビデオアンカーへのビジュアルリンクを含む。 Below video player window 110 there are three video anchors 120, 130, and 140. Each video anchor 120, 130, and 140 has a corresponding time indicator 122, 132, and 142 on the video player's progress bar 112. Each time indicator corresponds to a playback time specified by a time index value for the video anchor. Additionally, each video anchor 120, 130, and 140 includes a visual link from the corresponding time indicator 122, 132, and 142 to the video anchor.
各ビデオアンカー120、130、および140は、それぞれビデオフレーム124、134、および144を含む。各ビデオフレームは、ビデオ中の対応する再生時間に、またはその後に出現するビデオの一部分から選択される。画像フレームの識別方法および選択方法について、以下でより詳細に説明する。 Each video anchor 120, 130, and 140 includes video frames 124, 134, and 144, respectively. Each video frame is selected from a portion of the video that occurs at or after a corresponding playback time in the video. The method of identifying and selecting image frames will be described in more detail below.
各ビデオアンカー120、130、および140はまた、それぞれビデオ中の顕著なトピックを説明するテキストのラベル126、136、および146をそれぞれ含む。いくつかの実施形態では、各顕著なトピックは、それが新しいトピックまたはビデオのトピックの著しい変化であるとき、識別される。顕著なトピックの識別方法について、以下でより詳細に説明する。 Each video anchor 120, 130, and 140 also includes a textual label 126, 136, and 146, respectively, each describing a salient topic in the video. In some embodiments, each salient topic is identified when it is a new topic or a significant change in the topic of the video. The method for identifying salient topics is described in more detail below.
ユーザデバイス上のビデオプレーヤに、時間インデックス値によって指定された再生時間のビデオの再生を開始させるそれぞれの命令が、各ビデオアンカー120、130、および140に埋め込まれる。命令は、ビデオアンカーを選択すると実行される。たとえば、ユーザがビデオアンカー130を選択する場合、ビデオアンカー130およびプログレスバー112に示されるように、2:13の再生時間に、ビデオプレーヤウィンドウ110内のビデオの再生が始まる。 Embedded in each video anchor 120, 130, and 140 is a respective instruction that causes a video player on the user device to begin playing the video for the playback time specified by the time index value. The instructions are executed upon selection of the video anchor. For example, if the user selects video anchor 130, the video in video player window 110 begins to play at a playback time of 2:13, as indicated by video anchor 130 and progress bar 112.
ビデオアンカー120、130、および140の下に、さらなるビデオ検索結果150、152、154、および156がある。いくつかの実施形態では、別のビデオ検索結果を選択すると、ビデオ検索結果によって参照されるビデオをビデオプレーヤウィンドウ110に置くことによって、ビデオ検索結果に焦点を当てられる。さらに、新しく焦点を当てられたビデオが対応するビデオアンカーを有する場合、ビデオアンカー120、130、および140は、新しく焦点を当てられたビデオに対応するビデオアンカーに置き換えられる。いくつかの実装形態では、ビデオアンカーは、各ビデオ検索結果とともに供給され、検索システムへのその後の要求を減らすためにユーザデバイスにキャッシュされる。 Below video anchors 120, 130, and 140 are additional video search results 150, 152, 154, and 156. In some embodiments, selecting another video search result brings the video search result into focus by placing the video referenced by the video search result in the video player window 110. Additionally, if the newly focused video has a corresponding video anchor, video anchors 120, 130, and 140 are replaced with the video anchor corresponding to the newly focused video. In some implementations, a video anchor is provided with each video search result and cached on the user device to reduce subsequent requests to the search system.
3つのビデオアンカーのみが示されているが、他の実装形態ではより多くのビデオアンカーが示される場合がある。さらに、より多くのビデオアンカーが、プログレスバー112内の対応するさらなる時間インジケータによって示される場合があり、ビデオアンカーへのアクセスが、ジェスチャー入力によって実現されてもよく、たとえば、ビデオアンカー140の場所に次のビデオアンカーを導入することによってさらなるビデオアンカーを「スクロール」し、ビデオアンカー140をビデオアンカー130の位置にシフトし、同様にビデオアンカー130をビデオアンカー120の位置にシフトするために、右から左にスワイプすることによって実現されてもよい。また第1のビデオアンカー120は、ディスプレイから削除される。さらなるビデオアンカーにアクセスするために、何らかの他の適切な対話モデルが使用される場合もある。 Although only three video anchors are shown, other implementations may show more video anchors. Additionally, more video anchors may be indicated by corresponding further time indicators in the progress bar 112, and access to the video anchors may be achieved by gesture input, e.g., at the location of the video anchor 140. From the right to "scroll" further video anchors by introducing the next video anchor, shifting video anchor 140 to the position of video anchor 130, and likewise shifting video anchor 130 to the position of video anchor 120. This may be accomplished by swiping to the left. The first video anchor 120 is also removed from the display. Any other suitable interaction model may also be used to access additional video anchors.
いくつかの実装形態では、システムは、1つまたは複数のビデオフレーム含有基準に基づいて、ビデオアンカーにビデオフレームの画像を含むかどうかを決定することができる。各ビデオアンカーは、画面のスペースが限られているので、ビデオアンカーにビデオフレームから生成された画像を含むかどうかの決定は、各ビデオアンカーに対して表示されるデータが他の各ビデオアンカーとは区別を生じることを確実にする。言い換えれば、ビデオアンカーが対応する顕著なトピックの情報を与えないビデオフレームは、いくつかの実装形態では、ビデオアンカーから省略され得る。たとえば、ビデオが講義のビデオであり、話者のビデオしかない場合、各ビデオアンカーに対する話者の画像は情報を与えない。したがって、ビデオフレームを使用しないことによって、より記述的なテキストのラベルが使用されてもよく、各テキストのラベルが、話者が論じている主題を説明する。 In some implementations, the system may determine whether to include an image of a video frame in a video anchor based on one or more video frame inclusion criteria. Because each video anchor has limited space on the screen, the decision whether to include images generated from video frames in a video anchor is important because the data displayed for each video anchor is different from each other. ensures that a distinction occurs. In other words, video frames to which the video anchor does not give information of the corresponding salient topic may be omitted from the video anchor in some implementations. For example, if the video is a lecture video and there is only a video of the speaker, the image of the speaker for each video anchor does not give any information. Therefore, by not using video frames, more descriptive text labels may be used, with each text label explaining the subject matter being discussed by the speaker.
いくつかの実装形態では、選択されたビデオフレームから生成された画像は、ビデオフレームのサムネイルである。本明細書で使用する、ビデオフレームの「サムネイル」は、サムネイルが示す実際のビデオフレームよりも寸法的に小さいビデオフレームの任意の画像である。他の実装形態では、画像は、ビデオフレームの切り取られた部分であってもよく、たとえば、キーモーメント識別子に決定された顕著なトピックに最も関連する物体を含むビデオフレームの一部分であってもよい。任意の適切な物体検出プロセスは、ビデオフレームにおいて決定された物体を検出し、識別するために使用され得る。 In some implementations, the image generated from the selected video frame is a thumbnail of the video frame. As used herein, a "thumbnail" of a video frame is any image of a video frame that is dimensionally smaller than the actual video frame that the thumbnail represents. In other implementations, the image may be a cropped portion of a video frame, e.g., a portion of a video frame containing objects most related to salient topics determined by key moment identifiers. . Any suitable object detection process may be used to detect and identify the determined objects in the video frames.
「テキストのみ」のビデオアンカーの一例が、図2に示され、図2は、ビデオアンカーが表示される別のビデオ表示環境200の図である。表示環境200は、たとえば、ビデオ講義用のビデオプレーヤであってもよい。ビデオプレーヤウィンドウ202の下に、3つのビデオアンカー210、220、および230がある。各ビデオアンカー210、220、および230は、ビデオプレーヤのプログレスバー204に、対応する時間インジケータ212、222、および232を有する。各時間インジケータは、ビデオアンカーに対して時間インデックス値によって指定された再生時間に対応する。さらに、各ビデオアンカー210、220、および230は、対応する時間インジケータ212、222、および232からビデオアンカーへのビジュアルリンクを含む。 An example of a "text only" video anchor is shown in FIG. 2, which is a diagram of another video display environment 200 in which the video anchor is displayed. Display environment 200 may be, for example, a video player for video lectures. Below the video player window 202 there are three video anchors 210, 220, and 230. Each video anchor 210, 220, and 230 has a corresponding time indicator 212, 222, and 232 on the video player's progress bar 204. Each time indicator corresponds to a playback time specified by a time index value for the video anchor. Additionally, each video anchor 210, 220, and 230 includes a visual link from the corresponding time indicator 212, 222, and 232 to the video anchor.
各ビデオアンカー210、220、および230はまた、それぞれビデオ中の顕著なトピックを説明するテキストのラベル214、224、および234をそれぞれ含む。いくつかの実装形態では、ビデオフレームが含まれないとき、テキストのラベルは、ビデオフレームが含まれるときよりも記述的である。たとえば、ビデオフレームが含まれるとき、テキストのラベルは6語に限定され得るが、ビデオフレームが含まれないとき、テキストのラベルはより高い語数の制限、たとえば、15語であってもよい。当然、他の語数制限が使用される場合もある。 Each video anchor 210, 220, and 230 also includes a textual label 214, 224, and 234, respectively, each describing a salient topic in the video. In some implementations, the textual label is more descriptive when video frames are not included than when video frames are included. For example, when a video frame is included, the text label may be limited to 6 words, but when no video frame is included, the text label may be a higher word limit, eg, 15 words. Of course, other word limits may be used.
図2はまた、ビデオアンカーが、ビデオ検索結果が提供される環境以外の環境で使用され得ることを示している。詳細には、ビデオアンカーは、どんなビデオ再生環境でも提供され得る。 Figure 2 also shows that video anchors can be used in environments other than those in which video search results are provided. In particular, video anchors may be provided in any video playback environment.
図3は、ビデオ用のビデオアンカーを生成するシステム300のブロック図である。図3のアーキテクチャは、一例のアーキテクチャにすぎず、他のアーキテクチャもまた使用され得る。システム300の動作について、ビデオアンカーを生成するための例示的なプロセス400の流れ図である図4を参照しながら説明する。 FIG. 3 is a block diagram of a system 300 for generating video anchors for videos. The architecture of FIG. 3 is only one example architecture; other architectures may also be used. Operation of system 300 will be described with reference to FIG. 4, which is a flow diagram of an exemplary process 400 for generating a video anchor.
プロセス400は、ビデオ302に対して、キーモーメント識別子303を取得する(402)。各キーモーメント識別子303は、ビデオにおける再生時間を指定する時間インデックス値を含み、ビデオ302内の顕著なトピックを定義する関心基準312を満たすと決定されたビデオ320の主題を示している。ビデオ内の顕著なトピックを定義する関心基準312は、ビデオタイプによって異なる場合がある。たとえば、スポーツビデオの関心基準は、ゴールの得点、ボール保持(possession)の変化、ファウル、およびユーザに特に関心があると決定され得る他の何らかの事象を含むように、顕著なトピックを定義してもよい。別の例として、複数の異なるアイテムを連続して描くビデオの関心基準が、1つの主題アイテムから別の主題アイテムへの焦点変化(change focus)として、顕著なトピックを定義してもよく、たとえば、図1に示すように、キーモーメントが、それぞれのスマートフォンの各レビューの開始セグメントに発生する。また別の例は、主に話をするビデオ、たとえば講義であるビデオに対するものである。これらのビデオでは、キーモーメントは、講師があるトピックから次のトピックに移るときであってもよい。さらにまた別の例は、教育ビデオである。これらのビデオでは、関心基準は、各ステップまたは命令の始まりがキーモーメントであると指定してもよい。 Process 400 obtains key moment identifier 303 for video 302 (402). Each key moment identifier 303 includes a time index value that specifies the playback time in the video and is indicative of the subject matter of the video 320 that has been determined to meet the interest criteria 312 that define salient topics within the video 302. Interest criteria 312 that define salient topics within a video may vary depending on video type. For example, interest criteria for sports videos may define salient topics to include goals scored, changes in possession, fouls, and any other events that may be determined to be of particular interest to the user. Good too. As another example, the interest criteria for a video depicting multiple different items in succession may define a salient topic as a change focus from one subject item to another, e.g. ,As shown in Figure 1, a key moment occurs at the start,segment of each review for each smartphone. Yet another example is for videos that primarily talk, such as videos that are lectures. In these videos, key moments may be when the instructor moves from one topic to the next. Yet another example is educational videos. In these videos, the interest criteria may specify that the beginning of each step or instruction is a key moment.
キーモーメント識別子は、関心基準、または関心基準を参照する決定論的プロセスについて訓練された訓練済みニューラルネットワークによるなど、アルゴリズム的に決定されてもよく、またはビデオとともに人間のキュレータによって提供されてもよい。たとえば、テキスト信号306、視覚信号308、および手作業のキュレーション310が、キーモーメントを取得するために使用されてもよい。テキスト信号306に関しては、光学文字認識が、時間とともにビデオの主題を決定するためにビデオフレームで使用されてもよく、クローズドキャプションデータもまた、時間とともにビデオの主題、ならびにメタデータを決定するために使用されてもよい。機械学習システムが、時間とともにビデオの支配的意図を決定するために訓練されてもよく、支配的意図が変わるとき、変化がキーモーメントとして識別されてもよい。 The key moment identifier may be determined algorithmically, such as by a trained neural network trained on the criteria of interest, or a deterministic process that references the criteria of interest, or may be provided by a human curator along with the video. . For example, textual signals 306, visual signals 308, and manual curation 310 may be used to obtain key moments. Regarding the text signal 306, optical character recognition may be used on video frames to determine the subject matter of the video over time, and closed caption data may also be used to determine the subject matter of the video over time, as well as metadata. may be used. A machine learning system may be trained to determine the dominant intent of a video over time, and when the dominant intent changes, the change may be identified as a key moment.
視覚信号308はまた、テキスト信号の代わりに、またはテキスト信号に加えて使用されてもよい。たとえば、ニューラルネットワークによる視覚的埋込みが、オーディオ処理と同様に、時間とともにビデオの主題を決定するために使用されてもよい。オーディオ処理に関して、オーディオは、時間とともにビデオの主題を識別するためにテキストに変換されてもよい。この場合も、機械学習システムが、時間とともにビデオの支配的意図を決定するために訓練されてもよく、支配的意図が変わるとき、変化がキーモーメントとして識別されてもよい。 Visual signals 308 may also be used instead of or in addition to text signals. For example, visual embedding with neural networks may be used to determine the subject matter of a video over time, similar to audio processing. Regarding audio processing, audio may be converted to text to identify the subject matter of the video over time. Again, a machine learning system may be trained to determine the dominant intent of the video over time, and when the dominant intent changes, the change may be identified as a key moment.
いくつかのテキスト信号および視覚信号は、それら自体でキーモーメントを示す場合がある。そのような信号は、キーモーメントを意味的に示している、またはキーモーメントを視覚的に示している。たとえば、テキストまたはオーディオの「次のステップ」は、主題として新しい命令を導入することを示している。他のそのような信号は、「…に進む」、「次の章…」などを含む場合がある。そのようなテキストまたはオーディオは、キュレータによって指定されてもよく、または機械学習技法によってビデオのコーパス上で学習されてもよい。ビデオ信号に関して、シーン変化、またはある製品から別の製品への変化が、キーモーメントを示してもよい。同様に、ボールがゴールを通り抜ける画像、またはプレーヤがある行為を行っている(たとえば、ボールを打っている、ゴールラインを通過しているなど)画像は、キーモーメントを示してもよい。そのようなビデオフレームは、キュレータによって指定されてもよく、または機械学習技法によってビデオのコーパス上で学習されてもよい。キーモーメントが識別されると、キーモーメントでビデオにおける再生時間を指定する時間インデックス値が決定される。 Some textual and visual signals may themselves exhibit key moments. Such signals are semantically indicative of key moments or visually indicative of key moments. For example, a text or audio "next step" indicates the introduction of a new command as the subject. Other such signals may include "proceed to...", "next chapter...", etc. Such text or audio may be specified by a curator or learned on a corpus of videos by machine learning techniques. For video signals, a scene change or a change from one product to another may indicate a key moment. Similarly, images of a ball passing through a goal, or images of a player performing an action (eg, hitting the ball, crossing the goal line, etc.) may indicate key moments. Such video frames may be specified by a curator or learned on a corpus of videos by machine learning techniques. Once the key moment is identified, a time index value is determined that specifies the playback time in the video at the key moment.
プロセス400は、各キーモーメント識別子に対して、時間インデックス値によって指定された再生時間に始まるビデオの適切なサブセットを選択する(404)。ビデオの適切なサブセットは、ラベル生成、および対応するビデオアンカーに含めるための画像を生成するためのビデオフレーム選択のために選択される。多くの場合、キーモーメントは数分間隔であってもよく、インジケータ間のビデオ全体の処理は、識別された顕著なトピックからのわずかなトピック逸脱が生じる場合がある。したがって、ビデオの適切なサブセットのみが選択される。ビデオの適切なサブセットは、時間インデックス値によって指定された再生時間に始まり、時間インデックス値によって指定された再生時間に始まって、別のキーモーメントの別の時間インデックス値によって指定された次の最も近い再生時間に終わるビデオセグメントの長さよりも短い。いくつかの実装形態では、ビデオの適切なサブセットは、6秒の長さであってもよい。たとえば、図1を参照すると、第1のキーモーメントのためのビデオの適切なサブセットは、1:12～1:18である。他の時間の長さが使用される場合もある。 For each key moment identifier, process 400 selects (404) an appropriate subset of the video that begins at the playback time specified by the temporal index value. An appropriate subset of the video is selected for label generation and video frame selection to generate images for inclusion in the corresponding video anchor. In many cases, key moments may be several minutes apart, and processing the entire video between indicators may result in slight topic deviations from the identified salient topics. Therefore, only a suitable subset of videos is selected. A suitable subset of the video starts at the playback time specified by the time index value, starts at the playback time specified by the time index value, and then begins at the next nearest point specified by another time index value for another key moment. Less than the length of the video segment ending in playback time. In some implementations, a suitable subset of videos may be 6 seconds long. For example, referring to FIG. 1, a suitable subset of video for the first key moment is 1:12 to 1:18. Other lengths of time may be used.
プロセス400は、各キーモーメント識別子に対して、ビデオの適切なサブセットについて、キーモーメント識別子用のテキストのラベルを決定する(406)。上記で説明したように、テキスト信号、視覚信号、および精選されたデータ(curated data)が使用されてもよい。いくつかの実装形態では、テキストラベルは、ビデオの適切なサブセットについて最も関連するトピックを決定することに応じて生成される。たとえば、ビデオがレシピビデオであって、ビデオの適切なサブセットは、以下のオーディオを含むと仮定する。「次に、泡立て器またはスプーンを使って、ミキシングボール内で粉末材料を混ぜます。…ように、材料を十分によく混ぜます。」 機械学習済みシステム、または言語処理システムが、テキストを入力として受け取り、「粉末材料を混ぜる」というラベルを生成してもよい。 For each key moment identifier, the process 400 determines a textual label for the key moment identifier for the appropriate subset of videos (406). As explained above, textual signals, visual signals, and curated data may be used. In some implementations, the text labels are generated in response to determining the most relevant topics for the appropriate subset of videos. For example, suppose the video is a recipe video and a suitable subset of the video includes the following audio: "Next, use a whisk or spoon to mix the powdered ingredients in a mixing bowl. Mix the ingredients well enough so that..." A machine-learned system, or language processing system, takes the text as input. may be received and generate a label that reads "mix powder ingredients."
ラベルは、視覚的分析から生成される場合もある。たとえば、ビデオの各フレームが、フレームに示されるコンテンツを説明するラベルを決定する画像処理システムによって処理されてもよい。たとえば、レシピビデオから、フレームは最初にシェフについて、次いでミキシングボールおよび器具についてであると仮定する。そのようなラベルは、「シェフ、ミキシングボール、泡立て器、粉末」を含んでもよい。 Labels may also be generated from visual analysis. For example, each frame of a video may be processed by an image processing system that determines a label that describes the content shown in the frame. For example, suppose from a recipe video that the frames are first about the chef and then about the mixing bowl and utensils. Such labels may include "chef, mixing bowl, whisk, powder."
ラベルは、キーモーメントを最も良く説明する1つまたは複数のラベルを決定するために、スコアリングされてもよい。任意の適切な適合性スコアリングプロセスが使用されてもよい。さらに、ユーザによってより容易に理解される語句をラベルから生成するために、自然原語処理が使用されてもよい。 The labels may be scored to determine the label or labels that best describe the key moment. Any suitable suitability scoring process may be used. Furthermore, natural language processing may be used to generate phrases from the labels that are more easily understood by the user.
プロセス400は、各キーモーメント識別子に対して、ビデオの適切なサブセットからビデオフレームを選択するかどうかを決定するために、ビデオの適切なサブセットの各ビデオフレームを処理する。各ビデオアンカーは、画面のスペースが限られているので、ビデオアンカーにビデオフレームを含むかどうかの決定は、各ビデオアンカーに対して表示されるデータが他の各ビデオアンカーとは区別を生じることを確実にする。言い換えれば、アンカーが対応する顕著なトピックの情報を与えないビデオフレームは、ビデオアンカーに含まれない。たとえば、ビデオが講義のビデオである場合、各ビデオアンカーに対する話者の画像は情報を与えない。したがって、ビデオフレームを使用しないことによって、より記述的なテキストのラベルが使用されてもよく、各テキストのラベルは、話者が論じている主題を説明する。 For each key moment identifier, process 400 processes each video frame of the appropriate subset of the video to determine whether to select the video frame from the appropriate subset of the video. Because each video anchor has limited space on the screen, the decision to include video frames in a video anchor is important because the data displayed for each video anchor is distinct from each other. ensure that In other words, video frames that do not give information of the salient topic to which the anchor corresponds are not included in the video anchor. For example, if the video is a lecture video, the image of the speaker for each video anchor provides no information. Therefore, by not using video frames, more descriptive text labels may be used, each text label explaining the subject matter being discussed by the speaker.
いくつかの実装形態では、ビデオの適切なサブセットのビデオフレームを処理している間、ビデオフレーム選択エンジン320が、ビデオフレームごとに、ビデオフレームで説明されるコンテンツを説明するフレーム用のラベルのセットを決定する。ラベルは、上記のように決定されたラベルと同じであってもよい。その後、各ビデオフレームに対して、システムは、キーモーメント識別子用のテキストのラベルに対するビデオフレーム用のラベルのセットの類似性を測定する類似性測度を決定し、次いで、ビデオアンカーに含めるための画像を生成するために最も高い類似性測度を有するビデオフレームを選択する。任意の適切な類似性測定プロセスが使用され得る。 In some implementations, while processing the video frames of the appropriate subset of the video, video frame selection engine 320 selects, for each video frame, a set of labels for the frame that describe the content described in the video frame. Determine. The label may be the same as the label determined above. Then, for each video frame, the system determines a similarity measure that measures the similarity of the set of labels for the video frame to the textual labels for the key moment identifiers, and then the image for inclusion in the video anchor. Select the video frame with the highest similarity measure to generate . Any suitable similarity measurement process may be used.
たとえば、レシピビデオに戻ると、ビデオの適切なサブセットの最初の3秒が、単にシェフの話を示し、残りの3秒が、粉末材料および器具とともにミキシングボールを示すと仮定する。また、セグメントのラベルがテキストおよび視覚信号から決定され、決定されたラベルは「粉末材料を混ぜる」であると仮定する。シェフのビデオフレームは、粉末材料および混合に関するいくつかのラベルを有してもよいが、示される人物を説明するラベルもまた有する。ミキシングボール、材料、および器具を示すフレーム用のラベルは、ミキシングボール、粉末材料、および器具を説明するラベルを有する。したがって、ビデオアンカーに決定されたラベルに最も類似したラベルフレームは、ミキシングボール、材料、および器具を示す後者のフレームとなる。 For example, returning to a recipe video, assume that the first 3 seconds of a suitable subset of the video simply shows the chef talking, and the remaining 3 seconds show the mixing bowl along with the powdered ingredients and equipment. Also assume that the label of the segment is determined from text and visual signals, and that the determined label is "mix powder ingredients." The chef's video frame may have some labels regarding powder ingredients and mixing, but also labels describing the person shown. The label for the frame showing the mixing bowl, materials, and equipment has a label describing the mixing bowl, powdered material, and equipment. Therefore, the label frame most similar to the label determined for the video anchor will be the latter frame showing the mixing bowl, materials, and equipment.
いくつかの実装形態では、最も高い類似性測度を有するフレームは、それが、選択のための最低類似性測度を指定する選択しきい値を満たすときにのみ選択される。選択しきい値は、選択されたフレームが、識別されたキーモーメントのラベルによって説明される物体またはイベントを示す可能性が非常に高いように選択されてもよい。 In some implementations, the frame with the highest similarity measure is selected only when it meets a selection threshold that specifies the lowest similarity measure for selection. The selection threshold may be chosen such that the selected frame is highly likely to show the object or event described by the identified key moment's label.
処理要件をさらに下げるために、いくつかの実装形態では、適切なサブセットのビデオフレームは、多様性測度を決定するために互いに比較される。多様性測度は、ビデオフレームと、ビデオの適切なサブセット中の1つまたは複数の他のビデオフレームとの違いを測定する。ビデオフレームの選択は、その場合、多様性測度に部分的に基づいてもよい。 To further reduce processing requirements, in some implementations, appropriate subsets of video frames are compared to each other to determine a diversity measure. A diversity measure measures the difference between a video frame and one or more other video frames in a relevant subset of videos. The selection of video frames may then be based in part on the diversity measure.
再びレシピビデオに戻ると、ビデオの適切なサブセットの最初の3秒が、単にシェフの話を示し、残りの3秒が、粉末材料および器具とともにミキシングボールを示すと仮定する。多様性測度は、ビデオフレームの最初の3秒を非常に類似しているとして、およびビデオフレームの最後の3秒を非常に類似しているとしてグループ分けする。したがって、ビデオフレームを選択するには、各グループからただ1つのビデオフレームが処理される必要があり、ラベルへの最も高い類似性測度を持つビデオフレームが選択されることになる。 Returning to the recipe video again, assume that the first 3 seconds of the appropriate subset of the video simply shows the chef talking, and the remaining 3 seconds show the mixing bowl along with the powdered ingredients and equipment. The diversity measure groups the first 3 seconds of a video frame as very similar and the last 3 seconds of a video frame as very similar. Therefore, to select video frames, only one video frame from each group needs to be processed, and the video frame with the highest similarity measure to the label will be selected.
ビデオが、ビデオの適切なサブセット全体の間、シェフの話に焦点が当てられる場合など、フレームすべてが非常に類似しているので、多様性測度が、フレームすべてが単一のグループにグループ分けされると示す場合、1つのフレームのみが選択され、それの類似性測度が決定されてもよい。ビデオフレームが類似性しきい値を満たさない場合、そのビデオフレームは、ビデオアンカーに含めるための画像を生成するために選択されない。 If the video is focused on the chef's story for the entire appropriate subset of the video, the frames are all very similar, so the diversity measure is such that all the frames are grouped into a single group. , then only one frame may be selected and its similarity measure determined. If a video frame does not meet the similarity threshold, that video frame is not selected to generate an image for inclusion in the video anchor.
他の実装形態では、フレームすべてが非常に類似していて、多様性測度が、フレームすべてが単一のグループにグループ分けされると示す場合、ビデオフレームは選択されず、代わりにテキストのラベルのみがビデオアンカーに使用される。 In other implementations, if the frames are all very similar and the diversity measure indicates that they all group into a single group, no video frames are selected and instead only the text labels is used for video anchors.
プロセス400は、各キーモーメント識別子に対して、ビデオアンカーを生成する(410)。各ビデオアンカーは、キーモーメント識別子用のテキストのラベルと、ビデオフレームから生成された画像(画像が選択されると決定された場合)と、ユーザデバイス上のビデオプレーヤに、キーモーメント識別子の時間インデックス値によって指定された再生時間のビデオの再生を開始させる命令とを含む。ビデオアンカーは、次いでビデオアンカーインデックス330に記憶され、ビデオアンカーが対応するビデオに関連付けられる。 Process 400 generates a video anchor for each key moment identifier (410). Each video anchor includes a textual label for a key moment identifier, an image generated from the video frame (if an image is determined to be selected), and a temporal index of the key moment identifier in the video player on the user device. and an instruction to start playing the video for the playback time specified by the value. The video anchor is then stored in a video anchor index 330, which associates the video anchor with the video to which it corresponds.
その後に、ユーザデバイス370は、ビデオを要求するためにビデオ要求340を発行してもよい。要求は、ビデオ検索環境ではビデオ検索に応じる、または他のビデオ再生環境により直接ビデオにアクセスすることに応じるものであり得る。 Thereafter, user device 370 may issue a video request 340 to request video. The request may be in response to a video search in a video search environment, or in response to accessing the video directly by another video playback environment.
応答して、プロセス400は、ユーザデバイスにビデオプレーヤ環境にビデオアンカーの各々をレンダリングさせるデータをユーザデバイスに提供する(412)。 In response, process 400 provides data to the user device that causes the user device to render each of the video anchors in a video player environment (412).
ビデオアンカーのユーザデバイス側処理について、ユーザデバイスでビデオアンカーを処理するための例示的なプロセス400の流れ図である図5を参照しながら説明する。 User device-side processing of video anchors will be described with reference to FIG. 5, which is a flow diagram of an example process 400 for processing video anchors at a user device.
プロセス500はユーザデバイスにビデオアンカーのセットをビデオプレーヤにレンダリングさせるデータを受け取り(502)、次いでビデオアンカーのセット中の各ビデオアンカーをレンダリングする(504)。たとえば、データは、各ビデオアンカーに対して、ビデオプレーヤのプログレスバー内の時間インジケータであって、時間インデックス値によって指定された再生時間に対応する時間インジケータと、対応する時間インジケータからビデオアンカーへのビジュアルリンクとを定義するデータを含む。ビデオアンカーはまた、ビデオフレームがビデオアンカーに選択された場合、顕著なトピックを説明するラベルと、ビデオフレームを示す画像、たとえばビデオフレームのサムネイルまたはビデオフレームの切り取られた部分とを含む。 Process 500 receives data that causes a user device to render a set of video anchors to a video player (502) and then renders each video anchor in the set of video anchors (504). For example, for each video anchor, the data includes a time indicator in the video player's progress bar that corresponds to the playback time specified by the time index value, and a time indicator from the corresponding time indicator to the video anchor. Contains data that defines visual links. The video anchor also includes a label describing the salient topic and an image representing the video frame, such as a thumbnail of the video frame or a cropped portion of the video frame, if the video frame is selected to be the video anchor.
プロセス500は、ビデオプレーヤのビデオ再生ウィンドウにビデオの第1のフレームをレンダリングする(506)。たとえば、ビデオの再生の前に、ビデオプレーヤは初期状態であって、ビデオの第1のフレームがビデオ再生ウィンドウに表示される。 Process 500 renders a first frame of video to a video playback window of a video player (506). For example, before playing a video, the video player is in an initial state and the first frame of the video is displayed in the video playback window.
プロセス500は、ビデオアンカーの1つの選択に応じて、ユーザデバイスに、ビデオアンカーの時間インデックス値によって指定された再生時間からビデオの再生を開始させる(508)。たとえば、ユーザが図1のビデオアンカー130を選択する場合は、ビデオアンカーに埋め込まれた命令は、ユーザデバイスに、2:13の再生時間にビデオプレーヤウィンドウ110でビデオの再生を開始させる。 Process 500 causes the user device to begin playing the video at the playback time specified by the video anchor's time index value in response to selection of one of the video anchors (508). For example, if a user selects video anchor 130 of FIG. 1, the instructions embedded in the video anchor cause the user device to begin playing the video in video player window 110 at the 2:13 playback time.
いくつかの実装形態では、ビデオプレーヤのプログレスバー内の各対応する時間インジケータが、対応する時間インジケータの再生時間に始まるビデオの一部分の時間の長さを示す。そのように示されるビデオの一部分は、ラベルに関連していると決定されたビデオの一部分であってもよい。たとえば、図2に示すように、破線のインジケータ213、223、および233は、ビデオアンカーのラベルに最も関連していると決定されたビデオのそれぞれの部分に対応する。関連性は、上記で説明したプロセスによって決定され得る。 In some implementations, each corresponding time indicator in a progress bar of a video player indicates the length of time of a portion of the video starting at the corresponding time indicator's playback time. The portion of the video so shown may be the portion of the video determined to be associated with the label. For example, as shown in FIG. 2, dashed indicators 213, 223, and 233 correspond to respective portions of the video determined to be most relevant to the video anchor's label. Relevance may be determined by the process described above.
本明細書で説明するシステムがユーザについての個人情報を収集する、または個人情報を利用し得る状況において、ユーザは、アプリケーションまたは機能がユーザ情報(たとえば、ユーザのソーシャルネットワーク、社会的行為もしくは活動、職業、ユーザの選好、またはユーザの現在の位置についての情報)を収集するかどうかを制御するための機会、または、ユーザにより関連がある可能性があるコンテンツを受信するかどうか、および/もしくはどのように受信するかを制御するための機会を与えられ得る。加えて、いくつかのデータは、個人を識別できる情報が削除されるように、記憶または使用される前に1つまたは複数の方法で扱われ得る。たとえば、ユーザの識別情報は、個人を識別できる情報がユーザについて決定できないように扱われ得る、またはユーザの地理的位置は、ユーザの具体的な位置が決定できないように、位置情報が取得される場合に(都市、ZIPコード、もしくは州のレベルなどに)一般化され得る。したがって、ユーザは、情報がどのようにユーザについて収集されコンテンツサーバにより使用されるかを制御することができる。 In situations where the systems described herein may collect or make use of personal information about a user, the user may be aware that the application or functionality is the opportunity to control whether and/or whether information about your occupation, user preferences, or current location of the user is collected; You may be given the opportunity to control what you receive. Additionally, some data may be treated in one or more ways before being stored or used so that personally identifying information is removed. For example, a user's identity may be treated such that personally identifiable information cannot be determined about the user, or the user's geographic location may be treated such that location information is obtained such that the user's specific location cannot be determined. (e.g., to the city, ZIP code, or state level). Thus, the user can control how information is collected about the user and used by the content server.
本明細書で説明した主題および動作の実施形態は、デジタル電子回路において、または、本明細書で開示した構造およびそれらの構造等価物を含む、コンピュータソフトウェア、ファームウェア、もしくはハードウェアにおいて、またはそれらのうちの1つもしくは複数の組合せにおいて実装され得る。本明細書で説明した主題の実施形態は、データ処理装置による実行のために、またはデータ処理装置の動作を制御するために、コンピュータ記憶媒体上で符号化された1つまたは複数のコンピュータプログラム、すなわち、コンピュータプログラム命令の1つまたは複数のモジュールとして実装され得る。 Embodiments of the subject matter and operations described herein may be implemented in digital electronic circuits or in computer software, firmware, or hardware, including the structures disclosed herein and structural equivalents thereof. One or more of these may be implemented in combination. Embodiments of the subject matter described herein may include one or more computer programs encoded on a computer storage medium for execution by or for controlling the operation of a data processing apparatus; that is, may be implemented as one or more modules of computer program instructions.
コンピュータ記憶媒体は、コンピュータ可読記憶デバイス、コンピュータ可読記憶基板、ランダムもしくはシリアルアクセスメモリアレイもしくはデバイス、またはそれらのうちの1つもしくは複数の組合せであり得るか、またはそれらに含まれ得る。さらに、コンピュータ記憶媒体は、伝搬信号ではなく、コンピュータ記憶媒体は、人工的に生成された伝搬信号において符号化されたコンピュータプログラム命令のソースまたは宛先であり得る。コンピュータ記憶媒体はまた、1つまたは複数の別個の物理構成要素または媒体(たとえば、複数のCD、ディスク、または他の記憶デバイス)であり得るか、またはそれらに含まれ得る。 The computer storage medium can be or include a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more thereof. Further, a computer storage medium is not a propagated signal; a computer storage medium can be a source or destination for computer program instructions encoded in an artificially generated propagated signal. Computer storage media can also be or included in one or more separate physical components or media (eg, multiple CDs, disks, or other storage devices).
本明細書で説明される動作は、1つもしくは複数のコンピュータ可読記憶デバイス上に記憶されたまたは他のソースから受信されたデータに対してデータ処理装置によって実施される動作として実装され得る。 The operations described herein may be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
「データ処理装置」という用語は、例として、プログラマブルプロセッサ、コンピュータ、システムオンチップ、もしくは上記の複数のもの、または上記の組合せを含む、データを処理するためのすべての種類の装置、デバイス、および機械を包含する。装置は、専用論理回路、たとえば、FPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)を含むことができる。装置は、ハードウェアに加えて、当該のコンピュータプログラムのための実行環境を作成するコード、たとえば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、クロスプラットフォームランタイム環境、仮想マシン、またはそれらのうちの1つもしくは複数の組合せを構成するコードをも含むことができる。装置および実行環境は、ウェブサービス、分散コンピューティングおよびグリッドコンピューティングインフラストラクチャなど、様々な異なるコンピューティングモデルインフラストラクチャを実現することができる。 The term "data processing apparatus" includes, by way of example, a programmable processor, a computer, a system-on-a-chip, or more than one of the above, or a combination of the above, and Includes machines. The device may include special purpose logic circuitry, such as an FPGA (Field Programmable Gate Array) or an ASIC (Application Specific Integrated Circuit). In addition to the hardware, the device includes code that creates an execution environment for the computer program in question, such as processor firmware, protocol stacks, database management systems, operating systems, cross-platform runtime environments, virtual machines, or any of the above. It may also include codes constituting one or more combinations of. The devices and execution environments can implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
コンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、スクリプト、またはコードとしても知られている)は、コンパイル型言語またはインタプリタ型言語、宣言型言語または手続き型言語を含む任意の形態のプログラミング言語で書かれ得、スタンドアロンプログラムとして、またはモジュール、構成要素、サブルーチン、オブジェクト、もしくはコンピューティング環境において使用するのに適した他のユニットとしてを含む任意の形態で展開され得る。コンピュータプログラムは、ファイルシステムにおけるファイルに対応し得るが、そうである必要はない。プログラムは、他のプログラムもしくはデータ(たとえば、マークアップ言語文書に記憶された1つもしくは複数のスクリプト)を保持するファイルの一部分に、当該のプログラム専用の単一のファイルに、または複数の協調ファイル(たとえば、1つもしくは複数のモジュール、サブプログラム、またはコードの部分を記憶するファイル)に記憶され得る。コンピュータプログラムは、1つのコンピュータ上で、または、1つのサイトに配置されるかもしくは複数のサイトにわたって分散され、通信ネットワークによって相互接続される複数のコンピュータ上で実行されるように展開され得る。 A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, declarative languages, or procedural languages. may be deployed in any form, including as a stand-alone program, or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may correspond to a file in a file system, but need not. A program may be a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), a single file dedicated to the program, or multiple cooperating files. (eg, a file that stores one or more modules, subprograms, or portions of code). A computer program may be deployed to run on one computer or on multiple computers located at one site or distributed across multiple sites and interconnected by a communications network.
本明細書で説明したプロセスおよび論理フローは、入力データ上で動作し、出力を生成することによってアクションを行うために、1つまたは複数のコンピュータプログラムを実行する1つまたは複数のプログラマブルプロセッサによって実行され得る。プロセスおよび論理フローは、専用論理回路、たとえば、FPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)によっても実施され得、装置は、それらとしても実装され得る。 The processes and logic flows described herein are performed by one or more programmable processors that execute one or more computer programs to perform actions by operating on input data and producing output. can be done. The processes and logic flows may also be implemented by dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits), and the apparatus may also be implemented as such.
コンピュータプログラムの実行に好適なプロセッサは、例として、汎用マイクロプロセッサと専用マイクロプロセッサの両方、および任意の種類のデジタルコンピュータの任意の1つまたは複数のプロセッサを含む。一般にプロセッサは、読取り専用メモリまたはランダムアクセスメモリまたは両方から命令およびデータを受け取ることになる。コンピュータの必須要素は、命令に従ってアクションを行うためのプロセッサ、ならびに命令およびデータを記憶するための1つまたは複数のメモリデバイスである。一般に、コンピュータはまた、データを記憶するための1つまたは複数の大容量記憶デバイス、たとえば、磁気ディスク、光磁気ディスク、もしくは光ディスクを含むか、または、それらからデータを受信することもしくはそれらにデータを転送すること、もしくは両方を行うために動作可能に結合される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。さらに、コンピュータは、ほんの数例を挙げると、別のデバイス、たとえば、モバイル電話、携帯情報端末(PDA)、モバイルオーディオもしくはビデオプレーヤ、ゲームコンソール、全地球測位システム(GPS)受信機、またはポータブル記憶デバイス(たとえば、ユニバーサルシリアルバス(USB)フラッシュドライブ)に埋め込まれ得る。コンピュータプログラム命令およびデータを記憶するのに好適なデバイスは、例として、半導体メモリデバイス、たとえば、EPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば、内蔵ハードディスクまたはリムーバブルディスク、光磁気ディスク、ならびにCD-ROMおよびDVD-ROMディスクを含む、すべての形態の不揮発性メモリ、媒体およびメモリデバイスを含む。プロセッサおよびメモリは、専用論理回路によって補完され得るか、または専用論理回路に組み込まれ得る。 Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any type of digital computer. Generally, a processor will receive instructions and data from read-only memory and/or random access memory. The essential elements of a computer are a processor for performing actions according to instructions, and one or more memory devices for storing instructions and data. Generally, a computer also includes one or more mass storage devices for storing data, such as magnetic disks, magneto-optical disks, or optical disks, or receiving data from or transmitting data to them. and/or both. However, a computer does not need to have such a device. Additionally, the computer may be connected to another device, such as a mobile phone, personal digital assistant (PDA), mobile audio or video player, game console, Global Positioning System (GPS) receiver, or portable storage device, to name just a few. It may be embedded in a device (eg, a Universal Serial Bus (USB) flash drive). Devices suitable for storing computer program instructions and data include, by way of example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks, and CDs. -Includes all forms of non-volatile memory, media and memory devices, including ROM and DVD-ROM disks. The processor and memory may be supplemented by or incorporated into special purpose logic circuits.
ユーザとの対話を提供するために、本明細書で説明される主題の実施形態は、情報をユーザに表示するためのディスプレイデバイス、たとえば、CRT(陰極線管)またはLCD(液晶ディスプレイ)モニタと、それによってユーザが入力をコンピュータに提供することができるキーボードおよびポインティングデバイス、たとえば、マウスまたはトラックボールとを有するコンピュータ上で実装され得る。他の種類のデバイスも、ユーザとの対話を提供するために使用され得、たとえば、ユーザに提供されるフィードバックは、任意の形態の感覚フィードバック、たとえば、視覚フィードバック、聴覚フィードバック、または触覚フィードバックであり得、ユーザからの入力は、音響入力、音声入力、または触覚入力を含む任意の形態で受信され得る。加えて、コンピュータは、文書をユーザによって使用されるデバイスに送信し、文書をそのデバイスから受信することによって、たとえば、ユーザのクライアントデバイス上のウェブブラウザから受信された要求に応じてウェブページをそのウェブブラウザに送信することによって、ユーザと対話することができる。 To provide user interaction, embodiments of the subject matter described herein include a display device, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user; It may be implemented on a computer having a keyboard and pointing device, such as a mouse or trackball, by which a user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, for example, the feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback. Input from the user may be received in any form, including acoustic, audio, or tactile input. In addition, the computer may, for example, create a web page in response to a request received from a web browser on a user's client device by sending the document to and receiving the document from the device used by the user. You can interact with the user by sending it to a web browser.
本明細書で説明した主題の実施形態は、たとえば、データサーバとして、バックエンド構成要素を含むか、またはミドルウェア構成要素、たとえば、アプリケーションサーバを含むか、またはフロントエンド構成要素、たとえば、ユーザが本明細書で説明した主題の実装形態とそれを通して対話することができるグラフィカルユーザインターフェースもしくはウェブブラウザを有するユーザコンピュータを含むか、または1つもしくは複数のそのようなバックエンド構成要素、ミドルウェア構成要素、もしくはフロントエンド構成要素の任意の組合せを含む、コンピューティングシステムにおいて実装され得る。システムの構成要素は、デジタルデータ通信の任意の形態または媒体、たとえば、通信ネットワークによって相互接続され得る。通信ネットワークの例は、ローカルエリアネットワーク(「LAN」)およびワイドエリアネットワーク(「WAN」)、インターネットワーク(たとえば、インターネット)、およびピアツーピアネットワーク(たとえば、アドホックピアツーピアネットワーク)を含む。 Embodiments of the subject matter described herein may include a back-end component, e.g., a data server, or a middleware component, e.g., an application server, or a front-end component, e.g. including a user computer having a graphical user interface or web browser through which implementations of the subject matter described in the specification can be interacted with, or one or more such back-end components, middleware components, or It may be implemented in a computing system including any combination of front-end components. The components of the system may be interconnected by any form or medium of digital data communication, such as a communication network. Examples of communication networks include local area networks (“LANs”) and wide area networks (“WANs”), internetworks (eg, the Internet), and peer-to-peer networks (eg, ad hoc peer-to-peer networks).
コンピューティングシステムは、ユーザおよびサーバを含むことができる。ユーザおよびサーバは、一般に互いから離れており、典型的には通信ネットワークを通じて対話する。ユーザとサーバの関係は、それぞれのコンピュータ上で動作し、互いに対してユーザ-サーバ関係を有する、コンピュータプログラムによって生じる。いくつかの実施形態では、サーバは、(たとえば、ユーザデバイスと対話するユーザにデータを表示し、ユーザからユーザ入力を受信する目的で)ユーザデバイスにデータ(たとえば、HTMLページ)を送信する。ユーザデバイスにおいて生成されたデータ(たとえば、ユーザ対話の結果)は、サーバにおいてユーザデバイスから受信され得る。 A computing system may include users and servers. Users and servers are generally remote from each other and typically interact through a communications network. The user and server relationship is created by computer programs running on their respective computers and having a user-server relationship with each other. In some embodiments, the server sends data (eg, an HTML page) to the user device (eg, for the purpose of displaying the data to and receiving user input from the user interacting with the user device). Data generated at a user device (eg, results of user interaction) may be received from the user device at a server.
本明細書は多くの特定の実装形態の詳細を含んでいるが、これらは任意の特徴の範囲または特許請求され得るものの範囲に対する限定として解釈されるべきではなく、むしろ特定の実施形態に特有の特徴の説明として解釈されるべきである。本明細書で別個の実施形態の文脈で説明されるいくつかの特徴は、単一の実施形態において組み合わせて実装されることもある。逆に、単一の実施形態の文脈で説明される様々な特徴は、複数の実施形態において別々に、または任意の適切な部分的組合せで実装されることもある。さらに、特徴はいくつかの組合せにおいて働くものとして上記で説明され、そのようなものとして最初に特許請求されることさえあるが、特許請求される組合せからの1つまたは複数の特徴は、場合によっては、その組合せから削除されることがあり、特許請求される組合せは、部分組合せまたは部分組合せの変形形態を対象とする場合がある。 Although this specification contains many specific implementation details, these should not be construed as limitations on the scope of any feature or what may be claimed, but rather as limitations on the scope of any feature or what may be claimed. It should be interpreted as a description of the feature. Certain features that are described herein in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features are described above as operating in some combination, and may even be initially claimed as such, one or more features from the claimed combination may may be deleted from the combination, and the claimed combination may be directed to subcombinations or variations of subcombinations.
同様に、動作は、特定の順序で図面に示されるが、これは、望ましい結果を達成するために、そのような動作が図示された特定の順序でもしくは順番に行われること、または例示したすべての動作が行われることを必要とするものと理解されるべきではない。状況によっては、マルチタスキングおよび平行処理が有利であり得る。さらに、上記で説明した実施形態における様々なシステム構成要素の分離は、すべての実施形態においてそのような分離を必要とするものとして理解されるべきではなく、説明したプログラム構成要素およびシステムは一般に、単一のソフトウェア製品に一緒に組み込まれるか、または複数のソフトウェア製品にパッケージ化されることがあると理解されたい。 Similarly, although acts are shown in the drawings in a particular order, this does not mean that such acts may be performed in the particular order or sequence shown or in any order shown to achieve a desired result. should not be understood as requiring that such actions be performed. Multitasking and parallel processing may be advantageous in some situations. Furthermore, the separation of various system components in the embodiments described above is not to be understood as requiring such separation in all embodiments, and the program components and systems described generally It is to be understood that they may be incorporated together into a single software product or packaged into multiple software products.
以上、本主題の特定の実施形態について説明した。他の実施形態は、以下の特許請求の範囲の範囲内にある。場合によっては、特許請求の範囲に列挙されるアクションは、異なる順序で行われ、依然として望ましい結果を達成し得る。加えて、添付の図面に示したプロセスは、所望の結果を達成するために、必ずしも示した特定の順序または順番を必要としない。いくつかの実装形態では、マルチタスキングおよび平行処理が有利であり得る。 Certain embodiments of the present subject matter have been described above. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims may be performed in a different order and still achieve the desired result. Additionally, the processes depicted in the accompanying figures do not necessarily require the particular order or order shown to achieve desired results. In some implementations, multitasking and parallel processing may be advantageous.
100 例示的な環境
110 ビデオプレーヤウィンドウ
120 ビデオアンカー
122 時間インジケータ
124 ビデオフレーム
126 テキストのラベル
130 ビデオアンカー
132 時間インジケータ
134 ビデオフレーム
136 テキストのラベル
140 ビデオアンカー
142 時間インジケータ
144 ビデオフレーム
146 テキストのラベル
150 ビデオ検索結果
152 ビデオ検索結果
154 ビデオ検索結果
156 ビデオ検索結果
200 別のビデオ表示環境
202 ビデオプレーヤウィンドウ
204 プログレスバー
210 ビデオアンカー
212 時間インジケータ
214 テキストのラベル
220 ビデオアンカー
222 時間インジケータ
224 テキストのラベル
230 ビデオアンカー
232 時間インジケータ
234 テキストのラベル
300 システム
302 ビデオ
303 キーモーメント識別子
306 テキスト信号
308 視覚信号
310 手作業のキュレーション
312 関心基準
320 ビデオ
330 ビデオアンカーインデックス
370 ユーザデバイス
100 example environments
110 Video player window
120 video anchor
122 hour indicator
124 video frames
126 Text Label
130 Video Anchor
132 hour indicator
134 video frames
136 Text Label
140 video anchor
142 hour indicator
144 video frames
146 Text Label
150 video search results
152 video search results
154 video search results
156 video search results
200 different video display environments
202 Video player window
204 Progress bar
210 Video Anchor
212 hour indicator
214 Text Label
220 video anchor
222 hour indicator
224 Text Label
230 Video Anchor
232 hour indicator
234 Text Label
300 systems
302 videos
303 Key Moment Identifier
306 Text Signal
308 Visual Signals
310 Manual curation
312 Interest Criteria
320 videos
330 Video Anchor Index
370 user device
Claims (22)
ビデオに対して、複数のキーモーメント識別子を取得するステップであって、各キーモーメント識別子が、
前記ビデオにおける再生時間を指定する時間インデックス値を含み、
前記ビデオ内の顕著なトピックを定義する1つまたは複数の関心基準を満たすと決定された前記ビデオの主題を示す、取得するステップと、
各キーモーメント識別子に対して、
前記時間インデックス値によって指定された前記再生時間に始まる前記ビデオの適切なサブセットを選択するステップであって、前記ビデオの前記適切なサブセットが、前記時間インデックス値によって指定された前記再生時間に始まり、別のキーモーメント識別子の別の時間インデックス値によって指定された次の最も近い再生時間に終わるビデオセグメントの長さよりも短い、選択するステップと、
前記ビデオの前記適切なサブセットについて、前記キーモーメント識別子用のテキストのラベルを決定するステップと、
前記ビデオの前記適切なサブセットからビデオフレームを選択するかどうかを決定するために、前記ビデオの前記適切なサブセットの各ビデオフレームを処理するステップと、
各キーモーメント識別子に対して、ビデオアンカーを生成するステップであって、 前記ビデオアンカーが、
前記キーモーメント識別子用の前記テキストのラベルと、
前記ビデオの前記適切なサブセットのビデオフレームを選択する決定に応じて、前記ビデオフレームを示す画像と、
ユーザデバイス上のビデオプレーヤに、前記キーモーメント識別子の前記時間インデックス値によって指定された前記再生時間の前記ビデオの再生を開始させる命令と
を含む、生成するステップと、
ユーザデバイスにデータを提供するステップであって、前記データが前記ユーザデバイスのビデオプレーヤ環境で前記ユーザデバイスに、
前記ビデオアンカーの各々と、
各ビデオアンカーに対して、前記ビデオプレーヤのプログレスバー内の時間インジケータであって、前記時間インデックス値によって指定された前記再生時間に対応する時間インジケータと
をレンダリングさせる、提供するステップと
を含み、
各ビデオアンカーが、ユーザによって選択可能であり、前記ビデオアンカーが選択されると、前記ビデオアンカーの前記命令が、ユーザデバイス上の前記ビデオプレーヤに、前記時間インデックス値によって指定された前記再生時間の前記ビデオの再生を開始させる、コンピュータ実装方法。 A computer-implemented method, the method comprising:
obtaining a plurality of key moment identifiers for the video, each key moment identifier comprising:
a time index value specifying a playback time in the video;
obtaining an indication of the subject matter of the video determined to meet one or more interest criteria defining salient topics within the video;
For each key moment identifier,
selecting a suitable subset of the videos starting at the playback time specified by the time index value, wherein the suitable subset of videos starts at the playback time specified by the time index value; selecting a video segment that is shorter than the length of the video segment ending at the next closest playback time specified by another time index value of another key moment identifier;
determining a textual label for the key moment identifier for the appropriate subset of videos;
processing each video frame of the appropriate subset of the video to determine whether to select a video frame from the appropriate subset of the video;
generating a video anchor for each key moment identifier, the video anchor comprising:
the textual label for the key moment identifier;
an image depicting the video frames in response to a decision to select video frames of the appropriate subset of the video;
instructions for causing a video player on a user device to begin playing the video at the playback time specified by the time index value of the key moment identifier;
providing data to a user device, the data being transmitted to the user device in a video player environment of the user device;
each of the video anchors;
for each video anchor, causing a time indicator to be rendered in a progress bar of the video player, the time indicator corresponding to the playback time specified by the time index value;
Each video anchor is selectable by the user, and when the video anchor is selected, the instruction of the video anchor causes the video player on the user device to play for the playback time specified by the time index value. A computer-implemented method of starting playback of the video.
各ビデオフレームに対して、前記ビデオフレームで説明されるコンテンツを説明する前記フレーム用のラベルのセットを決定するステップと、
各ビデオフレームに対して、前記ビデオフレーム用の前記ラベルのセットの、前記キーモーメント識別子用の前記テキストのラベルに対する類似性を測定する類似性測度を決定するステップと、
前記ビデオアンカーに含めるための前記画像を生成するために最も高い類似性測度を有する前記ビデオフレームを選択するステップと
を含む、請求項1に記載のコンピュータ実装方法。 processing each video frame of the appropriate subset of videos to determine whether to select a video frame from the appropriate subset of videos;
determining, for each video frame, a set of labels for the frame that describe the content described in the video frame;
determining, for each video frame, a similarity measure that measures the similarity of the set of labels for the video frame to the textual labels for the key moment identifier;
and selecting the video frame with the highest similarity measure to generate the image for inclusion in the video anchor.
各ビデオフレームに対して、前記ビデオフレームと、前記ビデオの前記適切なサブセット中の1つまたは複数の他のビデオフレームとの違いを測定する多様性測度を決定するステップと、
前記多様性測度に部分的に基づいてビデオフレームを選択するかどうかを決定するステップと
を含む、請求項1に記載のコンピュータ実装方法。 processing each video frame of the appropriate subset of videos to determine whether to select a video frame from the appropriate subset of videos;
determining, for each video frame, a diversity measure that measures the difference between the video frame and one or more other video frames in the appropriate subset of videos;
and determining whether to select video frames based in part on the diversity measure.
前記多様性測度に基づいて、前記ビデオの前記適切なサブセットの前記ビデオフレームが、最小多様性を指定する多様性しきい値を満たすかどうかを決定するステップ
を含む、請求項4に記載のコンピュータ実装方法。 determining whether to select a video frame based in part on the diversity measure;
5. The computer of claim 4, comprising determining, based on the diversity measure, whether the video frames of the appropriate subset of the video meet a diversity threshold specifying a minimum diversity. How to implement.
前記ビデオフレームで説明されるコンテンツを説明する前記ビデオフレーム用のラベルのセットを決定するステップと、
前記ビデオフレーム用の前記ラベルのセットの、前記キーモーメント識別子用の前記テキストのラベルに対する類似性を測定する類似性測度を決定するステップと、
前記ビデオアンカーに含めるための前記画像を生成するために最も高い類似性測度を有する前記ビデオフレームを選択するステップと
をさらに含む、請求項5に記載のコンピュータ実装方法。 selecting one or more video frames from the video frames of the appropriate subset of videos in response to determining that the video frames of the appropriate subset of videos meet the diversity threshold; and for each of the selected video frames,
determining a set of labels for the video frame that describes content illustrated in the video frame;
determining a similarity measure that measures the similarity of the set of labels for the video frame to the text label for the key moment identifier;
6. The computer-implemented method of claim 5, further comprising selecting the video frame with the highest similarity measure to generate the image for inclusion in the video anchor.
をさらに含む、請求項5に記載のコンピュータ実装方法。 of the appropriate subset of the video to generate the images for inclusion in the video anchor in response to a determination that the video frames of the appropriate subset of the video do not meet the diversity threshold; 6. The computer-implemented method of claim 5, further comprising selecting one of the video frames.
をさらに含む、請求項5に記載のコンピュータ実装方法。 of the appropriate subset of the video to generate the images for inclusion in the video anchor in response to a determination that the video frames of the appropriate subset of the video do not meet the diversity threshold; 6. The computer-implemented method of claim 5, further comprising selecting no video frames.
データ処理装置と、
前記データ処理装置とデータ通信し、前記データ処理装置によって実行可能であって、そのような実行時に、前記データ処理装置に動作を行わせる命令を記憶する、非一時的コンピュータ可読媒体とを含み、前記動作が、
ビデオに対して、複数のキーモーメント識別子を取得する動作であって、各キーモーメント識別子が、
前記ビデオにおける再生時間を指定する時間インデックス値を含み、
前記ビデオ内の顕著なトピックを定義する1つまたは複数の関心基準を満たすと決定された前記ビデオの主題を示す、取得する動作と、
各キーモーメント識別子に対して、
前記時間インデックス値によって指定された前記再生時間に始まる前記ビデオの適切なサブセットを選択する動作であって、前記ビデオの前記適切なサブセットが、前記時間インデックス値によって指定された前記再生時間に始まり、別のキーモーメント識別子の別の時間インデックス値によって指定された次の最も近い再生時間に終わるビデオセグメントの長さよりも短い、選択する動作と、
前記ビデオの前記適切なサブセットについて、前記キーモーメント識別子用のテキストのラベルを決定する動作と、
前記ビデオの前記適切なサブセットからビデオフレームを選択するかどうかを決定するために、前記ビデオの前記適切なサブセットの各ビデオフレームを処理する動作と、
各キーモーメント識別子に対して、ビデオアンカーを生成する動作であって、前記ビデオアンカーが、
前記キーモーメント識別子用の前記テキストのラベルと、
前記ビデオの前記適切なサブセットのビデオフレームを選択する決定に応じて、前記ビデオフレームを示す画像と、
ユーザデバイス上のビデオプレーヤに、前記キーモーメント識別子の前記時間インデックス値によって指定された前記再生時間の前記ビデオの再生を開始させる命令と
を含む、生成する動作と、
ユーザデバイスにデータを提供する動作であって、前記データが前記ユーザデバイスのビデオプレーヤ環境で前記ユーザデバイスに、
前記ビデオアンカーの各々と、
各ビデオアンカーに対して、前記ビデオプレーヤのプログレスバー内の時間インジケータであって、前記時間インデックス値によって指定された前記再生時間に対応する時間インジケータと
をレンダリングさせる、提供する動作と
を含み、
各ビデオアンカーが、ユーザによって選択可能であり、前記ビデオアンカーが選択されると、前記ビデオアンカーの前記命令が、ユーザデバイス上の前記ビデオプレーヤに、前記時間インデックス値によって指定された前記再生時間の前記ビデオの再生を開始させる、システム。 A system,
a data processing device;
a non-transitory computer-readable medium in data communication with the data processing device and storing instructions that, when executed, cause the data processing device to perform operations; The said operation is
An operation of obtaining a plurality of key moment identifiers for a video, each key moment identifier being
a time index value specifying a playback time in the video;
an act of obtaining an indication of the subject matter of the video determined to meet one or more interest criteria defining salient topics within the video;
For each key moment identifier,
an act of selecting an appropriate subset of the videos beginning at the playback time specified by the time index value, wherein the appropriate subset of videos begins at the playback time specified by the time index value; an act of selecting a video segment that is less than the length of the video segment ending at the next closest playback time specified by another time index value of another key moment identifier;
an act of determining a textual label for the key moment identifier for the appropriate subset of videos;
an act of processing each video frame of the appropriate subset of the video to determine whether to select a video frame from the appropriate subset of the video;
an act of generating a video anchor for each key moment identifier, the video anchor comprising:
the textual label for the key moment identifier;
an image depicting the video frames in response to a decision to select video frames of the appropriate subset of the video;
instructions for causing a video player on a user device to begin playing the video for the playback time specified by the time index value of the key moment identifier;
an act of providing data to a user device, the data being provided to the user device in a video player environment of the user device;
each of the video anchors;
for each video anchor, causing a time indicator to be rendered in a progress bar of the video player, the time indicator corresponding to the playback time specified by the time index value;
Each video anchor is selectable by the user, and when the video anchor is selected, the instruction of the video anchor causes the video player on the user device to play for the playback time specified by the time index value. A system for starting playback of said video.
各ビデオフレームに対して、前記ビデオフレームで説明されるコンテンツを説明する前記フレーム用のラベルのセットを決定するステップと、
各ビデオフレームに対して、前記ビデオフレーム用の前記ラベルのセットの、前記キーモーメント識別子用の前記テキストのラベルに対する類似性を測定する類似性測度を決定するステップと、
前記ビデオアンカーに含めるための前記画像を生成するために最も高い類似性測度を有する前記ビデオフレームを選択するステップと
を含む、請求項12に記載のシステム。 processing each video frame of the appropriate subset of videos to determine whether to select a video frame from the appropriate subset of videos;
determining, for each video frame, a set of labels for the frame that describe the content described in the video frame;
determining, for each video frame, a similarity measure that measures the similarity of the set of labels for the video frame to the textual labels for the key moment identifier;
and selecting the video frame with the highest similarity measure to generate the image for inclusion in the video anchor.
各ビデオフレームに対して、前記ビデオフレームと、前記ビデオの前記適切なサブセット中の1つまたは複数の他のビデオフレームとの違いを測定する多様性測度を決定するステップと、
前記多様性測度に部分的に基づいてビデオフレームを選択するかどうかを決定するステップと
を含む、請求項12に記載のシステム。 processing each video frame of the appropriate subset of videos to determine whether to select a video frame from the appropriate subset of videos;
determining, for each video frame, a diversity measure that measures the difference between the video frame and one or more other video frames in the appropriate subset of videos;
and determining whether to select video frames based in part on the diversity measure.
前記多様性測度に基づいて、前記ビデオの前記適切なサブセットの前記ビデオフレームが、最小多様性を指定する多様性しきい値を満たすかどうかを決定するステップ
を含む、請求項16に記載のシステム。 determining whether to select a video frame based in part on the diversity measure;
17. The system of claim 16, comprising determining, based on the diversity measure, whether the video frames of the appropriate subset of the video meet a diversity threshold that specifies a minimum diversity. .
前記ビデオフレームで説明されるコンテンツを説明する前記ビデオフレーム用のラベルのセットを決定するステップと、
前記ビデオフレーム用の前記ラベルのセットの、前記キーモーメント識別子用の前記テキストのラベルに対する類似性を測定する類似性測度を決定するステップと、
前記ビデオアンカーに含めるための前記画像を生成するために最も高い類似性測度を有する前記ビデオフレームを選択するステップと
をさらに含む、請求項17に記載のシステム。 selecting one or more video frames from the video frames of the appropriate subset of videos in response to determining that the video frames of the appropriate subset of videos meet the diversity threshold; and for each of the selected video frames,
determining a set of labels for the video frame that describes content illustrated in the video frame;
determining a similarity measure that measures the similarity of the set of labels for the video frame to the text label for the key moment identifier;
18. The system of claim 17, further comprising: selecting the video frame with the highest similarity measure to generate the image for inclusion in the video anchor.
をさらに含む、請求項18に記載のシステム。 of the appropriate subset of the video to generate the images for inclusion in the video anchor in response to a determination that the video frames of the appropriate subset of the video do not meet the diversity threshold; 19. The system of claim 18, further comprising selecting one of the video frames.
をさらに含む、請求項17に記載のシステム。 of the appropriate subset of the video to generate the images for inclusion in the video anchor in response to a determination that the video frames of the appropriate subset of the video do not meet the diversity threshold; 18. The system of claim 17, further comprising the step of not selecting any video frames.
ビデオに対して、複数のキーモーメント識別子を取得する動作であって、各キーモーメント識別子が、
前記ビデオにおける再生時間を指定する時間インデックス値を含み、
前記ビデオ内の顕著なトピックを定義する1つまたは複数の関心基準を満たすと決定された前記ビデオの主題を示す、取得する動作と、
各キーモーメント識別子に対して、
前記時間インデックス値によって指定された前記再生時間に始まる前記ビデオの適切なサブセットを選択する動作であって、前記ビデオの前記適切なサブセットが、前記時間インデックス値によって指定された前記再生時間に始まり、別のキーモーメント識別子の別の時間インデックス値によって指定された次の最も近い再生時間に終わるビデオセグメントの長さよりも短い、選択する動作と、
前記ビデオの前記適切なサブセットについて、前記キーモーメント識別子用のテキストのラベルを決定する動作と、
前記ビデオの前記適切なサブセットからビデオフレームを選択するかどうかを決定するために、前記ビデオの前記適切なサブセットの各ビデオフレームを処理する動作と、
各キーモーメント識別子に対して、ビデオアンカーを生成する動作であって、前記ビデオアンカーが、
前記キーモーメント識別子用の前記テキストのラベルと、
前記ビデオの前記適切なサブセットのビデオフレームを選択する決定に応じて、前記ビデオフレームから生成された画像と、
ユーザデバイス上のビデオプレーヤに、前記キーモーメント識別子の前記時間インデックス値によって指定された前記再生時間に前記ビデオの再生を開始させる命令と
を含む、生成する動作と、
ユーザデバイスにデータを提供する動作であって、前記データが前記ユーザデバイスのビデオプレーヤ環境で前記ユーザデバイスに、
前記ビデオアンカーの各々と、
各ビデオアンカーに対して、前記ビデオプレーヤのプログレスバー内の時間インジケータであって、前記時間インデックス値によって指定された前記再生時間に対応する時間インジケータと、
各ビデオアンカーに対して、前記対応する時間インジケータから前記ビデオアンカーへのビジュアルリンクと
をレンダリングさせる、提供する動作と
を含み、
各ビデオアンカーが、ユーザによって選択可能であり、前記ビデオアンカーが選択されると、前記ビデオアンカーの前記命令が、ユーザデバイス上の前記ビデオプレーヤに、前記時間インデックス値によって指定された前記再生時間の前記ビデオの再生を開始させる、非一時的コンピュータ可読媒体。 a non-transitory computer-readable medium in data communication with a data processing device and storing instructions that, when executed, cause the data processing device to perform an operation; The action is
An operation of obtaining a plurality of key moment identifiers for a video, each key moment identifier being
a time index value specifying a playback time in the video;
an act of obtaining an indication of the subject matter of the video determined to meet one or more interest criteria defining salient topics within the video;
For each key moment identifier,
an act of selecting an appropriate subset of the videos beginning at the playback time specified by the time index value, wherein the appropriate subset of videos begins at the playback time specified by the time index value; an act of selecting a video segment that is less than the length of the video segment ending at the next closest playback time specified by another time index value of another key moment identifier;
an act of determining a textual label for the key moment identifier for the appropriate subset of videos;
an act of processing each video frame of the appropriate subset of the video to determine whether to select a video frame from the appropriate subset of the video;
an act of generating a video anchor for each key moment identifier, the video anchor comprising:
the textual label for the key moment identifier;
an image generated from the video frames in response to a decision to select video frames of the appropriate subset of the video;
instructions for causing a video player on a user device to begin playing the video at the playback time specified by the time index value of the key moment identifier;
an act of providing data to a user device, the data being provided to the user device in a video player environment of the user device;
each of the video anchors;
for each video anchor, a time indicator in a progress bar of the video player, the time indicator corresponding to the playback time specified by the time index value;
for each video anchor, causing and providing a visual link from the corresponding time indicator to the video anchor;
Each video anchor is selectable by the user, and when the video anchor is selected, the instruction of the video anchor causes the video player on the user device to play for the playback time specified by the time index value. A non-transitory computer-readable medium that initiates playback of the video.
各ビデオフレームに対して、前記ビデオフレームで説明されるコンテンツを説明する前記フレーム用のラベルのセットを決定するステップと、
各ビデオフレームに対して、前記ビデオフレーム用の前記ラベルのセットの、前記キーモーメント識別子用の前記テキストのラベルに対する類似性を測定する類似性測度を決定するステップと、
前記ビデオアンカーに含めるために最も高い類似性測度を有する前記ビデオフレームを選択するステップと
を含む、請求項21に記載の非一時的コンピュータ可読媒体。 processing each video frame of the appropriate subset of videos to determine whether to select a video frame from the appropriate subset of videos;
determining, for each video frame, a set of labels for the frame that describe the content described in the video frame;
determining, for each video frame, a similarity measure that measures the similarity of the set of labels for the video frame to the textual labels for the key moment identifier;
and selecting the video frame with the highest similarity measure for inclusion in the video anchor.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2023148643A JP2023165769A (en) | 2019-04-04 | 2023-09-13 | Video timed anchor |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2021559177A JP7350883B2 (en) | 2019-04-04 | 2019-04-04 | video time adjustment anchor |
PCT/GR2019/000029 WO2020201780A1 (en) | 2019-04-04 | 2019-04-04 | Video timed anchors |
JP2023148643A JP2023165769A (en) | 2019-04-04 | 2023-09-13 | Video timed anchor |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021559177A Division JP7350883B2 (en) | 2019-04-04 | 2019-04-04 | video time adjustment anchor |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023165769A true JP2023165769A (en) | 2023-11-17 |
Family
ID=66484097
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021559177A Active JP7350883B2 (en) | 2019-04-04 | 2019-04-04 | video time adjustment anchor |
JP2023148643A Pending JP2023165769A (en) | 2019-04-04 | 2023-09-13 | Video timed anchor |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021559177A Active JP7350883B2 (en) | 2019-04-04 | 2019-04-04 | video time adjustment anchor |
Country Status (4)
Country | Link |
---|---|
US (2) | US11823716B2 (en) |
JP (2) | JP7350883B2 (en) |
KR (2) | KR102574278B1 (en) |
WO (1) | WO2020201780A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210064648A1 (en) * | 2019-08-26 | 2021-03-04 | Koninklijke Philips N.V. | System for automated dynamic guidance for diy projects |
CN113949920A (en) * | 2021-12-20 | 2022-01-18 | 深圳佑驾创新科技有限公司 | Video annotation method and device, terminal equipment and storage medium |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP4227241B2 (en) | 1999-04-13 | 2009-02-18 | キヤノン株式会社 | Image processing apparatus and method |
JP4270118B2 (en) | 2004-11-30 | 2009-05-27 | 日本電信電話株式会社 | Semantic label assigning method, apparatus and program for video scene |
JP4599244B2 (en) | 2005-07-13 | 2010-12-15 | キヤノン株式会社 | Apparatus and method for creating subtitles from moving image data, program, and storage medium |
KR101644789B1 (en) | 2009-04-10 | 2016-08-04 | 삼성전자주식회사 | Apparatus and Method for providing information related to broadcasting program |
US20120263430A1 (en) * | 2011-03-31 | 2012-10-18 | Noah Spitzer-Williams | Bookmarking moments in a recorded video using a recorded human action |
US10204273B2 (en) * | 2015-10-20 | 2019-02-12 | Gopro, Inc. | System and method of providing recommendations of moments of interest within video clips post capture |
US10581857B2 (en) * | 2017-11-09 | 2020-03-03 | International Business Machines Corporation | Controlling access to a host site using a personal identification video |
-
2019
- 2019-04-04 WO PCT/GR2019/000029 patent/WO2020201780A1/en active Application Filing
- 2019-04-04 US US17/601,339 patent/US11823716B2/en active Active
- 2019-04-04 KR KR1020217033182A patent/KR102574278B1/en active IP Right Grant
- 2019-04-04 JP JP2021559177A patent/JP7350883B2/en active Active
- 2019-04-04 KR KR1020237029506A patent/KR20230129616A/en not_active Application Discontinuation
-
2023
- 2023-09-13 JP JP2023148643A patent/JP2023165769A/en active Pending
- 2023-10-17 US US18/488,605 patent/US20240046964A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2020201780A1 (en) | 2020-10-08 |
KR20210136122A (en) | 2021-11-16 |
US11823716B2 (en) | 2023-11-21 |
US20220165309A1 (en) | 2022-05-26 |
US20240046964A1 (en) | 2024-02-08 |
KR102574278B1 (en) | 2023-09-04 |
JP2022529225A (en) | 2022-06-20 |
KR20230129616A (en) | 2023-09-08 |
JP7350883B2 (en) | 2023-09-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10706100B2 (en) | Method of and system for recommending media objects | |
RU2632100C2 (en) | Method and server of recommended set of elements creation | |
RU2632144C1 (en) | Computer method for creating content recommendation interface | |
RU2632132C1 (en) | Method and device for creating contents recommendations in recommendations system | |
US10140368B2 (en) | Method and apparatus for generating a recommendation page | |
JP6938680B2 (en) | Efficient image enhancement with related content | |
JP2023165769A (en) | Video timed anchor | |
WO2017161784A1 (en) | Method and device for displaying recommended contents | |
US20190130185A1 (en) | Visualization of Tagging Relevance to Video | |
KR102148968B1 (en) | System and method for providing context information | |
JP2017021785A (en) | Extraction of knowledge point and relation from teaching material of learning | |
CN111580921B (en) | Content creation method and device | |
RU2609079C2 (en) | Search offer method and processing server | |
WO2017016101A1 (en) | Search result display method, device and search engine | |
CN115168433A (en) | Generating a contextual search presentation | |
US20140089239A1 (en) | Methods, Apparatuses and Computer Program Products for Providing Topic Model with Wording Preferences | |
US20210365164A1 (en) | User interface engagement heatmaps | |
WO2023071491A1 (en) | Encyclopedia information determination method and apparatus, encyclopedia information display method and apparatus, and device and medium | |
JP6162134B2 (en) | Social page trigger | |
CN113449144B (en) | Video processing method and device and electronic equipment | |
CN109791545A (en) | The contextual information of resource for the display including image | |
JP2020021489A (en) | Area-based item recommendation device and method | |
US20180137587A1 (en) | Contextual personalized list of recommended courses | |
US20190138575A1 (en) | Counterpointing system | |
JP6900334B2 (en) | Video output device, video output method and video output program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230921 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20230921 |