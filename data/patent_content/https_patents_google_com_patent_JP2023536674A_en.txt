JP2023536674A - Multi-camera video stabilization - Google Patents
Multi-camera video stabilization Download PDFInfo
- Publication number
- JP2023536674A JP2023536674A JP2022536617A JP2022536617A JP2023536674A JP 2023536674 A JP2023536674 A JP 2023536674A JP 2022536617 A JP2022536617 A JP 2022536617A JP 2022536617 A JP2022536617 A JP 2022536617A JP 2023536674 A JP2023536674 A JP 2023536674A
- Authority
- JP
- Japan
- Prior art keywords
- camera
- video
- image
- capture
- view
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/683—Vibration or motion blur correction performed by a processor, e.g. controlling the readout of an image memory
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6811—Motion detection based on the image signal
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/69—Control of means for changing angle of the field of view, e.g. optical zoom objectives or electronic zooming
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/90—Arrangement of cameras or camera modules, e.g. multiple cameras in TV studios or sports stadiums
Abstract
マルチカメラビデオ安定化のための、コンピュータ記憶媒体上に符号化されたコンピュータプログラムを含む、方法、システム、および装置。いくつかの実現例では、あるビデオキャプチャデバイスは、第１のカメラと第２のカメラとを有する。ビデオキャプチャデバイスは、ビデオ記録中に、あるデジタルズーム範囲内で、ユーザ指定の倍率変更を可能にする、デジタルズーム機能を提供する。ビデオキャプチャデバイスは、デジタルズーム範囲の異なる部分にわたって異なるカメラからのビデオデータを使用するよう構成される。ビデオキャプチャデバイスは、（ｉ）第２のカメラのための正準基準空間への第１の変換と、（ｉｉ）第１のカメラのための正準基準空間への第２の変換と、（ｉｉｉ）第１のカメラのための正準基準空間内の画像データに電子画像安定化を適用するための第３の変換とを含む変換のセットを適用することによって、第２のカメラを使用してキャプチャされた画像データを処理することができる。A method, system, and apparatus, including a computer program encoded on a computer storage medium, for multi-camera video stabilization. In some implementations, a video capture device has a first camera and a second camera. Video capture devices provide digital zoom functionality that allows user-specified magnification changes within a certain digital zoom range during video recording. The video capture device is configured to use video data from different cameras over different parts of the digital zoom range. The video capture device includes: (i) a first transformation to a canonical reference space for the second camera; (ii) a second transformation to the canonical reference space for the first camera; iii) using a second camera by applying a set of transformations comprising: a third transformation for applying electronic image stabilization to the image data in the canonical reference space for the first camera; The captured image data can be processed.
Description
背景
スマートフォンなどの一部のデバイスは、複数のカメラモジュールを含む。これらのカメラは、静止画像またはビデオを記録するために使用され得る。多くの状況において、手振れおよびデバイスの他の動きは、キャプチャされた画像およびビデオの品質を低下させる可能性がある。その結果、一部のデバイスは、記録された画像データの品質を改善するために画像安定化機能を含む。
Background Some devices, such as smartphones, include multiple camera modules. These cameras can be used to record still images or video. In many situations, hand shake and other movements of the device can degrade the quality of captured images and videos. As a result, some devices include image stabilization features to improve the quality of recorded image data.
概要
いくつかの実現例では、あるデバイスは、マルチビューカメラシステム、例えば、異なる視野を有する複数のカメラモジュールを伴うデバイスを含む。このデバイスは、デバイスのカメラモジュールのうちの１つ以上を使用してキャプチャされたビデオのためにビデオ安定化を提供する。これは、異なるズーム（例えば、拡大または倍率）レベルで様々なレベルの画像安定化を提供することと、画像安定化挙動を異なるズームレベルおよび異なるカメラモジュールに対して一貫するように管理することとを含むことができる。カメラモジュールが固定視野を有する場合であっても、デバイスは、例えば、デジタルズーム技術を使用して視野の範囲に沿って連続的または滑らかなズームを実施または近似することによって、ズーム機能を提供することができる。デバイスは、ビデオ記録中に異なるカメラモジュールのビデオキャプチャ間で遷移するときを含むズーム設定の範囲にわたって一貫して安定化されたビデオをキャプチャするための特徴を提供することができる。いくつかの実現例では、デバイスは、ビデオ記録中に画像キャプチャのためにカメラ間をいつ遷移すべきかを検出する。デバイスは、遷移を実行し、キャプチャされたデータを処理して、遷移の期間にわたってシームレスな出力、例えば、視野、画像安定化、露出、焦点、ノイズなどのパラメータを維持または円滑に調整する出力ビデオを生成する。これにより、システムは、異なるカメラからの異なる部分のビデオを、ユーザにとって妨害的でないカメラ間の遷移を伴って、例えば、単眼視差、目に見えるスタッター、目に見えるズームの休止、または他のグリッチなしに用いるビデオを生成することが可能になる。
Overview In some implementations, a device includes a multi-view camera system, eg, a device with multiple camera modules having different fields of view. The device provides video stabilization for videos captured using one or more of the device's camera modules. This includes providing different levels of image stabilization at different zoom (e.g. magnification or magnification) levels and managing image stabilization behavior consistently for different zoom levels and different camera modules. can include Even if the camera module has a fixed field of view, the device may provide zoom functionality, for example by using digital zoom techniques to perform or approximate a continuous or smooth zoom along the range of the field of view. can be done. The device can provide features for capturing consistently stabilized video across a range of zoom settings, including when transitioning between video captures of different camera modules during video recording. In some implementations, the device detects when to transition between cameras for image capture during video recording. The device executes the transition and processes the captured data to maintain or smooth out seamless output over the duration of the transition, e.g. parameters such as field of view, image stabilization, exposure, focus, noise, etc. Output video to generate This allows the system to display different portions of video from different cameras with transitions between cameras that are non-intrusive to the user, such as monocular parallax, visible stutter, visible zoom pauses, or other glitches. It is possible to generate videos that are used without
このシステムの主な目標は、ユーザに提示されるビデオシーンの円滑さを向上させることである。言い換えれば、このシステムは、シーンの時間的連続性（例えば、経時的な望ましくないカメラブレを低減すること）とシーンの空間的連続性（例えば、異なるカメラからキャプチャされたビデオ間の差を低減すること）との両方を達成しようと試みる。これは、２つの重要な技術、すなわち、（１）単一のカメラ上で経時的にシーンの連続性を提供して、ビデオフィード内に示されるシーンの時間的平滑化を効果的に提供する電子画像安定化（ＥＩＳ）、および（２）空間的平滑化方法を効果的に提供してカメラ間の遷移付近の混乱または中断を回避しながら異なるカメラ間でシーンの連続性を提供する、強化されたマルチカメラデジタルズーム（例えば、複数のカメラの出力を使用する漸次的、漸増的、または実質的に連続的なズーム）を伴う。 The main goal of this system is to improve the smoothness of the video scenes presented to the user. In other words, the system improves the temporal continuity of the scene (e.g., reducing unwanted camera shake over time) and the spatial continuity of the scene (e.g., reducing differences between videos captured from different cameras). ) and attempt to achieve both. It has two important techniques: (1) it provides continuity of the scene over time on a single camera, effectively providing temporal smoothing of the scene shown within the video feed; Electronic Image Stabilization (EIS), and (2) enhancements that effectively provide spatial smoothing methods to provide scene continuity between different cameras while avoiding clutter or interruptions near transitions between cameras. multi-camera digital zoom (eg, gradual, incremental, or substantially continuous zoom using the outputs of multiple cameras).
ＥＩＳおよびマルチカメラデジタルズームは、画像データを表現するために「正準」カメラ空間の使用を含む、以下でさらに論じる様々な技術および変換を使用することにより、効率的に組み合わせることができる。正準空間は、経時的に変化しない固定された固有特性を有する概念的なカメラのビューを表すことができる。例えば、正準空間は、光学画像安定化（ＯＩＳ）またはボイスコイルモータ（ＶＣＭ）位置およびローリングシャッタ効果等の要因によって影響されないものであり得る。 EIS and multi-camera digital zoom can be efficiently combined by using various techniques and transformations discussed further below, including the use of "canonical" camera space to represent the image data. A canonical space can represent a conceptual camera view with fixed intrinsic properties that do not change over time. For example, the canonical space may be unaffected by factors such as optical image stabilization (OIS) or voice coil motor (VCM) position and rolling shutter effects.
例として、あるデバイスは、あるシーンの異なる視野を提供する、第１のカメラおよび第２のカメラを含むことができる。本デバイスは、カメラモジュールのうちの１つ以上が固定された視野を有する場合であっても、ユーザが出力ビデオによって表されるズームまたは倍率を滑らかに変更することを可能にするズーム機能を可能にすることができる。いくつかの実現例では、固定焦点距離レンズを有する２つ以上のカメラモジュールを用いて、（１）あるズーム範囲の第１の部分に対して第１のカメラからの画像に基づくデジタルズーム（例えば、クロッピングおよび／または拡大）を使用し、（２）同ズーム範囲の第２の部分に対して第２のカメラからの画像に基づくデジタルズームを使用することによって、同範囲にわたって連続ズームをシミュレートすることができる。ビデオの全体的な品質を向上させるために、適用されるデジタルズームの現在のレベルに対して、画像安定化処理を動的に調整することができる。例えば、シミュレートされた連続ズーム範囲に沿ったズームの各変化は、画像安定化パラメータにおいて、対応する変化を有することができる。 As an example, a device may include a first camera and a second camera that provide different views of a scene. The device enables a zoom function that allows the user to smoothly change the zoom or magnification represented by the output video, even if one or more of the camera modules have a fixed field of view. can be Some implementations use two or more camera modules with fixed focal length lenses to (1) digitally zoom based on images from a first camera for a first portion of a zoom range (e.g., (2) using digital zoom based on images from a second camera for a second portion of the same zoom range to simulate a continuous zoom over the same range. can be done. The image stabilization process can be dynamically adjusted to the current level of digital zoom applied to improve the overall quality of the video. For example, each change in zoom along the simulated continuous zoom range can have a corresponding change in the image stabilization parameter.
デバイスによる画像処理は、ＥＩＳアプリケーションおよび焦点距離、露出などの他の画像キャプチャ態様の一貫性を維持しながら、異なるカメラからの画像キャプチャ間の遷移を管理して、実質的にシームレスな遷移を提供することができる。ズーム機能は、例えば、デバイスが第１のカメラによってキャプチャされた画像上でますますクロッピングするにつれて第１のカメラの出力を使用するデジタルズームを含むことができる。次いで、ズームの閾値レベルに達し、ズームインされるエリアが第２のカメラの視野内になると、デバイスは、第２のカメラを使用してキャプチャされるビデオを記録することに切り替わる。 Image processing by the device manages the transition between image captures from different cameras to provide a virtually seamless transition while maintaining consistency of EIS applications and other image capture aspects such as focal length, exposure, etc. can do. A zoom function may include, for example, a digital zoom that uses the output of the first camera as the device crops more and more on the image captured by the first camera. Then, when the zoom threshold level is reached and the area zoomed in is within the field of view of the second camera, the device switches to recording video captured using the second camera.
記録されたビデオが異なるカメラの出力間で滑らかな遷移を提供するようにするために、デバイスは、一連の変換を使用して、第２のカメラの出力を第１のカメラの出力に関連付けることができる。これらの変換は、ホモグラフィ行列を使用して、または他の形態で実現されてもよい。いくつかの実現例では、変換は、第２のカメラからローリングシャッタ効果、ＯＩＳレンズの動き等のカメラ特有の時間依存寄与を除去することによって、第２のカメラからの画像を正準カメラ空間にマッピングすることを伴う。第２の変換は、第２のカメラの正準画像空間内の画像データを第１のカメラの正準画像空間に投影することができる。これは、第２のカメラの視野を第１のカメラの視野と整列させ、デバイス内のカメラ間の空間的差異（例えば、オフセット）を考慮することができる。次いで、電子画像安定化（ＥＩＳ）処理を、第１のカメラの正準画像空間内の画像データに適用することができる。この一連の変換は、例えば、ＥＩＳ処理された第２のカメラ画像データをＥＩＳ処理された第１のカメラ画像データと関連付けおよび整列させようとするよりもはるかに効率的な処理技術を提供する。ＥＩＳ処理は、画像が画像キャプチャ中に異なる視野、異なる固有特性などを有する異なるカメラを使用してキャプチャされているにもかかわらず、単一のカメラ空間または基準フレームにおいて実行され得る。次いで、第１のカメラの正準画像空間におけるＥＩＳ処理の出力は、（ローカルおよび／もしくはリモートで）ビデオファイルとして記憶するために提供することができ、ならびに／または（ローカルおよび／もしくはリモートで）表示のためにストリーミングすることができる。 To ensure that the recorded video provides smooth transitions between different camera outputs, the device uses a series of transformations to relate the output of the second camera to the output of the first camera. can be done. These transformations may be implemented using homography matrices or in some other form. In some implementations, the transformation brings the image from the second camera into canonical camera space by removing camera-specific time-dependent contributions such as rolling shutter effects, OIS lens movements, etc. from the second camera. It involves mapping. A second transform may project the image data in the canonical image space of the second camera into the canonical image space of the first camera. This aligns the field of view of the second camera with the field of view of the first camera and can account for spatial differences (eg, offsets) between the cameras within the device. An electronic image stabilization (EIS) process can then be applied to the image data in the canonical image space of the first camera. This sequence of transformations provides a much more efficient processing technique than, for example, trying to associate and align EIS processed second camera image data with EIS processed first camera image data. EIS processing can be performed in a single camera space or frame of reference, even though the images are captured using different cameras with different fields of view, different intrinsic properties, etc. during image capture. The output of the EIS processing in the canonical image space of the first camera can then be provided (locally and/or remotely) for storage as a video file and/or (locally and/or remotely) Can be streamed for display.
これらの技術は、手振れおよび他の意図しないカメラの動きをより効果的に制御するために、現在のズームレベルに合わせて調整された画像安定化のレベルを適用することができる。加えて、ビデオキャプチャ中にカメラ間を滑らかに遷移させる能力は、中断的な遷移なしにビデオキャプチャの解像度を高めることができる。例えば、より広い視野を有するカメラの出力にデジタルズームがますます適用されるにつれて、解像度は低下する傾向がある。デジタルズームが増加するにつれて、結果として生じる出力画像は、画像センサのより小さい部分を表し、したがって、出力画像を生成するために画像センサのより少ない画素を使用する。第２のカメラは、より狭い視野を有するレンズを有することができ、より狭い視野が画像センサの全体でキャプチャされることを可能にする。出力フレームが第２のカメラの視野内に入る点までビデオがズームインされると、カメラは、ビデオキャプチャを、第２のカメラによってキャプチャされる画像データを使用することに遷移させることができる。したがって、ビデオファイルのキャプチャおよび記録を継続しながら、ＥＩＳが継続的かつ一貫して適用されている状態で、カメラは、実質的にシームレスな態様でビデオキャプチャのために異なるカメラを使用することを切り替えることができる。切り替えは、ユーザに対して透明にすることができ、したがって、カメラ間の切り替えは、キャプチャされたビデオ映像において、または任意選択的にユーザのためのユーザインタフェースにおいて、目立たない。 These techniques can apply a level of image stabilization tailored to the current zoom level to more effectively control camera shake and other unintended camera movements. In addition, the ability to smoothly transition between cameras during video capture can increase the resolution of video capture without interrupting transitions. For example, as digital zoom is increasingly applied to the output of cameras with wider fields of view, resolution tends to decrease. As the digital zoom increases, the resulting output image represents a smaller portion of the image sensor, thus using fewer pixels of the image sensor to generate the output image. The second camera may have a lens with a narrower field of view, allowing a narrower field of view to be captured across the image sensor. Once the video is zoomed in to the point where the output frame is within the field of view of the second camera, the camera can transition video capture to using image data captured by the second camera. Thus, with continued and consistent application of EIS while still capturing and recording video files, the camera encourages the use of different cameras for video capture in a substantially seamless manner. You can switch. Switching can be transparent to the user, so that switching between cameras is unobtrusive in the captured video footage or optionally in the user interface for the user.
概して、ビデオキャプチャおよび関連する画像処理を実行するプロセスは、特に高解像度ビデオキャプチャの場合、計算的に高価であり得る。本明細書で論じられる技術は、他の技術の中でもとりわけ、両方のカメラの画像データを単一の共通基準空間にマッピングした後に同じタイプのＥＩＳ処理をその基準空間内の画像データに適用することによって、カメラモジュール間の画像安定化処理および遷移を管理するための計算効率的な技術を提供する。基準空間は、時間依存の影響が除去されたものであってもよく、これは、異なるカメラからの画像を整列させることおよびＥＩＳ処理を適用することのために必要とされる計算をさらに低減する。 Generally, the process of performing video capture and associated image processing can be computationally expensive, especially for high resolution video capture. The techniques discussed herein, among other techniques, map the image data of both cameras into a single common reference space and then apply the same type of EIS processing to the image data in that reference space. provides a computationally efficient technique for managing image stabilization processing and transitions between camera modules. The reference space may have time-dependent effects removed, which further reduces the computation required for aligning images from different cameras and applying EIS processing. .
いくつかの実現例では、本明細書で説明する技術は、電話、タブレットコンピュータ、および他のモバイルデバイスなど、電力予算が限られ、計算リソースが限られた電池式デバイス上で実現される。議論される処理は、デバイスによってバッテリ電力上で効率的に行われることができ、安定化処理を、実質的にリアルタイムで、進行中のビデオキャプチャと同時に、例えば、ビデオキャプチャが継続するにつれて、画像安定化処理されたビデオ出力が保存またはストリーミングされる状態で、実行する。また、以下で説明するように、本技術は、たとえば、焦点、露出などのためにカメラモジュール設定を調整するために、およびどのカメラモジュールが異なる時間に使用されるかを切り替えるために、カメラモジュールを使用するビデオのキャプチャを調整することができる。これらの技術はまた、追加のビデオがキャプチャされ続けている間にビデオがキャプチャされ、処理され、記録されるときにリアルタイムで実行され得るように、効率的に実行される。 In some implementations, the techniques described herein are implemented on battery-operated devices with limited power budgets and limited computational resources, such as phones, tablet computers, and other mobile devices. The discussed processing can be efficiently performed by the device on battery power, and the stabilization processing can be performed substantially in real-time, concurrently with ongoing video capture, e.g., as video capture continues. Run with the stabilized video output stored or streamed. Also, as described below, the present technology can be used to adjust camera module settings, e.g., for focus, exposure, etc., and to switch which camera module is used at different times. You can adjust the capture of the video you use. These techniques are also efficiently performed so that they can be performed in real-time as video is captured, processed, and recorded while additional video continues to be captured.
１つの一般的な局面において、ある方法は、第１のカメラおよび第２のカメラを有するビデオキャプチャデバイスが、ビデオ記録中にデジタルズーム範囲内でユーザ指定の倍率変更を可能にするデジタルズーム機能を提供することを含み、ビデオキャプチャデバイスは、（ｉ）デジタルズーム範囲の第１の部分にわたって第１のカメラによってキャプチャされたビデオデータを使用し、（ｉｉ）デジタルズーム範囲の第２の部分にわたって第２のカメラによってキャプチャされたビデオデータを使用するよう構成され、本方法はさらに、デジタルズーム範囲の第２の部分においてあるズームレベルを提供するためにビデオキャプチャデバイスの第２のカメラを使用してビデオをキャプチャする間に、（ｉ）第２のカメラのための第２の正準基準空間への第１の変換と、（ｉｉ）第１のカメラのための第１の正準基準空間への第２の変換と、（ｉｉｉ）第１のカメラのための第１の正準基準空間内の画像データに電子画像安定化を適用するための第３の変換と含む変換のセットを適用することによって、第２のカメラを使用してキャプチャされた画像データを処理することを含む。 In one general aspect, a method provides a digital zoom feature that allows a video capture device having a first camera and a second camera to allow user-specified magnification changes within a digital zoom range during video recording. wherein the video capture device uses video data captured by (i) a first camera over a first portion of the digital zoom range; and (ii) by a second camera over a second portion of the digital zoom range. configured to use the captured video data, the method further comprising: while capturing video using a second camera of the video capture device to provide a zoom level in a second portion of the digital zoom range; (i) a first transformation to a second canonical reference space for the second camera, and (ii) a second transformation to the first canonical reference space for the first camera and (iii) a third transform for applying electronic image stabilization to the image data in the first canonical reference space for the first camera. including processing image data captured using the camera of
いくつかの実現例では、本方法は、デジタルズーム範囲の第１の部分において、あるズームレベルを提供するために、ビデオキャプチャデバイスの第１のカメラを使用してビデオをキャプチャする間に、（ｉ）第１のカメラのための第１の正準基準空間への変換と、（ｉｉ）第１のカメラのための第１の正準基準空間内のデータに電子画像安定化を適用するための変換とを含む変換のセットを適用することによって、第２のカメラを使用してキャプチャされた画像データを処理することを含む。 In some implementations, the method performs (i ) transforming to a first canonical reference space for the first camera; and (ii) applying electronic image stabilization to the data in the first canonical reference space for the first camera. and processing the image data captured using the second camera by applying a set of transforms.
いくつかの実現例では、第１のカメラおよび第２のカメラは異なる視野を有し、（ｉ）第２のカメラの視野は第１のカメラの視野内に含まれるか、または（ｉｉ）第１のカメラの視野は第２のカメラの視野内に含まれる。 In some implementations, the first camera and the second camera have different fields of view, and (i) the field of view of the second camera is contained within the field of view of the first camera, or (ii) the field of view of the second camera The field of view of one camera is contained within the field of view of the second camera.
いくつかの実現例では、第１のカメラおよび第２のカメラは各々、固定焦点距離レンズアセンブリを含む。 In some implementations, the first camera and the second camera each include a fixed focal length lens assembly.
いくつかの実現例では、第２のカメラのための正準基準空間および第１のカメラのための正準基準空間は、画像データを正準基準空間に投影することがビデオフレームのキャプチャ中の時間依存の影響を除去するように、所定の固定された組のカメラ固有特性によって定義される概念的カメラ空間である。 In some implementations, the canonical reference space for the second camera and the canonical reference space for the first camera are such that projecting image data into the canonical reference space is A conceptual camera space defined by a predetermined fixed set of camera-specific properties so as to remove time-dependent effects.
いくつかの実現例では、第１のカメラは、光学画像安定化（ＯＩＳ）システムを含み、第１のカメラのための第１の正準基準空間は、画像データが、一貫した所定のＯＩＳ位置を有して表されるものである。 In some implementations, the first camera includes an optical image stabilization (OIS) system, and the first canonical reference space for the first camera is such that image data is consistent with predetermined OIS locations. is represented by
いくつかの実現例では、第２のカメラは、光学画像安定化（ＯＩＳ）システムを含み、第２のカメラのための第２の正準基準空間は、画像データが、一貫した所定のＯＩＳ位置を有して表されるものである。 In some implementations, the second camera includes an optical image stabilization (OIS) system, and the second canonical reference space for the second camera is such that image data is consistent with predetermined OIS positions. is represented by
いくつかの実現例では、第１のカメラは、画像フレームの画像走査線を漸進的にキャプチャする画像データを提供し、第１のカメラのための第１の正準基準空間は、画像データが画像走査線について漸進的キャプチャによる歪みを除去するように補正されているものである。 In some implementations, the first camera provides image data that progressively captures image scanlines of image frames, and the first canonical reference space for the first camera is The image scan lines have been corrected to remove distortion due to gradual capture.
いくつかの実現例では、第２のカメラは、画像フレームの画像走査線を漸進的にキャプチャする画像データを提供し、第２のカメラのための第２の正準基準空間は、画像データが画像走査線について漸進的キャプチャによる歪みを除去するように補正されているものである。 In some implementations, a second camera provides image data that progressively captures image scanlines of image frames, and a second canonical reference space for the second camera is defined by the image data The image scan lines have been corrected to remove distortion due to gradual capture.
いくつかの実現例では、第２の変換は、第２のカメラの視野を第１のカメラの視野に整列させ、第１のカメラと第２のカメラとの間の空間オフセットについて調整する。 In some implementations, the second transform aligns the field of view of the second camera with the field of view of the first camera and adjusts for the spatial offset between the first and second cameras.
いくつかの実現例では、第１の変換、第２の変換、および第３の変換は各々、対応するホモグラフィ行列を有し、画像データを処理することは、ホモグラフィ行列を適用することを含む。 In some implementations, the first transform, the second transform, and the third transform each have a corresponding homography matrix, and processing the image data includes applying the homography matrix. include.
いくつかの実現例では、本方法は、第１のカメラを使用するビデオデータのキャプチャ、および電子画像安定化を適用するための第１のカメラからのビデオデータの処理中に、デジタルズーム範囲の第２の部分における特定のズームレベルへのズームレベルの変化を示すユーザ入力を受信することを含む。本方法は、ユーザ入力を受信することに応じて、所定のズームレベルに達するまで、第１のカメラを使用してキャプチャされるビデオフレームの倍率が漸増的に増加される、ビデオフレームのシーケンスを記録することと、第２のカメラを使用してビデオキャプチャを開始することと、第２のカメラを使用してキャプチャされるビデオフレームの第２のシーケンスを記録することとを含み、ビデオフレームの第２のシーケンスは、所定のズームレベルを提供し、特定のズームレベルに達するまで、第２のカメラを使用してキャプチャされるビデオフレームの、増加する倍率を提供する。 In some implementations, the method includes, during capture of video data using a first camera and processing of video data from the first camera to apply electronic image stabilization, a first digital zoom range. receiving user input indicating a change in zoom level to a particular zoom level in part 2; In response to receiving user input, the method produces a sequence of video frames in which the magnification of video frames captured using the first camera is incrementally increased until a predetermined zoom level is reached. recording, initiating video capture using a second camera, and recording a second sequence of video frames captured using the second camera; A second sequence provides a predetermined zoom level and provides increasing magnification of the video frames captured using the second camera until the specified zoom level is reached.
いくつかの実現例では、第２の変換は、第２のカメラの焦点距離に少なくとも部分的に基づいて決定される。 In some implementations, the second transform is determined based at least in part on the focal length of the second camera.
いくつかの実現例では、第１の変換は、第２のカメラを使用してキャプチャされる画像データの異なる走査線に対する複数の異なる調整を含む。 In some implementations, the first transformation includes multiple different adjustments for different scanlines of image data captured using the second camera.
いくつかの実現例では、第３の変換は、ビデオフレームのために、ビデオフレームのうちの特定のビデオフレームごとに、特定のビデオフレームの前の１つ以上のビデオフレームと特定のビデオフレームの後の１つ以上のビデオフレームとを使用する電子画像安定化を含む。 In some implementations, the third transform includes, for a video frame, for each particular video frame of the video frames, one or more video frames before the particular video frame and the electronic image stabilization using one or more subsequent video frames;
いくつかの実現例では、第２のカメラは、第１のカメラより小さい視野を有する。本方法は、第１のカメラを使用する画像キャプチャ中に、ビデオキャプチャに対してズームレベルの変更を示すユーザ入力を受信することと、ユーザ入力を受信することに応じて、変更されたズームレベルが所定の遷移ズームレベル以上であるか否かを判定することとを含み、所定の遷移ズームレベルは、第２のカメラの視野よりも小さい視野を表す。 In some implementations, the second camera has a smaller field of view than the first camera. The method comprises: receiving user input indicating a change in zoom level for video capture during image capture using a first camera; is greater than or equal to a predetermined transitional zoom level, the predetermined transitional zoom level representing a field of view that is smaller than the field of view of the second camera.
いくつかの実現例においては、本方法は、（ｉ）第１のカメラを使用するビデオキャプチャから第２のカメラを使用するビデオキャプチャに遷移するための第１の遷移ズームレベルと、（ｉｉ）第２のカメラを使用するビデオキャプチャから第１のカメラを使用するビデオキャプチャに遷移するための第２の遷移ズームレベルとを示すデータを記憶することを含み、第１の遷移ズームレベルは第２の遷移ズームレベルとは異なり、本方法はさらに、（ｉ）要求されたズームレベルが視野の減少に対応するとき、要求されたズームレベルを第１の遷移ズームレベルと比較することと、（ｉｉ）要求されたズームレベルが視野の増加に対応するとき、要求されたズームレベルを第２の遷移ズームレベルと比較することとによって、ビデオキャプチャのためにカメラ間で切り替えるかどうかを決定することを含む。 In some implementations, the method includes (i) a first transition zoom level for transitioning from video capture using a first camera to video capture using a second camera; and (ii) a second transition zoom level for transitioning from video capture using the second camera to video capture using the first camera, wherein the first transition zoom level is the second transition zoom level; , the method further includes (i) comparing the requested zoom level to a first transitional zoom level when the requested zoom level corresponds to a reduced field of view; and (ii ) determining whether to switch between cameras for video capture by comparing the requested zoom level to a second transitional zoom level when the requested zoom level corresponds to an increase in field of view; include.
いくつかの実現例では、第１の遷移ズームレベルは、第２の遷移ズームレベルよりも小さい視野に対応する。 In some implementations, the first transitional zoom level corresponds to a smaller field of view than the second transitional zoom level.
いくつかの実現例においては、本方法は、ビデオファイルの記録中に、カメラのうちの特定のカメラを使用してビデオをキャプチャすることから、カメラのうちの別のカメラを使用してビデオをキャプチャすることに切り替えることを決定することと、切り替えることを決定することに応答して、特定のカメラを使用する画像キャプチャのために使用されているビデオキャプチャパラメータの値を判断することと、判断されたビデオキャプチャパラメータに基づいて、別のカメラに対するビデオキャプチャパラメータの値を設定することと、別のカメラに対するビデオキャプチャパラメータの値を設定した後、第２のカメラからのビデオキャプチャを開始し、第２のカメラからのキャプチャされたビデオをビデオファイルに記録することとを含む。ビデオキャプチャパラメータを設定することは、第２のカメラについて、露出、画像センサ感度、ゲイン、画像キャプチャ時間、開口サイズ、レンズ焦点距離、ＯＩＳステータス、またはＯＩＳレベルのうちの１つ以上を調整することを含む。 In some implementations, the method moves from capturing video using a particular one of the cameras to capturing video using another of the cameras during recording of the video file. determining to switch to capturing; and responsive to determining to switch, determining values of video capture parameters being used for image capture using a particular camera; setting values of video capture parameters for another camera based on the determined video capture parameters; starting video capture from the second camera after setting the values of the video capture parameters for the another camera; and recording the captured video from the second camera to a video file. Setting video capture parameters includes adjusting one or more of exposure, image sensor sensitivity, gain, image capture time, aperture size, lens focal length, OIS status, or OIS level for the second camera. including.
この局面の他の実施形態および本明細書で論じられる他の実施形態は、コンピュータストレージデバイス上に符号化された、方法の動作を実行するよう構成された、対応するシステム、装置、およびコンピュータプログラムを含む。１つ以上のコンピュータまたは他のデバイスのシステムは、動作中にシステムにアクションを実行させる、システムにインストールされたソフトウェア、ファームウェア、ハードウェア、またはそれらの組合せによってそのように構成されることができる。１つ以上のコンピュータプログラムは、データ処理装置によって実行されるとデータ処理装置にアクションを実行させる命令を有することによってそのように構成されることができる。 Other embodiments of this aspect and other embodiments discussed herein provide corresponding systems, apparatus, and computer programs encoded on computer storage devices and configured to perform the acts of the methods. including. A system of one or more computers or other devices can be so configured by software, firmware, hardware, or combinations thereof installed on the system that cause the system to perform actions during operation. One or more computer programs may be so configured by having instructions which, when executed by a data processing apparatus, cause the data processing apparatus to perform actions.
別の一般的な態様では、１つ以上の機械可読媒体は、１つ以上のプロセッサによって実行されると動作の実行を引き起こす命令を記憶し、上記動作は、第１のカメラおよび第２のカメラを有するビデオキャプチャデバイスが、ビデオ記録中にデジタルズーム範囲内でユーザ指定の倍率変更を可能にするデジタルズーム機能を提供することを含み、ビデオキャプチャデバイスは、（ｉ）デジタルズーム範囲の第１の部分にわたって第１のカメラによってキャプチャされたビデオデータを使用し、（ｉｉ）デジタルズーム範囲の第２の部分にわたって第２のカメラによってキャプチャされたビデオデータを使用するよう構成され、上記動作はさらに、デジタルズーム範囲の第２の部分において、あるズームレベルを提供するために、ビデオキャプチャデバイスの第２のカメラを使用してビデオをキャプチャする間に、（ｉ）第２のカメラのための第２の正準基準空間への第１の変換と、（ｉｉ）第１のカメラのための第１の正準基準空間への第２の変換と、（ｉｉｉ）第１のカメラのための第１の正準基準空間内の画像データに電子画像安定化を適用するための第３の変換と含む変換のセットを適用することによって、第２のカメラを使用してキャプチャされた画像データを処理することを含む。 In another general aspect, one or more machine-readable media store instructions that, when executed by one or more processors, cause execution of actions, the actions being performed by the first camera and the second camera. provides a digital zoom capability that allows user-specified magnification changes within the digital zoom range during video recording, the video capture device comprising: (i) a first zoom range over a first portion of the digital zoom range; configured to use video data captured by one camera and (ii) use video data captured by a second camera over a second portion of the digital zoom range; In part 2, while capturing video using a second camera of the video capture device, to provide a certain zoom level, (i) a second canonical reference space for the second camera; (ii) a second transformation to the first canonical reference space for the first camera; and (iii) the first canonical reference space for the first camera. processing the image data captured using the second camera by applying a set of transforms including a third transform for applying electronic image stabilization to the image data within.
別の全般的な局面では、ビデオキャプチャデバイスは、第１の視野を有する第１のカメラと、第２の視野を有する第２のカメラと、１つ以上の位置または向きセンサと、１つ以上のプロセッサと、１つ以上のプロセッサによって実行されると動作の実行を引き起こす命令を記憶する１つ以上のデータ記憶装置とを備え、上記動作は、第１のカメラおよび第２のカメラを有するビデオキャプチャデバイスが、ビデオ記録中にデジタルズーム範囲内でユーザ指定の倍率変更を可能にするデジタルズーム機能を提供することを含み、ビデオキャプチャデバイスは、（ｉ）デジタルズーム範囲の第１の部分にわたって第１のカメラによってキャプチャされたビデオデータを使用し、（ｉｉ）デジタルズーム範囲の第２の部分にわたって第２のカメラによってキャプチャされたビデオデータを使用するよう構成され、上記動作はさらに、デジタルズーム範囲の第２の部分において、あるズームレベルを提供するために、ビデオキャプチャデバイスの第２のカメラを使用してビデオをキャプチャする間に、（ｉ）第２のカメラのための第２の正準基準空間への第１の変換と、（ｉｉ）第１のカメラのための第１の正準基準空間への第２の変換と、（ｉｉｉ）第１のカメラのための第１の正準基準空間内の画像データに電子画像安定化を適用するための第３の変換と含む変換のセットを適用することによって、第２のカメラを使用してキャプチャされた画像データを処理することを含む。 In another general aspect, a video capture device includes a first camera having a first field of view, a second camera having a second field of view, one or more position or orientation sensors, and one or more and one or more data storage devices storing instructions that, when executed by the one or more processors, cause execution of actions, the actions being performed by a video camera having a first camera and a second camera. The capture device includes a digital zoom capability that allows user-specified magnification changes within the digital zoom range during video recording, the video capture device comprising: (i) the first camera over a first portion of the digital zoom range; and (ii) using video data captured by a second camera over a second portion of the digital zoom range, the operation further comprising: In order to provide a certain zoom level, while capturing video using a second camera of the video capture device, (i) the second camera into a second canonical reference space for the second camera; (ii) a second transformation to the first canonical reference space for the first camera; and (iii) the image in the first canonical reference space for the first camera. Processing the image data captured using the second camera by applying a set of transforms including a third transform for applying electronic image stabilization to the data.
いくつかの実現例では、第１のカメラおよび第２のカメラは異なる視野を有し、第２のカメラの視野は第１のカメラの視野内に含まれる。第１のカメラおよび第２のカメラは各々、固定焦点距離レンズアセンブリを含むことができる。 In some implementations, the first camera and the second camera have different fields of view, and the field of view of the second camera is contained within the field of view of the first camera. The first camera and the second camera can each include fixed focal length lens assemblies.
本発明の１つ以上の実施形態の詳細は、添付の図面および以下の説明に記載される。本発明の他の特徴および利点は、説明、図面、および特許請求の範囲から明らかになるであろう。 The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features and advantages of the invention will become apparent from the description, drawings, and claims.
様々な図面における同様の参照番号および名称は、同様の要素を示す。
詳細な説明
図１Ａ～図１Ｂは、マルチカメラビデオ安定化を提供するデバイス１０２の例を示す図である。ビデオ安定化は、モバイルデバイスのカメラシステムにとって重要な特徴であることが多い。従来の単一カメラビデオ安定化では、ビデオフレームは、振れ易い実カメラ軌道中に実際にキャプチャされた画像データから、時間平滑化された仮想カメラ軌道のための安定化された出力に変換することができる。単一カメラでは、実カメラ軌道中に取得されたフレームは、平滑化された仮想カメラ軌道に沿ったフレームを表す変更された出力フレームに投影され得る。
Like reference numbers and designations in the various drawings indicate like elements.
DETAILED DESCRIPTION FIGS. 1A-1B are diagrams illustrating an
いくつかのデバイスは、同じシーンの異なる視野をキャプチャするように配置された、同じ方向を向く（例えば、デバイスの同じ側にある）複数のカメラを含む。マルチカメラシステムでは、デバイスは、単一のビデオが、異なるカメラを使用してキャプチャされたセグメントを含むように、ビデオをキャプチャする過程でカメラを切り替えてもよい。カメラ間の切り替えが発生すると、異なる実カメラが使用されているので、単一の実カメラ軌道はもはや存在しない。それにもかかわらず、デバイスは、カメラ間の遷移の期間中であっても、キャプチャされた映像の連続性および滑らかさを維持するために、同じ仮想カメラ軌道を維持すべきである。本明細書で論じられるように、仮想カメラ軌道は、複数のカメラ間で維持され、カメラ間の切り替えが気を散らしたりまたは中断効果（例えば、スタッター、画像オフセットまたは視野の急激な変化、安定化の中断または適用される安定化レベルの急激な変化など）を引き起こさないように、経時的に平滑化されることができる。 Some devices include multiple cameras pointing in the same direction (eg, on the same side of the device) arranged to capture different views of the same scene. In a multi-camera system, the device may switch cameras during the process of capturing video so that a single video contains segments captured using different cameras. When a switch between cameras occurs, there is no longer a single real camera trajectory since different real cameras are being used. Nevertheless, the device should maintain the same virtual camera trajectory, even during transitions between cameras, to maintain continuity and smoothness of the captured video. As discussed herein, a virtual camera trajectory is maintained across multiple cameras such that switching between cameras may cause distracting or disruptive effects (e.g., stutter, image offset or abrupt changes in field of view, stabilization). , or abrupt changes in the applied stabilization level, etc.).
マルチカメラビデオ安定化システムは、様々な利益を提供するように実現され得る。経時的なビデオの平滑化に加えて、異なるカメラのビデオキャプチャの期間と期間との間の遷移も平滑化することができる。加えて、本技術は、たとえば、継続的なビデオキャプチャと同時に、キャプチャされたビデオに平滑化を適用するために、リアルタイムで動作するのに、およびバッテリ動作デバイスによる長期使用を可能にするために電力を節約するのに、充分に効率的であり得る。 A multi-camera video stabilization system can be implemented to provide various benefits. In addition to smoothing video over time, transitions between periods of different camera video captures can also be smoothed. In addition, the technology can be used, for example, to apply smoothing to captured video concurrently with continuous video capture, to operate in real-time, and to enable long-term use by battery-operated devices. It can be efficient enough to save power.
図１Ａの例では、デバイス１０２は電話として示されているが、タブレットコンピュータ、カメラなどの別のタイプのデバイスであってもよい。デバイス１０２は、第１のカメラ１１０ａおよび第２のカメラ１１０ｂを含むマルチカメラモジュール１０８を含む。２つのカメラ１１０ａ、１１０ｂは、デバイス１０２の同じ側に配置され、したがって、両方とも、（例えば、図示の電話機の背面において、）デバイス１０２に面する同じシーン１０５の画像をキャプチャするように配置される。カメラ１１０ａ、１１０ｂは、カメラ１１０ａ、１１０ｂがデバイス１０２と同じ程度および同じ動きで共に移動するよう、デバイス１０２に共にしっかりと結合され得る。
In the example of FIG. 1A,
カメラは異なる視野を有する。例えば、第１のカメラ１１０ａは、第１の視野１２０を有し、第２のカメラ１１０ｂは、第１の視野１２０よりも狭い第２の視野１２２を有する。視野１２０、１２２は著しく異なってもよいが、２つのカメラ１１０ａ、１１０ｂは同様の画像解像度を有してもよい。２つの視野１２０、１２２は重なり合うことができる。特に、第２のカメラ１１０ｂの視野１２２は、第１のカメラ１１０ａの視野１２０内に完全に含まれることができる。例として、第１の視野１２０は７７度であってもよく、第２の視野１２２は５２度であってもよく、５２度の視野１２２は、ほとんどまたは完全に７７度の視野１２０内にある。この例では、カメラ１１０ａ、１１０ｂは各々、固定焦点距離レンズ、例えば光学ズームなしのレンズアセンブリを使用する。言い換えれば、各カメラ１１０ａ、１１０ｂのレンズ焦点距離は、フォーカスブリージングなどの焦点関連効果とは別に、固定されてもよい。
The cameras have different fields of view. For example, a
カメラ１１０ａ、１１０ｂのうちの１つ以上は、カメラブレおよびデバイス１０２の他の望ましくない動きの影響を低減するために、光学画像安定化（ＯＩＳ）モジュールを含んでもよい。カメラ１１０ａ、１１０ｂがＯＩＳモジュールを含むか否かにかかわらず、デバイス１０２は、電子画像安定化（ＥＩＳ）を使用して、キャプチャされたビデオを経時的に平滑化することができる。以下でさらに説明するように、ＥＩＳ処理は、キャプチャされたフレームに変換を適用して、ブレる実カメラ軌道に沿って撮られたキャプチャされたフレームを、仮想カメラの平滑化またはフィルタリングされた軌道を表す安定化された出力フレームに投影することができる。
One or more of
デバイス１０２は、カメラ１１０ａ、１１０ｂを使用して、異なるカメラ１１０ａ、１１０ｂからの異なる倍率レベルでの画像キャプチャを使用する有効ズーム範囲を提供する。デバイス１０２は、ズーム範囲の異なる部分において異なるカメラ１１０ａ、１１０ｂの出力に適用されるデジタルズーム技術を使用してズーム範囲を提供することができる。例として、本デバイスは、１．０ｘ～３．０ｘの全体ズーム範囲を提供してもよい。より広い視野１２０を提供するカメラ１１０ａは、ズーム範囲の第１の部分、たとえば１．０ｘ～１．８ｘの画像をキャプチャするために使用されてもよい。ズームが所定の遷移点、例えば１．８ｘなどの特定のレベルに達すると、デバイス１０２は、第１のカメラ１１０ａを用いてビデオをキャプチャすることから第２のカメラ１１０ｂを用いてビデオをキャプチャすることに切り替わる。より狭い視野１２２を提供するカメラ１１０ｂは、ズーム範囲の第２の部分、例えば、１．８ｘ～３．０ｘの画像をキャプチャするために使用されてもよい。
図示される例は、デバイス１０２が、ビデオ１３０がデバイス１０２によってキャプチャされ記録されているときにキャプチャされたビデオ１３０をユーザが見ることを可能にするディスプレイを含むことを示す。デバイス１０２は、ビデオキャプチャ中にズームレベルを動的に調整するための制御１３２をユーザに提供することができる。この例では、制御１３２は、デバイス１０２のタッチスクリーン上に示される画面上スライダ制御であり、ユーザに、ズームを、ズーム範囲に沿った所望の位置に設定させる。いくつかの実現例では、ズームレベルは、細かい粒度の増分で、たとえば、０．２ｘ、０．１ｘ、またはそれより小さいステップで調整され、ユーザが、サポートされたズーム範囲にわたって連続的なズームに実質的に近似する態様で、サポートされたズーム範囲にわたって徐々に移動することを可能にする。
The illustrated example shows that
図１Ｂは、カメラ１１０ａ、１１０ｂからの出力を用いて異なるズームレベルをどのように提供できるかの例を示す。それぞれの視野１２０、１２２は、異なるズームレベルを提供するために使用され得るクロッピングされた部分１４０ａ～１４０ｅとともに示される。出力フレーム１５０ａ～１５０ｅは、異なるズームレベルで提供され得る例示的なフレーム出力を示す。２つのカメラ１１０ａ、１１０ｂは、デバイス１０２内で互いに物理的にオフセットされているので、シーンのビューに違いがあることに留意されたい。たとえ２つのカメラが同じシーンを見ているとしても、シーン内の被写体は、単眼視差に起因して、カメラの出力においてわずかに異なる位置を有するであろう。以下で更に説明する画像変換は、カメラ１１０ａ、１１０ｂの出力間のこの視差および他の差を補正することができる。
FIG. 1B shows an example of how the output from
出力フレーム１５０ａ～１５０ｃは各々、より広い視野１２０を提供する第１のカメラ１１０ａからの画像データから導出される。ズームレベルが１．８ｘなどの閾値に達すると、デバイス１０２は、より狭い視野１２２を提供する第２のカメラ１１０ｂによってキャプチャされた画像データを使用することに切り替わる。カメラ１１０ａ、１１０ｂ間の遷移は、ズームレベルが第２のカメラ１１０ｂの視野１２２内に完全に含まれるズームレベルで起こることができ、その結果、所望のズームレベルで出力フレームを満たすのに充分な画像データが存在する。いくつかの実現例では、デバイス１０２は、ＥＩＳ処理のために使用され得るキャプチャされたデータのマージンを保持するために、ズームレベルが第２のカメラ１１０ｂの全出力よりも小さいエリアに対応すると、遷移を実行するよう構成される。
Each of the output frames 150a-150c is derived from image data from the
概して、出力品質を最大化するために、視野１２２が出力フレームを満たすことができるズームレベル付近で、より狭い視野１２２に切り替えることが有利である。これは、より狭い視野１２２が、シーンのその領域に対して、より高い解像度を提供するからである。２つのカメラ１１０ａ、１１０ｂは、同様の解像度を有してもよいが、より狭い視野１２２においては、カメラ１１０ｂは、シーンのビューをキャプチャするためにその全解像度を使用することができるが、同じビューは、より広いカメラ１１０ａの解像度のほんの一部でキャプチャされるであろう。この例では、第１のカメラ１１０ａのズームインされた視野１４０ｃが第２のカメラ１１０ｂの全視野１２２に一致するズームレベル、たとえば１．７ｘがある。この点で、第１のカメラ１１０ａのための画像センサのかなり小さい部分のみが、出力フレーム１５０ｃを提供するために使用されており、したがって、出力フレーム１５０ｃは、より広いズームレベルで提供されるよりも低い解像度または低い品質であり得る。対照的に、第２のカメラ１１０ｂの画像センサの全解像度は、そのレベルの有効ズームまたは倍率を提供するよう使用され得、より高い品質の出力をもたらす。平衡点（例えば、１．７ｘ）またはそのすぐ後において第２のカメラ１１０ｂに切り替えることによって、デバイス１０２は、そのズームレベルおよび更なるズームレベルにおいて、より高い品質を提供することができる。
Generally, to maximize output quality, it is advantageous to switch to a narrower field of view 122 around the zoom level where the field of view 122 can fill the output frame. This is because a narrower field of view 122 provides higher resolution for that region of the scene. The two
いくつかの実現例では、カメラ１１０ａ～１１０ｂを切り替えるために設定されるズームレベルは、第２のカメラ１１０ｂが画像フレームを埋めることができる点の後に設定される。例えば、遷移点は、カメラ１１０ａ、１１０ｂの異なる物理的位置から生じる単眼視差を考慮するマージンを提供するズームレベルに設定され得る。その結果、第２のカメラ１１０ｂが１．７ｘのズームレベルでフレームを満たすことができる場合、それにもかかわらず、デバイス１０２は、カメラ１１０ｂからの全センサ出力が（例えば、所望のフレームキャプチャエリアを表す領域を囲むエッジにおいて）画像データのバッファ領域を提供するように、例えば、１．８ｘまたは１．９ｘのズームまで、第２のカメラ１１０ｂを使用するキャプチャへの切り替えを遅延してもよく、したがって、単眼視差を補正するために必要とされる任意のオフセットまたは他の調整が、例えば、視野１２２を視野１２０の拡大されたエリアと整列させ、および依然として出力フレームを満たすよう、行われることができる。
In some implementations, the zoom level set for switching
デバイス１０２は、ビデオ安定化、例えば、デバイス１０２の動きの影響を低減または排除するために、経時的にフレーム内において見えるカメラの動きの平滑化を適用する。デバイス１０２は、ビデオ安定化、例えば、ＥＩＳ処理を、リアルタイムで、またはほぼリアルタイムで、例えば、安定化されているビデオに対する進行中のビデオキャプチャと同時に実行することができる。経時的な（例えば、複数のフレームにわたる）ビデオ安定化平滑化は、ズーム設定が変化するにつれてカメラ１１０ａとカメラ１１０ｂとの間の遷移と協調されることができる。それにもかかわらず、以下に論じられるように、第２のカメラ１１０ｂの出力を第１のカメラ１１０ａのための正準空間にマッピングする画像データ変換が使用され得、単一のＥＩＳ処理スキームが両方のカメラ１１０ａ、１１０ｂの出力のために一貫して使用されることを可能にする。
The
本明細書で論じる技術は、２つ以上の固定焦点距離レンズとともに効果的に使用することができる。任意選択的に、本明細書で論じる技術はまた、光学ズームを提供する１つ以上のレンズとともに使用することもできる。例えば、本技術は、（ｉ）交差または重複してもしなくてもよい異なる光学ズーム範囲を有する複数のカメラ、または（ｉｉ）光学ズームを提供する１つ以上のカメラおよび１つ以上の固定焦点距離カメラ、からの画像キャプチャを含む範囲にわたるシームレスな有効ズームを提供するために使用することができる。 The techniques discussed herein can be effectively used with two or more fixed focal length lenses. Optionally, the techniques discussed herein can also be used with one or more lenses that provide optical zoom. For example, the technology may use (i) multiple cameras with different optical zoom ranges that may or may not intersect or overlap, or (ii) one or more cameras providing optical zoom and one or more fixed focus It can be used to provide seamless effective zooming over a range, including image capture from range cameras.
図２は、ビデオ安定化を提供するデバイス１０２の例を示す図である。上記で説明したように、デバイス１０２は、第１のカメラ１１０ａと第２のカメラ１１０ｂとを含む。カメラ１１０ａ～１１０ｂのうちの１つ以上は、任意選択でＯＩＳモジュール２１５ａ～２１５ｂを含んでもよい。デバイス１０２は、ＯＩＳモジュール２１５ａ～２１５ｂが含まれる場合、ＯＩＳモジュール２１５ａ～２１５ｂを使用しながらビデオフレームをキャプチャして、フレームキャプチャ中にデバイス１０２の動きを少なくとも部分的に打ち消してもよい。デバイス１０２はまた、１つ以上のデバイス位置センサ２２０と、１つ以上のデータ記憶装置２３０と、ＥＩＳモジュール２５５とを含む。
FIG. 2 is a diagram illustrating an
デバイス１０２は、携帯電話、タブレットコンピュータ、カメラなどのカメラモジュールを含む様々なタイプのいずれかであることができる。いくつかの実現例では、デバイス１０２は、ソフトウェア、ハードウェア、またはそれらの何らかの組合せで実行されてもよい、ＥＩＳモジュール２５５の動作を実行するためのコンピューティングシステムを含むことができる。たとえば、デバイス１０２は、様々な処理構成要素、たとえば、１つ以上のプロセッサ、実行可能命令を記憶する１つ以上のデータ記憶装置、メモリ、入力／出力構成要素などを含んでもよい。ＥＩＳ処理を実行するプロセッサは、汎用プロセッサ（例えば、携帯電話または他のデバイスのメインＣＰＵ）、グラフィックスプロセッサ、コプロセッサ、画像プロセッサ、固定機能ＥＩＳプロセッサ、またはそれらの任意の組合せを含んでもよい。
ＥＩＳモジュール２５５は、デバイス位置センサ２２０およびＯＩＳモジュール２１５ａ～２１５ｂの両方からの位置データを使用して、記録デバイスによってキャプチャされたビデオを安定化させる。例えば、ＯＩＳモジュール２１５ａ～２１５ｂからの位置データは、デバイス位置データから推測されるであろう期待されるカメラビューに対する、ＯＩＳの動きの影響を表すオフセットを判断するために使用されることができる。これは、たとえＯＩＳモジュール２１５ａ～２１５ｂがデバイス位置に対してカメラのシーンのビューを変化させても、ＥＩＳモジュール２１５が画像センサの実際のビューを反映する有効カメラ位置を推定することを可能にする。本明細書で説明されている他の特徴とともに、これらの技術は、デバイス１０２がＯＩＳ処理とＥＩＳ処理とを同時に効果的に使用し、両方の技術の利益を実現することを可能にする。
The EIS module 255 uses position data from both the
概して、ＯＩＳは、カメラブレによる個々のフレーム内のぼやけを低減するのに非常に有効であり得、ＯＩＳは、一連のフレームにわたって見える動きを低減するのにいくらか有効であり得る。しかしながら、単独で使用されるＯＩＳは、多くの場合、種々の制限を受ける。ＯＩＳモジュールは、動きに応答する速度、および補償することができる動きの大きさにおいて、制限される場合がある。さらに、ＯＩＳモジュールの動作は、しばしば、ゆらぎのあるビデオなどの歪みを引き起こし、パニングなどの所望の動きに誤って対抗することがある。ＥＩＳモジュール２５５は、ＯＩＳモジュールの内部の動きを記述する位置データを使用して、これらの制限の影響を軽減することができる。 In general, OIS can be very effective in reducing blurring within individual frames due to camera shake, and OIS can be somewhat effective in reducing visible motion over a series of frames. However, OISs used alone are often subject to various limitations. OIS modules may be limited in the speed at which they respond to motion and the amount of motion they can compensate for. Furthermore, the operation of the OIS module often causes distortions such as jittery video and falsely counteracts desired motion such as panning. The EIS module 255 can mitigate the effects of these limitations using position data that describes motion within the OIS module.
ＯＩＳモジュール２１５ａ～２１５ｂは、記録デバイスの動きを補償しようと試みるので、デバイスの動きだけでは、ビデオキャプチャ中に使用される真のカメラビューを示さない場合がある。ＥＩＳ処理がデバイスの動きのみに基づいて動きを補償しようと試みる場合、ＥＩＳ処理は、ＯＩＳシステムによって既に補償された動きを補正しようと試みる場合がある。さらに、ＯＩＳは、概して、デバイスの動きの影響を部分的にのみ除去し、補償の量は、フレーム毎に変動し得る。高品質の安定化を提供するために、ＥＩＳモジュール２５５は、ＯＩＳ位置データをデバイスレベル位置データとともに使用して、各フレームに対して、およびいくつかの実現例ではフレームの個々の走査線に対しても適用される安定化の量を変化させる。この処理は、効果的な安定化を提供するとともに、ビデオ映像の歪みを低減または排除することができる。例えば、フレームをキャプチャする間のＯＩＳレンズシフト位置の変化は、特に、多くのカメラモジュールに典型的であるローリングシャッタと組み合わせられるとき、歪みを導入し得る。フレームキャプチャ中の異なる時間におけるＯＩＳレンズシフトに関する情報を用いて、ＥＩＳモジュール２５５は、フレームの異なる部分がキャプチャされたときのレンズ位置を推定し、画像を補正することができる。ＥＩＳモジュール２５５はまた、パニングに干渉するかまたはそうでなければ望ましくないＯＩＳレンズシフトの影響を低減するように補償することもできる。
Since the
ＥＩＳモジュール２５５がビデオを強化することができる別の方法は、後でキャプチャされたフレームについてのデータの分析によるものである。特定のフレームを処理するために、ＥＩＳ処理モジュールは、１つ以上の将来のフレームがキャプチャされた時間を含む時間ウィンドウ内のカメラ位置の組を評価してもよい。将来のフレームおよび対応する位置に関する情報は、いくつかの方法で使用することができる。第１に、ＥＩＳモジュール２５５は、カメラ位置の組にフィルタリングを適用して、フレームを変更するために画像変換を定義するよう使用される動きパターンを平滑化することができる。第２に、ＥＩＳモジュール２５５は、カメラ位置の組を使用して、一貫した動き（例えば、パニング）が存在するかまたは試みられる可能性を評価し、次いで、その可能性が高い場合には、この動きと一貫してフレームを調整することができる。第３に、ＥＩＳモジュール２５５は、フレームのカメラ位置を、将来のカメラ位置に対して評価し、将来の大きな動きに対して調整を行うことができる。例えば、将来のフレームに対して大きな急速な動きが識別された場合、ＥＩＳモジュール２５５は、その動きが始まる前にフレームの内容の調整を開始することができる。ＥＩＳモジュール２５５は、大きな目に見える動きを数フレームにわたって可能にするのではなく、その動きをより大きなフレームにわたって広げることができ、漸増的な画像シフトが、より早期のフレーム中に生じ、動きを、より多数のフレームにわたって徐々に広げる。 Another way the EIS module 255 can enhance the video is by analyzing data for later captured frames. To process a particular frame, the EIS processing module may evaluate a set of camera positions within a time window that includes the times at which one or more future frames were captured. Information about future frames and corresponding positions can be used in several ways. First, the EIS module 255 can apply filtering to the set of camera positions to smooth the motion patterns used to define the image transforms for changing frames. Second, the EIS module 255 uses the set of camera positions to assess the likelihood that consistent motion (e.g., panning) is present or attempted, and then, if likely, You can adjust the frame consistently with this movement. Third, the EIS module 255 can evaluate a frame's camera position against future camera positions and make adjustments for future large motions. For example, if significant rapid motion is identified for a future frame, EIS module 255 can begin adjusting the content of the frame before the motion begins. Rather than allowing large visible motion over a few frames, the EIS module 255 can spread that motion over larger frames, with incremental image shift occurring during earlier frames to reduce motion. , gradually spreading out over a larger number of frames.
ＥＩＳモジュール２５５は、出力フレームの領域的合成を実行し、例えば、画像フレームの各走査線に適用される変換を変更する。これにより、システムは、ローリングシャッタ歪み、ＯＩＳモジュール２１５ａ～２１５ｂの動き、および単一フレームのキャプチャ持続時間内に生じる様々なデバイスの動きを補正することが可能になる。
The EIS module 255 performs regional compositing of the output frames, eg, altering the transform applied to each scanline of the image frame. This allows the system to compensate for rolling shutter distortion,
さらに図２を参照すると、デバイス１０２は、ビデオデータをキャプチャするようカメラを有する任意の適切なデバイス、たとえば、カメラ、セルラーフォン、スマートフォン、タブレットコンピュータ、ウェアラブルコンピュータ、または他のデバイスであり得る。図２の例は、ビデオをキャプチャおよび処理する単一のデバイスを示すが、機能は、任意選択肢的に、複数のデバイスまたはシステムの間で拡散されてもよい。たとえば、第１のデバイスは、ビデオフレームをキャプチャしてもよく、また、位置データおよび他のパラメータをメタデータとして記録してもよい。第１のデバイスは、ビデオフレームおよびメタデータを、本明細書で説明するＥＩＳ処理を実行することができる第２のデバイス、たとえば、ローカルコンピューティングシステムまたはリモートサーバに提供してもよい。
Still referring to FIG. 2,
第１のカメラ１１０ａは、レンズ素子、画像センサ、センサ読取回路、および他の構成要素を含むことができる。ＯＩＳモジュール２１５ａ～２１５ｂは、センサと、可動要素と、プロセッサと、可動要素を動かすための駆動機構とを含むことができる。可動要素は、第１のカメラ１１０ａの光路内に位置する。例えば、可動要素は、反射または屈折要素、例えば、レンズ、ミラー、プリズムであってもよい。いくつかの実現例では、可動要素は第１のカメラ１１０ａの画像センサである。センサは、動きを検出するための１つ以上のジャイロスコープまたは他のセンサを含むことができる。プロセッサは、センサによって示される動きを補償するために可動要素に必要とされる動きの量および方向を決定し、次いで、可動要素を動かすように駆動機構に命令する。
The
デバイス１０２は、デバイス１０２の向きの変化を測定する１つ以上の位置センサ２２０を含む。いくつかの実現例では、デバイス１０２のための位置センサ２２０は、ＯＩＳモジュール２１５ａ～２１５ｂによって使用されるセンサとは別個である。位置センサ２２０は、１つ以上の軸の周りのデバイス１０２の回転を検出することができる。例として、デバイス位置センサ２２０は、３軸ジャイロスコープまたは慣性測定ユニット（ＩＭＵ）であってもよい。加えて、または代替として、他のセンサが、デバイス位置を判断するために使用されてもよい。たとえば、１つ以上の加速度計、１軸ジャイロスコープ、２軸ジャイロスコープなどを使用して、デバイス１０２の位置を判断してもよい。概して、デバイス１０２の回転位置が判断されることを可能にする、任意の適切なセンサまたはセンサの組み合わせを、使用することができる。
いくつかの例では、ＯＩＳモジュール２１５ａ～２１５ｂのジャイロスコープセンサからの位置データは、記録デバイス２２０の別個の位置センサ２２０を使用することに加えて、またはその代わりに、キャプチャされ記憶されてもよい。それにもかかわらず、デバイス１０２がＯＩＳセンサとは異なる特性を有するジャイロスコープセンサを使用することは有益であり得る。例えば、デバイス１０２のジャイロスコープセンサは、１００度／秒より大きい感知可能な回転範囲で約４００Ｈｚのレートで測定値を提供してもよい。デバイスレベルセンサと比較して、ＯＩＳモジュールの典型的なジャイロスコープセンサは、測定値を、異なるレートおよび範囲、例えば、５０００測定値／秒以上のレートを、約１０度／秒の感知可能な回転範囲で、提供してもよい。いくつかの実現例では、（たとえば大きい動きを記述するために、）デバイスレベルセンサの、より大きい感知可能な回転範囲を有することは、（例えば小さな変化または高周波パターンを検出するための）ＯＩＳモジュールセンサの、より頻繁な測定と同様に、有益である。したがって、両方のタイプのデータを、デバイス１０２の位置を判断するために、一緒に使用してもよい。
In some examples, position data from the gyroscope sensors of the
デバイス１０２は、第１のカメラ１１０ａを特徴付ける情報およびフレームキャプチャプロセスを記憶する１つ以上のデータ記憶装置２３０を含む。例えば、記憶されたデータは、ＯＩＳモジュール２１５ａ～２１５ｂの位置と画像データに生じる結果として生じるオフセットとの間の関係を示す較正データ２３２を含むことができる。同様に、較正データ２３２は、カメラモジュールレンズ焦点位置とそれら焦点位置に対する有効焦点距離との対応を（例えば各カメラ１１０ａ、１１０ｂに対して異なるマッピングで）示すことができ、システムがフォーカスブリージングを考慮することを可能にする。加えて、較正データ２３２または他の記憶されたデータは、カメラレンズ焦点位置と被写体距離との対応を示すことができ、オートフォーカスシステムによって選択されたレンズ焦点位置から、カメラのセンサ面からの合焦された被写体の距離を示す被写体距離への変換を可能にする。較正データ２３２はまた、一方のカメラ１１０ａの他方のカメラ１１０ｂに対する相対的な３Ｄ空間位置も示す。通常、これは、一方のカメラの他方のカメラに対する３Ｄ回転および３Ｄ並進を特定する較正データを含む。典型的には、較正は、最良のユーザ体験を保証するために、製造される各デバイスに対して行われる。その結果、較正データ２３２は、特定のカメラモジュール（例えば、カメラ１１０ａ、１１０ｂ、それらの取り付け構造など）の特性および製造後のモジュールの状態に対して非常に正確であることができる。記憶されたデータは、第１のカメラ１１０ａ内の画像センサの読取特性を示すことができる走査パターンデータ２３４を含むことができる。例えば、走査パターンデータ２３４は、走査の方向（例えば、走査線は、上から下へ読み取られる）、走査線が個別に読み取られるかまたはグループで読み取られるかなどを示すことができる。
ビデオキャプチャ中、カメラ１１０ａ～１１０ｂ、ＯＩＳモジュール２１５ａ～２１５ｂ、およびデバイス位置センサ２２０は各々、ビデオキャプチャプロセスについての情報を提供し得る。第１のカメラ１１０ａは、ビデオフレームデータ２４２ａ、例えば、ビデオ画像フレームのシーケンスを提供する。第２のカメラ１１０ｂは、同様に、ビデオフレームデータ２４２ｂを提供する。カメラ１１０ａ～１１０ｂはまた、フレーム露出データ２４４ａ～２４４ｂも提供し、これは、キャプチャされたフレームごとに、露出持続時間、および露出がいつ生じたかを示す基準時間（例えば、露出の開始時間または終了時間）の指示を含み得る。カメラ１１０ａ～１１０ｂはまた、各キャプチャされたフレームに対するレンズ焦点位置を示すレンズ焦点位置データ２４６ａ～２４６ｂも提供する。
During video capture,
ＯＩＳモジュール２１５ａ～２１５ｂは、ビデオキャプチャ中の様々な時間におけるＯＩＳモジュール２１５ａ～２１５ｂの可動要素の位置を示すＯＩＳ位置データ２４８ａ～２４８ｂを提供する。例えば、可動要素が、動きを補償するようにシフトする可動レンズである場合、ＯＩＳモジュール２１５ａ～２１５ｂは、各々における可動レンズの現在位置を特定するレンズシフト読取を提供することができる。デバイス１０２は、レンズシフト位置およびその位置が生じた時間を記録することができる。いくつかの実現例では、ＯＩＳ位置データ２４８ａ～２４８ｂは、各ビデオフレーム露出の持続時間にわたって複数の測定が行われるように、高い周波数で、たとえばビデオキャプチャのフレームレートよりも高いレートでキャプチャされる。
The
デバイス位置センサ２２０は、ビデオキャプチャ中のデバイス１０２の回転および／または他の動きを示すデバイス位置データ２５０を提供する。デバイス位置は、高い周波数、例えば２００Ｈｚ以上で測定することができる。したがって、多くの事例では、測定値は、各ビデオフレームのキャプチャ中に複数の異なる時間について取得されることができる。
レンズ焦点位置データ２４６、ＯＩＳ位置データ２４８、およびデバイス位置データ２５０はすべて、特定された位置が生じた時間を示すタイムスタンプとともに記録され得る。タイムスタンプは、例えば、最も近いミリ秒まで、精度よく作成することができ、様々な位置測定から得られたデータを時間的に整列させることができる。さらに、デバイス、ＯＩＳシステム、またはレンズ焦点機構の位置を補間して、測定間の時間における値を判断することができる。
Lens focus position data 246, OIS position data 248, and
データキャプチャの潜在的タイミングの例が、チャート２５２に示されている。図示のように、デバイス位置データ２５０（たとえば、ジャイロスコープデータ）およびＯＩＳ位置データ２４８（たとえば、レンズシフト位置データ）は、ビデオキャプチャフレームレート（例えば、３０フレーム／秒、６０フレーム／秒など）よりも高いレートでキャプチャされてもよく、デバイスおよびＯＩＳシステムの複数の位置は各ビデオフレームについて判断されることができる。その結果、シャッタ方向に応じて水平または垂直であってもよい各走査線について、異なるデバイス位置およびＯＩＳ設定を使用して、その走査線の変換を判断することができる。レンズ焦点位置データ２４６は、画像フレーム毎に少なくとも１回キャプチャされてもよい。この位置データは、例えば、ジャイロスコープセンサデータおよびＯＩＳ位置データが、画像フレーム露出の開始または終了を超過し、必ずしもそれと同期されないレートでサンプリングされることによって、フレーム露出に対して非同期的にキャプチャされてもよい。
An example of potential timing for data capture is shown in
カメラ１１０ａ～１１０ｂおよび他の構成要素から得られたデータは、処理のためにＥＩＳモジュール２５５に提供される。この処理は、ビデオキャプチャが進行している間に行われてもよい。例えば、ＥＩＳ処理は、実質的にリアルタイムで実行することができ、それにより、ビデオキャプチャの終了時にユーザにアクセス可能にされたビデオファイルは、ＥＩＳモジュール２５５によって安定化されている。いくつかの実現例では、ＥＩＳ処理は、後で、例えば、ビデオキャプチャが完了した後で、またはビデオを記録したのとは異なるデバイスによって、実行されてもよい。ＥＩＳモジュール２５５は、ハードウェア、ファームウェア、ソフトウェア、またはこれらの組合せもしくは部分的組合せで実現されることができる。
Data obtained from
図２は、ＥＩＳモジュール２５５の機能の一部のみを示す。図２の例は、カメラ１１０ａ～１１０ｂのうちの単一のカメラからキャプチャされたフレームを含むビデオフレームに対するＥＩＳ処理を示し、デジタルズームを提供するため、またはズームレベルに対してＥＩＳ処理を調整するために使用される特徴を説明しない。しかし、以下に説明するように、デバイス１０２は、両方のカメラからキャプチャされた画像データを使用して、ビデオキャプチャおよび記録中にデジタルズームを提供することができる。例えば、ユーザが閾値ズーム量にズームインすると、デバイス１０２は、第１のカメラ１１０ａからキャプチャされた画像の使用から、第２のカメラ１１０ｂからキャプチャされた画像の使用に切り替えることができる。さらに、ズームインはビデオ内の目に見える振れを強調し得るので、ズームレベルは、例えば、ズームレベルが増加するにつれてより大きな安定化レベルを適用するために、ＥＩＳモジュール２２５の変換および他の動作を調整することができる。複数のカメラ間のデジタルズームおよび遷移を考慮するＥＩＳ処理の調整を、図４から図６に関して論ずる。幾つかの実現例では、変換は、キャプチャされた画像の各ラインについて、異なる変換を適用することによって達成され得る。画像のラインごとに、デバイス１０２は、そのラインに対する特定のタイムスタンプを計算することができ、このラインは、対応するＯＩＳおよびデバイス位置データと関連付けられる。
FIG. 2 shows only part of the functionality of EIS module 255 . The example of FIG. 2 illustrates EIS processing for video frames including frames captured from a single one of
さらに図２を参照すると、ＥＩＳモジュール２５５は、デバイス位置センサ２２０から更新されたデバイス位置データ２５０を周期的または連続的に取得するデバイス位置データハンドラ２５６を含む。動きデータハンドラは、デバイス位置データ２５０から現在のカメラ姿勢を推定する。例えば、ジャイロスコープ信号を取得し、これを使用して、デバイス１０２のデバイス位置を、例えば２００Ｈｚなどの高周波数で推定することができる。所与の時間ｔにおけるこのデバイス位置は、以下ではＲ（ｔ）と呼ばれる。このデバイス位置は、例えば、１つ、２つ、または３つの軸に対するデバイス１０２の回転位置を示してもよい。デバイス位置は、回転行列として、または座標系に関して、または他の形態で表されてもよい。各計算されたデバイス位置は、デバイス１０２のその位置が生じた時間を示す時間でラベル付けすることができる。
Still referring to FIG. 2, EIS module 255 includes device
ＥＩＳモジュール２５５は、ＯＩＳ位置データハンドラ２５８を含み、これは、ＯＩＳ位置データ２４８として示されるＯＩＳ位置読取を周期的または連続的に取得する。ＯＩＳ位置データハンドラ２５８は、ＯＩＳ読取値を、デバイス位置と共に使用することができるオフセットに変換する。例えば、ＯＩＳレンズ位置を２次元画素オフセットに変換することができる。オフセットを生成するために、ＯＩＳ位置データハンドラ２５８は、ＯＩＳ位置から対応するオフセットに変換するよう変換係数または行列を提供し得る、記憶された較正データ２３２を使用することができる。ＯＩＳ位置によるオフセットを生成することは、例えば、第１のカメラ１１０ａが光学ズームが可能である場合、レンズ焦点位置および／またはレンズズーム位置の変化に起因する、経時的なカメラの有効焦点距離の変化を考慮に入れることができる。動きデータハンドラ２５６と同様に、ＯＩＳ位置データハンドラ２５８は、各測定値およびオフセットを、データが表す時間でラベル付けする。
EIS module 255 includes OIS
ＥＩＳモジュールは、デバイス位置データハンドラ２５６によって計算されたデバイス位置と、ＯＩＳ位置データハンドラ２５８によって計算されたＯＩＳオフセットとを受信する動きモデル構築部２６０を含む。このデータならびにフレーム露出データ２４４およびレンズ焦点位置データ２４６を用いて、動きモデル構築部２６０は、フレームのために第１の変換２６２を生成する。たとえば、第１の変換２６２は、カメラのビュー内の実世界のシーンをキャプチャされたフレームにマッピングする射影行列であり得る。このプロセスは、フレームごとに繰り返される。フレームのために第１の変換２６２を生成するとき、ＯＩＳモジュール２１５ａ～２１５ｂの位置は、ジャイロスコープデータから判断された主のデバイス位置からのオフセットとしてモデル化され得る。以下でさらに論じられるように、オフセットは、キャプチャ時のカメラの有効焦点距離を、その時点でのレンズ焦点位置に対する有効焦点距離を調べることによって、考慮に入れることができる。第１の変換２６２は、単一の画像フレームの異なる部分集合または領域の関係を別に記述することができる。たとえば、第１の変換２６２の異なる部分または成分は、フレームの異なる走査線が実世界のシーンにどのようにマッピングされるかを記述してもよい。デバイス位置、ＯＩＳモジュール位置、被写体距離（例えば、カメラから合焦された被写体の距離）、およびレンズ焦点位置はすべて、測定タイムスタンプを使用して整列させ、必要に応じて補間して、フレームの個々の走査線の露出時に正確な位置を提供することができる。オートフォーカスを伴うレンズの場合、焦点位置は、被写体がカメラからどれだけ離れているかに応じて設定される。したがって、レンズ焦点位置と被写体距離との関係を示すマップが存在する。マッピングは、生成および較正されることができ、被写体距離は、空間遷移のための後の計算で使用される。
The EIS module includes a
動きモデル構築部２６０によって生成された第１の変換２６２は、第２の変換２７２を決定する非線形動きフィルタリングエンジン２７０に提供される。この第２の変換２７２は、フレームの画像データを、そのフレームの安定化されたバージョンを表す出力フレームに投影する、第２の射影行列Ｐ’ｉ，ｊとすることができる。具体的には、第２の変換２７２は、キャプチャされた画像データで演算するのではなく、第１の変換２６２を使用して作成された画像投影Ｐｉ，ｊを出力フレームにマッピングすることができる。いくつかの実現例では、２つの変換２６２、２７２は、次いで、フレームの最初にキャプチャされた画像データで演算し、それを、安定化された出力フレームに直接マッピングする、単一の変換に結合され得る。
A
動きを効果的に安定させるために、非線形動きフィルタリングエンジン２７０は、処理されるフレームのキャプチャ後に将来生じる動きを考慮に入れるよう、第２の変換２７２を生成することができる。例えば、分析中の現在のフレームについて、記録デバイスの位置は、前のフレームから有意に移動していない場合がある。それにもかかわらず、エンジン２７０が、有意な動きが将来のフレームに生じると判断する場合、第２の変換２７２は、現在のフレームをシフトするかまたは別様に変更して、目に見える動きをビデオに導入するように生成され得、したがって、大きな将来の動きが、急激な変化ではなく、一連の漸次的な変化として拡散され得る。同様に、将来のフレームの安定化がクロッピングまたは他の変化をもたらす場合、第２の変換２７２は、一連のフレームにわたって、より漸次的かつ一貫した変化のために、それらの変化を以前のフレームに少なくとも部分的に伝搬するように生成され得る。
To effectively stabilize motion, the non-linear
非線形フィルタリングエンジン２７０は、フレームに対する仮想カメラ位置から第２の変換２７２を生成することができる。露出が生じたときのカメラの実際の位置を表すのではなく、仮想カメラ位置は、記録されているビデオを安定させるであろう、デバイス１０２の調整されたまたは仮想的な姿勢を表すことができる。仮想位置は、仮想カメラを配置するための所望の位置、例えば、シーンの特定のビューまたはパースペクティブをシミュレートするであろう位置を表すことができる。概して、任意のカメラ位置は、グローバルな基準フレームに対するその回転および並進によって表され得る。仮想カメラ位置は、回転行列、例えば基準位置に対する回転オフセットを示す行列として表すことができる。これは、３つの回転軸に対する回転オフセットを示す３×３行列であってもよい。いくつかの実現例では、ＥＩＳモジュールの安定化処理は、回転成分に関してのみ位置を定義するが、これは、それらが一般にハンドヘルドビデオの安定性に最大の影響を与えるからである。
A
フレームに対する仮想カメラ位置は、ビデオ安定化を強化し、歪みおよび動作を補正し、パニングを容易にし、および他の態様でビデオを強化するよう、推定されるカメラ位置に対する調整を反映することができる。フレームに対する仮想カメラ位置は、様々な要因に基づいて調整される初期カメラ位置を生成することによって判断され得る。例えば、仮想カメラ位置の調整は、フレームの前後に検出された動きに基づくデバイス位置のフィルタリングを通じて、フレーム内のぼけの量に基づいて、パニングが発生している可能性に基づいて、将来のフレーム内の動きに備えるよう、調整を通じて、および／または画像データが出力フレーム全体をカバーすることを確実にするために、行われ得る。様々な要因は、フレームに対する最終仮想カメラ位置を判断するために変更、混合、または他の態様で使用される、そのフレームの一連の仮想カメラ位置を生成することによって、考慮することができる。 The virtual camera position for the frame can reflect adjustments to the estimated camera position to enhance video stabilization, correct distortion and motion, facilitate panning, and otherwise enhance the video. . A virtual camera position for a frame may be determined by generating an initial camera position that is adjusted based on various factors. For example, adjustments to the virtual camera position can be made based on the amount of blur in the frame, through filtering of the device position based on motion detected before and after the frame, based on the likelihood that panning is occurring in future frames. This may be done through adjustments to provide for motion in the image, and/or to ensure that the image data covers the entire output frame. Various factors can be taken into account by generating a series of virtual camera positions for that frame that are modified, mixed, or otherwise used to determine the final virtual camera position for that frame.
ちょうど変換２６２、２７２が異なる走査線に対して異なるマッピングを有することができるように、フレームの異なる走査線に対して異なる仮想カメラ位置を判断して、フレームキャプチャ中のデバイス位置、ＯＩＳモジュール２１５ａ～２１５ｂの位置、および／またはレンズ焦点位置の変化について調整することができる。したがって、フレームの異なる部分に対して異なる仮想カメラ位置を使用することができる。効率のために、第２の変換２７２の仮想カメラ位置および対応する成分は、画像センサの走査線の適切な部分集合について計算され得、次いで、適切なデータが、残りの走査線について補間され得る。以下の様々な例では、簡潔にするために、画像センサの中央走査線などの単一の走査線について論じる。仮想カメラ位置および対応する射影行列成分を完全に計算するための技術は、画像フレームの複数の走査線に対して、さらには必要に応じて各走査線に対して個々に、使用され得る。
Determining different virtual camera positions for different scanlines of a frame to determine the device position during frame capture,
本明細書で使用するとき、デバイス位置とは、たとえば、デバイス位置データ２５０（たとえば、ジャイロスコープデータ）およびデバイス位置データハンドラ２５６の出力によって示されるような、デバイス１０２の位置を指す。このデバイスレベル位置は、第１のカメラ１１０ａのレンズの内部の動きまたはＯＩＳモジュール２１５ａ～２１５ｂの動きを考慮しない、デバイス１０２の姿勢または向きを示す。また、本明細書で使用されるように、カメラ位置とは、カメラの有効なビューまたは推定されるビューに対応する位置を示す。ＯＩＳモジュール２１５ａ～２１５ｂの動作、レンズブリージング、および他の要因によるシフトを考慮することによって、カメラ位置は、デバイス位置とは異なってもよい。さらに、カメラ位置は、仮想位置、例えば、カメラの実際のビューではなく、カメラの強化もしくは変更されたビューを反映する近似または仮想的な位置であってもよい。
As used herein, device location refers to the location of
次いで、ＥＩＳモジュール２５５は、画像ワーピングエンジン２８０を使用して、非線形動きフィルタリングエンジン２７０の出力を使用し、各キャプチャされた画像フレームを出力フレームにマッピングする。第２の投影２７２は、フレームの各部分が出力空間にマッピングされ、出力フレームの画素の各々が定義されるように、フレームの各走査線に対応する成分を含むことができる。ＥＩＳモジュール２５５の処理は、ビデオのフレームの各々について実行され得る。
EIS module 255 then uses
図３は、ビデオ安定化のために使用されるデータの例を示す図である。この図は、デバイス１０２によってキャプチャされる一連のフレーム３１０を示す。各フレームは、例えば、露出持続時間、露出基準時間（例えば、露出の開始時間、停止時間、または他の基準点）、レンズ焦点位置などを示す、メタデータ３１２の対応するセットでラベル付けされる。図示されていないが、デバイス位置データおよびＯＩＳモジュール位置データは、各露出中の様々な時間にキャプチャされ、タイムスタンプが付けられる。
FIG. 3 is a diagram showing an example of data used for video stabilization. The figure shows a series of
フレーム３１１の安定化処理を行うために、フレーム３１１のキャプチャの付近の時間範囲が規定される。この時間範囲またはフレームのウィンドウは、フレーム３１１をどのように変換するかを判断するために分析される。本明細書で使用されるように、フレームの時間「ｔ」は、概して、フレームのキャプチャの基準時間を表すために使用される、中央走査線のキャプチャの時間を指す。（例えば、フレームの中央走査線に対するキャプチャの主時間とは異なってもよい時間を考慮するために、）個々の走査線の時間に言及するとき、その時間はｔＬで示され、Ｌは特定の走査線のインデックスまたは識別子である。分析中のフレーム３１１の中央走査線の露出時間ｔは、分析に使用される時間範囲の中心として使用することができる。所定の時間オフセットＱを使用して、その範囲、例えば［ｔ－Ｑ，ｔ＋Ｑ］からの時間範囲を設定することができる。いくつかの実現例では、この時間オフセットＱは約２００ｍｓである。その結果、この範囲は、フレーム３１１の前に約７フレームおよび後に約７フレームを含むことになる。より大きいおよびより小さい時間オフセットＱを使用してもよい。ＥＩＳモジュール２５５は、処理中に将来のフレームのコンテキストを使用するので、フレームの処理は、適切な数の後続のフレームがキャプチャされるまで、遅延される。
In order to perform stabilization processing for
図３では、フレーム３１１は、（例えば、カメラ１１０ａ～１１０ｂのいずれかのカメラの）画像センサによってキャプチャされるように示されている。上記で説明したように、ＥＩＳモジュール２５５は、フレーム３１１のキャプチャ中のデバイス１０２の実際の位置、ならびにＯＩＳモジュール要素などのカメラ要素の位置およびレンズ焦点位置を示すデータから、第１の変換２６２を定義する。第１の変換２６２を適用した結果は、出力フレームターゲット３３５に関して示される投影された画像３３０である。いくつかの実現例では、第１の変換２６２は、フレーム３１１のキャプチャに対応するデータのみを使用して決定される。変換２６２は、カメラの実レンズ位置に対応し、したがって、投影された画像３３０は、画像データとカメラのビュー内の実際のシーンとの間のマッピングを推定する。
In FIG. 3,
ＥＩＳモジュール２５５は、第２の変換２７２を使用して、フレーム３１１の画像データをさらに調整する。この第２の変換２７２は、仮想レンズ位置、例えば、フレーム３１１をキャプチャするために使用される場合、より安定したビデオをもたらすであろう仮想的な位置に対応する。この第２の変換２７２は、フレーム３１１に適用されると、出力フレーム３３５のためのデータを完全に定義する投影された画像３４０を生成する。
EIS module 255 further adjusts the image data for
投影された画像３４０を生成する第２の変換２７２は、［ｔ－Ｑ，ｔ＋Ｑ］からの時間範囲内の各フレームに対応するデータから生成されてもよい。この期間にわたるデバイス１０２の位置Ｒ（ｔ）は、例えば、ガウスフィルタを使用してその範囲にわたって動きを平滑化するようフィルタリングすることができる。ここでいう位置の集合Ｒ（ｔ）は、［ｔ－Ｑ，ｔ＋Ｑ］の範囲内で発生する中央走査線キャプチャ時間の各々におけるデバイス１０２の位置を含む集合である。この範囲が、処理されている現在のフレームの中央走査線キャプチャ時間ｔ０と、処理されている現在のフレームの前後の７つのフレームの中央走査線キャプチャ時間とを包含する例を考える。フィルタリングされる位置の集合は集合｛Ｒ（ｔ－７），Ｒ（ｔ－６），…，Ｒ（ｔ－１），Ｒ（ｔ０），Ｒ（ｔ１），…，Ｒ（ｔ６），Ｒ（ｔ－７）｝となるであろう。時間ｔにおけるフィルタリングの結果、すなわちフレーム３１１の中央走査線の露出は、初期仮想カメラ位置Ｖ０（ｔ）として使用することができる。フィルタリングを用いても、デバイス位置における望ましくない動き、または望ましくない動きをもたらす他の要因が存在し得る。その結果、初期仮想カメラ位置Ｖ０（ｔ）は、一連のさらなる動作を通じて更新され得る。いくつかの実現例では、フィルタリングされる位置Ｒ（ｔ）は、ＯＩＳの動きを仮定しない位置であり、したがって、ＯＩＳ位置データ２４８を考慮しないデバイス位置データ２５０に基づくことができる。他の実現例では、ＯＩＳの動きおよびオフセットは、初期仮想カメラ位置Ｖ０（ｔ）を生成するようフィルタ処理される位置の集合に要素として含められ得る。
A
例えば、第２の仮想カメラ位置Ｖ１（ｔ）は、位置Ｖ０（ｔ）を、将来のフレームにわたって生じる動きの量に基づいて、前のフレームに対する最終カメラ位置ＶＦ（ｔｐｒｅ）で補間することによって生成され得る。最終カメラ位置ＶＦ（ｔｐｒｅ）は、フレーム３１１の直前にキャプチャされたフレームの中央走査線に対する（例えば、記録された出力フレームを生成するために使用される）仮想カメラ位置であり得る。前のフレームのカメラ位置は、最終的な仮想カメラ位置、例えば前のフレームの安定化された出力バージョンを生成するために使用される変換に対応する位置であり得る。補間は、フレーム３１１と前のフレームとの間の動きの目に見える変化を、フレーム３１１と将来のフレームとの間の動きの目に見える変化と整列させることができる。
For example, a second virtual camera position V 1 (t) interpolates position V 0 (t) with the final camera position VF(t pre ) relative to the previous frame based on the amount of motion that occurs over future frames. can be generated by Final camera position VF(t pre ) may be the virtual camera position (eg, used to generate the recorded output frame) relative to the center scanline of the frame captured immediately before
第３の仮想カメラ位置Ｖ２（ｔ）は、フレーム３１１内に存在するカメラのモーションブラーの量に基づいて、Ｖ１（ｔ）を実デバイス位置Ｒ（ｔ）で補間することによって生成され得る。これは、観視者のぶれの知覚を低減するために適用される安定化の量を低減し得る。モーションブラーは一般に除去することができないので、これは、より自然な結果を生成するために、適切なときにビデオの安定性を低下させ得る。
A third virtual camera position V 2 (t) may be generated by interpolating V 1 (t) with the real device position R(t) based on the amount of camera motion blur present in
第４の仮想カメラ位置Ｖ３（ｔ）は、時間範囲［ｔ－Ｑ，ｔ＋Ｑ］にわたってデバイス１０２の一貫した動き中に生じる位置をシミュレートまたは表すために生成され得る。この位置は、ドメイン変換フィルタなどの安定したフィルタを、推定された実際のデバイス位置Ｒ（ｔ）に、時間範囲にわたって適用することによって、決定されてもよい。フィルタは、Ｖ０（ｔ）を生成するために使用される同じデバイス位置の組に適用されるが、このステップは、異なるタイプのフィルタリングを表す。例えば、Ｖ０（ｔ）は、平滑化するが、概して、経時的に、推定された実際のデバイス位置の変化に従い、所定の形状またはパターンを課さないフィルタリングを通して生成されてもよい。対照的に、Ｖ３（ｔ）は、デバイス１０２のユーザによって潜在的に意図されてもよい実質的に線形のパニングまたは他の動きなどの所定の一貫した動きパターンに適合するようデバイスパターンをフィルタリングすることによって、生成される。
A fourth virtual camera position V 3 (t) may be generated to simulate or represent positions that occur during consistent motion of
第５の仮想カメラ位置Ｖ４（ｔ）は、Ｖ３（ｔ）およびＶ２（ｔ）の補間として生成され得る。ＥＩＳモジュール２５５は、経時的なデバイス位置の変化がデバイス１０２のパニングを表す可能性が高いかどうかを評価し、それに応じて補間を重み付けまたは調整することができる。パニングの可能性が高いと判定された場合、Ｖ４（ｔ）は推定されたパニング位置Ｖ３（ｔ）に近い。パニングの可能性が低いと判断された場合、Ｖ４（ｔ）は位置Ｖ２（ｔ）により近くなることになる。
A fifth virtual camera position V4 (t) may be generated as an interpolation of V3 (t) and V2 (t). The EIS module 255 can assess whether changes in device position over time are likely to represent panning of the
第５の仮想カメラ位置Ｖ４（ｔ）で、ＥＩＳモジュール２５５は、対応する変換が出力フレーム３３５に提供するであろうカバレッジを評価することができる。出力フレーム３３５全体を埋め、いかなる画素も定義されないままにしないことが望ましいので、ＥＩＳモジュール２５５は、仮想カメラ位置Ｖ４（ｔ）からのシーンのビューを表す射影行列などの変換を決定し、投影された画像が出力フレーム３３５をカバーするであろうことを検証し得る。将来のフレームにおける動きを考慮するために、変換は、将来の画像フレームによってキャプチャされるシーンの部分に適用され得る。変換および対応する仮想カメラ位置Ｖ４（ｔ）は、現在のフレームおよび将来のフレームの組の各々が、変換を使用してマッピングされるときに、すべて完全に出力フレーム３３５を定義するであろうように、調整され得る。結果として生じる変換は、変換２７２として設定されてもよく、フレーム３１１に対する安定化された出力フレーム３３５を生成するために使用され得る。
At the fifth virtual camera position V 4 (t), EIS module 255 can evaluate the coverage that the corresponding transform would provide in
いくつかの実現例では、フレーム３１１に対して安定化された出力フレーム３３５を生成することは、フレーム３１１の１つ以上の他の走査線について、時間ｔＬにおいて露出された走査線Ｌについて説明したＥＩＳ処理技術を実行することを含む。例えば、その処理は、走査線に対して、ある間隔（例えば、１００走査線ごと、５００走査線ごとなど）で、またはある基準点（例えば、フレームを横切る１／４および３／４、またはフレームの上部およびフレームの底部）で、実行されてもよい。仮想カメラ位置および第２の変換２７２がフレーム３１１の走査線の適切な部分集合のみに対して決定されるとき、走査線に対する変換（例えば、射影行列の対応する部分）は、計算された位置間で補間される。このようにして、適切な変換が各走査線に対して決定され、各走査線は、異なる変換が結果として適用されてもよい。いくつかの実現例では、仮想カメラ位置および第２の変換２７２を生成する全処理は、異なる走査線のデータ間の補間に依拠することなく、各フレームの各走査線について行われてもよい。
In some implementations, generating stabilized
フレーム３１１が出力フレーム３３５にマッピングされると、その結果が保存され、ＥＩＳモジュール２５５は次のフレームの処理を開始する。プロセスは、ビデオのフレームの各々が処理されるまで継続する。
Once
仮想カメラ位置および結果として生じる変換を生成するために使用される種々の要素は、組み合わせて、または別々に使用することができる。たとえば、実現例に応じて、仮想カメラ位置Ｖ０（ｔ）～Ｖ４（ｔ）を作成するために使用される補間および調整のいくつかは省略されてもよい。たとえば、異なる実現例では、フィルタ処理されたカメラ位置Ｖ０（ｔ）～Ｖ３（ｔ）のいずれかを用いて、データを出力フレームに投影するための変換を決定し、Ｖ４（ｔ）をその目的のために使用する代わりとしてもよい。したがって、安定化変換を生成するためにフィルタ処理されたカメラ位置Ｖ０（ｔ）、Ｖ１（ｔ）、およびＶ２（ｔ）のいずれかを使用することは、依然としてビデオの安定性を改善し得る。同様に、Ｖ３（ｔ）は、パニングが生じているビデオを安定化するのに有効であってもよい。多くの他の変形例が、議論される異なる要素の部分集合を考慮に入れる場合であっても、本開示の範囲内である。 The various factors used to generate the virtual camera positions and resulting transformations can be used in combination or separately. For example, depending on the implementation, some of the interpolations and adjustments used to create virtual camera positions V 0 (t)-V 4 (t) may be omitted. For example, different implementations use any of the filtered camera positions V 0 (t) to V 3 (t) to determine the transform for projecting the data into the output frame, and V 4 (t) may be used instead of for that purpose. Therefore, using any of the filtered camera positions V 0 (t), V 1 (t), and V 2 (t) to generate the stabilizing transform still improves video stability. can. Similarly, V 3 (t) may be effective in stabilizing video in which panning is occurring. Many other variations are within the scope of the disclosure, even taking into account different subsets of the elements discussed.
論じられる技術は、様々な方法で適用され得る。例えば、２つの変換２６２、２７２を画像データに順次適用するのではなく、記録デバイスは、両方の組合された効果を反映する単一の組合された変換を生成することができる。したがって、変換２６２、２７２を使用して安定化された画像データを生成することは、変換２６２、２７２を直接適用するのではなく、画像データを安定化するために最終的に使用されるさらなる変換または関係の生成を包含してもよい。画像安定化のための種々の技術が説明され、ここに引用により援用する、２０１９年１０月２９日に発行された米国特許１０，４６２，３７０で論じられる技術等の他の技術が、加えて、または代替として、使用されることができる。
The techniques discussed can be applied in various ways. For example, rather than applying the two
図４は、図１Ａ～図１Ｂのデバイス１０２による処理の追加の例を示すブロック図である。そのうちのいくつかが再び図４に表される図２に示される要素に加えて、デバイス１０２は、図４に表される追加の機能を提供するためのハードウェアおよび／またはソフトウェア要素を含むことができる。
FIG. 4 is a block diagram illustrating additional example processing by the
デバイス１０２は、ズームレベルにおいて要求された変更を示すデバイス１０２へのユーザ入力を処理するズーム入力処理モジュール４１０を含むことができる。デバイス１０２がビデオをキャプチャするとき、たとえＥＩＳが使用中でも、デバイス１０２は、ズームレベルを変更するためのユーザ入力を受信することができる。これは、画面上スライダ制御を移動させる入力、タッチスクリーン上のジェスチャ、または他の入力を含むことができる。１．０ｘのズームレベルは、ＥＩＳが使用状態でカメラ１１０ａを使用して利用可能な最も広い視野を表すことができる。これは、ＥＩＳ処理のためにマージンを提供するよう、ネイティブ画像センサ解像度のクロッピングされたセクションであってもよい。モジュール４１０は、ユーザ入力に基づいて所望のズームレベルを判断して、例えば、現在のズームレベル（例えば、１．０ｘ）から変更または所望されるズームレベル（例えば、２．０ｘ）に移動することができる。
要求されたズームレベルを示すデータは、ビデオ記録に使用されるカメラを切り替えるかどうかを判断するカメラセレクタモジュール４２０に提供される。例えば、カメラセレクタモジュール４２０は、カメラ間の遷移が生じるべきズームレベルを示す記憶されたカメラ遷移閾値４３０を受信および使用することができる。例えば、第２のカメラ１１０ｂは、１．７ｘのズームレベルに対応する視野を有することができ、遷移閾値は、１．８ｘのズームレベルに設定されることができる。カメラセレクタ４２０は、所望のズームレベル２．０が１．８ｘ閾値を満たす（例えば、閾値以上である）と判断し、したがって、カメラの変更は、１．８ｘ以降のズームレベルを表すキャプチャから適切である。ズームイン（例えば、視野を狭くする）のための第１の閾値、およびズームアウト（例えば、視野を拡大する）のための第２の閾値等、複数の閾値が定義されてもよい。これらの閾値は異なり得る。例えば、第１の閾値は１．８であり得、第２の閾値は１．７であり得、したがって、遷移点は、ユーザがズームインしているかまたはズームアウトしているかに応じて異なる。
Data indicating the requested zoom level is provided to
いくつかの実現例では、カメラ１１０ａ、１１０ｂ間の切り替えは、単にしきい値を超えることによって制御される。デバイスがカメラ１１０ｂを使用してビデオを記録しており、ユーザが、より広角のカメラ１１０ａを代わりに使用するよう切り替えを引き起こすレベルにズームアウトを開始する状況を考える。ズームイン時にカメラ１１０ａからカメラ１１０ｂに切り替えるための閾値を１．８ｘとし、カメラ１１０ｂが使用できる最低ズーム位置を１．７ｘ（例えば、現在のカメラ１１０ｂの全最大視野を表すズームレベル）とする。その結果、ズームアウト時には、遷移はズームレベル１．７ｘでなされなければならず、なぜならば、第２のカメラ１１０ｂはそれよりも広い視野を提供できないからである。この状況に対処するために、ズームレベルが１．９ｘに低減されると、カメラ１１０ａは、出力フレームがカメラ１１０ｂによって出力される現在の画像データに依然として基づいているにもかかわらず、画像データのストリーミングを開始する。これは、例えば、１．９ｘから１．８ｘへのズームレベル遷移中に、両方のカメラ１１０ａ、１１０ｂがシーンの画像データをキャプチャおよびストリーミングしている期間を提供する。これは、カメラ１１０ａでキャプチャを初期化するための調整期間を提供するので、カメラ１１０ａは「ウォームアップ」することができ、オートフォーカス、オート露出、および他のプロセスが収束し始めることになる。この調整期間は、カメラ１１０ａの設定に対するマージンを提供して、カメラ１１０ａの設定およびキャプチャが初期化および整列されて、切り替え前にカメラ１１０ａによって現在使用されているものとマッチするようにしてもよい。
In some implementations, switching between
このようにして、デバイス１０２は、現在のズームレベル、ズーム変更の方向、およびデバイス１０２とのユーザ対話などの要因に基づいて、カメラ間の切り替えの必要性を予測し、それに備えることができる。例えば、ズームを１．９ｘに減少させるように命令するユーザ入力に基づいて、カメラ切り替えが必要とされる可能性が高いことを検出すると、デバイス１０２は、遷移が目標とされるズームレベルをユーザが命令する前に、またはそれが必要になるときに、使用されるべき次のカメラ１１０ａでキャプチャを開始する設定を命令することができる。例えば、１．９ｘのズームレベルがユーザによって指示されたときにデバイス１０２がビデオキャプチャおよび設定調整を開始し、ユーザが１．８ｘのズームレベルを指示するときまでにカメラ１１０ａが準備ができて所望の設定で動作している場合、デバイス１０２は、１．８ｘのズームレベルが指示されるとカメラを切り替えることができる。それにもかかわらず、何らかの理由で、１．７ｘのズームレベル（例えば、現在のカメラ１１０ｂの最大視野）が指示される時点までに設定の収束または実施が完了していない場合、デバイス１０２は、ズームレベルが１．７ｘに達したときに切り替えを強制することになる。
In this manner,
カメラセレクタ４２０は、カメラ選択信号または他の制御データをカメラ制御モジュール４４０に提供する。カメラ制御モジュール４４０は、カメラ１１０ａ、１１０ｂからビデオキャプチャパラメータを読み出し、ビデオキャプチャパラメータを設定するよう命令または設定も送信する。ビデオキャプチャパラメータは、例えば、どのカメラ１１０ａ、１１０ｂが画像データをキャプチャしているか、フレームキャプチャのレート（例えば、２４フレーム／秒（ｆｐｓ）、３０ｆｐｓ、６０ｆｐｓなど）、露出設定、画像センサ感度、（例えば、画像キャプチャの前または後に適用される）ゲイン、画像キャプチャ時間（例えば、各走査線がフレーム中に光をキャプチャする有効な「シャッタ速度」または持続時間）、レンズ開口サイズ、被写体距離（例えば、カメラから合焦された被写体の距離）、レンズ焦点位置または焦点距離、ＯＩＳステータス（例えば、ＯＩＳが有効にされているか否か、使用されるＯＩＳのモードなど）、ＯＩＳレンズ位置（例えば、水平および垂直オフセット、回転位置など）、適用されるＯＩＳの強度またはレベルなどを含むことができる。カメラ制御モジュール４４０は、一般的なビデオキャプチャのためのこれらおよび他のビデオキャプチャパラメータ、例えば露出、フレームレート等を設定することができる。カメラ制御モジュール２３２はまた、較正データ２３２を受信するかまたはそれにアクセスすることができる。較正手順は、デバイスの製造および品質保証の一部として、各デバイスについて、例えばデバイス１０２について行うことができる。較正データは、例えば、カメラ１１０ａ、１１０ｂの互いに対する関係、異なるレンズ位置に対するレンズ焦点位置と被写体距離（例えば、焦点面の距離）との間の関係などを微調整するためのデータを示すことができる。
カメラ制御モジュール４４０はまた、適切な時間にカメラ１１０ａ、１１０ｂを有効化および無効化して、カメラセレクタ４２０によって示されるカメラ切り替えを生じさせることができる。例えば、入力が１．０ｘから２．０ｘへのズームレベルの変化を示し、カメラセレクタ４２０からのデータが１．８ｘで開始する第２のカメラ１１０ｂへの変更を示す場合、カメラ制御モジュール４４０は、この遷移を生じさせるための制御命令を生成することができる。要求されたズーム速度に関する記憶された情報、およびズームが一貫して実行され得る速度に関する任意の制限を使用して、カメラ制御モジュール４４０は、切り替えを行う時間、例えば、画像出力に反映される漸増的または漸次的なズームが１．８ｘカメラ遷移点に到達する時間を判断する。この遷移のための時間は、デジタルズームが１．８ｘズームレベルに滑らかに到達し得るまで第１のカメラ１１０ａでデータをキャプチャし続ける時間の持続時間またはフレーム数、または１．８ｘズームレベルに到達することをユーザ入力が指定する時間に基づくことができる。
Camera control module 440 can also enable and disable
カメラ遷移を予期して、カメラ制御モジュール４４０は、現在のカメラ（例えば、第１のカメラ１１０ａ）からビデオキャプチャパラメータを読み出し、遷移後に使用されるべきカメラ（例えば、第２のカメラ１１０ｂ）のための対応するビデオキャプチャパラメータを設定することができる。これは、同じフレームレート、同じ露出レベル、同じレンズ開口、同じ焦点距離、同じＯＩＳモードまたはステータス（例えば、有効とされるかどうか）などを設定することを含むことができる。概して、カメラ間の遷移の前には、両方のカメラ１１０ａ、１１０ｂが能動的にビデオデータを同時にキャプチャしている期間がある。例えば、第１のカメラ１１０ａから第２のカメラ１１０ｂに切り替える場合、ズームレベルが、記録されたビデオのためにカメラ１１０ｂの出力を使用するよう切り替えるための閾値に達する前に、カメラ１１０ｂは開き、ビデオキャプチャを開始することになる。設定のための初期値は、カメラ１１０ａに現在使用されている設定の値に基づく。カメラ１１０ｂは、例えば所望の動作モード（例えば、適切な開口設定、正しい焦点距離設定、正しいＯＩＳ設定など）に収束するよう、指示された設定に向けてその動作の調整を開始する。このプロセスは、カメラ１１０ｂまたはデバイス１０２が、正しい焦点距離を判断するためのオートフォーカスプロセスなどを用いて、適切な設定を判断することを含んでもよい。カメラ１１０ｂのために設定の収束や演算が行われた後、録画に使用するビデオストリームは、カメラ１１０ｂが出力するビデオストリームに切り替わることになる。場合によっては、カメラ１１０ａ、１１０ｂのパラメータは同じでなくてもよいが、カメラ１１０ｂのパラメータは、それにもかかわらず、カメラ１１０ａによって使用されているキャプチャのためのパラメータに基づいて設定されてもよく、出力間の一貫性を促進または保つように設定されてもよい。例えば、カメラ１１０ａ、１１０ｂは、同じ開口範囲が利用可能でなくてもよく、したがって、カメラ制御モジュール４４０は、２つのカメラ１１０ａ、１１０ｂに対して同等またはほぼ同等の露出レベルを設定してもよいが、感度／ゲイン、キャプチャ時間（例えば、シャッタ速度）、および開口に対する設定の異なる組み合わせで設定してもよい。カメラ制御モジュール４４０は、遷移されるべきカメラを、適切な設定が、遷移に先立って、そのカメラを記録された最終出力フレームのために使用することに適用される状態で、起動することができ、したがって、画像キャプチャおよび入来ビデオフィードは、遷移時または遷移時より前に利用可能である。
Anticipating a camera transition, camera control module 440 reads the video capture parameters from the current camera (eg,
ビデオデータは、画像処理モジュール４５０を使用して処理される。このモジュール４５０は、ビデオキャプチャのために現在選択されているカメラ１１０ａ、１１０ｂからストリーミングされるキャプチャされたビデオフレームを受信することができる。モジュール４５０はまた、ジャイロスコープ、慣性測定ユニット（ＩＭＵ）、加速度計などのデバイス位置センサからセンサデータを受信する。モジュール４５０はまた、（例えば、カメラ制御モジュールまたはメモリから、）ビデオフレームをキャプチャするために使用されるパラメータを示すビデオキャプチャパラメータ値も受信する。これは、各フレームについて、さらにはフレームの異なる部分について、さらにはフレームをキャプチャまたは読み出すプロセス内の特定の走査線または時点について、ＯＩＳ要素位置、カメラ焦点位置、被写体距離、フレームキャプチャ時間、シャッタ速度／キャプチャ持続時間などを示すメタデータを含むことができる（図２、チャート２５２参照）。モジュール４５０はまた、デジタルズームレベル（例えば、倍率レベル、等価レンズ焦点距離、結果として生じる視野、クロッピングのレベルなどとして表されてもよい）を示すデータも受信する。画像処理モジュール４５０は、キャプチャされた画像フレームに変換を適用して、ローリングシャッタ、ＯＩＳシステムの動き、フォーカスブリージングなどによるアーチファクトを除去する。例えば、モジュール４５０は、ビデオフレームを取得し、フレームをキャプチャしたカメラの正準空間に変換または投影することができ、正準空間では、フレームのキャプチャの時変局面（例えば、ＯＩＳ要素の動き、ローリングシャッタなど）が除去される。
Video data is processed using
モジュール４５０は、画像フレームが第１のカメラ１１０ａまたは第２のカメラ１１０ｂでキャプチャされたかどうかにかかわらず、画像フレームを第１のカメラ１１０ａの正準カメラ空間に変換することができる。たとえば、第２のカメラ１１０ｂを使用してキャプチャされた画像の場合、モジュール４５０は、デバイス上のカメラ１１０ａ、１１０ｂの位置間の空間的差異およびカメラ１１０ｂの焦点位置などの他の要因を補正することによって、データを第２のカメラ１１０ｂの正準空間から第１のカメラ１１０ａの正準空間に変換する。これは、シーンのビューが両方のカメラ１１０ａ、１１０ｂから記録されたビデオにわたって一貫するように、第２のカメラ１１０ｂを使用してキャプチャされた画像データを第１のカメラの視野の部分に整列させることができる。この技術は、図５Ａ～図５Ｃおよび図６に関して以下でさらに論じられる。
デバイス１０２は、主カメラの正準空間に変換された画像データを受信および処理するＥＩＳ処理モジュール４６０を含むことができる。「主カメラ」とは、複数のカメラのうち、他のカメラの基準として予め指定されたカメラのことである。例えば、主カメラは、最も広い視野を有する第１のカメラ１１０ａとすることができ、任意の他のカメラ（例えば、第２のカメラ１１０ｂ）の出力は、第１のカメラ１１０ａの正準空間に変換またはマッピングすることができる。モジュール４５０は、両方のカメラ１１０ａ、１１０ｂの画像データを、フレーム内の時間依存変動（例えば、フレームの異なる走査線のキャプチャ時間、デバイス位置、ＯＩＳ位置等の差異）を補償した共通の標準化された正準空間にマッピングする。これは、ＥＩＳ処理モジュール４６０がフレーム内の時変キャプチャ特性を考慮に入れる必要性をなくすことによって、ＥＩＳ処理を著しく簡略化する。それはまた、単一のＥＩＳ処理ワークフローが、カメラ１１０ａ、１１０ｂのいずれかを使用してキャプチャされたビデオデータのために使用されることを可能にする。ＥＩＳ処理モジュール４６０は、潜在的にはフレームごとに、望ましいズーム設定、例えばズームレベルまたは視野も、受信する。これにより、ＥＩＳ処理モジュールは、適切な量の安定化を、フレームごとに、フレームに使用されるズームのレベルに従って、適用することが可能になる。ズームレベルが増加し、画像が拡大されるにつれて、カメラの動きの影響も拡大される。したがって、ＥＩＳ処理モジュール４６０は、ズームレベルが増加するにつれて、より強い安定化を適用して、出力ビデオにおいてほぼ一貫した安定性のレベルを維持することができる。ＥＩＳ処理モジュールは、図２および図３に関して上で説明されたＥＩＳ処理技術のいずれかまたはすべてを使用することができる。安定化は、画像データを主カメラの正準画像空間から仮想カメラ空間に投影すると考えることができ、画像データは、あたかもカメラがビデオキャプチャ中に実際のカメラが有するよりも滑らかな動きの軌道を有するかのように、出力をシミュレートするよう変換される。
ＥＩＳ処理モジュール４６０がフレームの画像データを安定化した後、そのフレームの画像データは出力され、および／またはデータ記憶装置（例えば、フラッシュメモリなどの不揮発性記憶媒体）に記録される。モジュール４１０、４４０がフレームについて判断するズームレベルは、フレームに必要とされるデジタルズームレベルをクロッピング、アップスケール、または他の態様で適用するために使用され得る。これらの技術の結果として、デバイス１０２は、キャプチャ中に２つのカメラ１１０ａ、１１０ｂの間でシームレスに遷移することができ、その遷移は、ユーザによって設定されたズームレベルに基づいてデバイス１０２によって自動的に管理される。したがって、結果として生じるビデオファイルは、ビデオファイルを通して散在される、異なるカメラ１１０ａ、１１０ｂを使用してキャプチャされたビデオセグメントを含むことができ、データは、両方のカメラ１１０ａ、１１０ｂからのセグメントにわたって一貫したＥＩＳ処理を維持しながら平滑なズーム遷移を示すよう整列および変換される。
After the
デバイス１０２は、ビデオ出力および／または記録モジュール４７０を含むことができる。ビデオ出力および／または記録モジュール４７０は、ＥＩＳ処理モジュール４６０の出力を受信し、これを、ビデオファイルとしてデバイスにローカルに、および／または遠隔で格納するために、提供するよう構成されてもよい。加えて、または代替として、ビデオ出力および／または記録モジュール４７０は、出力をストリーミングして、デバイス１０２上にローカルに、および／または例えばネットワークを介して別の表示装置に遠隔で、表示してもよい。
図５Ａ～図５Ｃは、マルチカメラビデオ安定化のための技術の例を示す図である。以下で説明する例では、カメラ１１０ａ、１１０ｂの一方が主カメラとして指定され、他方が副カメラとして指定される。以下に論じられる処理および変換を通して、記録されたビデオが副カメラを使用してキャプチャされると、副カメラの出力は、主カメラの正準空間にマッピングされる。説明を明確にするために、第１のカメラ１１０ａは主カメラとして使用され、第２のカメラ１１０ｂは副カメラとして使用される。これは、これらの例では、より広い視野を有するカメラが主カメラとして指定されることを意味する。これはいくつかの実現例では望ましいが、必須ではない。代替として、これらの技術は、狭い視野を有するカメラを主カメラとした状態で、使用されてもよい。
5A-5C are diagrams illustrating example techniques for multi-camera video stabilization. In the example described below, one of the
一般に、ホモグラフィ変換は、あるカメラ空間から別のカメラ空間に変更するために使用される変換である。表記ＡＨＢは、点をカメラ空間Ｂからカメラ空間Ａに変換するホモグラフを表す。仮想カメラとは、（ユーザに渡される、および/またはビデオファイルに記録される）最終的なシーンが生成される仮想カメラなどの合成カメラビューを指す。この有効カメラ位置は、可能な限り多くの時間的および空間的連続性を提供するために、典型的にはビデオ全体の持続時間にわたって安定化される（例えば、位置および向きにおいて可能な限り静止しているように見える）ことになる。本明細書で使用されるようにとおりでは、「主」カメラは、出力ビデオを生成するための基準フレームを定義するために使用される主たるカメラである。主カメラは、仮想カメラと空間的に同じ位置に位置するように定義され得るが、時間的に同じ位置に位置しなくてもよい。以下のほとんどの例では、第１のカメラ１１０ａが主カメラとして使用される。副カメラは、主カメラと対になっている。副カメラは、仮想カメラから空間的に離れるように定義され、副カメラの出力は、副カメラが先行カメラである場合、仮想カメラ空間に移動される（warped）ことになる。以下のほとんどの例では、第２のカメラ１１０ｂが副カメラとして使用される。「先行カメラ」は、ビデオキャプチャのために現在開いているカメラ、例えば、保存された出力ビデオを生成するために現在の画像データが使用されているカメラを指す。「後続カメラ」は、先行カメラと対にされる。後続カメラは、現在、ビデオキャプチャのためにオープンまたはアクティブではない。それにもかかわらず、後続カメラの出力が実際に出力カメラで使用されるかまたは出力カメラに保存される前に、先行カメラとしてステータスを取得することを予期して、後続カメラがビデオをキャプチャし始める起動期間があり得る。正準カメラは、経時的に変化しない固定された固有パラメータを有する概念的なカメラであり、例えば、正準カメラは、ＯＩＳの動作またはボイスコイルモータ（ＶＣＭ）レンズシフト（焦点合わせのためなど）の影響を受けない。カメラの各々について、正準カメラ（および対応する画像空間）、例えば、正準主カメラ空間および正準副カメラ空間がある。
Generally, a homography transform is a transform used to change from one camera space to another. The notation AHB represents a homograph that transforms a point from camera space B to camera space A. A virtual camera refers to a synthetic camera view, such as a virtual camera, from which the final scene (passed to the user and/or recorded in a video file) is generated. This effective camera position is typically stabilized for the duration of the entire video to provide as much temporal and spatial continuity as possible (e.g., as stationary as possible in position and orientation). appear to be). As used herein, the "primary" camera is the primary camera used to define the frame of reference for generating the output video. The primary camera may be defined to be spatially co-located with the virtual camera, but may not be temporally co-located. In most of the examples below, the
これらのパラメータは、カメラ１１０ａ、１１０ｂのうちのどれがキャプチャのために使用されているかに応じて、２つの異なる使用事例につながる。主カメラが先行している場合、画像データを物理カメラの空間と空間との間でマッピングする空間移動は必要とされない。合成ズームおよびＥＩＳ処理は、単に主カメラに対するＥＩＳ処理で行うことができる。他方、副カメラが先行している場合、システムは、２つのカメラ１１０ａ、１１０ｂによるキャプチャ間の切り替えにわたる一貫性のために、空間移動を適用して、出力を主カメラの視点にマッピングする。これは、２つのカメラ１１０ａ、１１０ｂの出力間の一貫性を再び保つために、焦点距離または被写体距離を変更するシーンにおける焦点の変化を考慮する。
These parameters lead to two different use cases depending on which of the
記録中にデジタルズームを提供する課題の１つは、特に、上述のような複数のカメラを使用する増分ズームまたは連続ズームのタイプに関して、ＥＩＳ処理をデジタルズーム機能とともに効率的に組み込むことである。ビデオキャプチャ中、デバイス１０２は、ＥＩＳ処理が、一連の時間的に安定化された画像シーケンスを取得するよう有効にされた状態で、ビデオを記録することができる。デバイス１０２は、ズーム機能およびＥＩＳ機能のためにホモグラフィを連結して、この効果を達成することができる。
One of the challenges of providing digital zoom during recording is efficiently incorporating EIS processing with digital zoom capability, especially for the type of incremental or continuous zoom using multiple cameras as described above. During video capture,
ＥＩＳ用のホモグラフィと、複数のカメラを用いたデジタルズーム用のホモグラフィとは、異なる目的を果たす。ＥＩＳのためのホモグラフィは、画像データを、画像センサの現在の（例えば実際の）出力フレーム（下付き文字「Ｒ」または「real」によって示される）から仮想フレーム（下付き文字「Ｖ」または「virt」によって示される）に変換する。以下の様々な等式および式において、変数ｔ（例えば、小文字ｔ）は、フレームのタイムスタンプであり、これは通常、フレームの中央走査線のキャプチャの時間に関連する。しかしながら、異なる走査線が、異なる時間にキャプチャされるため、他の走査線に対するタイムスタンプは、変動し得る。（例えば、フレームフレームの中央走査線に対するキャプチャの主時間とは異なる時間を考慮するために）個々の走査線の時間に言及するとき、時間はｔＬで示され、これは走査線Ｌのキャプチャタイムスタンプの時間を表す。ローリングシャッタを有するカメラを使用するとき、フレームの各走査線に対するタイムスタンプはわずかに異なり、したがって、項ｔＬは、異なる走査線に対してわずかに異なり得、また、カメラ位置も、異なる走査線に対してわずかに異なり得る。異なる項ＴＥは、主カメラ１１０ａと副カメラ１１０ｂとの間の外部並進（extrinsic translation）を指し、いかなるタイムスタンプも表さないことに留意されたい。同様に、ｎＴは、以下で論じられる平面ノルム（plane norm）であり、外部並進および時間項とは無関係である。
Homography for EIS and homography for digital zoom with multiple cameras serve different purposes. Homography for EIS converts image data from the image sensor's current (e.g., real) output frame (indicated by subscript "R" or "real") to a virtual frame (subscript "V" or (indicated by "virt"). In the various equations and formulas below, the variable t (eg, lowercase t) is the timestamp of the frame, which is typically related to the time of capture of the middle scanline of the frame. However, since different scanlines are captured at different times, the timestamps for other scanlines may vary. When referring to the time of an individual scan line (e.g., to allow for a different time than the main time of capture for the center scan line of a frame frame), the time is denoted tL , which is the capture of scan line L. Represents the time of the timestamp. When using a camera with a rolling shutter, the timestamp for each scanline of a frame is slightly different, so the term tL may be slightly different for different scanlines, and the camera position is also different for different scanlines. can be slightly different for Note that the different term TE refers to the extrinsic translation between the
ＥＩＳホモグラフィは、ＶＨＲまたはＨｅｉｓとして示される。ＥＩＳからのホモグラフィは、現在のまたは「実際の」フレームからの点を３Ｄ空間に投影解除し、次いで、それを仮想空間に投影し戻すことによって、データを現在のまたは「実際の」フレームから仮想フレームに変換するよう構成される。このホモグラフィは、以下のように表すことができる： EIS homographies are denoted as VHR or Heis . Homography from EIS transforms data from the current or 'real' frame by unprojecting the points from the current or 'real' frame into 3D space and then projecting them back into virtual space. configured to convert to virtual frames; This homography can be expressed as:
式１において、ＲＶは仮想カメラの３×３回転行列を表し、ＲＣは現在使用されている実カメラ１１０ａ～１１０ｂの３×３回転行列を表し、ＲＣおよびＲＶは両方ともカメラ位置データ（例えば、ジャイロスコープデータ）から得ることができる。Ｋは内部行列であり、ＫＶは仮想カメラの内部行列であり、ＫＣは現在のカメラ（例えば、カメラ１１０ａ～１１０ｂのどちらが使用されていようとも）の内部行列である。カメラ内部データ（例えば、カメラ形状、較正データ、およびカメラ特性に基づく）は、１つ以上のデータ記憶装置２３０に記憶され、そこから取り出され得る。内部行列は、下記の式２のように表すことができる。
In Equation 1, R V represents the 3×3 rotation matrix of the virtual camera, R C represents the 3×3 rotation matrix of the
式２において、ｆは焦点距離であり、ｏｘおよびｏｙは主点である。上記の式において、ＲＣ、ＲＶおよびＫＣ
－１は時間依存性であり、時間ｔの関数として示される。これらの値は、時間的連続性を保証するために、以前のフレームまたは以前の出力フレーム生成プロセスのための情報を保持するかまたはそれに基づくことができる。ＲＶは、仮想空間のための３×３回転行列であり、過去の仮想カメラフレームの軌道のフィルタに基づいて計算され、遅延および「先を見る」戦略が使用される場合には、潜在的に、いくつかの将来のフレームに対しても計算される。ＲＣは、現在のフレームのための３×３回転行列であり、現在のフレームを第１のフレームに変換するジャイロスコープまたはデバイス位置センサ２２０からのデータに基づいて計算される。ＫＣ
－１は、光学主中心と現在のＯＩＳ値とに基づいて算出される。
In Equation 2, f is the focal length and o x and o y are the principal points. In the above equations R C , R V and K C −1 are time dependent and are shown as a function of time t. These values may retain or be based on information for previous frames or previous output frame generation processes to ensure temporal continuity. RV is a 3×3 rotation matrix for virtual space, computed based on a filter of the trajectories of past virtual camera frames, and potentially Also computed for some future frames. R C is a 3×3 rotation matrix for the current frame, calculated based on data from the gyroscope or
ズーム処理のためのホモグラフィは、あるカメラ１１０ａのビューから別のカメラ１１０ｂのビューに変換される。たとえば、このホモグラフィは、主カメラ１１０ａの現在のフレームから、副カメラ１１０ｂのフレームに変換し、mainHsecとして示される。このホモグラフィは、このホモグラフィを計算するために４点手法を使用して計算してもよいが、このホモグラフィをユークリッドホモグラフィとして単純化することによって、行列自体も以下のように分解することができる：
The homography for zooming is transformed from one
副カメラ（例えば、望遠カメラ１１０ｂ）画像上の点Psecを、主カメラ（例えば、広角カメラ１１０ａ）画像上の対応する点Pmainに、Ｓの任意のスカラーまでもたらす、ユークリッドホモグラフィ変換を仮定すると、このホモグラフィ行列は以下のように分解することができる：
Assume a Euclidean homography transformation that brings a point P sec on the secondary camera (e.g.,
この式において、Ext(t)は外部変換であり、それは、平面の深さに依存する行列である： In this formula, Ext(t) is the extrinsic transformation, which is a matrix that depends on the depth of the plane:
その結果、組み合わせは以下のように示される： The resulting combinations are shown below:
式４において、ｎＴは平面ノルム（例えば、焦点面に垂直なベクトル）であり、Ｄは焦点面が位置する深さまたは距離である。変数ＲＥおよびＴＥは、主カメラ１１０ａと副カメラ１１０ｂとの間の外部回転および外部並進をそれぞれ示す。
In Equation 4, nT is the plane norm (eg, vector normal to the focal plane) and D is the depth or distance at which the focal plane is located. Variables R E and T E denote the external rotation and translation, respectively, between
変数KmainおよびKsecは、主カメラ１１０ａおよび副カメラ１１０ｂの内部行列であり、これらは、上述したＥＩＳホモグラフィと同じフォーマットを有する。式４において、Kmain、Ksec、Ｄは時間依存である。しかしながら、ＥＩＳ定式化における対応するバージョンとは異なり、Kmain、Ksec、およびＤは、ズーム定式化において過去の情報を保持しない。KmainおよびKsecは両方とも、フレームごとに変化する、（例えば、ＯＩＳ位置データ２４８ａ～２４８ｂを使用する）現在のＶＣＭ値およびＯＩＳ値に基づいて計算される。変数Ｄは、オートフォーカス中の焦点距離値に関連し、ビデオキャプチャおよび記録中に時間とともに変化してもよい被写体距離に対応する。上述のように、被写体距離は、カメラのために選択された現在の焦点に対する、デバイス１０２からの焦点面の距離を指す。被写体距離は、例えば、焦点位置データ２４６ａ～２４６ｂからのカメラの焦点位置と、合焦された被写体がカメラセンサ面からどれだけ遠くにあるかを示す被写体距離に対する焦点設定または焦点要素位置の対応を示すルックアップテーブルなどの較正データとを使用して判断することができる。カメラ１１０ａ、１１０ｂ間の位置のオフセットを前提として、キャプチャされた画像間の関係は、被写体距離に応じていくらか変動し得、変換は、被写体距離、レンズ焦点位置、および／または他のデータを使用して、これらの影響を考慮することができる。
The variables K main and K sec are the internal matrices of the
上記の２つのホモグラフィ分解は、一連の動作、すなわち、（１）ソースカメラから投影解除すること、（２）ソースカメラからワールド３次元基準フレームに変換すること、（３）ワールドからターゲット３次元基準フレームに変換すること、（４）ターゲットカメラに再投影して戻すこと、としてまとめることができる。これらは、以下の表１に要約され、図５Ａ～図５Ｃおよび図６に関しても論じられる。 The above two homography decompositions are a sequence of operations: (1) unprojecting from the source camera; (2) transforming from the source camera to the world 3D reference frame; (4) reprojecting back to the target camera. These are summarized in Table 1 below and also discussed with respect to FIGS. 5A-5C and FIG.
次のセクションは、ズームホモグラフィとＥＩＳホモグラフィとを効果的かつ計算効率の良い態様で組み合わせるための技術を説明する。使用可能な技術の１つは、１つのカメラ、典型的には最も広い視野を有するカメラを主カメラとして設定し、処理された出力を主カメラに関してマッピングすることである。 The next section describes techniques for combining zoom homography and EIS homography in an efficient and computationally efficient manner. One technique that can be used is to set one camera, typically the one with the widest field of view, as the primary camera and map the processed output with respect to the primary camera.
主カメラ（例えば広角カメラ１１０ａ）がビデオキャプチャに使用される場合、項mainHsecは恒等である。その結果、結合されたホモグラフィは、式７に示されるように、単純にＥＩＳホモグラフィHeisであり得る。
If the main camera (eg, wide-
項ＲＤは、ジャイロスコープまたは他の動きセンサからのデータに基づいて計算されたデバイス回転を示し、主カメラ１１０ａおよび副カメラ１１０ｂの両方について同じ値を有する。
The term RD indicates device rotation calculated based on data from a gyroscope or other motion sensor and has the same value for both
具体的には、３つのホモグラフィ変換は以下の通りである：
●sec_canHsec：（０，０）ＯＩＳ運動、０ローリングシャッタ時間、固定焦点距離、およびフレーム中心での回転を伴う、実際の副カメラから正準副カメラ空間への変換。
Specifically, the three homography transformations are:
• sec_can H sec : Transformation from real secondary camera to canonical secondary camera space with (0,0) OIS motion, 0 rolling shutter time, fixed focal length, and rotation about the frame center.
ここで、Rsec
-1(t)*Ksec
-1(t)は、必要とされるものに応じて、各走査線またはフレーム中心から取得され、これは、所与のＯＩＳ／ＶＣＭ値と協働することになる。Rsec_can(t)は、各走査線におけるRsec
-1(t)とは反対に、フレーム中心における回転である。
●main_canHsec_can：正準副カメラから正準主カメラ空間への変換。
where R sec -1 (t)*K sec -1 (t) is obtained from each scan line or frame center, depending on what is required, which is the same for a given OIS/VCM value and will collaborate. R sec_can (t) is the rotation about the frame center as opposed to R sec −1 (t) at each scanline.
● main_can H sec_can : Transform from canonical secondary camera to canonical main camera space.
この式において、Ext(t)は、被写体距離（例えば、カメラセンサからの焦点面の空間における深さ）に依存する行列である外部変換である： In this equation, Ext(t) is the extrinsic transform, a matrix that depends on the object distance (e.g. depth in space of the focal plane from the camera sensor):
ここで、ｎＴは平面ノルムであり、Ｄは平面ｎＴの深さであり、ＲＥおよびＴＥは、副カメラと主カメラとの間の外部回転および並進である。
●virtHmain_can：正準副カメラから安定化された仮想カメラ空間への変換であり、この回転はＥＩＳアルゴリズムによってフィルタリングされる。
where nT is the plane norm, D is the depth of plane nT , and RE and TE are the extrinsic rotations and translations between the secondary and primary cameras.
- virt H main_can : transformation from the canonical secondary camera to the stabilized virtual camera space, this rotation is filtered by the EIS algorithm.
Rmain_can=Rsec_canは、フレーム中心における現在の実際のカメラ回転であることに留意されたく、なぜならば、主カメラおよび副カメラは互いに強固に取り付けられるからである。 Note that R main_can =R sec_can is the current actual camera rotation at the frame center, because the main and secondary cameras are rigidly attached to each other.
３つのホモグラフィを連結すると、最終的なホモグラフィが得られる： Concatenating the three homographies gives the final homography:
ここで、中間項main_canHsec_canはズーム処理に由来する。
主カメラが先行している場合、最終的なホモグラフィは、ＥＩＳ処理のためのホモグラフィだけに単純化する。
上記の式は次式となる：
Here, the intermediate terms main_can H sec_can are derived from the zoom processing.
If the main camera is leading, the final homography simplifies to just the homography for EIS processing.
The above equation becomes:
これは、main_canHsec_can＝恒等を設定し、上記の式は、元のHeis式になる： This sets main_can H sec_can = identity and the above formula becomes the original Heis formula:
上記のセクションから、概して、２つの式を有することになる：
副カメラが先行しているとき、式は以下の通りである：
From the above section, we generally have two formulas:
When the secondary camera is leading, the formula is as follows:
主カメラが先行しているとき、式は以下の通りである： When the main camera is leading, the formula is:
エンジニアリングの観点から、ＥＩＳの実現中、副カメラは存在せず、したがって、すべての仮想カメラは、現在の先行カメラ上に位置し、これは、元のＥＩＳパイプラインについて以下を意味する：
主カメラが先行しているとき、
From an engineering point of view, there is no secondary camera during the EIS realization, so all virtual cameras are located above the current leading camera, which means for the original EIS pipeline:
When the main camera is ahead,
副カメラが先行しているとき、 When the secondary camera is leading,
ＥＩＳの基本式から、 From the basic EIS formula,
項ＫＶは常に現在のカメラと同じに定義され、定義によれば、Kv_mainとKv_secとの間の唯一の差は、２つのカメラ間のＦＯＶである（仮想カメラは画像の中心に配置されるので、主点は同じである）。 The term KV is always defined to be the same as the current camera, and by definition the only difference between Kv_main and Kv_sec is the FOV between the two cameras (the virtual camera is centered in the image the principal point is the same).
上記の場合に対応するために、システムは、主カメラおよび副カメラにおける視野が切り替え点で互いに一致することを意図的に保証することができる。この視野整合は、すべての動作に先立って、副カメラから主カメラに視野をスケーリングするハードウェアクロッピングを通して効率的に行われ、行列Ｓは、以下の式で使用される。 To accommodate the above cases, the system can intentionally ensure that the fields of view in the primary and secondary cameras coincide with each other at the switching point. This field-of-view matching is efficiently done through hardware cropping that scales the field-of-view from the secondary camera to the primary camera prior to any operation, and the matrix S is used in the equation below.
ソフトウェア側から、ホモグラフィの式は、以下に適合する： From the software side, the homography formula fits:
最終的に適用されるホモグラフィは、以下のようになり得る： The final applied homography can be:
その変換は、正準副空間から、正準副空間と同じ視野（例えばスケール）を有するが、カメラ外部因子によって引き起こされるすべての並進／回転が中和された正規化された正準副空間にマッピングする。 The transformation is from the canonical subspace to a normalized canonical subspace with the same field of view (e.g. scale) as the canonical subspace, but with all translations/rotations caused by camera extrinsic factors neutralized. map.
副カメラ（例えば、望遠カメラ１１０ｂ）と見なされるカメラがビデオキャプチャに使用されるとき、異なるカメラ１１０ａ、１１０ｂを使用するキャプチャの期間にわたって一貫した画像特性が維持されることを可能にするために、副カメラからのビューを主たるカメラからのビューに変換または投影するために使用される一連の変換がある。これらの変換は、（１）ソースカメラ（例えば、第２のカメラ１１０ｂ）から投影解除して、ＯＩＳモーションおよびローリングシャッタのようなカメラ固有の効果を除去すること、（２）３次元世界等の正準基準フレームに変換すること、（３）正準基準フレームから主カメラ基準に変換すること（例えば、第１のカメラ１１０ａによってキャプチャされるであろうフレームに整列すること）、および（４）主カメラ１１０ａのフレームから電子安定化が適用された仮想カメラフレームに再投影することを含むことができる。変換の順序に基づいて、本技術は、平行化第一または安定化第一で行われ得、それ異なる性能結果をもたらす。図５Ａ～図５Ｃは、第２のカメラ１１０ｂからのキャプチャされた画像を、第１のカメラ１１０ａのために生成される安定化されたビューと整列されかつそれと一貫する安定化されたビューに変換するための異なる技術を示す。
When a camera considered a secondary camera (e.g.,
図５Ａは、平行化を第一に実行するマルチカメラビデオ安定化のための例示的な技術を示す図である。この技術は、第２のカメラ１１０ｂ（たとえば、この例では望遠カメラ）からの視界を最初に第１のカメラ１１０ａ（例えば、この例では広角カメラ）に平行化し、次いで安定化を適用する。この技術は、ＥＩＳホモグラフィと主たる副から主へのホモグラフィとの単純な連結によって表されるという利点を提供し、これは、ビデオフィードを処理する効率を高める。これを次式に示す：
FIG. 5A illustrates an exemplary technique for multi-camera video stabilization that performs parallelization first. This technique first collimates the view from the
図５Ｂは、安定化を第一に実行するマルチカメラビデオ安定化のための例示的な技術を示す図である。このオプションは、副カメラフレームからの画像を安定化された仮想主カメラフレームに安定化し、次いで、平行化を適用して仮想副カメラフレームを主カメラフレームに投影する。しかしながら、この技術は、副カメラから主カメラへの変換が主カメラフレーム座標系において定義されるため、図５Ａの手法ほど単純ではない。したがって、同じ変換を実行するために、安定化された仮想主カメラフレームは、仮想副フレームから回転させて戻るようにしなければならない。これは、以下の式で表され、「virt_main」は主カメラの仮想フレーム（安定化された）を指し、「virt_sec」は副カメラの仮想フレーム（安定化された）を指し、「real_main」は主カメラの実フレーム（安定化されていない）を指し、「real_sec」は副カメラの実フレームを指す。 FIG. 5B illustrates an exemplary technique for multi-camera video stabilization that performs stabilization first. This option stabilizes the image from the secondary camera frame into a stabilized virtual primary camera frame, then applies parallelization to project the virtual secondary camera frame into the primary camera frame. However, this technique is not as simple as the approach of Figure 5A because the transformation from the secondary camera to the primary camera is defined in the primary camera frame coordinate system. Therefore, the stabilized virtual primary camera frame must be rotated back from the virtual secondary frame to perform the same transformation. This is expressed in the following formula, where "virt_main" refers to the primary camera's virtual frame (stabilized), "virt_sec" refers to the secondary camera's virtual frame (stabilized), and "real_main" refers to It refers to the real frame of the primary camera (not stabilized), and "real_sec" refers to the real frame of the secondary camera.
項目を連結することによって、結果は、図５Ａと同じ全体的な変換である。たとえ異なる幾何学的定義が存在しても、画像に対する全体的な効果は同じである。 By concatenating the items, the result is the same overall transformation as in FIG. 5A. Even if different geometric definitions exist, the overall effect on the image is the same.
図５Ｃは、第一に正準カメラに平行化するマルチカメラビデオ安定化のための例示的な技術を示す図である。上で論じた幾何学定義を使用して、実現例は、現在の実カメラビューから正準カメラビューに平行化することによって、より効率的にすることができ、正準カメラビューは、現在の先行カメラの仮想カメラとして定義されるが、固定ＯＩＳ（ＯＩＳ＿Ｘ＝０，ＯＩＳ＿Ｙ＝０）および固定ＶＣＭ（ＶＣＭ＝３００）を有する。 FIG. 5C shows an exemplary technique for multi-camera video stabilization that first collimates to the canonical camera. Using the geometry definitions discussed above, implementations can be made more efficient by collimating from the current real camera view to the canonical camera view, which is the current Defined as a virtual camera of the leading camera, but with fixed OIS (OIS_X=0, OIS_Y=0) and fixed VCM (VCM=300).
この結果は、図５Ａの第１の場合における式と同様である。例えば、これは以下を与える： The result is similar to the equation in the first case of FIG. 5A. For example this gives:
しかしながら、この場合、Kmain -1(t)Kmain(t)対を挿入する代わりに、Kmain_can -1(t)*Kmain_can(t)を挿入することになり、ここで、Kmain_canは、正準主カメラの固有の特性を示す。項 H'eisおよび mainH'secは、Kmain_canを使用して計算される。 However, in this case, instead of inserting K main -1 (t)K main (t) pairs, we would insert K main_can -1 (t)*K main_can (t), where K main_can is , denote the intrinsic properties of the canonical primary camera. The terms H' eis and main H' sec are calculated using K main_can .
この手法は、システムが副カメラおよび主カメラの両方のメタデータについて同時に問い合わせる必要がないので、著しい利点を提供する。 This approach provides a significant advantage because the system does not have to query metadata for both secondary and primary cameras at the same time.
組み合わされたホモグラフィ、Hcombined は、正準位置表現および走査線単位処理とともに使用することができる。まず、正準位置について説明する。議論された Heis および mainHsec の元の定義から、システムは、デジタルズーム処理のためのホモグラフィにおいて使用された Ksec および Kmain 項の両方についてＯＩＳおよびＶＣＭ情報を使用する。このシステムはまた、ＥＩＳホモグラフィで使用された項ＫＣも使用する。 The combined homography, H combined , can be used with canonical position representation and line-by-line processing. First, the canonical position will be explained. From the original definitions of Heis and main Hsec discussed, the system uses OIS and VCM information for both the Ksec and Kmain terms used in the homography for the digital zoom process. This system also uses the term K C used in the EIS homography.
以下の式において、ある変数は、主カメラおよび副カメラのそれぞれについて、現在のＯＩＳ値およびＶＣＭ値に依存する時間依存変数である。これらの時間依存変数は、 Kmain
-1(t), Kmain(t), および Ksec
-1(t)を含む。これらの項の時間依存性は、システムが両方のカメラ１１０ａ、１１０ｂのＯＩＳ／ＶＣＭメタデータの両方を同時にストリーミングする必要があるであろうことを意味する。しかしながら、正準表現を使用すると、データ収集要件を大幅に低減することができる。例えば、項 Ksec
C および Kmain
Cが、ＯＩＳおよびＶＣＭの両方が所定の標準または正準位置に配置された正準カメラモデルとして定義されることを含み、正準位置は、ＯＩＳＸ＝ＯＩＳＹ＝０およびＶＣＭ＝３００を有する所定の位置である、例示的な表現が以下に示される。元の Hcombined = Heis * mainHsecについて、その表現は以下のように分解することができる：
In the equations below, certain variables are time-dependent variables that depend on the current OIS and VCM values for the primary and secondary cameras, respectively. These time dependent variables include K main -1 (t), K main (t), and K sec -1 (t). The time dependence of these terms means that the system will need to stream both OIS/VCM metadata for both
上記の式において、項 Csec(t) は、現在のカメラビューを正準カメラビューに変換する（例えば、ＯＩＳレンズ位置の影響を除去する）補正行列である。デジタルズームホモグラフィは、第２のカメラ１１０ｂ（例えば、望遠カメラ）からビデオをキャプチャするときにのみアクティブであるため、現在の副カメラビューを正準副カメラビューに変換する、１つの補正行列項 Csec(t) のみが必要とされる。この新しい式において、Kmain Cおよび Ksec Cは両方とも、デジタルズームのためのホモグラフィについて、経時的に一定である： In the above equation, the term C sec (t) is a correction matrix that transforms the current camera view to a canonical camera view (eg, removes the effects of OIS lens position). One correction matrix term C Only sec (t) is required. In this new formula, both K main C and K sec C are constant over time for the homography for digital zoom:
結果として、唯一の時間依存成分は D(t) であり、これは、カメラから焦点面までの距離であり得る、焦点のための被写体距離に依存する。したがって、必要とされる唯一の新たなメタデータは、被写体距離であろう。 As a result, the only time-dependent component is D(t), which depends on the subject distance for focus, which can be the distance from the camera to the focal plane. Therefore, the only new metadata that would be needed would be subject distance.
次に、各走査線の補正について説明する。走査線ごとの補正は、上述の式の右側の別の行列 S(tL, L) の使用を含むことができる。この追加の行列は、走査線Ｌごとに潜在的に示される異なる調整を用いて、走査線に特定である補正を行うために使用することができる。走査線ごとの補正行列 S(tL, L) は、（例えば、走査線がキャプチャされた適切な時間のジャイロスコープセンサデータを取得するための）走査線の現在の時間 tL 、および（例えば、各走査線を中央走査線と整列させるために変形またはシフトを補正するために使用される）走査線番号 L に依存する。その結果、行列 S(tL, L) は、各走査線 L に対する補正およびその対応するキャプチャ時間 tL を含むことができる。 Next, correction of each scanning line will be described. Scanline-by-scanline correction can involve using another matrix S(t L , L) on the right side of the above equation. This additional matrix can be used to make corrections that are scanline specific, with different adjustments potentially shown for each scanline L. The per-scanline correction matrix S(t L , L) is the scanline's current time t L (e.g. to obtain gyroscope sensor data for the appropriate time the scanline was captured), and (e.g. , used to correct for deformations or shifts to align each scanline with the central scanline) depending on the scanline number L. As a result, the matrix S(t L , L) can contain the correction for each scan line L and its corresponding capture time t L .
元の分解された式から、走査線ごとの補正行列の加算は、以下を与える： From the original decomposed formula, the scanline-by-scanline addition of the correction matrix gives:
最終的なホモグラフィは、正準位置および走査線ごとの項を加算することによって判断することができる： The final homography can be determined by adding the canonical position and scanline-wise terms:
この例では、HC eis = KV*RV(t)* RD -1(t) *Kmain C-1 は、正準主カメラのみにおける安定化行列を表す。加えて、項 mainHC sec = Kmain C* (RE -1 - D(t)*TE*nT) * Ksec C-1は、正準副ビューから正準主ビューへのホモグラフィを表す。項 Csec(t) は、現在の副カメラビューを正準副カメラビューにするホモグラフィである。項 S(tL, L) は、各走査線の変形を中央線の基準位置にもってくる走査線ごとの補正である。 In this example, H C eis = K V *R V (t)* R D -1 (t) *K main C-1 represents the stabilization matrix for the canonical main camera only. In addition, the term main H C sec = K main C * (R E -1 - D(t)*T E *n T ) * K sec C-1 is the homo represents graphics. The term C sec (t) is a homography that makes the current secondary camera view the canonical secondary camera view. The term S(t L , L) is a line-by-line correction that brings the deformation of each line to the centerline reference position.
図６は、マルチカメラビデオ安定化を効率的に提供するために使用され得る例示的な変換を示す図である。ＥＩＳと連続ズームとの組合せは、３つのホモグラフィまたは変換の組合せとして解釈され得る。 FIG. 6 is a diagram illustrating exemplary transforms that may be used to efficiently provide multi-camera video stabilization. The combination of EIS and continuous zoom can be interpreted as a combination of three homographies or transformations.
この例では、画像フレーム６１１は、第２のカメラ１１０ｂによってキャプチャされた画像フレームを表す。図に表される複数の変換を通して、画像データは、視覚的アーチファクト（例えば、ローリングシャッタ、ＯＩＳの動きなど）を除去するように処理され、第１のカメラ１１０ａの視野と整列され、前述のＥＩＳ技術を使用して安定化される。別個の変換６１０、６２０、６３０は、様々な画像６１１～６１４と同様に、例示の目的で示されている。実現例は、中間画像を別に生成する必要なく、論じた動作および機能を組み合わせることができる。
In this example,
第１の変換６１０は、実際の副カメラ１１０ｂ（たとえば、望遠カメラ）からのキャプチャされた画像６１１に対して動作し、画像６１１を正準の第２のカメラ画像６１２に変換する。変換６１０は、正準の第２のカメラ画像６１２がＯＩＳの動きを含まず、ローリングシャッタを含まず、固定焦点距離を含み、フレーム中心での回転を伴うように、画像６１１の画像データを調整する。いくつかの実現例では、第１の変換６１０は、画像６１１のキャプチャの過程にわたる第２のカメラ１１０ｂの動き、画像６１１のキャプチャの過程にわたる焦点距離の変化、画像６１１のキャプチャ中のＯＩＳシステムの動きなどの影響を除去または低減するために、画像６１１の各画像走査線を個々に調整してもよい。この変換６１０は、sec_canHsec、つまり第２のカメラビュー（「sec」）から正準の第２のカメラビュー（「sec_can」）への変換として表される。画像６１１のキャプチャ中にＯＩＳ安定化が使用されてもよいが、画像６１１も画像６１２もＥＩＳ処理を使用して安定化されていない。
A
第２変換６２０は、正準の第２のカメラ画像６１２から正準の第１のカメラ画像６１３への変換である。これは、画像データを画像６１２の同じ正準空間内に維持することができるが、画像データを、主カメラ、例えば、第１の（例えば、広角）カメラ１１０ａによってキャプチャされるシーンに整列させることができる。この変換６２０は、第２のカメラ１１０ｂと第１のカメラ１１０ａとの間の空間的差異を補正することができる。２つのカメラ１１０ａ、１１０ｂは、それらの間の空間的オフセットおよび潜在的に位置または向きにおける他の差異を伴って、電話または他のデバイス上に位置する。第２の変換６１２は、これらの差異を補正して、画像６１２を第１のカメラ１１０ａの視野の対応する部分に投影することができる。加えて、カメラ１１０ａ、１１０ｂのビュー間の差は、現在の焦点深度に応じて変動し得る。例えば、カメラ１１０ａ、１１０ｂの一方または両方は、焦点距離に応じて有効視野を調整するフォーカスブリージングを経験する場合がある。第２の変換６２０は、これらの差異を考慮に入れることができ、デバイス１０２が、焦点距離を使用して、主カメラ正準視野に対する画像６１２の整列を微調整することを可能にする。典型的には、ＯＩＳ位置等のための同じ正準パラメータが、主カメラ正準表現および副カメラ正準表現の両方のために使用されるが、差異がある場合、これらは、第２の変換６２０を使用することに関して補正されることができる。
The
第３の変換６２０は、安定化された画像６１４を生成するために、画像６１３にＥＩＳを適用する。例えば、この変換６２０は、正準主カメラビューからの画像データを、カメラ位置が平滑化またはフィルタリングされる仮想カメラビューに変換することができる。例えば、この変換６３０は、図３に関して上で論じた第２の投影とすることができ、仮想カメラ位置が経時的にフィルタリングされ、フレーム間の位置の変化が用いられ、動きがぼやけに起因して潜在的に許容され、パニング対偶発的な動きの可能性が考慮され、出力フレームを満たすための将来の動きまたは調整の適応が実行されるなどする。ＥＩＳ処理は、処理されている現在のフレーム、ならびに前のフレームのウィンドウおよび将来のフレームのウィンドウを使用して実行され得る。当然ながら、ＥＩＳ処理は、処理に必要な「将来のフレーム」を収集する画像キャプチャタイミングから遅れ得る。
A
上述のように、変換６１０、６２０、６３０は、効率のために組み合わせるかまたは統合することができ、中間画像６１２および６１３を生成する必要はない。むしろ、デバイス１０２は、適切な変換６１０、６２０、６３０を決定し、第１のカメラ１１０ａを使用してキャプチャされた画像から生成された安定化された画像データと整列され、それと一貫する安定化された画像６１４を直接生成してもよい。
As noted above,
図６の例は、第２のカメラ１１０ｂからのビデオフレームから第１のカメラ１１０ａの安定化された空間への変換を示すが、これは、ズームレベルが第２のカメラ１１０ｂの視野と同じかまたはそれより小さい視野に対応するときに使用される。第１のカメラ１１０ａが使用される場合、例えば、視野が第２のカメラ１１０ｂの視野よりも大きい場合、２つの変換のみが必要とされる。第１の変換６１０と同様に、ビデオフレームの異なる走査線に対して変化する条件などの時間依存の影響をキャプチャされた画像から除去するために変換が適用される。変換６１０と同様に、これはローリングシャッタ、ＯＩＳ位置などを補償する。しかしながら、変換は、画像データを主カメラ（例えば、カメラ１１０ａ）の正準カメラ空間に直接投影する。主カメラの正準カメラ空間から、ＥＩＳ変換、例えば、変換６３０のみが必要とされる。したがって、画像データが主カメラを使用してキャプチャされるとき、２つのカメラ１１０ａ、１１０ｂの空間特性を関係付ける必要はなく、なぜならば、全体的な画像キャプチャおよびＥＩＳ処理が、たとえば、主カメラのための正準の非時間依存基準フレームから、主カメラのための基準フレームにおいて一貫して行われるからである。
The example of FIG. 6 shows the transformation from the video frames from the
図６の処理は、第２のカメラ１１０ｂの出力を第１のカメラ１１０ａの安定化された出力に変換またはマッピングするために使用され得る様々なプロセスを示す。これは、ビデオキャプチャおよび記録中に、異なるカメラ１１０ａ、１１０ｂによってキャプチャされたビデオを使用する間に遷移するときのビューにおける一貫性を提供する。例えば、第２のカメラ１１０ｂからのビデオは、第１のカメラ１１０ａからのビデオと整列させられるが、視野を正しい位置に位置決めするだけでなく、ＥＩＳ特性も整合させる。その結果、画像キャプチャ中の（例えば、閾値ズームレベルを上回るまたは下回るデジタルズームによる）カメラ間の切り替えは、ＥＩＳがアクティブである状態で、カメラ１１０ａ、１１０ｂ間の遷移点におけるぎくしゃくした動き、突然の画像シフトまたはビューのシフト、および目に見えるビデオの滑らかさのシフト（たとえば、ＥＩＳ適用における変化）を最小限に抑えるかまたは回避する方法でビデオフィードを整合させて、行うことができる。この技術の別の利点は、カメラ１１０ａからのビデオを、第２のカメラ１１０ｂとの一貫性のためにいかなる調整または処理も伴わずに使用することができることである。第２のカメラ１１０ｂの出力のみが調整され、第１のカメラ１１０ａのビューおよび特性に対して一貫するように整列させられる。
The process of FIG. 6 illustrates various processes that may be used to transform or map the output of the
デバイス１０２はまた、異なるカメラ１１０ａ、１１０ｂを使用してキャプチャされるビデオの特性をより良く合致させるように、ビデオキャプチャ中に、ビデオキャプチャ設定を調整することができる。例えば、第１のカメラ１１０ａを使用するキャプチャから第２のカメラ１１０ｂを使用するキャプチャに遷移するとき、デバイス１０２は、遷移の直前に第１のカメラ１１０ａのために使用されている、焦点距離、ＯＩＳパラメータ（例えば、ＯＩＳが有効にされているかどうか、適用されるＯＩＳの強度またはレベルなど）、露出設定（例えば、ＩＳＯ、センサ感度、ゲイン、フレームキャプチャ時間またはシャッタ速度など）などの特性を判断することができる。デバイス１０２は、次いで、第２のカメラ１１０ｂに、それらの設定、または同等の結果を提供する設定を、第２のカメラ１１０ｂのためのキャプチャのために使用させることができる。
The
これは、第２のカメラ１１０ｂからのビデオを、記録されたビデオに含めるために、遷移に先立って設定変更をなすことを伴い得る。たとえば、動作中に調整を行うための時間を提供するために、デバイス１０２は、カメラ切り替えが適切であるかまたは必要であることを検出し、それに応答して、第１のカメラ１１０ａの現在の設定を判断し、第１のカメラ１１０ａによって使用される設定と同等の設定を使用して開始するように第２のカメラ１１０ｂに命令してもよい。これは、例えば、第２のカメラ１１０ｂの電源を投入し、第２のカメラ１１０ｂのためにＯＩＳシステムの安定化を作動および達成し、第１のカメラ１１０ａの露出に合致するように第２のカメラ１１０ｂの露出を調整し、第１のカメラ１１０ａの焦点位置に合致するように第２のカメラ１１０ｂの焦点位置を設定するなどのために、充分な時間を提供することができる。第２のカメラ１１０ｂが、例えば、第１のカメラ１１０ａのビデオキャプチャパラメータと一貫するビデオキャプチャパラメータを用いて、適切なモードで動作していると、デバイス１０２は、ビデオキャプチャのために第１のカメラ１１０ａを使用することから第２のカメラ１１０ｂを使用することに切り替える。同様に、第２のカメラ１１０ｂから第１のカメラ１１０ａに遷移するとき、デバイス１０２は、第２のカメラ１１０ｂによって使用されるビデオキャプチャパラメータ値を判断することができ、遷移を行う前に第１のカメラ１１０ａのために対応するビデオキャプチャパラメータ値を設定することができる。
This may involve making a setting change prior to the transition to include the video from the
いくつかの実現例について説明した。それにもかかわらず、本開示の精神および範囲から逸脱することなく、様々な修正がなされ得ることが理解されるであろう。例えば、上に示されたフローを、ステップを並べ替え、追加し、または除去して、様々な形態で用いてもよい。別の例示的修正として、上記の説明は、主に、画像データの処理が、第１および／または第２のカメラによってビデオをキャプチャしている間に実行されることを説明しているが、いくつかの実現例では、第２のカメラのための第２の正準基準空間への第１の変換、第２の正準基準空間から第１のカメラのための第１の正準基準空間への第２の変換、および第１のカメラのための第１の正準基準空間内の画像データに電子画像安定化を適用するための第３の変換が、代わりに、（デバイス１０２または別の、例えば遠隔デバイスによって、）後で、例えばビデオがもはやキャプチャされていないときに適用されてもよいことが理解されるであろう。
Several implementation examples have been described. Nevertheless, it will be understood that various modifications can be made without departing from the spirit and scope of the disclosure. For example, the flows shown above may be used in a variety of ways, with steps reordered, added, or removed. As another exemplary modification, although the above description primarily describes processing of the image data being performed while capturing video by the first and/or second camera, In some implementations, a first transformation to a second canonical reference space for the second camera, the second canonical reference space to the first canonical reference space for the first camera and a third transformation for applying electronic image stabilization to the image data in the first canonical reference space for the first camera (
本発明の実施形態および本明細書に記載される機能的動作のすべては、デジタル電子回路において、または本明細書に開示される構造およびそれらの構造的等価物を含むコンピュータソフトウェア、ファームウェア、もしくはハードウェアにおいて、またはそれらの１つ以上の組み合わせにおいて実現され得る。本発明の実施形態は、１つ以上のコンピュータプログラム製品、たとえば、データ処理装置によって実行するために、またはデータ処理装置の動作を制御するために、コンピュータ可読媒体上に符号化されたコンピュータプログラム命令の１つ以上のモジュールとして実現され得る。コンピュータ可読媒体は、機械可読記憶装置、機械可読記憶基板、メモリデバイス、機械可読伝搬信号をもたらす物質の組成、またはそれらの１つ以上の組み合わせであり得る。「データ処理装置」という用語は、例として、プログラマブルプロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む、データを処理するためのすべての装置、デバイス、および機械を包含する。装置は、ハードウェアに加えて、当該コンピュータプログラムの実行環境を作成するコード、例えば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、またはこれらの１つ以上の組合せを構成するコードを含むことができる。伝搬信号は、人工的に生成された信号、例えば、好適な受信機装置への伝送のために情報を符号化するために生成される、機械生成電気、光学、または電磁信号である。 Embodiments of the invention and all of the functional acts described herein may be implemented in digital electronic circuitry or in computer software, firmware, or hardware containing the structures disclosed herein and their structural equivalents. hardware, or in a combination of one or more thereof. Embodiments of the present invention may comprise one or more computer program products, e.g., computer program instructions encoded on a computer readable medium for execution by or for controlling the operation of a data processing apparatus. can be implemented as one or more modules of A computer-readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter that provides a propagating machine-readable signal, or a combination of one or more thereof. The term "data processor" encompasses all apparatus, devices and machines for processing data including, by way of example, a programmable processor, computer, or multiple processors or computers. In addition to hardware, the apparatus includes code that creates an execution environment for the computer program, e.g., code that makes up processor firmware, protocol stacks, database management systems, operating systems, or combinations of one or more of these. can be done. A propagated signal is an artificially generated signal, eg, a machine-generated electrical, optical, or electromagnetic signal generated to encode information for transmission to suitable receiver equipment.
コンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーション、スクリプトまたはコードとしても公知である）は、コンパイル型言語または解釈型言語を含む任意の形式のプログラミング言語で記述され得、それは、スタンドアロンプログラムとして、または、モジュール、コンポーネント、サブルーチン、もしくは、コンピューティング環境で使用するのに好適な他のユニットとして含む、任意の形態で展開され得る。コンピュータプログラムは、必ずしもファイルシステム内のファイルに対応するとは限らない。プログラムは、他のプログラムまたはデータを保持するファイルの一部分（例えば、マークアップ言語ドキュメントに格納された１つ以上のスクリプト）、当該プログラムに専用の単一ファイル、または複数の協調ファイル（たとえば、１つ以上のモジュール、サブプログラム、もしくはコードの一部を記憶するファイル）に記憶することができる。コンピュータプログラムは、１つのコンピュータ、または１つのサイトに位置し、もしくは複数のサイトにわたって分散され、通信ネットワークによって相互接続された複数のコンピュータ上で実行されるように展開され得る。 A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and may be written as a stand-alone program or as a module. , components, subroutines, or other units suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be part of a file holding other programs or data (e.g., one or more scripts stored in a markup language document), a single file dedicated to that program, or multiple collaborative files (e.g., one files that store one or more modules, subprograms, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers located at one site or distributed across multiple sites and interconnected by a communication network.
本明細書に記載されるプロセスおよび論理フローは、入力データを操作し出力を生成することにより機能を実行するよう１つ以上のプログラマブルプロセッサが１つ以上のコンピュータプログラムを実行することによって実行され得る。本プロセスおよび論理フローの実行、ならびに本装置の実施は、たとえばＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）といった特殊目的論理回路系によってもなされ得る。 The processes and logic flows described herein can be performed by one or more programmable processors executing one or more computer programs to perform functions by manipulating input data and generating output. . Execution of the process and logic flow, as well as implementation of the device, may also be done by special purpose logic circuitry such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits), for example.
コンピュータプログラムの実行に好適であるプロセッサは、例として、汎用マイクロプロセッサおよび特殊目的マイクロプロセッサの両方、ならびに任意の種類のデジタルコンピュータの任意の１つ以上のプロセッサを含む。概して、プロセッサは、読み取り専用メモリもしくはランダムアクセスメモリまたは両方から命令およびデータを受信することになる。コンピュータの必須要素は、命令を実行するためのプロセッサ、ならびに命令およびデータを記憶するための１つ以上のメモリデバイスである。一般に、コンピュータはさらに、たとえば磁気ディスク、光磁気ディスクまたは光ディスクといった、データを格納するための１つ以上の大容量記憶装置を含むか、当該１つ以上の大容量記憶装置からデータを受取るかもしくは当該１つ以上の大容量記憶装置にデータを転送するよう作動的に結合されるか、またはその両方を行うことにもなる。しかしながら、コンピュータは、そのようなデバイスを有する必要はない。さらに、コンピュータは、別のデバイス、たとえば、ほんの数例を挙げると、タブレットコンピュータ、携帯電話、携帯情報端末（ＰＤＡ）、モバイルオーディオプレーヤ、全地球測位システム（ＧＰＳ）受信機に組み込まれ得る。コンピュータプログラム命令およびデータを格納するのに好適であるコンピュータ可読媒体は、例として、たとえばＥＰＲＯＭ、ＥＥＰＲＯＭおよびフラッシュメモリデバイスといった半導体メモリデバイスを含むすべての形態の不揮発性メモリ、媒体およびメモリデバイス；たとえば内部ハードディスクまたはリムーバブルディスクといった磁気ディスク；光磁気ディスク；ならびにＣＤ－ＲＯＭおよびＤＶＤ－ＲＯＭディスクを含む。プロセッサおよびメモリは、特殊目的論理回路によって補足され得るか、または特殊目的論理回路に組み込まれ得る。 Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from read-only memory or random-access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer also includes one or more mass storage devices, such as magnetic, magneto-optical or optical discs, for storing data, or receiving data from such one or more mass storage devices, or It would also be operatively coupled to transfer data to the one or more mass storage devices, or both. However, a computer need not have such devices. Additionally, the computer may be embedded in another device such as a tablet computer, a mobile phone, a personal digital assistant (PDA), a mobile audio player, a global positioning system (GPS) receiver, just to name a few. Computer readable media suitable for storing computer program instructions and data include, by way of example, all forms of non-volatile memory, media and memory devices including semiconductor memory devices such as EPROM, EEPROM and flash memory devices; Includes magnetic disks, such as hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and memory may be supplemented by, or incorporated into, special purpose logic circuitry.
ユーザとの対話を提供するために、本発明の実施形態は、ユーザに情報を表示するための表示装置、例えば、ＣＲＴ（陰極線管）またはＬＣＤ（液晶ディスプレイ）モニタと、ユーザがコンピュータに入力を提供することができるキーボードおよびポインティングデバイス、例えば、マウスまたはトラックボールとを有するコンピュータ上で実現することができる。他の種類のデバイスを使用して、ユーザとの対話を提供することもでき；例えば、ユーザに提供されるフィードバックは、任意の形態の感覚フィードバック、例えば、視覚フィードバック、聴覚フィードバック、または触覚フィードバックであり得；ユーザからの入力は、音響入力、音声入力、または触覚入力を含む、任意の形態で受信され得る。 To provide interaction with the user, embodiments of the present invention include a display device, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user, and a user input to the computer. It can be implemented on a computer with a keyboard and pointing device, eg mouse or trackball, which can be provided. Other types of devices can also be used to provide interaction with the user; for example, the feedback provided to the user can be any form of sensory feedback, e.g., visual, auditory, or tactile feedback. Possible; input from a user may be received in any form, including acoustic, speech, or tactile input.
本発明の実施形態は、たとえばデータサーバとしてバックエンドコンポーネントを含む計算システムにおいて実現され得るか、たとえばアプリケーションサーバといったミドルウェアコンポーネントを含む計算システムにおいて実現され得るか、たとえば本発明の実現例とユーザが対話することが可能であるグラフィカルユーザインターフェイスもしくはウェブブラウザを有するクライアントコンピュータといったフロントエンドコンポーネントを含む計算システムにおいて実現され得るか、または１つ以上のそのようなバックエンドコンポーネント、ミドルウェアコンポーネントもしくはフロントエンドコンポーネントの任意の組合せの計算システムにおいて実現され得る。システムの構成要素は、デジタルデータ通信の任意の形態または媒体、例えば、通信ネットワークによって相互接続することができる。通信ネットワークの例は、ローカルエリアネットワーク（「ＬＡＮ」）および広域ネットワーク（「ＷＡＮ」）、例えばインターネットを含む。 Embodiments of the present invention may be implemented in a computing system that includes backend components, such as data servers, or may be implemented in a computing system that includes middleware components, such as application servers, or may be implemented in a computing system that includes middleware components, such as an application server, for example, and how a user interacts with implementations of the present invention. or any of one or more such back-end components, middleware components or front-end components can be implemented in a computing system of the combination of The components of the system can be interconnected by any form or medium of digital data communication, eg, a communication network. Examples of communication networks include local area networks (“LAN”) and wide area networks (“WAN”), such as the Internet.
コンピューティングシステムは、クライアントおよびサーバを含むことができる。クライアントとサーバとは一般に互いから遠隔にあり、典型的には通信ネットワークを通じて対話する。クライアントとサーバとの関係は、それぞれのコンピュータ上で実行されるとともに互いに対してクライアント－サーバ関係を有するコンピュータプログラムによって生ずる。 The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
本明細書は多くの詳細を含むが、これらは、本発明の範囲または特許請求され得るものの範囲に対する限定として解釈されるべきではなく、むしろ、本発明の特定の実施形態に特有の特徴の説明として解釈されるべきである。本明細書において別々の実施形態の文脈で記載される特定の特徴は、単一の実施形態において組合せでも実現され得る。逆に、単一の実施形態の文脈において記載されるさまざまな特徴も、複数の実施形態において別々に、または任意の好適な部分的組合わせでも実現され得る。さらに、特徴は、ある組合せにおいて作用すると上で記載され、最初はそのように請求されていさえする場合もあるが、請求される組合せからの１つ以上の特徴はいくつかの場合には当該組合せから削除され得、請求される組合せは、部分的組合わせまたは部分的組合わせの変形例に向けられ得る。 While this specification contains many details, these should not be construed as limitations on the scope of the invention or what may be claimed, but rather a description of the features characteristic of particular embodiments of the invention. should be interpreted as Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Further, while features may be described above and even initially claimed to act in certain combinations, one or more features from a claimed combination may in some cases be may be deleted from and claimed combinations may be directed to subcombinations or variations of subcombinations.
同様に、動作が図においては特定の順に示されているが、そのような動作は、望ましい結果を達成するために、示された当該特定の順もしくは連続した順で実行される必要があると理解されるべきではなく、または、すべての示された動作が実行される必要があると理解されるべきではない。特定の状況では、マルチタスク化および並列処理化が有利である場合もある。さらに、上述の実施形態における様々なシステムコンポーネントの分離は、すべての実施形態においてそのような分離を必要とすると理解されるべきではなく、記載されるプログラムコンポーネントおよびシステムは一般に単一のソフトウェア製品に統合され得るかまたは複数のソフトウェア製品にパッケージ化され得ることが理解されるべきである。 Similarly, although acts have been presented in a particular order in the figures, it should be understood that such acts must be performed in the particular order presented or in a sequential order to achieve desirable results. It should not be understood, or that all indicated acts must be performed. Multitasking and parallelism can be advantageous in certain situations. Furthermore, the separation of the various system components in the above-described embodiments should not be understood to require such separation in all embodiments, and the program components and systems described are generally combined into a single software product. It should be understood that it may be integrated or packaged into multiple software products.
本発明の特定の実施形態について説明してきた。他の実施形態は以下の請求の範囲内にある。たとえば、請求項において記載されるステップは、異なる順で実行され得、それでも望ましい結果を達成し得る。 Particular embodiments of the invention have been described. Other embodiments are within the following claims. For example, the steps recited in the claims can be performed in a different order and still achieve desirable results.
Claims (15)
第１のカメラおよび第２のカメラを有するビデオキャプチャデバイスが、ビデオ記録中にデジタルズーム範囲内でユーザ指定の倍率変更を可能にするデジタルズーム機能を提供することを含み、前記ビデオキャプチャデバイスは、（ｉ）前記デジタルズーム範囲の第１の部分にわたって前記第１のカメラによってキャプチャされたビデオデータを使用し、（ｉｉ）前記デジタルズーム範囲の第２の部分にわたって前記第２のカメラによってキャプチャされたビデオデータを使用するよう構成され、前記方法はさらに、
前記デジタルズーム範囲の前記第２の部分において、あるズームレベルを提供するために、前記ビデオキャプチャデバイスの前記第２のカメラを使用してビデオをキャプチャする間に、（ｉ）前記第２のカメラのための第２の正準基準空間への第１の変換と、（ｉｉ）前記第２のカメラのための前記第２の正準基準空間から前記第１のカメラのための第１の正準基準空間への第２の変換と、（ｉｉｉ）前記第１のカメラのための前記第１の正準基準空間内の画像データに電子画像安定化を適用するための第３の変換と含む変換のセットを適用することによって、前記第２のカメラを使用してキャプチャされた画像データを処理することを含む、方法。 a method,
A video capture device having a first camera and a second camera provides a digital zoom capability that allows user-specified magnification changes within a digital zoom range during video recording, the video capture device comprising: (i ) using video data captured by said first camera over a first portion of said digital zoom range; and (ii) using video data captured by said second camera over a second portion of said digital zoom range. and the method further comprises:
(i) while capturing video using the second camera of the video capture device to provide a zoom level in the second portion of the digital zoom range; (ii) a first transformation for the first camera from the second canonical reference space for the second camera to a second canonical reference space for a second transformation to reference space; and (iii) a third transformation for applying electronic image stabilization to image data in said first canonical reference space for said first camera. processing image data captured using the second camera by applying a set of .
前記デジタルズーム範囲の前記第１の部分において、あるズームレベルを提供するために、前記ビデオキャプチャデバイスの前記第１のカメラを使用してビデオをキャプチャする間に、（ｉ）前記第１のカメラのための前記第１の正準基準空間への前記第１の変換と、（ｉｉ）前記第１のカメラのための前記第１の正準基準空間内の画像データに電子画像安定化を適用するための前記第３の変換とを含む変換のセットを適用することによって、前記第２のカメラを使用してキャプチャされた画像データを処理することを含む、請求項１に記載の方法。 The method further comprises:
(i) while capturing video using the first camera of the video capture device to provide a zoom level in the first portion of the digital zoom range; and (ii) applying electronic image stabilization to image data in the first canonical reference space for the first camera. 2. The method of claim 1, comprising processing image data captured using the second camera by applying a set of transforms comprising the third transform for .
前記第２のカメラは、光学画像安定化（ＯＩＳ）システムを含み、前記第２のカメラのための前記第２の正準基準空間は、画像データが、一貫した所定のＯＩＳ位置を有して、表されるものである、請求項５に記載の方法。 The first camera includes an optical image stabilization (OIS) system, and the first canonical reference space for the first camera is such that image data has a consistent predetermined OIS position. , or
The second camera includes an optical image stabilization (OIS) system, and the second canonical reference space for the second camera is such that image data has a consistent predetermined OIS position. 6. The method of claim 5, wherein .
前記第２のカメラは、画像フレームの画像走査線を漸進的にキャプチャする画像データを提供し、前記第２のカメラのための前記第２の正準基準空間は、画像データが、前記画像走査線の漸進的キャプチャに起因する歪みを除去するように、補正されたものである、請求項６に記載の方法。 The first camera provides image data that progressively captures image scanlines of image frames, and the first canonical reference space for the first camera is defined such that the image data corresponds to the image scan. corrected to remove distortion due to progressive capture of the line, or
The second camera provides image data that progressively captures image scanlines of image frames, and the second canonical reference space for the second camera is defined such that the image data corresponds to the image scans. 7. The method of claim 6, corrected to remove distortion due to progressive capture of lines.
前記第１のカメラを使用するビデオデータのキャプチャ、および電子画像安定化を適用するための前記第１のカメラからのビデオデータの処理中に、前記デジタルズーム範囲の前記第２の部分における特定のズームレベルへのズームレベルの変化を示すユーザ入力を受信することと、
前記ユーザ入力を受信することに応じて、
所定のズームレベルに達するまで、前記第１のカメラを使用してキャプチャされるビデオフレームの倍率が漸増的に増加される、ビデオフレームのシーケンスを記録することと、
前記第２のカメラを使用してビデオキャプチャを開始することと、
前記第２のカメラを使用してキャプチャされるビデオフレームの第２のシーケンスを記録することとを含み、前記ビデオフレームの第２のシーケンスは、前記所定のズームレベルを提供し、前記特定のズームレベルに達するまで、前記第２のカメラを使用してキャプチャされるビデオフレームの、増加する倍率を提供する、請求項１～９のいずれか１項に記載の方法。 The method further comprises:
a particular zoom in the second portion of the digital zoom range during capture of video data using the first camera and processing of video data from the first camera to apply electronic image stabilization; receiving user input indicating a change in zoom level to level;
In response to receiving said user input,
recording a sequence of video frames in which the magnification of video frames captured using the first camera is incrementally increased until a predetermined zoom level is reached;
initiating video capture using the second camera;
recording a second sequence of video frames captured using the second camera, wherein the second sequence of video frames provides the predetermined zoom level and the specified zoom. A method according to any preceding claim, providing increasing magnification of video frames captured using the second camera until a level is reached.
前記第２の変換は、前記第２のカメラの焦点距離に少なくとも部分的に基づいて決定され、
前記第３の変換は、ビデオフレームのために、前記ビデオフレームのうちの特定のビデオフレームごとに、前記特定のビデオフレームの前の１つ以上のビデオフレームと前記特定のビデオフレームの後の１つ以上のビデオフレームとを使用する電子画像安定化を含む、請求項１～１０のいずれか１項に記載の方法。 the first transformation includes a plurality of different adjustments to different scanlines of image data captured using the second camera;
the second transform is determined based at least in part on a focal length of the second camera;
The third transform includes, for video frames, one or more video frames before the particular video frame and one after the particular video frame, for each particular one of the video frames. A method according to any preceding claim, comprising electronic image stabilization using one or more video frames.
前記方法はさらに、
前記第１のカメラを使用する画像キャプチャ中に、ビデオキャプチャに対してズームレベルの変更を示すユーザ入力を受信することと、
前記ユーザ入力を受信することに応じて、前記変更されたズームレベルが所定の遷移ズームレベル以上であるか否かを判定することとを含み、前記所定の遷移ズームレベルは、前記第２のカメラの前記視野よりも小さい視野を表す、請求項１～１１のいずれか１項に記載の方法。 the second camera has a smaller field of view than the first camera;
The method further comprises:
receiving user input indicating a change in zoom level for video capture during image capture using the first camera;
determining whether the changed zoom level is greater than or equal to a predetermined transitional zoom level in response to receiving the user input, wherein the predetermined transitional zoom level is equal to or greater than the second camera. A method according to any one of claims 1 to 11, representing a field of view that is smaller than the field of view of .
（ｉ）前記第１のカメラを使用するビデオキャプチャから前記第２のカメラを使用するビデオキャプチャに遷移するための第１の遷移ズームレベルと、（ｉｉ）前記第２のカメラを使用するビデオキャプチャから前記第１のカメラを使用するビデオキャプチャに遷移するための第２の遷移ズームレベルとを示すデータを記憶することを含み、前記第１の遷移ズームレベルは前記第２の遷移ズームレベルとは異なり、前記方法はさらに、
（ｉ）要求されたズームレベルが視野の減少に対応するとき、前記要求されたズームレベルを前記第１の遷移ズームレベルと比較することと、（ｉｉ）前記要求されたズームレベルが視野の増加に対応するとき、前記要求されたズームレベルを前記第２の遷移ズームレベルと比較することとによって、ビデオキャプチャのためにカメラ間で切り替えるかどうかを決定することを含む、請求項１～１２のいずれか１項に記載の方法。 The method further comprises:
(i) a first transition zoom level for transitioning from video capture using the first camera to video capture using the second camera; and (ii) video capture using the second camera. a second transitional zoom level for transitioning from to video capture using the first camera, wherein the first transitional zoom level is different from the second transitional zoom level In contrast, the method further comprises:
(i) comparing the requested zoom level to the first transitional zoom level when the requested zoom level corresponds to a decreased field of view; and (ii) the requested zoom level corresponds to an increased field of view. determining whether to switch between cameras for video capture by comparing the requested zoom level with the second transition zoom level when corresponding to A method according to any one of paragraphs.
ビデオファイルの記録中に、前記第１のカメラおよび前記第２のカメラのうちの特定のカメラを使用してビデオをキャプチャすることから、前記第１のカメラおよび前記第２のカメラのうちの別のカメラを使用してビデオをキャプチャすることに切り替えることを決定することと、
前記切り替えることを決定することに応答して、
前記特定のカメラを使用する画像キャプチャのために使用されているビデオキャプチャパラメータの値を判断することと、
前記判断されたビデオキャプチャパラメータに基づいて、前記別のカメラに対するビデオキャプチャパラメータの値を設定することと、
前記別のカメラに対する前記ビデオキャプチャパラメータの前記値を設定した後、前記第２のカメラからのビデオキャプチャを開始し、前記第２のカメラからのキャプチャされたビデオを前記ビデオファイルに記録することとを含み、
前記ビデオキャプチャパラメータを設定することは、前記第２のカメラについて、露出、画像センサ感度、ゲイン、画像キャプチャ時間、開口サイズ、レンズ焦点距離、ＯＩＳステータス、またはＯＩＳレベルのうちの１つ以上を調整することを含む、請求項１～１３のいずれか１項に記載の方法。 The method further comprises:
During recording of a video file, a particular one of said first camera and said second camera is used to capture video, and another camera of said first camera and said second camera is used to capture video. deciding to switch to capturing video using the camera of
In response to deciding to switch,
determining values of video capture parameters being used for image capture using the particular camera;
setting values of video capture parameters for the another camera based on the determined video capture parameters;
After setting the values of the video capture parameters for the another camera, initiating video capture from the second camera and recording captured video from the second camera to the video file. including
Setting the video capture parameters adjusts one or more of exposure, image sensor sensitivity, gain, image capture time, aperture size, lens focal length, OIS status, or OIS level for the second camera. The method of any one of claims 1-13, comprising:
第１の視野を有する第１のカメラと、
第２の視野を有する第２のカメラと、
１つ以上の位置または向きセンサと、
１つ以上のプロセッサと、
前記１つ以上のプロセッサによって実行されると請求項１～１４のいずれか１項に記載の方法を実行させる命令を記憶する１つ以上のデータ記憶装置とを備える、ビデオキャプチャデバイス。 a video capture device,
a first camera having a first field of view;
a second camera having a second field of view;
one or more position or orientation sensors;
one or more processors;
and one or more data storage devices storing instructions which, when executed by said one or more processors, cause the method of any one of claims 1 to 14 to be performed.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/941,775 US11190689B1 (en) | 2020-07-29 | 2020-07-29 | Multi-camera video stabilization |
US16/941,775 | 2020-07-29 | ||
PCT/US2021/040378 WO2022026126A1 (en) | 2020-07-29 | 2021-07-02 | Multi-camera video stabilization |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023536674A true JP2023536674A (en) | 2023-08-29 |
Family
ID=77168422
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022536617A Pending JP2023536674A (en) | 2020-07-29 | 2021-07-02 | Multi-camera video stabilization |
Country Status (7)
Country | Link |
---|---|
US (3) | US11190689B1 (en) |
EP (1) | EP4052456A1 (en) |
JP (1) | JP2023536674A (en) |
KR (1) | KR20230044137A (en) |
CN (1) | CN114788260A (en) |
DE (1) | DE112021000208T5 (en) |
WO (1) | WO2022026126A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10462370B2 (en) | 2017-10-03 | 2019-10-29 | Google Llc | Video stabilization |
US10171738B1 (en) | 2018-05-04 | 2019-01-01 | Google Llc | Stabilizing video to reduce camera and face movement |
EP3984214B1 (en) * | 2019-06-27 | 2023-08-23 | Huawei Technologies Co., Ltd. | Multifocal display device and method |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
KR102480820B1 (en) * | 2020-08-12 | 2022-12-22 | 코어포토닉스 리미티드 | Optical Image Stabilization of Scanning Folded Cameras |
KR20220025600A (en) * | 2020-08-24 | 2022-03-03 | 삼성전자주식회사 | Method and apparatus for generating image |
KR102274270B1 (en) * | 2020-12-10 | 2021-07-08 | 주식회사 케이티앤씨 | System for acquisiting iris image for enlarging iris acquisition range |
US11610338B2 (en) * | 2021-03-26 | 2023-03-21 | Fotonation Limited | Method of controlling a camera |
KR20230023506A (en) * | 2021-08-10 | 2023-02-17 | 삼성전자주식회사 | Electronic device performing image stabilization and operating method thereof |
US20230222754A1 (en) * | 2022-01-07 | 2023-07-13 | Sony Interactive Entertainment Inc. | Interactive video playback techniques to enable high fidelity magnification |
WO2023139528A1 (en) * | 2022-01-21 | 2023-07-27 | Swiss Rig Gmbh | Algorithmic camera zoom controller method |
WO2024025182A1 (en) * | 2022-07-26 | 2024-02-01 | 삼성전자 주식회사 | Electronic device for acquiring image by using multiple cameras and method therefor |
WO2024072722A1 (en) * | 2022-09-29 | 2024-04-04 | Google Llc | Smooth continuous zooming in a multi-camera system by image-based visual features and optimized geometric calibrations |
WO2024076363A1 (en) * | 2022-10-04 | 2024-04-11 | Google Llc | Field of view correction techniques for shutterless camera systems |
WO2024076176A1 (en) * | 2022-10-07 | 2024-04-11 | 삼성전자 주식회사 | Method for controlling camera, and electronic device |
CN116320784B (en) * | 2022-10-27 | 2023-11-28 | 荣耀终端有限公司 | Image processing method and device |
Family Cites Families (123)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4637571A (en) | 1985-09-03 | 1987-01-20 | The United States Of America As Represented By The Secretary Of The Army | Electronic image stabilization |
GB2220319B (en) | 1988-07-01 | 1992-11-04 | Plessey Co Plc | Improvements in or relating to image stabilisation |
US20030038927A1 (en) | 2001-08-27 | 2003-02-27 | Alden Ray M. | Image projector with integrated image stabilization for handheld devices and portable hardware |
US6877863B2 (en) | 2002-06-12 | 2005-04-12 | Silicon Optix Inc. | Automatic keystone correction system and method |
KR20040077240A (en) | 2003-02-28 | 2004-09-04 | 주식회사 대우일렉트로닉스 | Method for controlling recording speed according to motion detecting in a timelapse image recording and reproducing apparatus |
US8199222B2 (en) | 2007-03-05 | 2012-06-12 | DigitalOptics Corporation Europe Limited | Low-light video frame enhancement |
US7420592B2 (en) | 2004-06-17 | 2008-09-02 | The Boeing Company | Image shifting apparatus for enhanced image resolution |
US7643062B2 (en) | 2005-06-08 | 2010-01-05 | Hewlett-Packard Development Company, L.P. | Method and system for deblurring an image based on motion tracking |
JP4340915B2 (en) | 2006-02-01 | 2009-10-07 | ソニー株式会社 | Captured image signal distortion correction method, captured image signal distortion correction apparatus, imaging method, and imaging apparatus |
US7697725B2 (en) | 2006-04-03 | 2010-04-13 | Sri International | Method and apparatus for autonomous object tracking |
US7952612B2 (en) | 2006-06-22 | 2011-05-31 | Nokia Corporation | Method and system for image construction using multiple exposures |
US8781162B2 (en) | 2011-01-05 | 2014-07-15 | Ailive Inc. | Method and system for head tracking and pose estimation |
JP4702233B2 (en) | 2006-09-11 | 2011-06-15 | ソニー株式会社 | Image data processing apparatus and image data processing method |
KR100819301B1 (en) | 2006-12-20 | 2008-04-03 | 삼성전자주식회사 | Method and apparatus for optical image stabilizer on mobile camera module |
US7559017B2 (en) * | 2006-12-22 | 2009-07-07 | Google Inc. | Annotation framework for video |
US8213685B2 (en) | 2007-01-05 | 2012-07-03 | American Traffic Solutions, Inc. | Video speed detection system |
US7796872B2 (en) | 2007-01-05 | 2010-09-14 | Invensense, Inc. | Method and apparatus for producing a sharp image from a handheld device containing a gyroscope |
WO2008114264A2 (en) | 2007-03-21 | 2008-09-25 | Mantis Vision Ltd | A method and apparatus for video image stabilization |
JP5111088B2 (en) | 2007-12-14 | 2012-12-26 | 三洋電機株式会社 | Imaging apparatus and image reproduction apparatus |
JP4539729B2 (en) | 2008-02-15 | 2010-09-08 | ソニー株式会社 | Image processing apparatus, camera apparatus, image processing method, and program |
JP5075757B2 (en) | 2008-08-05 | 2012-11-21 | オリンパス株式会社 | Image processing apparatus, image processing program, image processing method, and electronic apparatus |
US8102428B2 (en) | 2008-08-28 | 2012-01-24 | Adobe Systems Incorporated | Content-aware video stabilization |
JP5144487B2 (en) | 2008-12-15 | 2013-02-13 | キヤノン株式会社 | Main face selection device, control method thereof, imaging device, and program |
KR101547556B1 (en) | 2009-02-06 | 2015-08-26 | 삼성전자주식회사 | Image display method and apparatus |
US8970690B2 (en) | 2009-02-13 | 2015-03-03 | Metaio Gmbh | Methods and systems for determining the pose of a camera with respect to at least one object of a real environment |
WO2011046633A1 (en) | 2009-10-14 | 2011-04-21 | Zoran Corporation | Method and apparatus for image stabilization |
US8599238B2 (en) | 2009-10-16 | 2013-12-03 | Apple Inc. | Facial pose improvement with perspective distortion correction |
US8553275B2 (en) * | 2009-11-09 | 2013-10-08 | Xerox Corporation | Architecture for controlling placement and minimizing distortion of images |
US8416277B2 (en) | 2009-12-10 | 2013-04-09 | Apple Inc. | Face detection as a metric to stabilize video during video chat session |
US8558903B2 (en) | 2010-03-25 | 2013-10-15 | Apple Inc. | Accelerometer / gyro-facilitated video stabilization |
US10614289B2 (en) | 2010-06-07 | 2020-04-07 | Affectiva, Inc. | Facial tracking with classifiers |
US20120050570A1 (en) | 2010-08-26 | 2012-03-01 | Jasinski David W | Audio processing based on scene type |
CN103201765B (en) | 2010-09-28 | 2016-04-06 | 马普科技促进协会 | For recovering the method and apparatus of digital picture from viewed digital image sequence |
WO2012064106A2 (en) | 2010-11-12 | 2012-05-18 | Samsung Electronics Co., Ltd. | Method and apparatus for video stabilization by compensating for view direction of camera |
US9077890B2 (en) | 2011-02-24 | 2015-07-07 | Qualcomm Incorporated | Auto-focus tracking |
GB2492529B (en) | 2011-05-31 | 2018-01-10 | Skype | Video stabilisation |
US8913140B2 (en) | 2011-08-15 | 2014-12-16 | Apple Inc. | Rolling shutter reduction based on motion sensors |
US8493459B2 (en) | 2011-09-15 | 2013-07-23 | DigitalOptics Corporation Europe Limited | Registration of distorted images |
TWI469062B (en) | 2011-11-11 | 2015-01-11 | Ind Tech Res Inst | Image stabilization method and image stabilization device |
FR2982678B1 (en) | 2011-11-14 | 2014-01-03 | Dxo Labs | METHOD AND SYSTEM FOR IMAGE SEQUENCE CAPTURE WITH COMPENSATION OF GRADING VARIATIONS |
US9246543B2 (en) | 2011-12-12 | 2016-01-26 | Futurewei Technologies, Inc. | Smart audio and video capture systems for data processing systems |
US8743222B2 (en) | 2012-02-14 | 2014-06-03 | Nokia Corporation | Method and apparatus for cropping and stabilization of video images |
WO2013130082A1 (en) | 2012-03-01 | 2013-09-06 | Geo Semiconductor Inc. | Method and system for adaptive perspective correction of ultra wide-angle lens images |
US20130314558A1 (en) | 2012-05-24 | 2013-11-28 | Mediatek Inc. | Image capture device for starting specific action in advance when determining that specific action is about to be triggered and related image capture method thereof |
EP2680616A1 (en) | 2012-06-25 | 2014-01-01 | LG Electronics Inc. | Mobile terminal and audio zooming method thereof |
US9280810B2 (en) | 2012-07-03 | 2016-03-08 | Fotonation Limited | Method and system for correcting a distorted input image |
US8928730B2 (en) | 2012-07-03 | 2015-01-06 | DigitalOptics Corporation Europe Limited | Method and system for correcting a distorted input image |
US8736692B1 (en) | 2012-07-09 | 2014-05-27 | Google Inc. | Using involuntary orbital movements to stabilize a video |
JP6097522B2 (en) | 2012-10-22 | 2017-03-15 | キヤノン株式会社 | Image blur correction apparatus, image blur correction method, and imaging apparatus |
US9288395B2 (en) | 2012-11-08 | 2016-03-15 | Apple Inc. | Super-resolution based on optical image stabilization |
CN113472989A (en) | 2012-11-28 | 2021-10-01 | 核心光电有限公司 | Multi-aperture imaging system and method for acquiring images by multi-aperture imaging system |
CN103853908B (en) | 2012-12-04 | 2017-11-14 | 中国科学院沈阳自动化研究所 | A kind of maneuvering target tracking method of adaptive interaction formula multi-model |
US9071756B2 (en) | 2012-12-11 | 2015-06-30 | Facebook, Inc. | Systems and methods for digital video stabilization via constraint-based rotation smoothing |
JP5997645B2 (en) | 2013-03-26 | 2016-09-28 | キヤノン株式会社 | Image processing apparatus and method, and imaging apparatus |
US9232138B1 (en) | 2013-06-03 | 2016-01-05 | Amazon Technologies, Inc. | Image stabilization techniques |
CN108989647B (en) | 2013-06-13 | 2020-10-20 | 核心光电有限公司 | Double-aperture zooming digital camera |
US10474921B2 (en) | 2013-06-14 | 2019-11-12 | Qualcomm Incorporated | Tracker assisted image capture |
US9857568B2 (en) | 2013-07-04 | 2018-01-02 | Corephotonics Ltd. | Miniature telephoto lens assembly |
EP3779565A3 (en) | 2013-07-04 | 2021-05-05 | Corephotonics Ltd. | Miniature telephoto lens assembly |
CN108718376B (en) | 2013-08-01 | 2020-08-14 | 核心光电有限公司 | Thin multi-aperture imaging system with auto-focus and method of use thereof |
WO2015048694A2 (en) | 2013-09-27 | 2015-04-02 | Pelican Imaging Corporation | Systems and methods for depth-assisted perspective distortion correction |
US9952756B2 (en) | 2014-01-17 | 2018-04-24 | Intel Corporation | Dynamic adjustment of a user interface |
US9413963B2 (en) | 2014-05-30 | 2016-08-09 | Apple Inc. | Video image stabilization |
US9357132B2 (en) | 2014-05-30 | 2016-05-31 | Apple Inc. | Video rolling shutter correction for lens movement in optical image stabilization cameras |
US20150362989A1 (en) | 2014-06-17 | 2015-12-17 | Amazon Technologies, Inc. | Dynamic template selection for object detection and tracking |
WO2015198478A1 (en) | 2014-06-27 | 2015-12-30 | 株式会社 市川ソフトラボラトリー | Image distortion correction apparatus, information processing apparatus and image distortion correction method |
US9674438B2 (en) | 2014-07-06 | 2017-06-06 | Apple Inc. | Low light video image stabilization strength modulation |
EP3167417A1 (en) | 2014-07-11 | 2017-05-17 | Google, Inc. | Hands-free offline communications |
CN105306804B (en) | 2014-07-31 | 2018-08-21 | 北京展讯高科通信技术有限公司 | Intelligent terminal and its video image stabilization method and device |
US9392188B2 (en) | 2014-08-10 | 2016-07-12 | Corephotonics Ltd. | Zoom dual-aperture camera with folded lens |
US9596411B2 (en) | 2014-08-25 | 2017-03-14 | Apple Inc. | Combined optical and electronic image stabilization |
WO2016061565A1 (en) | 2014-10-17 | 2016-04-21 | The Lightco Inc. | Methods and apparatus for using a camera device to support multiple modes of operation |
US20170351932A1 (en) | 2014-12-19 | 2017-12-07 | Nokia Technologies Oy | Method, apparatus and computer program product for blur estimation |
JP6530602B2 (en) | 2014-12-22 | 2019-06-12 | キヤノン株式会社 | Image pickup apparatus and control method thereof |
WO2016107635A1 (en) | 2014-12-29 | 2016-07-07 | Metaio Gmbh | Method and system for generating at least one image of a real environment |
GB2533788A (en) | 2014-12-30 | 2016-07-06 | Nokia Technologies Oy | Method for determining the position of a portable device |
CN107209404B (en) | 2015-01-03 | 2021-01-15 | 核心光电有限公司 | Miniature telephoto lens module and camera using the same |
US9426362B2 (en) | 2015-01-16 | 2016-08-23 | Mems Drive, Inc. | Three-axis OIS for super-resolution imaging |
JP7106273B2 (en) | 2015-01-27 | 2022-07-26 | インターデジタル マディソン パテント ホールディングス， エスアーエス | Methods, systems and apparatus for electro-optical and opto-electrical conversion of images and video |
KR101914894B1 (en) | 2015-04-02 | 2018-11-02 | 코어포토닉스 리미티드 | Dual voice coil motor structure of dual optical module camera |
WO2016168415A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Light guided image plane tiled arrays with dense fiber optic bundles for light-field and high resolution image acquisition |
KR101963546B1 (en) | 2015-04-16 | 2019-03-28 | 코어포토닉스 리미티드 | Auto focus and optical imagestabilization in a compact folded camera |
CN110687655B (en) | 2015-05-28 | 2022-10-21 | 核心光电有限公司 | Bi-directional stiffness for optical image stabilization and auto-focus in dual aperture digital cameras |
KR101992040B1 (en) | 2015-06-24 | 2019-06-21 | 코어포토닉스 리미티드 | Low-Profile 3-Axis Actuator for Foldable Lens Cameras |
KR102253997B1 (en) | 2015-08-13 | 2021-05-20 | 코어포토닉스 리미티드 | Dual aperture zoom camera with video support and switching/non-switching dynamic control |
KR102225727B1 (en) | 2015-09-06 | 2021-03-10 | 코어포토닉스 리미티드 | Auto focus and optical image stabilization with roll compensation in a compact folded camera |
KR102457617B1 (en) | 2015-09-16 | 2022-10-21 | 한화테크윈 주식회사 | Method and apparatus of estimating a motion of an image, method and apparatus of image stabilization and computer-readable recording medium for executing the method |
US9967461B2 (en) | 2015-10-14 | 2018-05-08 | Google Inc. | Stabilizing video using transformation matrices |
CN106709932B (en) | 2015-11-12 | 2020-12-04 | 创新先进技术有限公司 | Face position tracking method and device and electronic equipment |
US9953217B2 (en) | 2015-11-30 | 2018-04-24 | International Business Machines Corporation | System and method for pose-aware feature learning |
US9674439B1 (en) | 2015-12-02 | 2017-06-06 | Intel Corporation | Video stabilization using content-aware camera motion estimation |
KR102140882B1 (en) | 2015-12-29 | 2020-08-04 | 코어포토닉스 리미티드 | Dual-aperture zoom digital camera with automatic adjustable tele field of view |
US9773196B2 (en) | 2016-01-25 | 2017-09-26 | Adobe Systems Incorporated | Utilizing deep learning for automatic digital image segmentation and stylization |
US10194089B2 (en) * | 2016-02-08 | 2019-01-29 | Qualcomm Incorporated | Systems and methods for implementing seamless zoom function using multiple cameras |
US9743001B1 (en) | 2016-02-19 | 2017-08-22 | Fotonation Limited | Method of stabilizing a sequence of images |
JP6640620B2 (en) | 2016-03-17 | 2020-02-05 | ソニーモバイルコミュニケーションズ株式会社 | Image stabilizing device, image stabilizing method, and electronic device |
US10482663B2 (en) | 2016-03-29 | 2019-11-19 | Microsoft Technology Licensing, Llc | Virtual cues for augmented-reality pose alignment |
KR102469567B1 (en) | 2016-04-11 | 2022-11-22 | 삼성전자주식회사 | Imaging device and operating method thereof |
CN105741789B (en) | 2016-05-06 | 2018-06-01 | 京东方科技集团股份有限公司 | A kind of driving method and driving device of high dynamic contrast display screen |
US10027893B2 (en) | 2016-05-10 | 2018-07-17 | Nvidia Corporation | Real-time video stabilization for mobile devices based on on-board motion sensing |
CN111965919B (en) | 2016-05-30 | 2022-02-08 | 核心光电有限公司 | Rotary ball guided voice coil motor |
CN109639954B (en) | 2016-06-19 | 2020-10-23 | 核心光电有限公司 | Frame synchronization system and method in dual aperture camera |
CN106101535B (en) | 2016-06-21 | 2019-02-19 | 北京理工大学 | A kind of video stabilizing method based on part and mass motion disparity compensation |
US9774798B1 (en) | 2016-06-29 | 2017-09-26 | Essential Products, Inc. | Apparatus and method for a wide field of view image sensor |
CN107770433B (en) * | 2016-08-15 | 2020-08-04 | 广州立景创新科技有限公司 | Image acquisition device and image smooth scaling method thereof |
US9888179B1 (en) | 2016-09-19 | 2018-02-06 | Google Llc | Video stabilization for mobile devices |
CN106780674B (en) | 2016-11-28 | 2020-08-25 | 网易（杭州）网络有限公司 | Lens moving method and device |
KR20180073327A (en) | 2016-12-22 | 2018-07-02 | 삼성전자주식회사 | Display control method, storage medium and electronic device for displaying image |
US10104334B2 (en) | 2017-01-27 | 2018-10-16 | Microsoft Technology Licensing, Llc | Content-adaptive adjustment of display device brightness levels when rendering high dynamic range content |
EP3579040B1 (en) | 2017-02-23 | 2021-06-23 | Corephotonics Ltd. | Folded camera lens designs |
US10645286B2 (en) | 2017-03-15 | 2020-05-05 | Corephotonics Ltd. | Camera with panoramic scanning range |
CN106954024B (en) | 2017-03-28 | 2020-11-06 | 成都通甲优博科技有限责任公司 | Unmanned aerial vehicle and electronic image stabilizing method and system thereof |
US10957297B2 (en) | 2017-07-25 | 2021-03-23 | Louis Yoelin | Self-produced music apparatus and method |
US10462370B2 (en) | 2017-10-03 | 2019-10-29 | Google Llc | Video stabilization |
US10979814B2 (en) | 2018-01-17 | 2021-04-13 | Beijing Xiaoniao Tingling Technology Co., LTD | Adaptive audio control device and method based on scenario identification |
US10171738B1 (en) | 2018-05-04 | 2019-01-01 | Google Llc | Stabilizing video to reduce camera and face movement |
JP7271653B2 (en) | 2018-08-08 | 2023-05-11 | グーグル エルエルシー | Optical Image Stabilization Operation to Generate a Super-Resolved Image of a Scene |
US10665250B2 (en) | 2018-09-28 | 2020-05-26 | Apple Inc. | Real-time feedback during audio recording, and related devices and systems |
CN113472976B (en) | 2018-10-16 | 2022-11-25 | 华为技术有限公司 | Microspur imaging method and terminal |
US10665204B1 (en) | 2019-10-08 | 2020-05-26 | Capital One Services, Llc | Automatically adjusting screen brightness based on screen content |
US10726579B1 (en) * | 2019-11-13 | 2020-07-28 | Honda Motor Co., Ltd. | LiDAR-camera calibration |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
CN111738230B (en) | 2020-08-05 | 2020-12-15 | 深圳市优必选科技股份有限公司 | Face recognition method, face recognition device and electronic equipment |
-
2020
- 2020-07-29 US US16/941,775 patent/US11190689B1/en active Active
-
2021
- 2021-07-02 EP EP21749423.6A patent/EP4052456A1/en active Pending
- 2021-07-02 CN CN202180007086.9A patent/CN114788260A/en active Pending
- 2021-07-02 JP JP2022536617A patent/JP2023536674A/en active Pending
- 2021-07-02 KR KR1020227020000A patent/KR20230044137A/en unknown
- 2021-07-02 WO PCT/US2021/040378 patent/WO2022026126A1/en unknown
- 2021-07-02 DE DE112021000208.2T patent/DE112021000208T5/en active Pending
- 2021-10-29 US US17/515,187 patent/US11856295B2/en active Active
-
2023
- 2023-11-28 US US18/520,855 patent/US20240107163A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11190689B1 (en) | 2021-11-30 |
WO2022026126A1 (en) | 2022-02-03 |
EP4052456A1 (en) | 2022-09-07 |
CN114788260A (en) | 2022-07-22 |
KR20230044137A (en) | 2023-04-03 |
US20220053133A1 (en) | 2022-02-17 |
US11856295B2 (en) | 2023-12-26 |
DE112021000208T5 (en) | 2022-09-08 |
US20240107163A1 (en) | 2024-03-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11856295B2 (en) | Multi-camera video stabilization | |
US20230336873A1 (en) | Video Stabilization | |
US9979889B2 (en) | Combined optical and electronic image stabilization | |
US7961222B2 (en) | Image capturing apparatus and image capturing method | |
JP5744217B2 (en) | Method and system for processing video for stabilization and retargeting | |
WO2008053765A1 (en) | Image generating device and image generating method | |
Bell et al. | A non-linear filter for gyroscope-based video stabilization | |
CN109191506B (en) | Depth map processing method, system and computer readable storage medium | |
CN112585644A (en) | System and method for creating background blur in camera panning or movement | |
KR102003460B1 (en) | Device and Method for dewobbling | |
US20230217067A1 (en) | Producing and adapting video images for presentation displays with different aspect ratios | |
JP2018142983A (en) | Image processing device and method of controlling the same, program, and storage medium | |
KR101741150B1 (en) | An imaging photographing device and an imaging photographing method using an video editing | |
KR102629883B1 (en) | Video support in a multi-aperture mobile camera with a scanning zoom camera | |
Peddigari et al. | Real-time implementation of zoom tracking on TI DM processor | |
KR101725932B1 (en) | An imaging photographing device and an imaging photographing method using an video editing | |
KR101206298B1 (en) | Method for generating stereoscopic image using mono camera | |
JP2014175739A (en) | Moving image processing apparatus | |
KR20070089314A (en) | Apparatus and method for providing motion information of object |