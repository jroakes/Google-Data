US20180307707A1 - System and method for presenting condition-specific geographic imagery - Google Patents
System and method for presenting condition-specific geographic imagery Download PDFInfo
- Publication number
- US20180307707A1 US20180307707A1 US15/492,583 US201715492583A US2018307707A1 US 20180307707 A1 US20180307707 A1 US 20180307707A1 US 201715492583 A US201715492583 A US 201715492583A US 2018307707 A1 US2018307707 A1 US 2018307707A1
- Authority
- US
- United States
- Prior art keywords
- images
- time
- image
- geographical location
- future point
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/5866—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, manually generated location and time information
-
- G06F17/30268—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/248—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/51—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/535—Filtering based on additional data, e.g. user or group profiles
-
- G06F17/3028—
-
- G06F17/3053—
-
- G06F17/30554—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/20—Drawing from basic elements, e.g. lines or circles
Definitions
- Various services such as Google Maps may provide street level images, often called “Street Views”, of geographic locations to users.
- a client computer may request street level images of a particular location and receive images, such as digitized panoramic 360 photographs, taken at street level in response.
- the images are received from different sources and may be grouped by locations where the images were taken.
- these images may include street level photographs of real world locations, which may allow users to view these locations from a person's perspective at ground level.
- User searches for maps and photos using such services for displaying street level images may often return images of a place at the most popular times for taking pictures and under perfect conditions. However, if the users are en route to a place or planning a trip at a certain time, the returned images may not match how the place may currently look upon the user's arrival.
- the method includes receiving a request for images of a location.
- a set of images depicting different conditions at the location may be identified.
- Each image of the set of images is associated with condition information defining the conditions depicted in that image.
- the method further includes determining when a user will be at the location at a future point in time.
- Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received.
- the set of images are ranked based on the received environmental information and the condition information for each image of the set of images.
- An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images.
- the selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- the future point in time may be based on at least one of: a search query performed by the user, a current location of the user, and the user's search history.
- the search history includes information indicating items related to a given time period corresponding to the future point in time.
- Ranking the set of images comprises identifying visual features disposed in each image of the set of images. The visual features disposed in a highest ranked image corresponds to the received environmental information
- the environmental condition information includes information representing weather conditions at the location for a time corresponding to the future point in time.
- the environmental condition information further includes information representing a time of day at the location for the future point in time.
- the environmental condition information further includes information related to a seasonal event that occurred at location. The event occurred at a time corresponding to the future point in time.
- the method further includes receiving a request for images of a location.
- a set of images depicting different conditions at the location may be identified.
- Each image of the set of images is associated with condition information defining the conditions depicted in that image.
- the method includes determining when a user will be at the location at a future point in time.
- Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received.
- the set of images are ranked based on the received environmental information and the condition information for each image of the set of images.
- An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images.
- the selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- Yet another aspect of the present technology provides a system that includes a memory for storing images and a processor coupled to the memory.
- the processor may be configured to receive a request for images of a location. In response to the request, a set of images depicting different conditions at the location may be identified. Each image of the set of images is associated with condition information defining the conditions depicted in that image.
- the processor may be further configured to determine when a user will be at the location at a future point in time. Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received.
- the set of images are ranked based on the received environmental information and the condition information for each image of the set of images.
- An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images. The selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- FIG. 1 is an example set of images including street level images in accordance with aspects of the disclosure.
- FIG. 2 is another example set of images including street level in accordance with aspects of the disclosure.
- FIG. 3 is yet another example set of images including street level images in accordance with aspects of the disclosure.
- FIG. 4 is an example of a ranked set of images in accordance with aspects of the disclosure.
- FIG. 5 is a block diagram of a system in accordance with aspects of the disclosure.
- FIG. 6 is a flow diagram depicting an example of a method in accordance with aspects of the disclosure.
- the present disclosure relates generally to ranking identified images of a location based on current conditions at that location. For example, when a user initiates a request to view images, such as street level images, of a geographic location, the techniques disclosed herein may automatically select real-world street level images that are most visually relevant to the current conditions (e.g., weather, time of day, holiday decor, etc.) at the location expected at a future point in time. While aspects of the disclosure are discussed in connection with street-level imagery, these techniques can also be used with other types of imagery such as aerial imagery or indoor imagery.
- a set of images for a location can be identified where the images correspond to images depicting different conditions at the location.
- Each image of the set of images is associated with condition information depicted in that image. It may be determined when a user will be at the location at a future point in time. For example, for a user currently traveling to a location, the user's current Global Positioning System (GPS) coordinates may be used to determine the user's expected time of arrival at the location.
- GPS Global Positioning System
- the images are ranked by a processor based on received environment information and the condition information associated with each image.
- the received environment information may include information indicating current environmental conditions at the location for the future point in time.
- An image from the set of images may be selected based on its rank relative to other images of the set of images. The selected image may depict current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- FIG. 1 is an example set of images 100 including a set of street level images 102 - 104 of a location.
- the set of images 100 may depict objects at the location such as a street, buildings, various terrains and different environmental conditions. Typically, these images are captured, for example, by cameras or image capturing devices at the locations in a direction generally parallel to the ground.
- the set of images 100 may be identified in response to a user's request for imagery of a location.
- a user may request images of a location the user may plan to visit at a future point in time.
- the search request or query for images of the location may include relevant information for identifying the location.
- the user's query may include information indicating a specific location (e.g., “Vermont Mountain Resort”), a longitude/latitude position, a street address or other types identifying information that may be used to determine the location of which the user desires to view street level imagery.
- the user may enter the identifying information into a browser on client device 109 .
- a server in communication with the client device 109 may identify one or more images also associated with the identifying information, and send these images to the client device 109 .
- the client device 109 may receive the image and display them to the user.
- a street level image may be shown to a user on a display of the client device 109 along with controls for zooming in on the image as well as controls for changing the orientation of the view (which may require retrieving more street level images).
- the techniques disclosed herein may be implemented in connection with an Internet browser such as Google Chrome (not shown) for displaying street level images as well as other information.
- Other navigation controls may be included as well, such as panning controls in the form of arrows disposed along the street. Such arrows may be selected by a user (by clicking or by dragging along the street line) to change the vantage point from up to down the street.
- the set of images 100 identified in response to the user's request may include images, such as street level images 102 and 104 that may depict the location 107 under different conditions (e.g., at daytime and nighttime).
- the user's expected time of arrival at a location can be estimated, for example, based on the user's current geolocated position.
- the user's client device 109 may include a positioning component, such as a GPS receiver, to determine a location of the device. Based on the device's current position, it can be approximated how far the user may be from the desired location.
- an arrival time of the user at the location can be estimated based on the user's known position and the user's given rate of travel. In some aspects, the rate of travel of the user may be interpolated, for example, from the GPS signal.
- This particular user may be traveling by vehicle 105 to the location 107 and may arrive at a certain time of day, such as 12:00 am midnight.
- a nighttime image 104 of location 107 may be selected to display to the user in a browser because it most closely matches the time of day (e.g., current environmental conditions) expected at the time the user will arrive in the future.
- the time of day at a location may be determined, for example, by using a latitude/longitude coordinate of the requested location to determine a time zone at that location.
- the time zone may then be used along with an internal clock of the client device 109 to calculate an expected local arrival time of the user at the location.
- weather and other types of conditions for a location can be determined based on available information, such as current weather conditions, weather forecasts as well as historical information about the location. This information may then be used as described above to select images.
- FIG. 2 is an example of such conditions.
- a set of images 200 includes street level images 202 - 208 depicting different weather conditions at a location (e.g., raining, sunny, snow, partly cloudy).
- a possible source of information regarding current weather conditions at a location may be a weather service that may provide current weather conditions as well as a prediction of the weather at that location over a range of time.
- the weather service may provide information about weather conditions at the desired destination.
- the image closely matching the weather conditions expected at the time of arrival of user may be displayed on client device 109 in response to the user's request for images of location 107 .
- the weather service provides information indicating that it may be raining at the user's desired destination
- image 204 which depicts the location in the rain may be selected for display to the user.
- other information regarding conditions at a location may include a current time of day, holiday events or seasonal conditions expected when the user will arrive.
- the user's search query may include signals indicating which seasonal conditions reflect the current conditions expected at the location when the user arrives, this may include information indicating a specific time period the user plans to be at the location.
- a set of images 300 may include street level images 302 - 308 that depict a location 107 under different seasonal conditions, such as spring, summer, fall and winter.
- a user planning a trip to the location 107 at some future point in time may wish to view images of that location beforehand.
- the query may indicate the user's plans for an upcoming ski trip by including information for “Vermont Ski Resort”, etc., indicating that the user desires to view imagery of the location that corresponds to winter conditions for the location.
- a user's navigation history may also include signals indicating a specific time period the user plans to be at the location.
- the user's navigation history may indicate that the user has viewed airline or hotel bookings for the location for a specific date or recently purchased items for the trip that are typically used during certain months like skies or ski boots. The user may have searched for driving direction to the location, e-mailed a friend or performed a number of other browsing activities which can be used as an indicator to determine a specific time period related to user's expected arrival at a location.
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, preferences or a user's current location), or to control whether and/or how to receive information that may be of interest to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, preferences or a user's current location
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's current and historical location may be generalized where location information is obtained (such as to a city, ZIP code or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code or state level
- a user may also be provided with the opportunity to decide whether, and to control how such information (such as search and location history) is used and stored on a user's device and by servers with which the device communicates.
- FIG. 4 is an example of a ranked set of images 400 including street level images 402 - 408 .
- the set of images 402 - 408 may be identified in response to the user request for images of a location 107 .
- the images may be ranked for display to a user on client device 409 based on how closely they match the current conditions. For example, conditions depicted in a highest ranking image may correspond to the current conditions expected at the location.
- the image depicting location 107 during winter months ranks higher than other images 404 , 406 and 408 identified for that location.
- the winter image 402 may depict conditions that are more aligned with the user's planned ski trip to the location during the winter or simply at the time of day or the current weather conditions at the user's time of arrival.
- a number of indicators may be used to determine current conditions depicted in the images.
- a date and time may be associated with the images indicate a time of day depicted in the images, such as the time of day when the image was captured.
- image analysis may be performed on the set of images to extract features that may be used to identify various ambient lighting or weather conditions depicted in the images. For example, a mostly dark image may depict a night scene of a location as opposed to depicting a day scene. In other examples, certain objects in an image may be identified as being snow covered identifying the images as winter month images versus objects depicted on a bright sunny summer day.
- the current conditions depicted in the images may also be identified base on information stored in the image (e.g., metadata).
- the information associated with an image may indicate that a certain white balance was used on a camera when capturing the image. This may indicate that the picture was taken at a location at night and, thus represent a nighttime depiction of the location.
- Other metadata may include specific camera settings used to capture the image that may indicate the environment conditions depicted in the image, such as a camera setting for cloudy or daytime mode.
- a user may tag or otherwise label the images with information identifying seasonal events and celebrations, or other meaningful condition information depicted in the images.
- the images may be labeled based on identified Christmas lights and other related decorations on structures within the image indicating that the images are related to a specific holiday event and/or time frame.
- FIG. 5 is a block diagram of an example system 500 , which includes a server 501 coupled to a network 595 and one or more client devices (only client device 510 being shown in FIG. 5 for clarity) capable of communicating with the server 501 over the network 595 .
- the server 501 may include a processor 502 , memory 504 , and other components typically present in general purpose computers.
- the memory 504 of server 501 may store information accessible by the processor 502 , including instructions 506 that may be executed by the processor 502 , and data 508 .
- the memory 504 may be of a type of memory operative to store information accessible by the processor 502 , including a non-transitory computer-readable medium, or other medium that stores data that may be read with the aid of an electronic device, such as a hard-drive, memory card, read-only memory (“ROM”), random access memory (“RAM”), digital versatile disc (“DVD”) or other optical disks, as well as other write-capable and read-only memories.
- ROM read-only memory
- RAM random access memory
- DVD digital versatile disc
- the subject matter disclosed herein may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media.
- FIG. 5 functionally illustrates the processor 502 and memory 504 as being within the same block
- the processor 502 and memory 504 may actually include multiple processors and memories that may or may not be stored within the same physical housing.
- some of the instructions and data may be stored on removable CD-ROM and others within a read-only computer chip.
- Some or all of the instructions and data may be stored in a location physically remote from, yet still accessible by, the processor 502 .
- the processor 504 may actually comprise a collection of processors, which may or may not operate in parallel.
- Data 508 may be retrieved, stored or modified by processor 502 in accordance with the instructions 506 .
- the data 508 may be stored in computer registers, in a relational database as a table having a plurality of different fields and records, XML documents, or flat files.
- the data 508 may also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode.
- the data 508 may be stored as bitmaps comprised of pixels that are stored in compressed or uncompressed, or various image formats (e.g., JPEG), vector-based formats (e.g., SVG) or computer instructions for drawing graphics.
- the data 508 may comprise information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories (including other network locations) or information used by a function to calculate the relevant data.
- a typical system can include a large number of connected computers, with each different computer being at a different node of the network 595 .
- the network 595 and intervening nodes, may comprise various configurations and protocols including the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi and HTTP, and various combinations of the foregoing.
- Such communication may be facilitated by a device capable of transmitting data to and from other computers, such as modems (e.g., dial-up, cable or fiber optic) and wireless interfaces.
- the client device 510 such as client device 109 of FIGS. 1-2 and client device 309 of FIGS. 3-4 , may be configured similarly to the server 501 , with a processor 512 , memory 514 , instructions 516 , and all of the internal components normally found in a personal computer.
- the client device 510 may include a central processing unit (CPU), display device 511 (for example, a monitor having a screen, a projector, a touch-screen, a small LCD screen, a television, or another device such as an electrical device operable to display information processed by the processor 512 ), CD-ROM, hard-drive, user input device 517 (for example, a keyboard, mouse, touch-screen or microphone), speakers, modem and/or network interface device (telephone, cable or otherwise) and all of the components used for connecting these elements to one another.
- CPU central processing unit
- display device 511 for example, a monitor having a screen, a projector, a touch-screen, a small LCD screen, a television, or another device such as an electrical device operable to display information processed by the processor 512
- CD-ROM compact disc read-only memory
- hard-drive for example, a keyboard, mouse, touch-screen or microphone
- speakers modem and/or network interface device (telephone, cable or otherwise)
- the client device 510 may be a computing device.
- client device 510 may be a laptop computer, a netbook, a desktop computer, and a portable personal computer such as a wireless-enabled PDA, a tablet PC or another type of computing device capable of obtaining information via a network like the Internet.
- a portable personal computer such as a wireless-enabled PDA, a tablet PC or another type of computing device capable of obtaining information via a network like the Internet.
- aspects of the disclosure generally relate to a single client device 510
- the client device 510 may be implemented as multiple devices with both portable and non-portable components (e.g., software executing on a rack-mounted server with an interface for gathering location information).
- client devices 510 may include a full-sized personal computer, the subject matter of the present disclosure may also be used in connection with mobile devices capable of wirelessly exchanging data.
- client device 510 may be a wireless-enabled mobile device, such as a Smartphone, or an Internet-capable cellular phone.
- the user may input information using a small keyboard, a keypad, a touch screen or other means of user input.
- the client devices and computers described herein may comprise a device capable of processing instructions and transmitting data to and from humans and other devices and computers including general purpose computers, network computers lacking local storage capability, game consoles, and set-top boxes for televisions.
- the client device 510 may include a component, such as circuits, to determine the location of the device.
- the client device 510 may include a GPS positioning component or receiver 518 .
- the positioning component 518 may include software for determining the position of the device based on other signals received at the client device 510 , such as signals received at a cell phone's antenna from one or more cell phone towers if the mobile device is a cell phone. In that regard, the provision of location identification data may occur automatically based on information received from such a component.
- the instructions 516 of the client device 610 may be a set of instructions to be executed directly (such as machine code) or indirectly (such as scripts) by the processor.
- the terms “instructions,” “steps” and “programs” may be used interchangeably herein.
- the instructions 516 may be stored in object code format for direct processing by the processor, or in other computer languages including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods and routines of the instructions are explained in more detail below.
- the instructions 516 may include a browser for displaying network content, and a navigation history of a user of the client device 510 .
- the browser provides for the display of network content, such as street level images, a set of search results or other types of network content, to a user of the client device 510 by sending and receiving data across the network 595 .
- the network content may be received in response to a search query that includes a search text including an indication of a location.
- the search results returned are associated with locations within the geographic region.
- the search results can be a number of street level images of different buildings or landscapes within the geographic region.
- Memory 514 may store a user's navigation history.
- the user's navigation history may represent data collected by using one or more browser add-ons, scripts or toolbars.
- the navigation history may be maintained on a remote server, such as server 501 and provided to the client device 510 .
- server 501 may be provided to the client device 510 .
- users may be provided with an opportunity to control whether programs or features of the subject matter disclosed herein may collect user information or to control whether and/or control whether and/or how to receive information that may be of interest to the user.
- Certain data may also be treated in one or more ways before it is stored or used, so that no personally identifiable information can be determined for the user.
- Image database 507 of sever 501 may store image data representing street level images 509 , which may be transmitted to client device 510 .
- Each street level image may be stored as image data, which includes an image, date, a visual orientation of the image and other data related to the images like an exposure setting used on a device to capture the images.
- the street level images 509 may include images of objects at locations, such as buildings, streets, and terrains.
- the street level images may be captured by cameras at the locations from a perspective a few feet above the ground.
- a typical street level image may include as many geographic objects (street lights, mountains, trees, bodies of water, vehicles, people, etc.) in as much detail as a camera can possibly capture.
- the image database 507 can include information relevant to a location of the street level images 509 .
- the street level images 509 may include or be associated with latitude/longitude coordinates representing locations where the images were captured.
- latitude/longitude positions may be used when referencing locations of the street level images 509 of system 500 .
- System 500 may further include a source that provides information about the current conditions at locations depicted in the street level images 509 .
- the source may be stored at the server 501 or may include external sources such as websites.
- one possible external source of information is weather service.
- the weather service may provide information about current and forecast weather at the location.
- system 500 may perform a search query for street level images 509 in response to a user request.
- the user may enter a search query for street level images using the client device 510 .
- a set of street level images 509 associated with a location may be returned.
- the user may view these street level images 509 , for example, on the display 511 of client device 510 .
- the images retuned by the server 501 may accurately depict the location, the user's interest may be directed towards viewing images of the location that depict current environmental conditions at a specific period of time, such as a time the user will at the location.
- the search request may include signals indicating which images are relevant to the user's interest. For example, a user request for “Vermont ski resorts” may indicate that the user does not just want images of these resorts, but specific imagery depicting the location during the winter months. Other types of signals indicating a user's interest in specific imagery of a location may include information about the user such as their current distance from a location or their navigation history. For example, if a user's navigation history discloses that the user has been searching for cold weather type items like ski boots; this may indicate that the user wants specific imagery of the location depicted in winter months.
- System 500 can be configured to rank the selected images that are most visually relevant to the user's interest.
- the instructions 506 of server 501 may further include an image ranking module 503 .
- the image ranking module 503 may be operable in conjunction with the server 210 from where it may receive the above described signals indicating which images are visual relevant to a user's interest.
- the functionally of this module can exist in a fewer or greater number of modules than shown in FIG. 5 , with such modules residing at one or more computing devices, which may be geographically dispersed.
- the image ranking module 503 may perform image analysis on the set of street level images 509 in order to determine a priority order for displaying the images to a user. For example, the image analysis may identify various lighting or weather conditions depicted in the street level images 509 . In this respect, a highest ranking image may reflect the current conditions expected at the location at the specific time period, such as a time the user will arrive at the location.
- the image ranking module 503 may classify the conditions depicted in the images based on information stored with the image or identified by a user or system administrator. For example, system 500 may send the images to a system administrator that may prompt the users to manually input a label identifying the environmental conditions depicted in the images before they are stored the image database 507 . When selecting images to display to a user, the system 500 may select an image from the set of images where the identified environmental conditions depicted in the image correspond to the current environmental information received regarding the location.
- FIG. 6 is a flow diagram 600 depicting an example of some of the aspects described above.
- a request for images of a location may be received.
- a server may receive a request for real-world street level imagery of a location from a client device, such as client device 510 of FIG. 5 .
- the user may enter a search query using, for example, a browser on a display of the client device.
- the user's query may include information indicating real-world coordinates of the location, such as a longitude/latitude position, a street address or other types identifying information that may be used to determine the location of which the user desires to view street level imagery.
- a set of images associated with the location may be identified. For example, these images may depict different environmental conditions at of the location.
- the different environmental conditions associated with each image may be represented as condition information that may be identified, for example, within in the image itself, as metadata with each image or as a user defined label/tag.
- the images are stored in a database coupled to a server and are captured at the location over a span of time.
- a number of indicators can be used to determine when a user will be at the location.
- One such indicator for a user traveling to the location may include the user's current GPS location. For example, from this GPS information, the user's expected time of arrival at the location can be interpolated given the user rate of travel.
- a user's search query may include information indicating a specific time period the user plans to be at the location, such as an indicator of the user's plans for an upcoming ski trip.
- the user's Internet browsing history may also indicate certain items purchased (e.g., ski lift tickets), travel bookings or communications related to a specific date or time period the user plans to be at the location.
- the environmental conditions are related to conditions at the location for the future point in time, which may be a time the user arrives at the location.
- the identified environmental conditions may include current environmental conditions, such as weather conditions at a location as well as a forecast prediction of weather at that location over a range of time.
- Other identified environmental conditions may include information regarding seasonal holiday events that occur at the location or a current time of day. This information may be received from various sources in response to those sources receiving a location identifier, such as the identified location from stage 610 .
- the set of images may be ranked based on the received environmental information and the future point in time.
- the ranking may identify one or more images that depict different environmental conditions at the location.
- the ranking may be based on how closely the images match the current environmental condition expected at the future point in time, such as a time of arrival of a user at the location, identified at stage 630 .
- a number of indicators are used to determine conditions depicted in the images. For example, a date and time may be associated with the images indicate a time of day depicted in the images.
- image analysis may be performed on the set of images to extract features that may be used to identify various ambient lighting or weather conditions depicted in the images. For example, using the image analysis, image feature can be identified in the images related to certain environmental climate conditions (e.g., snow on an object). The environmental conditions depicted in the images may also be determined base on information stored with the image or by a user.
- an image may be selected based on its rank relative to other images. For example, a highest ranked image may be selected, which depicts current environmental conditions at the location most closely matching the current environmental conditions received at stage 640 .
- the selected image may be displayed to a user on a client device, such as on client device 510 of FIG. 5 .
Abstract
Description
- Various services such as Google Maps may provide street level images, often called “Street Views”, of geographic locations to users. Typically in such services, a client computer may request street level images of a particular location and receive images, such as digitized panoramic 360 photographs, taken at street level in response. The images are received from different sources and may be grouped by locations where the images were taken. In some examples, these images may include street level photographs of real world locations, which may allow users to view these locations from a person's perspective at ground level.
- User searches for maps and photos using such services for displaying street level images may often return images of a place at the most popular times for taking pictures and under perfect conditions. However, if the users are en route to a place or planning a trip at a certain time, the returned images may not match how the place may currently look upon the user's arrival.
- Aspects of this disclosure may be advantageous for providing techniques in searching for images of a location that depict current conditions at that location for a future point in time, such as an expected arrival time of a user. One aspect of the present technology provides a method. The method includes receiving a request for images of a location. In response to the request, a set of images depicting different conditions at the location may be identified. Each image of the set of images is associated with condition information defining the conditions depicted in that image. The method further includes determining when a user will be at the location at a future point in time. Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received. The set of images are ranked based on the received environmental information and the condition information for each image of the set of images. An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images. The selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- In one example, the future point in time may be based on at least one of: a search query performed by the user, a current location of the user, and the user's search history. In this regard, if the future point in time is based on the user's search history, the search history includes information indicating items related to a given time period corresponding to the future point in time. Ranking the set of images comprises identifying visual features disposed in each image of the set of images. The visual features disposed in a highest ranked image corresponds to the received environmental information
- In one example, the environmental condition information includes information representing weather conditions at the location for a time corresponding to the future point in time.
- In one example, the environmental condition information further includes information representing a time of day at the location for the future point in time.
- In one example, the environmental condition information further includes information related to a seasonal event that occurred at location. The event occurred at a time corresponding to the future point in time.
- Another aspects of the present technology provides a non-transitory computer readable medium including instructions that, when executed by a processor, cause the processor to perform a method. The method further includes receiving a request for images of a location. In response to the request, a set of images depicting different conditions at the location may be identified. Each image of the set of images is associated with condition information defining the conditions depicted in that image. The method includes determining when a user will be at the location at a future point in time. Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received. The set of images are ranked based on the received environmental information and the condition information for each image of the set of images. An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images. The selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
- Yet another aspect of the present technology provides a system that includes a memory for storing images and a processor coupled to the memory. The processor may be configured to receive a request for images of a location. In response to the request, a set of images depicting different conditions at the location may be identified. Each image of the set of images is associated with condition information defining the conditions depicted in that image. The processor may be further configured to determine when a user will be at the location at a future point in time. Environmental information including information identifying environmental conditions expected at the location for the future point in time may be received. The set of images are ranked based on the received environmental information and the condition information for each image of the set of images. An image from the set of images may be selected for display on a client device based on the selected image's rank relative to other images of the set of images. The selected image depicts current conditions at the location most closely matching the environmental conditions expected at the future point in time.
-
FIG. 1 is an example set of images including street level images in accordance with aspects of the disclosure. -
FIG. 2 is another example set of images including street level in accordance with aspects of the disclosure. -
FIG. 3 is yet another example set of images including street level images in accordance with aspects of the disclosure. -
FIG. 4 is an example of a ranked set of images in accordance with aspects of the disclosure. -
FIG. 5 is a block diagram of a system in accordance with aspects of the disclosure. -
FIG. 6 is a flow diagram depicting an example of a method in accordance with aspects of the disclosure. - Aspects, features and advantages of this disclosure will be appreciated when considered with reference to the following description of embodiments and accompanying figures. The same reference numbers in different drawings may identify the same or similar elements. Furthermore, the following description is not limiting; the scope of the present technology is defined by the appended claims and equivalents. While certain processes in accordance with example embodiments are shown in the figures as occurring in a linear fashion, this is not a requirement unless expressly stated herein. Different processes may be performed in a different order or concurrently. Steps may also be added or omitted unless otherwise stated.
- The present disclosure relates generally to ranking identified images of a location based on current conditions at that location. For example, when a user initiates a request to view images, such as street level images, of a geographic location, the techniques disclosed herein may automatically select real-world street level images that are most visually relevant to the current conditions (e.g., weather, time of day, holiday decor, etc.) at the location expected at a future point in time. While aspects of the disclosure are discussed in connection with street-level imagery, these techniques can also be used with other types of imagery such as aerial imagery or indoor imagery.
- According to aspects, in response to a user request, a set of images for a location can be identified where the images correspond to images depicting different conditions at the location. Each image of the set of images is associated with condition information depicted in that image. It may be determined when a user will be at the location at a future point in time. For example, for a user currently traveling to a location, the user's current Global Positioning System (GPS) coordinates may be used to determine the user's expected time of arrival at the location. In order to determine which images of the set of images are displayed to the user, the images are ranked by a processor based on received environment information and the condition information associated with each image. The received environment information may include information indicating current environmental conditions at the location for the future point in time. An image from the set of images may be selected based on its rank relative to other images of the set of images. The selected image may depict current conditions at the location most closely matching the environmental conditions expected at the future point in time.
-
FIG. 1 is an example set ofimages 100 including a set of street level images 102-104 of a location. The set ofimages 100 may depict objects at the location such as a street, buildings, various terrains and different environmental conditions. Typically, these images are captured, for example, by cameras or image capturing devices at the locations in a direction generally parallel to the ground. - The set of
images 100 may be identified in response to a user's request for imagery of a location. For example, a user may request images of a location the user may plan to visit at a future point in time. The search request or query for images of the location may include relevant information for identifying the location. For example, the user's query may include information indicating a specific location (e.g., “Vermont Mountain Resort”), a longitude/latitude position, a street address or other types identifying information that may be used to determine the location of which the user desires to view street level imagery. In one example, the user may enter the identifying information into a browser onclient device 109. Thereupon, a server in communication with theclient device 109 may identify one or more images also associated with the identifying information, and send these images to theclient device 109. - The
client device 109 may receive the image and display them to the user. As an example, a street level image may be shown to a user on a display of theclient device 109 along with controls for zooming in on the image as well as controls for changing the orientation of the view (which may require retrieving more street level images). For example, the techniques disclosed herein may be implemented in connection with an Internet browser such as Google Chrome (not shown) for displaying street level images as well as other information. Other navigation controls may be included as well, such as panning controls in the form of arrows disposed along the street. Such arrows may be selected by a user (by clicking or by dragging along the street line) to change the vantage point from up to down the street. - Although more than one image may be identified for a location, a particular user may be most interested in viewing an image that depicts conditions at the location for a time closely matching an expected time of arrival. For example, the user may be en route to a
location 107 when making a request for imagery of that location. As shown in the example ofFIG. 1 , the set ofimages 100 identified in response to the user's request may include images, such asstreet level images location 107 under different conditions (e.g., at daytime and nighttime). - The user's expected time of arrival at a location can be estimated, for example, based on the user's current geolocated position. For example, the user's
client device 109 may include a positioning component, such as a GPS receiver, to determine a location of the device. Based on the device's current position, it can be approximated how far the user may be from the desired location. In turn, an arrival time of the user at the location can be estimated based on the user's known position and the user's given rate of travel. In some aspects, the rate of travel of the user may be interpolated, for example, from the GPS signal. - This particular user, however, may be traveling by
vehicle 105 to thelocation 107 and may arrive at a certain time of day, such as 12:00 am midnight. In this case, anighttime image 104 oflocation 107 may be selected to display to the user in a browser because it most closely matches the time of day (e.g., current environmental conditions) expected at the time the user will arrive in the future. The time of day at a location may be determined, for example, by using a latitude/longitude coordinate of the requested location to determine a time zone at that location. The time zone, in turn, may then be used along with an internal clock of theclient device 109 to calculate an expected local arrival time of the user at the location. - Other types of current conditions may also be used to select and provide images. For example, weather and other types of conditions for a location can be determined based on available information, such as current weather conditions, weather forecasts as well as historical information about the location. This information may then be used as described above to select images.
-
FIG. 2 is an example of such conditions. In this example, a set ofimages 200 includes street level images 202-208 depicting different weather conditions at a location (e.g., raining, sunny, snow, partly cloudy). As an example, a possible source of information regarding current weather conditions at a location may be a weather service that may provide current weather conditions as well as a prediction of the weather at that location over a range of time. For example, in response to receiving a location identifier, such as a desired destination of the user traveling viavehicle 105, the weather service may provide information about weather conditions at the desired destination. - In this instance, the image closely matching the weather conditions expected at the time of arrival of user may be displayed on
client device 109 in response to the user's request for images oflocation 107. For example, if the weather service provides information indicating that it may be raining at the user's desired destination,image 204 which depicts the location in the rain may be selected for display to the user. - Still further, other information regarding conditions at a location may include a current time of day, holiday events or seasonal conditions expected when the user will arrive. In this regard, there are many indicators that can be used to estimate when a user will be at a location. For example, the user's search query may include signals indicating which seasonal conditions reflect the current conditions expected at the location when the user arrives, this may include information indicating a specific time period the user plans to be at the location.
- As shown in the example of
FIG. 3 , a set ofimages 300 may include street level images 302-308 that depict alocation 107 under different seasonal conditions, such as spring, summer, fall and winter. For example, a user planning a trip to thelocation 107 at some future point in time may wish to view images of that location beforehand. In this example, the query may indicate the user's plans for an upcoming ski trip by including information for “Vermont Ski Resort”, etc., indicating that the user desires to view imagery of the location that corresponds to winter conditions for the location. - In some aspects, a user's navigation history may also include signals indicating a specific time period the user plans to be at the location. For example, the user's navigation history may indicate that the user has viewed airline or hotel bookings for the location for a specific date or recently purchased items for the trip that are typically used during certain months like skies or ski boots. The user may have searched for driving direction to the location, e-mailed a friend or performed a number of other browsing activities which can be used as an indicator to determine a specific time period related to user's expected arrival at a location.
- For situations in which the subject matter described herein collects information about users, or may make use of user-related information, the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, preferences or a user's current location), or to control whether and/or how to receive information that may be of interest to the user. In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's current and historical location may be generalized where location information is obtained (such as to a city, ZIP code or state level), so that a particular location of a user cannot be determined. A user may also be provided with the opportunity to decide whether, and to control how such information (such as search and location history) is used and stored on a user's device and by servers with which the device communicates.
- Before selecting an image to be displayed to the user, images identified based on the conditions as described above may be ranked.
FIG. 4 is an example of a ranked set ofimages 400 including street level images 402-408. As noted above, the set of images 402-408 may be identified in response to the user request for images of alocation 107. The images may be ranked for display to a user onclient device 409 based on how closely they match the current conditions. For example, conditions depicted in a highest ranking image may correspond to the current conditions expected at the location. - As shown in the example of
FIG. 4 , theimage depicting location 107 during winter months ranks higher thanother images winter image 402 may depict conditions that are more aligned with the user's planned ski trip to the location during the winter or simply at the time of day or the current weather conditions at the user's time of arrival. - In order to determine a ranking of the identified set of images, a number of indicators may be used to determine current conditions depicted in the images. In one aspect, a date and time may be associated with the images indicate a time of day depicted in the images, such as the time of day when the image was captured. In some aspects, image analysis may be performed on the set of images to extract features that may be used to identify various ambient lighting or weather conditions depicted in the images. For example, a mostly dark image may depict a night scene of a location as opposed to depicting a day scene. In other examples, certain objects in an image may be identified as being snow covered identifying the images as winter month images versus objects depicted on a bright sunny summer day.
- The current conditions depicted in the images may also be identified base on information stored in the image (e.g., metadata). For example, the information associated with an image may indicate that a certain white balance was used on a camera when capturing the image. This may indicate that the picture was taken at a location at night and, thus represent a nighttime depiction of the location. Other metadata may include specific camera settings used to capture the image that may indicate the environment conditions depicted in the image, such as a camera setting for cloudy or daytime mode.
- In other situations, a user may tag or otherwise label the images with information identifying seasonal events and celebrations, or other meaningful condition information depicted in the images. For example, the images may be labeled based on identified Christmas lights and other related decorations on structures within the image indicating that the images are related to a specific holiday event and/or time frame.
-
FIG. 5 is a block diagram of anexample system 500, which includes aserver 501 coupled to anetwork 595 and one or more client devices (only client device 510 being shown inFIG. 5 for clarity) capable of communicating with theserver 501 over thenetwork 595. Theserver 501 may include aprocessor 502,memory 504, and other components typically present in general purpose computers. - The
memory 504 ofserver 501 may store information accessible by theprocessor 502, includinginstructions 506 that may be executed by theprocessor 502, anddata 508. Thememory 504 may be of a type of memory operative to store information accessible by theprocessor 502, including a non-transitory computer-readable medium, or other medium that stores data that may be read with the aid of an electronic device, such as a hard-drive, memory card, read-only memory (“ROM”), random access memory (“RAM”), digital versatile disc (“DVD”) or other optical disks, as well as other write-capable and read-only memories. The subject matter disclosed herein may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media. - Although
FIG. 5 functionally illustrates theprocessor 502 andmemory 504 as being within the same block, theprocessor 502 andmemory 504 may actually include multiple processors and memories that may or may not be stored within the same physical housing. For example, some of the instructions and data may be stored on removable CD-ROM and others within a read-only computer chip. Some or all of the instructions and data may be stored in a location physically remote from, yet still accessible by, theprocessor 502. Similarly, theprocessor 504 may actually comprise a collection of processors, which may or may not operate in parallel. -
Data 508 may be retrieved, stored or modified byprocessor 502 in accordance with theinstructions 506. For instance, although the systems and method disclosed herein are not limited by a particular data structure, thedata 508 may be stored in computer registers, in a relational database as a table having a plurality of different fields and records, XML documents, or flat files. Thedata 508 may also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode. By further way of example only, thedata 508 may be stored as bitmaps comprised of pixels that are stored in compressed or uncompressed, or various image formats (e.g., JPEG), vector-based formats (e.g., SVG) or computer instructions for drawing graphics. Moreover, thedata 508 may comprise information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories (including other network locations) or information used by a function to calculate the relevant data. - A typical system can include a large number of connected computers, with each different computer being at a different node of the
network 595. Thenetwork 595, and intervening nodes, may comprise various configurations and protocols including the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi and HTTP, and various combinations of the foregoing. Such communication may be facilitated by a device capable of transmitting data to and from other computers, such as modems (e.g., dial-up, cable or fiber optic) and wireless interfaces. - The
client device 510, such asclient device 109 ofFIGS. 1-2 andclient device 309 ofFIGS. 3-4 , may be configured similarly to theserver 501, with aprocessor 512,memory 514,instructions 516, and all of the internal components normally found in a personal computer. By way of example only, theclient device 510 may include a central processing unit (CPU), display device 511 (for example, a monitor having a screen, a projector, a touch-screen, a small LCD screen, a television, or another device such as an electrical device operable to display information processed by the processor 512), CD-ROM, hard-drive, user input device 517 (for example, a keyboard, mouse, touch-screen or microphone), speakers, modem and/or network interface device (telephone, cable or otherwise) and all of the components used for connecting these elements to one another. - The
client device 510 may be a computing device. For example,client device 510 may be a laptop computer, a netbook, a desktop computer, and a portable personal computer such as a wireless-enabled PDA, a tablet PC or another type of computing device capable of obtaining information via a network like the Internet. Although aspects of the disclosure generally relate to asingle client device 510, theclient device 510 may be implemented as multiple devices with both portable and non-portable components (e.g., software executing on a rack-mounted server with an interface for gathering location information). - Although the
client devices 510 may include a full-sized personal computer, the subject matter of the present disclosure may also be used in connection with mobile devices capable of wirelessly exchanging data. For example,client device 510 may be a wireless-enabled mobile device, such as a Smartphone, or an Internet-capable cellular phone. In either regard, the user may input information using a small keyboard, a keypad, a touch screen or other means of user input. In various aspects, the client devices and computers described herein may comprise a device capable of processing instructions and transmitting data to and from humans and other devices and computers including general purpose computers, network computers lacking local storage capability, game consoles, and set-top boxes for televisions. - The
client device 510 may include a component, such as circuits, to determine the location of the device. For example, theclient device 510 may include a GPS positioning component orreceiver 518. By way of example only, thepositioning component 518 may include software for determining the position of the device based on other signals received at theclient device 510, such as signals received at a cell phone's antenna from one or more cell phone towers if the mobile device is a cell phone. In that regard, the provision of location identification data may occur automatically based on information received from such a component. - The
instructions 516 of theclient device 610 may be a set of instructions to be executed directly (such as machine code) or indirectly (such as scripts) by the processor. In that regard, the terms “instructions,” “steps” and “programs” may be used interchangeably herein. Theinstructions 516 may be stored in object code format for direct processing by the processor, or in other computer languages including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods and routines of the instructions are explained in more detail below. - The
instructions 516 may include a browser for displaying network content, and a navigation history of a user of theclient device 510. The browser provides for the display of network content, such as street level images, a set of search results or other types of network content, to a user of theclient device 510 by sending and receiving data across thenetwork 595. The network content may be received in response to a search query that includes a search text including an indication of a location. The search results returned are associated with locations within the geographic region. For example, the search results can be a number of street level images of different buildings or landscapes within the geographic region. -
Memory 514 may store a user's navigation history. The user's navigation history may represent data collected by using one or more browser add-ons, scripts or toolbars. In some aspects, the navigation history may be maintained on a remote server, such asserver 501 and provided to theclient device 510. As discussed above, users may be provided with an opportunity to control whether programs or features of the subject matter disclosed herein may collect user information or to control whether and/or control whether and/or how to receive information that may be of interest to the user. Certain data may also be treated in one or more ways before it is stored or used, so that no personally identifiable information can be determined for the user. -
Image database 507 ofsever 501 may store image data representingstreet level images 509, which may be transmitted toclient device 510. Each street level image may be stored as image data, which includes an image, date, a visual orientation of the image and other data related to the images like an exposure setting used on a device to capture the images. Thestreet level images 509 may include images of objects at locations, such as buildings, streets, and terrains. The street level images may be captured by cameras at the locations from a perspective a few feet above the ground. In many aspects, a typical street level image may include as many geographic objects (street lights, mountains, trees, bodies of water, vehicles, people, etc.) in as much detail as a camera can possibly capture. - The
image database 507 can include information relevant to a location of thestreet level images 509. For example, thestreet level images 509 may include or be associated with latitude/longitude coordinates representing locations where the images were captured. Although the subject matter disclosed herein is not limited to a particular positional reference system, latitude/longitude positions may be used when referencing locations of thestreet level images 509 ofsystem 500. -
System 500 may further include a source that provides information about the current conditions at locations depicted in thestreet level images 509. The source may be stored at theserver 501 or may include external sources such as websites. As noted above, one possible external source of information is weather service. For example, in response to receiving a location overnetwork 595, the weather service may provide information about current and forecast weather at the location. - In one aspect,
system 500 may perform a search query forstreet level images 509 in response to a user request. For example, the user may enter a search query for street level images using theclient device 510. In response to the search query, a set ofstreet level images 509 associated with a location may be returned. The user may view thesestreet level images 509, for example, on thedisplay 511 ofclient device 510. Although the images retuned by theserver 501 may accurately depict the location, the user's interest may be directed towards viewing images of the location that depict current environmental conditions at a specific period of time, such as a time the user will at the location. - The search request may include signals indicating which images are relevant to the user's interest. For example, a user request for “Vermont ski resorts” may indicate that the user does not just want images of these resorts, but specific imagery depicting the location during the winter months. Other types of signals indicating a user's interest in specific imagery of a location may include information about the user such as their current distance from a location or their navigation history. For example, if a user's navigation history discloses that the user has been searching for cold weather type items like ski boots; this may indicate that the user wants specific imagery of the location depicted in winter months.
System 500 can be configured to rank the selected images that are most visually relevant to the user's interest. - In order to facilitate the image ranking operations of
system 500, theinstructions 506 ofserver 501 may further include animage ranking module 503. Theimage ranking module 503 may be operable in conjunction with the server 210 from where it may receive the above described signals indicating which images are visual relevant to a user's interest. The functionally of this module can exist in a fewer or greater number of modules than shown inFIG. 5 , with such modules residing at one or more computing devices, which may be geographically dispersed. - The
image ranking module 503 may perform image analysis on the set ofstreet level images 509 in order to determine a priority order for displaying the images to a user. For example, the image analysis may identify various lighting or weather conditions depicted in thestreet level images 509. In this respect, a highest ranking image may reflect the current conditions expected at the location at the specific time period, such as a time the user will arrive at the location. - In some aspects, the
image ranking module 503 may classify the conditions depicted in the images based on information stored with the image or identified by a user or system administrator. For example,system 500 may send the images to a system administrator that may prompt the users to manually input a label identifying the environmental conditions depicted in the images before they are stored theimage database 507. When selecting images to display to a user, thesystem 500 may select an image from the set of images where the identified environmental conditions depicted in the image correspond to the current environmental information received regarding the location. -
FIG. 6 is a flow diagram 600 depicting an example of some of the aspects described above. Atstage 610, a request for images of a location may be received. For example, a server may receive a request for real-world street level imagery of a location from a client device, such asclient device 510 ofFIG. 5 . The user may enter a search query using, for example, a browser on a display of the client device. The user's query may include information indicating real-world coordinates of the location, such as a longitude/latitude position, a street address or other types identifying information that may be used to determine the location of which the user desires to view street level imagery. - At
stage 620, in response to the user's request, a set of images associated with the location may be identified. For example, these images may depict different environmental conditions at of the location. The different environmental conditions associated with each image may be represented as condition information that may be identified, for example, within in the image itself, as metadata with each image or as a user defined label/tag. The images are stored in a database coupled to a server and are captured at the location over a span of time. - At
stage 630, it may be determined when a user will be at the location at a future point in time, such as the user's time of arrival at the location. For example, a number of indicators can be used to determine when a user will be at the location. One such indicator for a user traveling to the location may include the user's current GPS location. For example, from this GPS information, the user's expected time of arrival at the location can be interpolated given the user rate of travel. - Other such indicators may include a search query or the user's Internet browsing history. For example, a user's search query may include information indicating a specific time period the user plans to be at the location, such as an indicator of the user's plans for an upcoming ski trip. The user's Internet browsing history may also indicate certain items purchased (e.g., ski lift tickets), travel bookings or communications related to a specific date or time period the user plans to be at the location.
- At
stage 640, information identifying environmental conditions at the location may be received. The environmental conditions are related to conditions at the location for the future point in time, which may be a time the user arrives at the location. For example, the identified environmental conditions may include current environmental conditions, such as weather conditions at a location as well as a forecast prediction of weather at that location over a range of time. Other identified environmental conditions may include information regarding seasonal holiday events that occur at the location or a current time of day. This information may be received from various sources in response to those sources receiving a location identifier, such as the identified location fromstage 610. - At
stage 650, the set of images may be ranked based on the received environmental information and the future point in time. The ranking may identify one or more images that depict different environmental conditions at the location. The ranking may be based on how closely the images match the current environmental condition expected at the future point in time, such as a time of arrival of a user at the location, identified atstage 630. - In order to determine a ranking order for the identified images, a number of indicators are used to determine conditions depicted in the images. For example, a date and time may be associated with the images indicate a time of day depicted in the images. In some aspects, image analysis may be performed on the set of images to extract features that may be used to identify various ambient lighting or weather conditions depicted in the images. For example, using the image analysis, image feature can be identified in the images related to certain environmental climate conditions (e.g., snow on an object). The environmental conditions depicted in the images may also be determined base on information stored with the image or by a user.
- At
stage 660, an image may be selected based on its rank relative to other images. For example, a highest ranked image may be selected, which depicts current environmental conditions at the location most closely matching the current environmental conditions received atstage 640. The selected image may be displayed to a user on a client device, such as onclient device 510 ofFIG. 5 . - As these and other variations and combinations of the features discussed above can be utilized without departing from the disclosure as defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the disclosure as defined by the claims. It will also be understood that the provision of examples of the disclosure (as well as clauses phrased as “such as,” “e.g.”, “including” and the like) should not be interpreted as limiting the disclosure to the specific examples; rather, the examples are intended to illustrate only some of many possible embodiments.
Claims (23)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/492,583 US10331733B2 (en) | 2013-04-25 | 2017-04-20 | System and method for presenting condition-specific geographic imagery |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/870,419 US9672223B2 (en) | 2013-04-25 | 2013-04-25 | Geo photo searching based on current conditions at a location |
US15/492,583 US10331733B2 (en) | 2013-04-25 | 2017-04-20 | System and method for presenting condition-specific geographic imagery |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/870,419 Continuation US9672223B2 (en) | 2013-04-25 | 2013-04-25 | Geo photo searching based on current conditions at a location |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180307707A1 true US20180307707A1 (en) | 2018-10-25 |
US10331733B2 US10331733B2 (en) | 2019-06-25 |
Family
ID=63852749
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/492,583 Active US10331733B2 (en) | 2013-04-25 | 2017-04-20 | System and method for presenting condition-specific geographic imagery |
Country Status (1)
Country | Link |
---|---|
US (1) | US10331733B2 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230044871A1 (en) * | 2020-12-29 | 2023-02-09 | Google Llc | Search Results With Result-Relevant Highlighting |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030103086A1 (en) * | 2001-11-30 | 2003-06-05 | Eastman Kodak Company | Method for viewing geolocated images linked to a context |
US20060123001A1 (en) * | 2004-10-13 | 2006-06-08 | Copernic Technologies, Inc. | Systems and methods for selecting digital advertisements |
US20070012783A1 (en) * | 2005-06-20 | 2007-01-18 | Mercolino Thomas J | Systems and methods for product authentication |
US20070043748A1 (en) * | 2005-08-17 | 2007-02-22 | Gaurav Bhalotia | Method and apparatus for organizing digital images with embedded metadata |
US20130013591A1 (en) * | 2011-07-08 | 2013-01-10 | Microsoft Corporation | Image re-rank based on image annotations |
US20130024449A1 (en) * | 2011-07-20 | 2013-01-24 | Ness Computing, Inc. | Method and apparatus for allowing users to augment searches |
US20130110822A1 (en) * | 2011-10-26 | 2013-05-02 | Google Inc. | Indicating Location Status |
US20130226915A1 (en) * | 2012-02-29 | 2013-08-29 | Inrix, Inc. | Organization of search results based upon availability of respective providers comprised therein |
US20140297575A1 (en) * | 2013-04-01 | 2014-10-02 | Google Inc. | Navigating through geolocated imagery spanning space and time |
US8898148B1 (en) * | 2009-01-08 | 2014-11-25 | Google Inc. | Targeting to physical environment |
US8995716B1 (en) * | 2012-07-12 | 2015-03-31 | Google Inc. | Image search results by seasonal time period |
Family Cites Families (64)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6977679B2 (en) | 2001-04-03 | 2005-12-20 | Hewlett-Packard Development Company, L.P. | Camera meta-data for content categorization |
US20020181739A1 (en) | 2001-06-04 | 2002-12-05 | Massachusetts Institute Of Technology | Video system for monitoring and reporting weather conditions |
EP1751690B1 (en) | 2004-02-04 | 2018-10-03 | Digimarc Corporation | Digital watermarking image signals on-chip and photographic travel logs through digital watermarking |
US8150617B2 (en) | 2004-10-25 | 2012-04-03 | A9.Com, Inc. | System and method for displaying location-specific images on a mobile device |
MX2007008283A (en) | 2005-01-06 | 2007-12-05 | Alan Shulman | Navigation and inspection system. |
US20060224583A1 (en) | 2005-03-31 | 2006-10-05 | Google, Inc. | Systems and methods for analyzing a user's web history |
WO2006121986A2 (en) | 2005-05-06 | 2006-11-16 | Facet Technology Corp. | Network-based navigation system having virtual drive-thru advertisements integrated with actual imagery from along a physical route |
US20070150188A1 (en) | 2005-05-27 | 2007-06-28 | Outland Research, Llc | First-person video-based travel planning system |
US7925995B2 (en) | 2005-06-30 | 2011-04-12 | Microsoft Corporation | Integration of location logs, GPS signals, and spatial resources for identifying user activities, goals, and context |
US8842197B2 (en) * | 2005-11-30 | 2014-09-23 | Scenera Mobile Technologies, Llc | Automatic generation of metadata for a digital image based on ambient conditions |
US7617246B2 (en) | 2006-02-21 | 2009-11-10 | Geopeg, Inc. | System and method for geo-coding user generated content |
US7486201B2 (en) * | 2006-01-10 | 2009-02-03 | Myweather, Llc | Combined personalized traffic and weather report and alert system and method |
US7813870B2 (en) | 2006-03-03 | 2010-10-12 | Inrix, Inc. | Dynamic time series prediction of future traffic conditions |
US7872593B1 (en) | 2006-04-28 | 2011-01-18 | At&T Intellectual Property Ii, L.P. | System and method for collecting image data |
US20080005068A1 (en) | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Context-based search, retrieval, and awareness |
US8594702B2 (en) | 2006-11-06 | 2013-11-26 | Yahoo! Inc. | Context server for associating information based on context |
US20080147730A1 (en) | 2006-12-18 | 2008-06-19 | Motorola, Inc. | Method and system for providing location-specific image information |
US7752188B2 (en) | 2007-02-16 | 2010-07-06 | Sony Ericsson Mobile Communications Ab | Weather information in a calendar |
US20080222132A1 (en) | 2007-03-07 | 2008-09-11 | Jiangyi Pan | Personalized shopping recommendation based on search units |
US7720844B2 (en) | 2007-07-03 | 2010-05-18 | Vulcan, Inc. | Method and system for continuous, dynamic, adaptive searching based on a continuously evolving personal region of interest |
US20090012955A1 (en) | 2007-07-03 | 2009-01-08 | John Chu | Method and system for continuous, dynamic, adaptive recommendation based on a continuously evolving personal region of interest |
US9612126B2 (en) | 2007-12-03 | 2017-04-04 | Nokia Technologies Oy | Visual travel guide |
US8321431B2 (en) | 2008-08-28 | 2012-11-27 | Frogzog, Llc | Iterative and interactive context based searching |
US20100131499A1 (en) | 2008-11-24 | 2010-05-27 | Van Leuken Reinier H | Clustering Image Search Results Through Folding |
US8391615B2 (en) | 2008-12-02 | 2013-03-05 | Intel Corporation | Image recognition algorithm, method of identifying a target image using same, and method of selecting data for transmission to a portable electronic device |
EP2380093B1 (en) | 2009-01-21 | 2016-07-20 | Telefonaktiebolaget LM Ericsson (publ) | Generation of annotation tags based on multimodal metadata and structured semantic descriptors |
US9683853B2 (en) * | 2009-01-23 | 2017-06-20 | Fuji Xerox Co., Ltd. | Image matching in support of mobile navigation |
US20100217525A1 (en) | 2009-02-25 | 2010-08-26 | King Simon P | System and Method for Delivering Sponsored Landmark and Location Labels |
US9415723B2 (en) | 2009-03-31 | 2016-08-16 | Konica Minolta Holdings, Inc. | Image integration unit and image integration method |
US20100250581A1 (en) * | 2009-03-31 | 2010-09-30 | Google Inc. | System and method of displaying images based on environmental conditions |
US20100287178A1 (en) | 2009-05-08 | 2010-11-11 | Google Inc. | Refining location estimates and reverse geocoding based on a user profile |
US8274571B2 (en) * | 2009-05-21 | 2012-09-25 | Google Inc. | Image zooming using pre-existing imaging information |
US20100306249A1 (en) | 2009-05-27 | 2010-12-02 | James Hill | Social network systems and methods |
US9767209B2 (en) * | 2009-05-28 | 2017-09-19 | Apple Inc. | Search filtering based on expected future time and location |
US8275649B2 (en) | 2009-09-18 | 2012-09-25 | Microsoft Corporation | Mining life pattern based on location history |
US8719198B2 (en) | 2010-05-04 | 2014-05-06 | Microsoft Corporation | Collaborative location and activity recommendations |
US8954425B2 (en) | 2010-06-08 | 2015-02-10 | Microsoft Corporation | Snippet extraction and ranking |
US8639803B2 (en) | 2010-09-01 | 2014-01-28 | Telefonaktiebolaget L M Ericsson (Publ) | Systems and method for predicting the future location of an entity |
US8866847B2 (en) | 2010-09-14 | 2014-10-21 | International Business Machines Corporation | Providing augmented reality information |
US20120084248A1 (en) * | 2010-09-30 | 2012-04-05 | Microsoft Corporation | Providing suggestions based on user intent |
JP5549515B2 (en) | 2010-10-05 | 2014-07-16 | カシオ計算機株式会社 | Imaging apparatus and method, and program |
US8958822B2 (en) | 2010-10-25 | 2015-02-17 | Alohar Mobile Inc. | Determining points of interest of a mobile user |
US20120316902A1 (en) | 2011-05-17 | 2012-12-13 | Amit Kumar | User interface for real time view of web site activity |
KR101864892B1 (en) | 2011-05-31 | 2018-06-05 | 삼성전자주식회사 | Device and method for providing search pattern of user in wireless terminal |
US20130036139A1 (en) * | 2011-08-01 | 2013-02-07 | Kung Solutions, LLC | Travel Planning Decision Tool |
US8694456B2 (en) | 2011-08-19 | 2014-04-08 | Bank Of America Corporation | Predicting future travel based on a user's historical financial institution transaction data and providing offers based on the predicted future travel |
US20130053054A1 (en) | 2011-08-24 | 2013-02-28 | Microsoft Corporation | Using predictive technology to intelligently choose communication |
KR20130037031A (en) | 2011-10-05 | 2013-04-15 | 삼성전자주식회사 | Apparatus and method for analyzing user preference about domain using multi-dimensional and multi-layer context structure |
US20130101159A1 (en) * | 2011-10-21 | 2013-04-25 | Qualcomm Incorporated | Image and video based pedestrian traffic estimation |
US9279693B2 (en) | 2012-02-17 | 2016-03-08 | Blackberry Limited | Navigation system and method for determining a route based on sun position and weather |
US20130232168A1 (en) * | 2012-02-17 | 2013-09-05 | Lauren Leigh McGregor | Presenting a Temporal Sequence of Geographic Location-Specific Digital Data |
US8972318B2 (en) | 2012-05-31 | 2015-03-03 | Qualcomm Incorporated | Predicting user behavior using feedback on previously run predictive searches |
US8996305B2 (en) | 2012-06-07 | 2015-03-31 | Yahoo! Inc. | System and method for discovering photograph hotspots |
CN109633785A (en) * | 2012-06-22 | 2019-04-16 | 谷歌有限责任公司 | Weather forecast based on desired location |
WO2014020995A1 (en) * | 2012-07-31 | 2014-02-06 | 古野電気株式会社 | Meteorological information display system, human navigation device, meteorological information display program and meteorological information display method |
US9009093B1 (en) | 2012-10-04 | 2015-04-14 | Amazon Technologies, Inc. | Deal scheduling based on user location predictions |
US10242412B2 (en) | 2012-11-20 | 2019-03-26 | Facebook, Inc. | Ambient-location-push notification |
US9677886B2 (en) | 2013-02-10 | 2017-06-13 | Qualcomm Incorporated | Method and apparatus for navigation based on media density along possible routes |
US10467553B2 (en) | 2013-03-13 | 2019-11-05 | Airbnb, Inc. | Automated determination of booking availability for user sourced accommodations |
US9672223B2 (en) * | 2013-04-25 | 2017-06-06 | Google Inc. | Geo photo searching based on current conditions at a location |
US9618343B2 (en) * | 2013-12-12 | 2017-04-11 | Microsoft Technology Licensing, Llc | Predicted travel intent |
US20150310601A1 (en) | 2014-03-07 | 2015-10-29 | Digimarc Corporation | Methods and arrangements for identifying objects |
US20160203137A1 (en) * | 2014-12-17 | 2016-07-14 | InSnap, Inc. | Imputing knowledge graph attributes to digital multimedia based on image and video metadata |
WO2016106358A1 (en) * | 2014-12-22 | 2016-06-30 | Robert Bosch Gmbh | System and methods for interactive hybrid-dimension map visualization |
-
2017
- 2017-04-20 US US15/492,583 patent/US10331733B2/en active Active
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030103086A1 (en) * | 2001-11-30 | 2003-06-05 | Eastman Kodak Company | Method for viewing geolocated images linked to a context |
US20060123001A1 (en) * | 2004-10-13 | 2006-06-08 | Copernic Technologies, Inc. | Systems and methods for selecting digital advertisements |
US20070012783A1 (en) * | 2005-06-20 | 2007-01-18 | Mercolino Thomas J | Systems and methods for product authentication |
US20070043748A1 (en) * | 2005-08-17 | 2007-02-22 | Gaurav Bhalotia | Method and apparatus for organizing digital images with embedded metadata |
US8898148B1 (en) * | 2009-01-08 | 2014-11-25 | Google Inc. | Targeting to physical environment |
US20130013591A1 (en) * | 2011-07-08 | 2013-01-10 | Microsoft Corporation | Image re-rank based on image annotations |
US20130024449A1 (en) * | 2011-07-20 | 2013-01-24 | Ness Computing, Inc. | Method and apparatus for allowing users to augment searches |
US20130110822A1 (en) * | 2011-10-26 | 2013-05-02 | Google Inc. | Indicating Location Status |
US20130226915A1 (en) * | 2012-02-29 | 2013-08-29 | Inrix, Inc. | Organization of search results based upon availability of respective providers comprised therein |
US8995716B1 (en) * | 2012-07-12 | 2015-03-31 | Google Inc. | Image search results by seasonal time period |
US20140297575A1 (en) * | 2013-04-01 | 2014-10-02 | Google Inc. | Navigating through geolocated imagery spanning space and time |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230044871A1 (en) * | 2020-12-29 | 2023-02-09 | Google Llc | Search Results With Result-Relevant Highlighting |
Also Published As
Publication number | Publication date |
---|---|
US10331733B2 (en) | 2019-06-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9672223B2 (en) | Geo photo searching based on current conditions at a location | |
AU2014248420B2 (en) | Navigating through geolocated imagery spanning space and time | |
US9286545B1 (en) | System and method of using images to determine correspondence between locations | |
US8164599B1 (en) | Systems and methods for collecting and providing map images | |
US8532916B1 (en) | Switching between best views of a place | |
US8792676B1 (en) | Inferring locations from an image | |
US8405501B2 (en) | Creating and monitoring alerts for a geographical area | |
US8533187B2 (en) | Augmentation of place ranking using 3D model activity in an area | |
US9258373B2 (en) | System and method for generating three-dimensional geofeeds, orientation-based geofeeds, and geofeeds based on ambient conditions based on content provided by social media content providers | |
US10191635B1 (en) | System and method of generating a view for a point of interest | |
US20150116360A1 (en) | Generating A Viewpoint On a Digital Map For a Point of Interest | |
US9600932B2 (en) | Three dimensional navigation among photos | |
US9195987B2 (en) | Systems and methods of correlating business information to determine spam, closed businesses, and ranking signals | |
US10331733B2 (en) | System and method for presenting condition-specific geographic imagery | |
US20150379040A1 (en) | Generating automated tours of geographic-location related features |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:RAPOPORT, EVAN;PACK, JEREMY BRYANT;SIGNING DATES FROM 20130415 TO 20130425;REEL/FRAME:042755/0247 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |