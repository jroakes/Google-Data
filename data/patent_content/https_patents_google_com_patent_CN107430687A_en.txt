CN107430687A - The segmentation of the time based on entity of video flowing - Google Patents
The segmentation of the time based on entity of video flowing Download PDFInfo
- Publication number
- CN107430687A CN107430687A CN201680019489.4A CN201680019489A CN107430687A CN 107430687 A CN107430687 A CN 107430687A CN 201680019489 A CN201680019489 A CN 201680019489A CN 107430687 A CN107430687 A CN 107430687A
- Authority
- CN
- China
- Prior art keywords
- entity
- video
- video frame
- fragment
- sample video
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000011218 segmentation Effects 0.000 title claims abstract description 61
- 239000012634 fragment Substances 0.000 claims abstract description 151
- 238000000034 method Methods 0.000 claims abstract description 28
- 230000000977 initiatory effect Effects 0.000 claims description 23
- 238000004590 computer program Methods 0.000 claims description 12
- 238000012549 training Methods 0.000 claims description 9
- 238000003062 neural network model Methods 0.000 claims description 7
- 238000003860 storage Methods 0.000 claims description 4
- 238000004458 analytical method Methods 0.000 abstract description 6
- 238000001514 detection method Methods 0.000 description 27
- 241000282326 Felis catus Species 0.000 description 15
- 230000006870 function Effects 0.000 description 14
- 230000001174 ascending effect Effects 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 230000004044 response Effects 0.000 description 5
- 230000000007 visual effect Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 241000009328 Perro Species 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000005012 migration Effects 0.000 description 2
- 238000013508 migration Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000005520 cutting process Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 238000005315 distribution function Methods 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 230000008707 rearrangement Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/85—Assembly of content; Generation of multimedia applications
- H04N21/854—Content authoring
- H04N21/8547—Content authoring involving timestamps for synchronizing content
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/91—Television signal processing therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2411—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on the proximity to a decision surface, e.g. support vector machines
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/764—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using classification, e.g. of video objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/49—Segmenting video sequences, i.e. computational techniques such as parsing or cutting the sequence, low-level clustering or determining units such as shots or scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/19—Recognition using electronic means
- G06V30/192—Recognition using electronic means using simultaneous comparisons or correlations of the image signals with a plurality of references
- G06V30/194—References adjustable by an adaptive method, e.g. learning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
Abstract
A kind of analysis of entity based on to being identified in the frame of video of video is provided come the solution of segmentation video on the time.Video is decoded as multiple frame of video, and selects multiple frame of video to be annotated.The entity that annotation procedure identification is present in Sample video frame, and the entity each identified has timestamp and indicates the confidence score for the possibility that the entity is accurately identified.For each identified entity, generate the time series being made up of timestamp and corresponding confidence score and it is carried out smoothly to annotate noise to reduce.The piece section boundary in time series by detecting entity includes one or more fragments of entity to obtain in the length of video.Split according to the individual time of the entity each identified in video, generate the semanteme of the overall time segmentation, wherein overall time segmentation reflecting video of video.
Description
Background technology
Described embodiment relates generally to Video processing, and more particularly, to video flowing based on entity
Time is split.
Pass through such as YOUTUBE trusteeship service shared video generation that there are various different contents and encode in different formats
Growing demand of the table to effectively tissue, index and management.Most of existing solutions for video tour and retrieval
Scheme is to be divided into camera lens in time based on camera lens, wherein video flowing.The camera lens of video flowing is shot from a camera
Video flowing uninterrupted sequence of frames of video；The two times upper adjacent fragment split using the time based on camera lens is in vision
On be different.
Many multimedia application be present, it points to the semanteme of video scene, rather than the time being directed towards between camera lens
Vision difference.A challenge in time segmentation based on camera lens is the high-level semantics by primitive rudimentary video data and video flowing
Field is associated (for example, finding the appropriate expression of the semantic vision content of reflecting video).To fly and land towards runway
Aircraft cinestrip exemplified by, on semantic level, cinestrip includes two scenes：One description aircraft flight, it is another
It is individual on aircraft landing.If the transition between two scenes is smooth, the segmentation based on camera lens can not differentiate between two
Scene.
The content of the invention
Described method, system and computer program product is provided based on the reality to being identified in the frame of video of video
The analysis of body carrys out on the time to split the solution of video.
One embodiment includes a kind of computer implemented method for splitting video on the time.This method includes regarding
The step of frequency is decoded as multiple frame of video.Multiple frame of video are selected to be annotated.Annotation procedure identification is present in Sample video frame
In entity, and the entity each identified has timestamp and indicates the confidence for the possibility that the entity is accurately identified
Spend score.For each identified entity, the time sequence for generating and being smoothly made up of timestamp and corresponding confidence score
Arrange to reduce annotation noise.The piece section boundary in time series by detecting entity includes to obtain in the length of video
One or more fragments of entity.Split from the individual time of the entity each identified in video, generate the totality of video
Time is split, wherein the semanteme of overall time segmentation reflecting video.
Feature and advantage described in this specification do not include entirely, especially, for ordinary skill people
Member for many supplementary features and advantage will due to accompanying drawing, specification and claims and it is apparent.In addition, it should note
Meaning, language as used in this specification is primarily to readable select with purpose is instructed, without being selected to describe
Or the theme that limitation is disclosed.
Brief description of the drawings
Fig. 1 be show be according to the video trusteeship service of the time segmentation module having based on entity of one embodiment
The block diagram of system view.
Fig. 2 is the example with the dog being branded as and the frame of video of the corresponding annotation of dog and cap.
Fig. 3 is the block diagram for showing the segmentation module according to one embodiment.
Fig. 4 shows corresponding at the various time instances of the time series of the entity identified in video and entity in video
Confidence score example.
Fig. 5 is the example of the time series application smooth function of the entity to being identified in video.
Fig. 6 is the example for the segment boundaries for detecting the entity identified in video.
Fig. 7 A are to generate the totality of video according to the individual segmentation based on the entity identified in video of one embodiment
The example of segmentation.
Fig. 7 B are the overall segmentations generated after the individual segmentation sequence to the entity of identification with the video shown in Fig. 7 A
Corresponding example.
Fig. 8 is the flow chart split according to the time based on entity of one embodiment.
Accompanying drawing depicts various embodiments of the present invention only for the purpose of illustration, and the invention is not restricted to these to show
The embodiment gone out.Those skilled in the art will be readily appreciated that from following discussion, not depart from invention as described herein
In the case of principle, the alternative embodiment of structures and methods shown in this article can be used.
Specific embodiment
I.System survey
Fig. 1 is to show the video trusteeship service with module 102 is split according to the time based on entity of one embodiment
The block diagram of 100 system view.Multiple user/beholders are carried using client 110A-N to use by video trusteeship service 100
The service (for example, video is uploaded and retrieved from video trustship website) of confession, and receive what is asked from video trusteeship service 100
Service.Video trusteeship service 100 communicates via network 130 with one or more client 110A-N.Video trusteeship service 100 from
Client 110A-N receives the video trusteeship service request to video, splits module 102 to video by the time based on entity
Split and indexed, and asked video is returned to client 110A-N.
In one embodiment, user asks video trusteeship service using client 110.For example, user uses client
110 are held to send the request of the video of index or storage upload.Client 110 can be such as personal computer (for example, desk-top
Computer, notebook, laptop computer) any kind of computer installation and such as mobile phone, individual
Digital assistants, enable IP video player device.Client 110 generally includes processor, display device (or is output to
Display device), client 110 store execution task in data used in user local storage (for example, hard drive
Device or flash memory device) and it is coupled to via network 130 network interface of video trusteeship service 100.Client 110 also has
For playing the video player of video flowing.
Network 130 to be communicated between client computer 110 and video trusteeship service 100.In one embodiment,
Network 130 is internet, and enables client 110 and video trusteeship service using currently known or subsequent development
The standardised networks interconnected communication technology and agreement of 100 communications.
Video trusteeship service 100 includes time segmentation module 102, video server 104 and video data based on entity
Storehouse 106.Video server 104 is the Video service from video database 106 in response to user video trusteeship service request.Depending on
Frequency database 106 stores the video of user's upload, splits module from the video of internet collection and by the time based on entity
The video of 102 segmentations.In one embodiment, video database 106 is stored for the big of the time segmentation module 102 based on entity
Type video corpus is to train annotation model.
Time segmentation module 102 based on entity is real based on one or more of frame of video to being present in input video
Input video is divided into multiple time semantic segments by the analysis of body.Entity in frame of video represents semantically having for frame of video
The space-time region of meaning.For example, the frame for the video that cat and dog are played together can include dog or cat or dog and cat two
Person, wherein dog and/or cat are the entities of frame of video.For the semanteme of fragment, two times upper adjacent language of input video
Adopted fragment includes different scenes (for example, dog scene is to cat scene).
In one embodiment, the time segmentation 102 based on entity has decoder module 140, annotations module 150 and segmentation
Module 300.Decoder module 140 decodes input video, and the video decoded has multiple frame of video.According to determining for implementer
Fixed, decoder module 140 can use any decoding scheme known to persons of ordinary skill in the art.In one embodiment, solve
Code module 140 is by performing the inverse of each stage of corresponding cataloged procedure for encoding input video according to video compression standard
Process decodes input video, and above-mentioned inverse process includes inverse transformation (discrete cosine transform or the small echo of incoming video signal
Conversion), re-quantization and inverse entropy code.
Annotations module 150 selects multiple frame of video from the video of decoding and frame of video to each selection annotates.
In one embodiment, annotations module 150 be based on timing information (for example, input video every five seconds for example select a frame of video) or
Position (for example, according to the display order of the frame of video of decoding select the every ten frame of video) selects frame of video.In order to institute
The frame of video of selection is annotated, and annotations module 150 identifies the entity in selected frame of video and the reality each to identify
Body assigns confidence score.In one embodiment, housebroken annotation model is applied to input video by annotations module 150
Each frame of video, and generate description each identify entity annotation parameter sets (for example, class label, comprising what is identified
The bounding box and confidence score of entity).The class label of the entity identified describes entity (for example, real in a manner of people is readable
The descriptive text of body).Bounding box comprising the entity identified defines the area in the frame of video comprising the entity identified
Domain.Bounding box is defined by its size and width and the coordinate of one angle pixel.The confidence score associated with entity
The possibility that the entity is accurately identified is indicated, for example, it is dog that the dog identified in frame of video, which has 90% probability,.In frame of video
In there is high confidence score entity more likely there are in the frame of video, rather than be contained therein same entity and have
In another frame of video of relatively low confidence score.
In one embodiment, annotations module 150 trains framework to train note using the annotation of such as DisBelief frameworks
Model is released, the training framework trains depth in a distributed fashion using the video being stored in video database 106 using iteratively faster
Layer neural network model.For example, annotations module 150 on the computing cluster with thousands of machines for 16,000,000 images and
The data set of 21000 classifications is trained using asynchronous stochastic gradient descent process and various distributed batch processing optimization process
Annotation model.Annotations module 150 extracts visual signature from training image, learns the invariant features in extracted visual signature,
And build training pattern from the study of visual signature.The other embodiments of annotations module 150 can use other machine learning
Technology trains annotation model.
Fig. 2 is the example with wear a hat 220 dog 220 and the frame of video 810 of the corresponding annotation of dog and cap.
Housebroken annotation model is applied to frame of video 210 by annotations module 150.Based on the application, annotations module 150 identifies frame of video
Two entities in 210：Dog 220 and tool cap 230 brimmy.For the entity of each identification, annotations module 150 uses
Class label (for example, dog, cap and the bounding box for including the entity identified) identifies entity.Annotations module 150 is also based on
Put by the analysis of the housebroken annotation model pair visual signature associated with entity by each entity assignment identified
Confidence score (not shown).
The analysis of entity based on the identification of one or more of frame of video to input video, segmentation module 300 will be defeated
Enter Video segmentation into multiple time semantic segments.In one embodiment, each being known based on input video of module 300 is split
Time of other entity is split to generate the segmentation of the overall time of input video, and by the entity of all identifications of input video
Time segmentation combination with generate the overall time of whole input video segmentation.Segmentation is further described referring to Fig. 3 to Fig. 8
Module 300.
II.Time semantic segmentation based on entity
Fig. 3 is the block diagram for showing the segmentation module 300 according to one embodiment.The embodiment of segmentation module 300 in Fig. 3
Including entity module 310, Leveling Block 320, fragment detection module 330 and scene cut module 340.Those skilled in the art will
Recognize, the other embodiments of segmentation module 300 can have the module different from module described here and/or other moulds
Block, can distribution function in a different manner between the modules.
Entity module 310 is interacted to receive identified entity and its corresponding with splitting the annotations module 150 of module 150
Confidence score, and the reality that is each identified of the generation with corresponding confidence score in the whole length of input video
The time series of body.In one embodiment, the time series of the entity identified is expressed as S by entity module 310e, wherein
Parameter e represents the entity identified in frame of video.Time series SeIncluding a series ofRight, wherein parameter i refers to frame
Number, parameterIt is the timestamp of the i-th frame,Refer to entity e in timestampThe confidence score at place.
Referring now to Fig. 4, Fig. 4 shows the time series of the entity identified in input video with entity in input video
The example of corresponding confidence score at various time instances.Fig. 4 shows one in the whole length of input video
The time series 430 of the entity (for example, dog in the video that cat is played together with dog) of identification.Trunnion axis 410 represents time sequence
The timing information (for example, timestamp of the length of video and the frame of video of video) of row 430, when vertical axis 420 represents and be each
Between the associated confidence score (for example, 430a-420h) of entity at example.For example, time instance t1The frame at place, which has, to be put
Confidence score 430a, it represents time instance t1The frame at place has the possibility of the entity identified in frame of video.
Leveling Block 320 is removed by the time series application smooth function of the entity of each identification to input video
Potential pseudo- fragment.Because noise (for example, the motion blur as caused by camera shake when capturing input video) is based on video
Original visual feature may misidentify entity in the frame of video of video.Therefore, know in the whole length of input video
The confidence score of other entity may change very greatly due to the small change of time upper follow-up frame, and this may cause input to regard
The pseudo- fragment of frequency.
In one embodiment, Leveling Block 320 using smooth each the identified entity of moving window time sequence
Row, to generate the smooth time series of the entity each identified.Moving window is defined by size and step-length.Entity when
Between moving window selection in sequence want the confidence score of smooth entity.Leveling Block 320 is to the confidence in moving window
Degree score is averaged to generate average confidence score, and smooth confidence level of its presentation-entity in moving window obtains
Point.Window is moved to the next part of the time series of entity for next portion of smoothingtime sequence by Leveling Block 320
Confidence score in point.
Fig. 5 is the example of the time series application smooth function of the entity to being identified in video.The original of the entity identified
Beginning time series is represented by smooth and continuous curve 530.Smooth function is the moving window to being defined by its size and step-length
Confidence score in 540 carries out average average function.The smooth time series of entity is represented that it is removed by curve 550
Annotation noise in the frame of video of input video.
Fragment detection module 330 detects the fragment of the entity each identified in input video.In one embodiment, piece
Section detection module 330 is detected by detecting the piece section boundary of the entity comprising identification in the time series of the entity of identification
Edge in frame of video.Fragment detection module 330 since selected by fragment detection module 330 the very first time stamp, according to when
Between the ascending order pair of timestamp of sequence be ranked up with the associated confidence score of smooth time series of the entity identified.
Fragment detection module 330 is based on predefined starting and offset threshold come a pair of borders of the fragment in detection time sequence.Piece
The beginning of the fragment of entity of the initiation threshold instruction comprising identification of section boundary；The offset threshold instruction of the entity of identification includes
The end of the fragment of the entity of identification.Frame of video between the time instance associated with the beginning and end of fragment, which is formed, to be included
The fragment of the entity of identification.The entity of identification in the frame of video captured between corresponding time instance, which has, to be equal to or more than
The smooth confidence score of initiation threshold.
In order to determine the length of the fragment of the entity of identification, fragment detection module 330 is based on stabbing phase with two continuous times
The derivative of the confidence score of association come determine whether to start at time instance new segment or terminate current clip.In a reality
Apply in example, fragment detection module 330 such as in following formula (1) by derivative calculations be confidence score at two continuous time stamps it
Difference：
WhereinRepresentThe confidence score at place,Represent future time stampPlace
Confidence score and two timestamps are assumed to be and are evenly spaced Δ t in time.Fragment detection module 330 will be calculated
Derivative and the first derivative threshold (also referred to as " starting derivative threshold ") compared with.Exceed starting in response to the derivative calculated
Derivative threshold, fragment detection module 330 start the new segment of identified entity.
Similarly, fragment detection module 330 can be by the derivative calculated and flection threshold value (also referred to as " skew derivative
Threshold value ") it is compared.It is less than skew derivative threshold in response to the derivative calculated, fragment detection module 330 terminates working as entity
Preceding fragment.
Fig. 6 is shown based on configurable starting derivative threshold and skew derivative threshold to detect the reality of the identification in video
The example of the segment boundaries of body (for example, dog).The time series of dog entity is represented by curve 660.Time instance t1+ΔtThe reality at place
Body has corresponding confidence score b, and it is chosen as the initiation threshold for indicating the beginning 630 of the fragment of dog entity.Time instance
tjThe entity at place has corresponding confidence score c, and it is chosen as the offset threshold for indicating the end 650 of the fragment of dog entity.
Time instance t1+ΔtAnd tjBetween frame of video formed and include the fragment of dog entity.In time instance t1+ΔtAnd tjBetween capture
Each dog entity in frame of video, which has, is equal to or more than initiation threshold (that is, confidence score b) confidence score.
Assuming that t1And t1+ΔtThe time instance at place is continuous, then fragment detection module 330 calculates according to formula (1) above
t1And t1+ΔtBetween confidence score derivative.Fragment detection module 330 is by the derivative calculated and predetermined start derivative threshold
Value is compared.In the example of fig. 6, t1And t1+ΔtBetween the derivative of confidence score exceed predetermined start derivative threshold.
Fragment detection module 330 determines the new segment of dog entity in time instance t1+ΔtPlace starts.
Similarly, fragment detection module 330 calculates t according to formula (1) abovejAnd tj+ΔtBetween confidence score lead
Number, and by the derivative calculated compared with predetermined migration derivative threshold.In the example of fig. 6, tjAnd tj+ΔtBetween
The derivative of confidence score is less than predetermined migration derivative threshold.Fragment detection module 330 determines that the fragment of dog entity is real in the time
Example tjTerminate at place.
It should be noted that starting derivative threshold and skew derivative threshold are configurable.In one embodiment, fragment detects
Module 330 is tested to select to originate derivative threshold using the video for the selection being stored in video database 106 based on Video segmentation
Value and skew derivative threshold, wherein the video selected has known segmentation information and represents to lead for deriving starting and skew
The True Data of number threshold value.In another embodiment, ascending order of the entity fragment detection module 330 based on confidence score is just led
Several selected percentiles come select originate derivative threshold；Negative the leading of descending of the fragment detection module 330 based on confidence score
Several selected percentiles come select offset derivative threshold.
In order to further show the starting based on percentile/skew derivative threshold selection, it is assumed that the time series of entity
Sequence derivative it is as follows：
{ -0.9, -0.6, -0.5, -0.3, -0.1,0,0,0,0,0.1,0.2,0.3,0.3,0.5 },
The wherein positive derivative of ascending order is { 0,0,0,0,0.1,0.2,0.3,0.3,0.5 }, the negative derivative of descending for -0.1, -
0.3, -0.5, -0.6-0.9 }, fragment detection module 330 selects 0.3 percentile of the positive derivative of ascending order as initiation threshold
And 0.3 percentile of the negative derivative of descending is selected as offset threshold.0.3 percentile of the positive derivative of ascending order will
Starting derivative threshold is set as 0.2, and 0.3 percentile of the negative derivative of descending is set as -0.3 by derivative threshold is offset.
Originate the beginning of the fragment of derivative threshold instruction entity, the end of the fragment of skew derivative threshold instruction entity.
In another embodiment, such as following formula (2) of fragment detection module 330 calculates the confidence level between two continuous time stamps
The reduction percentage of score：
The selection of fragment detection module 230 reduces the threshold value of percentage, and the Percentage_Reduction that will be calculated
Compared with selected threshold value.It is less than selected threshold value, piece in response to the Percentage_Reduction calculated
Section detection module 230 is in timestampLocate end fragment.
In order to prevent causing unnecessary segmentation due to the frame of video that minority is lost in the cutting procedure to entity, piece
Section detection module 330 merges fragment close on the time during the gentle phase.According to the feature of the content of such as input video, can
With the various factors of computing resource (for example, quantity of computer processor), the gentle phase can be continued for some time (for example, five
Second).During the gentle phase, even if meeting the condition for indicating the end of above-mentioned fragment, the fragment of entity is still allowed to continue.
Input video generally has many frame of video and continued for some time.Each frame of video can include in the video frame
More than one entity.Above embodiments describe the entity generation individual segmentation for each identifying.The base of scene cut module 340
The overall segmentation of whole input video is generated in the individual segmentation of each identified entity.The overall segmentation bag of input video
Include one or more time semantic segments of each set with entity；Any two have the collection of different entities adjacent to fragment
Close.
In one embodiment, the scene cut mould that module 300 has the overall segmentation for being used to generate input video is split
Block 340.Scene cut module 340 obtains the individual segmentation of the entity of each identification of input video from fragment detection module 330,
And it is ranked up according to individual segmentation of the timestamp associated with individual segmentation to the entity of identification.From the individual point of sequence
Cut, scene cut module 340 records the beginning and end associated with individual segmentation and generates the fragment for including different entities.
Referring now to Fig. 7, Fig. 7 be according to individual segmentation of the one embodiment based on the entity identified in input video come
Generate the example of the overall segmentation of input video.Example in Fig. 7 has the four individual pieces generated by segmentation detection module 230
Section：Time instance t1With time instance t3Between dog entity fragment；Time instance t5With time instance t7Between another dog
The fragment of entity；Time instance t2With time instance t4Between cat entity fragment；Time instance t6With time instance t8Between
Another cat entity fragment.
As shown in fig. 7, scene cut module 340 is stabbed come to dog according to the start and end time associated with individual fragment
The individual fragment of entity and cat entity is ranked up.Scene cut module 340 records 4 time started stamp (that is, time instances
t1、t2、t5And t6The timestamp at place) and 4 ending time stamp (that is, time instance t3、t4、t7And t8The timestamp at place).Scene
Segmentation module 340 is stabbed according to the start and end time of sequence to be used for combine the individual fragment of dog entity and cat entity with generating
The new segment of input video.For example, the timestamp instruction following six new segment of the sequence of individual fragment：
Timestamp t1And t2Between fragment, it is the fragment of only dog；
Timestamp t2And t3Between fragment, it is cat and dog fragment；
Timestamp t3And t4Between fragment, it is the fragment of only cat；
Timestamp t5And t6Between fragment, it is the fragment of only dog；
Timestamp t6And t7Between fragment, it is cat and dog fragment；
Timestamp t7And t8Between fragment, it is the fragment of only cat.
Scene cut module 340 can further be ranked up and delete to new segment and include and another entity identical
The fragment of entity sets.For example, timestamp t1And t2Between fragment and timestamp t5And t6Between fragment be only dog
Fragment.Scene cut module 340 can select one in the two fragments (for example, timestamp t5And t6Between fragment) come
Represent the fragment of the only dog of input video.Similarly, scene cut module 340 can stab t with selection time7And t8Between piece
Section represents the fragment of only cat.After further sequence, the input that the generation of scene cut module 340 includes three fragments regards
The overall segmentation of frequency：Only the fragment of dog, only cat fragment and cat and dog fragment.Fig. 7 B show that the input after sequence regards
The example of the overall segmentation of frequency.
In another embodiment, scene cut module 340 can be further according to the confidence score associated with entity
New segment is ranked up.For example, corresponding confidence score of the scene cut module 340 based on fragment is come the entity to identification
The fragment of (for example, dog) is ranked up.In response to the search inquiry to entity, scene cut module 340 can be returned and inquired about
Entity all fragments each with the confidence score more than threshold value subset, or return to the institute of inquired about entity
There is fragment.
III.The exemplary operation of time semantic segmentation based on entity
Fig. 8 is the flow chart split according to the time based on entity of one embodiment.Initially, the time based on entity point
Module 102 is cut to be decoded input video (810).The input video of decoding has multiple frame of video, and each frame of video has
One or more entities.Time segmentation module 102 based on entity selects one or more Sample video frames for segmentation
(820).For example, the time segmentation module 102 based on entity selects a frame of video from every five frame of video of input video.
For the frame of video of each selection, housebroken annotation model is applied to selected by the time segmentation module 102 based on entity
Sample video frame (830).Time segmentation module 102 based on entity is identified every based on the application of housebroken annotation model
Each entity (840) in the Sample video frame of individual selection.When the entity of each identification in the Sample video frame of selection has
Between the confidence score of possibility that is accurately identified of stamp, the label of entity and instruction entity.
Time segmentation module 102 based on entity generates the time series (850) of the entity each identified, wherein this when
Between sequence be included in input video whole length on each time instance at the entity identified and its corresponding confidence
Spend score.Time segmentation module 102 based on entity is being annotated to the time series application smoothing function of each entity with eliminating
The noise (860) generated during process.
For the entity of each identification, it is raw in the whole length of input video that the time based on entity splits module 102
Into the individual fragment of the entity comprising identification.The individual fragment of entity has starting point and end point, and it limits the length of fragment.
In one embodiment, the time segmentation module 102 based on entity is limited based on predefined starting and offset threshold to detect
A pair of borders (870) of fragment.The rearrangement and analysis of individual fragment based on the entity to identification, the time based on entity
Segmentation module 102 generates the overall segmentation of whole input video.
Including foregoing description to show the operation of preferred embodiment, the scope of the present invention is not intended to limit.The present invention
Scope be only limited by the claims that follow.From the discussion above, many changes are for those skilled in the relevant art
It will be evident that it will also be covered by the spirit and scope of the present invention.
The present invention is described in particular detail with for a possible embodiment.It will be understood by those skilled in the art that this
Invention can be put into practice in other embodiments.First, the specific name of component, the capital and small letter of term, attribute, data structure or
Any other programming or configuration aspects are not enforceable or relevant, and the mechanism for realizing the present invention or its feature can be with
With different titles, form or agreement.In addition, system can be realized such as the combination via hardware and software, or
Realized completely in hardware element.In addition, the particular division of the function between various system components described herein is only to show
Example property, rather than it is enforceable；The function of being performed by individual system assembly can be performed conversely by multiple components, and by more
The function that individual component performs can be performed conversely by single component.
Claims (according to the 19th article of modification of treaty)
1. a kind of method for splitting video on the time, methods described includes：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Using neural network model annotation model is trained on the corpus of training image；
Utilize each Sample video frame in the selected Sample video frame of housebroken annotation model annotation；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video represent the semantically meaningful space-time region of the video；
And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
2. according to the method for claim 1, wherein, annotate each Sample video frame bag in selected Sample video frame
Include：
Housebroken annotation model is applied to selected Sample video frame；
One or more be present in selected Sample video frame is identified based on the application of housebroken annotation model
Individual entity, the entity identified of the video represent the object of interest in selected Sample video frame；And
The entity each identified is represented by annotating parameter sets.
3. according to the method for claim 2, wherein, the annotation for the entity in selected Sample video frame is joined
The one of the semantic descriptive label that manifold conjunction includes describing the entity, the selected Sample video frame comprising the entity
The confidence score for the possibility that part and the instruction entity are accurately identified.
4. according to the method for claim 1, wherein, the annotation based on selected Sample video frame will be selected
Multiple fragments that Sample video frame is divided into each entity of the video include：
For each entity of the video：
The time series of the entity is generated, the time series includes the selected Sample video frame comprising the entity
The corresponding confidence score of multiple timestamps and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
5. the method according to claim 11, wherein, by the time series that is generated of the smooth function applied to the entity
Including：
Moving window is applied to the time series of the entity, the moving window is defined by size and step-length, and
Multiple confidence scores of timestamp of the moving window selection in the moving window；And
Calculate the average confidence score of the confidence score selected by the moving window.
6. according to the method for claim 4, wherein, identifying the piece section boundary of entity includes：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
7. a kind of non-transitory for the executable computer program instruction for being stored with the segmentation video on the time is computer-readable
Storage medium, the computer program instructions comprise instructions that the instruction causes computer processor when executed：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Using neural network model annotation model is trained on the corpus of training image；
Each Sample video frame in the Sample video frame is annotated using housebroken annotation model；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video represent the semantically meaningful space-time region of the video；
And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
8. computer-readable medium according to claim 7, wherein, it is every in selected Sample video frame for annotating
The computer program instructions of individual Sample video frame comprise instructions that the instruction causes the calculating when executed
Machine processor：
Housebroken annotation model is applied to selected Sample video frame；
One or more be present in selected Sample video frame is identified based on the application of housebroken annotation model
Individual entity, the entity identified of the video represent the object of interest in selected Sample video frame；And
The entity each identified is represented by annotating parameter sets.
9. computer-readable medium according to claim 8, wherein, for the entity in selected Sample video frame
The annotation parameter sets include describing the semantic descriptive label of the entity, the selected sample comprising the entity
The confidence score for the possibility that a part and the instruction entity for frame of video is accurately identified.
10. computer-readable medium according to claim 1, wherein, for the institute based on selected Sample video frame
State the computer program for the multiple fragments for annotating each entity that selected Sample video frame is divided into the video
Instruction comprises instructions that the instruction causes the computer processor when executed：
For each entity of the video：
The time series of the entity is generated, the time series includes the selected Sample video frame comprising the entity
The corresponding confidence score of multiple timestamps and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
11. computer-readable medium according to claim 10, wherein, for smooth function to be applied into the entity
The computer program instructions of the time series generated comprise instructions that the instruction causes described when executed
Computer processor：
Moving window is applied to the time series of the entity, the moving window is defined by size and step-length, and
Multiple confidence scores of timestamp of the moving window selection in the moving window；And
Calculate the average confidence score of the confidence score selected by the moving window.
12. computer-readable medium according to claim 11, wherein, for identifying described in the piece section boundary of entity
Computer program instructions comprise instructions that the instruction causes the computer processor when executed：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
13. a kind of computer system for splitting video on the time, the system includes：
Computer processor, the computer processor perform step, and the step includes：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Using neural network model annotation model is trained on the corpus of training image；
Each Sample video frame in the Sample video frame is annotated using housebroken annotation model；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video reflect the semanteme of the video；And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
14. system according to claim 13, wherein, annotate each Sample video frame in selected Sample video frame
Including：
Housebroken annotation model is applied to selected Sample video frame；
One or more be present in selected Sample video frame is identified based on the application of housebroken annotation model
Individual entity, the entity identified of the video represent the object of interest in selected Sample video frame；And
The entity each identified is represented by annotating parameter sets.
15. system according to claim 14, wherein, the annotation for the entity in selected Sample video frame
Semantic descriptive label that parameter sets include describing the entity, the selected Sample video frame comprising the entity
The confidence score for the possibility that a part and the instruction entity are accurately identified.
16. system according to claim 13, wherein, the annotation based on selected Sample video frame is by selected by
Multiple fragments of Sample video frame each entity for being divided into the video include：
For each entity of the video：
The time series of the entity is generated, the time series includes the selected Sample video frame comprising the entity
The corresponding confidence score of multiple timestamps and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
17. system according to claim 16, wherein, identifying the piece section boundary of entity includes：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
Claims (20)
1. a kind of method for splitting video on the time, methods described includes：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Annotate each Sample video frame in the Sample video frame；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video represent the semantically meaningful space-time region of the video；
And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
2. according to the method for claim 1, wherein, annotation Sample video frame includes：
Housebroken annotation model is applied to the Sample video frame；
One or more of described Sample video frame is present in fact to identify based on the application of housebroken annotation model
Body, the entity identified of the video represent the object of interest in the Sample video frame；And
The entity each identified is represented by annotating parameter sets.
3. the method according to claim 11, wherein, the annotation parameter set for the entity in the Sample video frame
Close the semantic descriptive label for including describing the entity, a part for the Sample video frame comprising the entity and
Indicate the confidence score for the possibility that the entity is accurately identified.
4. according to the method for claim 1, wherein, annotation Sample video frame also includes：
Using neural network model annotation model is trained on the corpus of training image.
5. according to the method for claim 1, wherein, the annotation based on selected Sample video frame will be selected
Multiple fragments that Sample video frame is divided into each entity of the video include：
For each entity of the video：
The time series of the entity is generated, the time series includes the multiple of the Sample video frame comprising the entity
The corresponding confidence score of timestamp and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
6. the method according to claim 11, wherein, by the time series that is generated of the smooth function applied to the entity
Including：
Moving window is applied to the time series of the entity, the moving window is defined by size and step-length, and
Multiple confidence scores of timestamp of the moving window selection in the moving window；And
Calculate the average confidence score of the confidence score selected by the moving window.
7. according to the method for claim 5, wherein, identifying the piece section boundary of entity includes：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
8. a kind of non-transitory for the executable computer program instruction for being stored with the segmentation video on the time is computer-readable
Storage medium, the computer program instructions comprise instructions that the instruction causes computer processor when executed：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Annotate each Sample video frame in the Sample video frame；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video represent the semantically meaningful space-time region of the video；
And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
9. computer-readable medium according to claim 8, wherein, for annotating the computer journey of Sample video frame
Sequence instruction comprises instructions that the instruction causes the computer processor when executed：
Housebroken annotation model is applied to the Sample video frame；
One or more of described Sample video frame is present in fact to identify based on the application of housebroken annotation model
Body, the entity identified of the video represent the object of interest in the Sample video frame；And
The entity each identified is represented by annotating parameter sets.
10. computer-readable medium according to claim 9, wherein, the institute for the entity in the Sample video frame
Stating annotation parameter sets includes describing the semantic descriptive label of the entity, the Sample video frame comprising the entity
A part and the confidence score of possibility that is accurately identified of the instruction entity.
11. computer-readable medium according to claim 8, wherein, for annotating the computer of Sample video frame
Programmed instruction also comprises instructions that the instruction causes the computer processor when executed：
Using neural network model annotation model is trained on the corpus of training image.
12. computer-readable medium according to claim 1, wherein, for the institute based on selected Sample video frame
State the computer program for the multiple fragments for annotating each entity that selected Sample video frame is divided into the video
Instruction comprises instructions that the instruction causes the computer processor when executed：
For each entity of the video：
The time series of the entity is generated, the time series includes the multiple of the Sample video frame comprising the entity
The corresponding confidence score of timestamp and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
13. computer-readable medium according to claim 12, wherein, for smooth function to be applied into the entity
The computer program instructions of the time series generated comprise instructions that the instruction causes described when executed
Computer processor：
Moving window is applied to the time series of the entity, the moving window is defined by size and step-length, and
Multiple confidence scores of timestamp of the moving window selection in the moving window；And
Calculate the average confidence score of the confidence score selected by the moving window.
14. computer-readable medium according to claim 13, wherein, for identifying described in the piece section boundary of entity
Computer program instructions comprise instructions that the instruction causes the computer processor when executed：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
15. a kind of computer system for splitting video on the time, the system includes：
Computer processor, the computer processor perform step, and the step includes：
Sample video frame is selected from the frame of video of multiple decodings of the video；
Annotate each Sample video frame in the Sample video frame；
Based on the annotation of selected Sample video frame, selected Sample video frame is divided into each of the video
Multiple fragments of entity, the fragment of the entity of the video reflect the semanteme of the video；And
The multiple fragment of each entity based on the video is split to generate the overall time of the video.
16. system according to claim 15, wherein, annotation Sample video frame includes：
Housebroken annotation model is applied to the Sample video frame；
One or more of described Sample video frame is present in fact to identify based on the application of housebroken annotation model
Body, the entity identified of the video represent the object of interest in the Sample video frame；And
The entity each identified is represented by annotating parameter sets.
17. system according to claim 16, wherein, the annotation parameter for the entity in the Sample video frame
Gather the semantic descriptive label for including describing the entity, a part for the Sample video frame comprising the entity with
And indicate the confidence score for the possibility that the entity is accurately identified.
18. system according to claim 15, wherein, annotation Sample video frame also includes：
Using neural network model annotation model is trained on the corpus of training image.
19. system according to claim 15, wherein, the annotation based on selected Sample video frame is by selected by
Multiple fragments of Sample video frame each entity for being divided into the video include：
For each entity of the video：
The time series of the entity is generated, the time series includes the multiple of the Sample video frame comprising the entity
The corresponding confidence score of timestamp and the entity；
Smooth function is applied to the time series generated of the entity；And
The confidence score of smoothed time series based on the entity identifies each fragment comprising the entity
Border.
20. system according to claim 19, wherein, identifying the piece section boundary of entity includes：
The initiation threshold of the fragment is selected, the initiation threshold indicates the beginning of the fragment；
The offset threshold of the fragment is selected, the offset threshold indicates the end of the fragment；
By the confidence score of the smoothed time series of the entity and the initiation threshold and the offset threshold
It is compared；And
The comparison of the confidence score of smoothed time series based on the entity identifies the fragment
The border.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/712,071 US9607224B2 (en) | 2015-05-14 | 2015-05-14 | Entity based temporal segmentation of video streams |
US14/712,071 | 2015-05-14 | ||
PCT/US2016/027330 WO2016182665A1 (en) | 2015-05-14 | 2016-04-13 | Entity based temporal segmentation of video streams |
Publications (3)
Publication Number | Publication Date |
---|---|
CN107430687A true CN107430687A (en) | 2017-12-01 |
CN107430687B CN107430687B (en) | 2022-03-04 |
CN107430687B9 CN107430687B9 (en) | 2022-04-08 |
Family
ID=57249260
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680019489.4A Active CN107430687B9 (en) | 2015-05-14 | 2016-04-13 | Entity-based temporal segmentation of video streams |
Country Status (8)
Country | Link |
---|---|
US (1) | US9607224B2 (en) |
EP (1) | EP3295678A4 (en) |
JP (1) | JP6445716B2 (en) |
KR (1) | KR101967086B1 (en) |
CN (1) | CN107430687B9 (en) |
DE (1) | DE112016002175T5 (en) |
GB (1) | GB2553446B8 (en) |
WO (1) | WO2016182665A1 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109145784A (en) * | 2018-08-03 | 2019-01-04 | 百度在线网络技术（北京）有限公司 | Method and apparatus for handling video |
CN109410145A (en) * | 2018-11-01 | 2019-03-01 | 北京达佳互联信息技术有限公司 | Timing smoothing method, device and electronic equipment |
WO2019196099A1 (en) * | 2018-04-09 | 2019-10-17 | 深圳大学 | Method for positioning boundaries of target object in medical image, storage medium, and terminal |
CN111898461A (en) * | 2020-07-08 | 2020-11-06 | 贵州大学 | Time sequence behavior segment generation method |
CN114342353A (en) * | 2019-09-10 | 2022-04-12 | 华为技术有限公司 | Method and system for video segmentation |
CN114550300A (en) * | 2022-02-25 | 2022-05-27 | 北京百度网讯科技有限公司 | Video data analysis method and device, electronic equipment and computer storage medium |
CN117095317A (en) * | 2023-10-19 | 2023-11-21 | 深圳市森歌数据技术有限公司 | Unmanned aerial vehicle three-dimensional image entity identification and time positioning method |
Families Citing this family (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10051344B2 (en) * | 2016-09-27 | 2018-08-14 | Clarifai, Inc. | Prediction model training via live stream concept association |
CN108510982B (en) * | 2017-09-06 | 2020-03-17 | 腾讯科技（深圳）有限公司 | Audio event detection method and device and computer readable storage medium |
DE102017124600A1 (en) * | 2017-10-20 | 2019-04-25 | Connaught Electronics Ltd. | Semantic segmentation of an object in an image |
US10417501B2 (en) | 2017-12-06 | 2019-09-17 | International Business Machines Corporation | Object recognition in video |
EP3621021A1 (en) | 2018-09-07 | 2020-03-11 | Delta Electronics, Inc. | Data search method and data search system thereof |
CN111480166B (en) * | 2018-12-05 | 2023-05-05 | 北京百度网讯科技有限公司 | Method and device for locating target video clips from video |
CN110602527B (en) * | 2019-09-12 | 2022-04-08 | 北京小米移动软件有限公司 | Video processing method, device and storage medium |
CN110704681B (en) | 2019-09-26 | 2023-03-24 | 三星电子（中国）研发中心 | Method and system for generating video |
CN110933462B (en) * | 2019-10-14 | 2022-03-25 | 咪咕文化科技有限公司 | Video processing method, system, electronic device and storage medium |
CN110958489A (en) * | 2019-12-11 | 2020-04-03 | 腾讯科技（深圳）有限公司 | Video processing method, video processing device, electronic equipment and computer-readable storage medium |
CN114025216B (en) * | 2020-04-30 | 2023-11-17 | 网易（杭州）网络有限公司 | Media material processing method, device, server and storage medium |
CN111738173B (en) * | 2020-06-24 | 2023-07-25 | 北京奇艺世纪科技有限公司 | Video clip detection method and device, electronic equipment and storage medium |
KR20220090158A (en) * | 2020-12-22 | 2022-06-29 | 삼성전자주식회사 | Electronic device for editing video using objects of interest and operating method thereof |
US11935253B2 (en) | 2021-08-31 | 2024-03-19 | Dspace Gmbh | Method and system for splitting visual sensor data |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010005430A1 (en) * | 1997-07-29 | 2001-06-28 | James Warnick | Uniform intensity temporal segments |
EP1132835A1 (en) * | 2000-03-08 | 2001-09-12 | Lg Electronics Inc. | Method of generating synthetic key frame and video browsing system using the same |
CN1945628A (en) * | 2006-10-20 | 2007-04-11 | 北京交通大学 | Video frequency content expressing method based on space-time remarkable unit |
US7559017B2 (en) * | 2006-12-22 | 2009-07-07 | Google Inc. | Annotation framework for video |
CN101527043A (en) * | 2009-03-16 | 2009-09-09 | 江苏银河电子股份有限公司 | Video picture segmentation method based on moving target outline information |
CN101789124A (en) * | 2010-02-02 | 2010-07-28 | 浙江大学 | Segmentation method for space-time consistency of video sequence of parameter and depth information of known video camera |
CN102160084A (en) * | 2008-03-06 | 2011-08-17 | 阿明·梅尔勒 | Automated process for segmenting and classifying video objects and auctioning rights to interactive video objects |
CN102663015A (en) * | 2012-03-21 | 2012-09-12 | 上海大学 | Video semantic labeling method based on characteristics bag models and supervised learning |
Family Cites Families (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH07175816A (en) * | 1993-10-25 | 1995-07-14 | Hitachi Ltd | Video associative retrieving device and method |
AU1468500A (en) * | 1998-11-06 | 2000-05-29 | Trustees Of Columbia University In The City Of New York, The | Systems and methods for interoperable multimedia content descriptions |
JP4404172B2 (en) * | 1999-09-02 | 2010-01-27 | 株式会社日立製作所 | Media scene information display editing apparatus, method, and storage medium storing program according to the method |
US7042525B1 (en) * | 2000-07-06 | 2006-05-09 | Matsushita Electric Industrial Co., Ltd. | Video indexing and image retrieval system |
JP4192703B2 (en) * | 2003-06-30 | 2008-12-10 | 日本電気株式会社 | Content processing apparatus, content processing method, and program |
GB0406512D0 (en) * | 2004-03-23 | 2004-04-28 | British Telecomm | Method and system for semantically segmenting scenes of a video sequence |
US7551234B2 (en) * | 2005-07-28 | 2009-06-23 | Seiko Epson Corporation | Method and apparatus for estimating shot boundaries in a digital video sequence |
US7555149B2 (en) * | 2005-10-25 | 2009-06-30 | Mitsubishi Electric Research Laboratories, Inc. | Method and system for segmenting videos using face detection |
EP1959449A1 (en) | 2007-02-13 | 2008-08-20 | British Telecommunications Public Limited Company | Analysing video material |
DE102007028175A1 (en) * | 2007-06-20 | 2009-01-02 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | Automated method for temporal segmentation of a video into scenes taking into account different types of transitions between image sequences |
US8170342B2 (en) * | 2007-11-07 | 2012-05-01 | Microsoft Corporation | Image recognition of content |
US20090278937A1 (en) * | 2008-04-22 | 2009-11-12 | Universitat Stuttgart | Video data processing |
JP2012038239A (en) * | 2010-08-11 | 2012-02-23 | Sony Corp | Information processing equipment, information processing method and program |
CN102402536A (en) * | 2010-09-13 | 2012-04-04 | 索尼公司 | Method and equipment for extracting key frame from video |
US9118886B2 (en) * | 2012-07-18 | 2015-08-25 | Hulu, LLC | Annotating general objects in video |
US20140181668A1 (en) * | 2012-12-20 | 2014-06-26 | International Business Machines Corporation | Visual summarization of video for quick understanding |
US10482777B2 (en) * | 2013-02-22 | 2019-11-19 | Fuji Xerox Co., Ltd. | Systems and methods for content analysis to support navigation and annotation in expository videos |
US9154761B2 (en) * | 2013-08-19 | 2015-10-06 | Google Inc. | Content-based video segmentation |
BR112016006860B8 (en) * | 2013-09-13 | 2023-01-10 | Arris Entpr Inc | APPARATUS AND METHOD FOR CREATING A SINGLE DATA STREAM OF COMBINED INFORMATION FOR RENDERING ON A CUSTOMER COMPUTING DEVICE |
KR101507272B1 (en) * | 2014-02-12 | 2015-03-31 | 인하대학교 산학협력단 | Interface and method for semantic annotation system for moving objects in the interactive video |
US10664687B2 (en) * | 2014-06-12 | 2020-05-26 | Microsoft Technology Licensing, Llc | Rule-based video importance analysis |
US9805268B2 (en) * | 2014-07-14 | 2017-10-31 | Carnegie Mellon University | System and method for processing a video stream to extract highlights |
JP2016103714A (en) * | 2014-11-27 | 2016-06-02 | 三星電子株式会社Ｓａｍｓｕｎｇ Ｅｌｅｃｔｒｏｎｉｃｓ Ｃｏ．，Ｌｔｄ． | Video recording and reproducing device |
-
2015
- 2015-05-14 US US14/712,071 patent/US9607224B2/en active Active
-
2016
- 2016-04-13 JP JP2017551249A patent/JP6445716B2/en active Active
- 2016-04-13 EP EP16793129.4A patent/EP3295678A4/en not_active Ceased
- 2016-04-13 GB GB1715780.1A patent/GB2553446B8/en active Active
- 2016-04-13 WO PCT/US2016/027330 patent/WO2016182665A1/en active Application Filing
- 2016-04-13 CN CN201680019489.4A patent/CN107430687B9/en active Active
- 2016-04-13 DE DE112016002175.5T patent/DE112016002175T5/en active Pending
- 2016-04-13 KR KR1020177028040A patent/KR101967086B1/en active IP Right Grant
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010005430A1 (en) * | 1997-07-29 | 2001-06-28 | James Warnick | Uniform intensity temporal segments |
EP1132835A1 (en) * | 2000-03-08 | 2001-09-12 | Lg Electronics Inc. | Method of generating synthetic key frame and video browsing system using the same |
CN1945628A (en) * | 2006-10-20 | 2007-04-11 | 北京交通大学 | Video frequency content expressing method based on space-time remarkable unit |
US7559017B2 (en) * | 2006-12-22 | 2009-07-07 | Google Inc. | Annotation framework for video |
CN102160084A (en) * | 2008-03-06 | 2011-08-17 | 阿明·梅尔勒 | Automated process for segmenting and classifying video objects and auctioning rights to interactive video objects |
CN101527043A (en) * | 2009-03-16 | 2009-09-09 | 江苏银河电子股份有限公司 | Video picture segmentation method based on moving target outline information |
CN101789124A (en) * | 2010-02-02 | 2010-07-28 | 浙江大学 | Segmentation method for space-time consistency of video sequence of parameter and depth information of known video camera |
CN102663015A (en) * | 2012-03-21 | 2012-09-12 | 上海大学 | Video semantic labeling method based on characteristics bag models and supervised learning |
Non-Patent Citations (5)
Title |
---|
ALPESH DABHI ET AL: "《A Neural Network Model for Automatic Image Annotation and Annotation Refinement: A survey》", 《INTERNATIONAL JOURNAL OF ENGINEERING DEVELOPMENT AND RESEARCH》 * |
CHING-YUNG LIN ET AL；: "《VideoAL: A Novel End-to-End MPEG-7 Video Automatic Labeling System》", 《IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING》 * |
RADIM BURGET ET AL: "《Supervised Video Scene Segmentation using Similarity Measures》", 《IEEE》 * |
SHAOFEI WU ET AL: "《Study on a New Video Scene Segmentation Algorithm》", 《APPLIED MATHEMATICS & INFORMATION SCIENCES》 * |
STEPHAN BLOEHDORN ET AL；: "《Semantic Annotation of Images and Videos for Multimedia Analysis》", 《SPRINGER-VERLAG BERLIN HEIDELBERG》 * |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019196099A1 (en) * | 2018-04-09 | 2019-10-17 | 深圳大学 | Method for positioning boundaries of target object in medical image, storage medium, and terminal |
CN109145784A (en) * | 2018-08-03 | 2019-01-04 | 百度在线网络技术（北京）有限公司 | Method and apparatus for handling video |
CN109410145A (en) * | 2018-11-01 | 2019-03-01 | 北京达佳互联信息技术有限公司 | Timing smoothing method, device and electronic equipment |
CN109410145B (en) * | 2018-11-01 | 2020-12-18 | 北京达佳互联信息技术有限公司 | Time sequence smoothing method and device and electronic equipment |
CN114342353A (en) * | 2019-09-10 | 2022-04-12 | 华为技术有限公司 | Method and system for video segmentation |
CN111898461A (en) * | 2020-07-08 | 2020-11-06 | 贵州大学 | Time sequence behavior segment generation method |
CN111898461B (en) * | 2020-07-08 | 2022-08-30 | 贵州大学 | Time sequence behavior segment generation method |
CN114550300A (en) * | 2022-02-25 | 2022-05-27 | 北京百度网讯科技有限公司 | Video data analysis method and device, electronic equipment and computer storage medium |
CN117095317A (en) * | 2023-10-19 | 2023-11-21 | 深圳市森歌数据技术有限公司 | Unmanned aerial vehicle three-dimensional image entity identification and time positioning method |
Also Published As
Publication number | Publication date |
---|---|
WO2016182665A1 (en) | 2016-11-17 |
GB2553446B (en) | 2021-08-04 |
GB2553446A (en) | 2018-03-07 |
EP3295678A4 (en) | 2019-01-30 |
JP6445716B2 (en) | 2018-12-26 |
KR101967086B1 (en) | 2019-04-08 |
CN107430687B (en) | 2022-03-04 |
KR20170128771A (en) | 2017-11-23 |
CN107430687B9 (en) | 2022-04-08 |
US9607224B2 (en) | 2017-03-28 |
DE112016002175T5 (en) | 2018-01-25 |
US20160335499A1 (en) | 2016-11-17 |
EP3295678A1 (en) | 2018-03-21 |
GB201715780D0 (en) | 2017-11-15 |
GB2553446B8 (en) | 2021-12-08 |
JP2018515006A (en) | 2018-06-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107430687A (en) | The segmentation of the time based on entity of video flowing | |
US10566009B1 (en) | Audio classifier | |
CN109977262B (en) | Method and device for acquiring candidate segments from video and processing equipment | |
US20220027634A1 (en) | Video processing method, electronic device and storage medium | |
US9727584B2 (en) | Refining image annotations | |
US9047376B2 (en) | Augmenting video with facial recognition | |
US10104345B2 (en) | Data-enhanced video viewing system and methods for computer vision processing | |
WO2017011745A1 (en) | Apparatus and methods for facial recognition and video analytics to identify individuals in contextual video streams | |
CN114342353A (en) | Method and system for video segmentation | |
WO2023011094A1 (en) | Video editing method and apparatus, electronic device, and storage medium | |
KR102156440B1 (en) | Apparatus and method for generating visual annotation based on visual language | |
CN109408672B (en) | Article generation method, article generation device, server and storage medium | |
JP6236154B2 (en) | How to generate a video tag cloud that represents objects that appear in video content | |
JP2013196703A (en) | Object identification in images or image sequences | |
JP6787831B2 (en) | Target detection device, detection model generation device, program and method that can be learned by search results | |
US20240037142A1 (en) | Systems and methods for filtering of computer vision generated tags using natural language processing | |
CN113301382B (en) | Video processing method, device, medium, and program product | |
CN114187558A (en) | Video scene recognition method and device, computer equipment and storage medium | |
US9667886B2 (en) | Apparatus and method for editing video data according to common video content attributes | |
KR101640317B1 (en) | Apparatus and method for storing and searching image including audio and video data | |
EP2887259A1 (en) | Method for annotating an object in a multimedia asset | |
CN110163043B (en) | Face detection method, device, storage medium and electronic device | |
JP6836985B2 (en) | Programs, devices and methods for estimating the context of human behavior from captured images | |
Masneri et al. | Towards semi-automatic annotation of video and audio corpora | |
CN117061815A (en) | Video processing method, video processing device, computer readable medium and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant | ||
CI03 | Correction of invention patent | ||
CI03 | Correction of invention patent |
Correction item: ClaimsCorrect: Claims submitted on December 6, 2021False: Claims submitted on October 8, 2021Number: 09-02Page: ??Volume: 38 |