US8964546B1 - Indirect measurement of user traffic on links - Google Patents
Indirect measurement of user traffic on links Download PDFInfo
- Publication number
- US8964546B1 US8964546B1 US13/755,277 US201313755277A US8964546B1 US 8964546 B1 US8964546 B1 US 8964546B1 US 201313755277 A US201313755277 A US 201313755277A US 8964546 B1 US8964546 B1 US 8964546B1
- Authority
- US
- United States
- Prior art keywords
- link
- traffic
- endpoint
- endpoints
- processor
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/25—Flow control; Congestion control with rate being modified by the source upon detecting a change of network conditions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L43/00—Arrangements for monitoring or testing data switching networks
- H04L43/08—Monitoring or testing based on specific metrics, e.g. QoS, energy consumption or environmental parameters
- H04L43/0876—Network utilisation, e.g. volume of load or congestion level
- H04L43/0882—Utilisation of link capacity
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L43/00—Arrangements for monitoring or testing data switching networks
- H04L43/08—Monitoring or testing based on specific metrics, e.g. QoS, energy consumption or environmental parameters
- H04L43/0876—Network utilisation, e.g. volume of load or congestion level
- H04L43/0894—Packet rate
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/24—Traffic characterised by specific attributes, e.g. priority or QoS
Definitions
- the invention relates to systems and methods for measuring the amount of traffic flow on a link.
- packets of data flow along paths in the form of traffic between endpoints. Routers located along the paths direct the data packets to other routers, until the data packets reach their destinations.
- Network links carry data packets directly between two routers, and each link is associated with a maximum flow capacity, or a maximum amount of data that the link can transfer within a unit of time.
- traffic on a link is congested, meaning traffic is near or exceeds the flow capacity of the link, delays and possible loss of data packets occur.
- the invention relates to a system that measures the amount of traffic flow on a link.
- the system includes a processor that is configured to generate aggregate egress data for multiple endpoints by aggregating amounts of egress traffic detected by servers processing the egress traffic at the endpoints.
- the processor is also configured to obtain the proportions of traffic flows that traverse each path between each pair of endpoints. Each path includes multiple network links such that a single network link may be shared across multiple paths.
- the processor computes an amount of traffic flow on a shared link based on the aggregate egress data and the obtained proportions.
- the invention relates to a method for measuring the amount of traffic flow on a link.
- the method comprises generating aggregate egress data for multiple endpoints by aggregating amounts of egress traffic detected by servers processing the egress traffic at the endpoints. Then the proportions of traffic flows that traverse each path between each pair of endpoints are obtained. Finally, the amount of traffic flow on a shared link is computed based on the aggregate egress data and the obtained proportions.
- FIGS. 1A and 1B are diagrams of a system for measuring the amount of traffic flow that traverses a link in a network, according to an illustrative embodiment of the invention.
- FIG. 2 is a simulated screenshot 200 of a user interface that displays the amount of traffic on a link, according to an illustrative embodiment of the invention.
- FIG. 3 is a flowchart of a method for measuring the amount of traffic flow that traverses a link in a network, according to an illustrative embodiment of the invention.
- FIG. 4 is a flowchart of a method for adjusting the traffic on the paths crossing a link if the measured amount of traffic flow exceeds a threshold, according to an illustrative embodiment of the invention.
- FIG. 1A is a diagram of a network 100 in which data traverses paths between two pairs of endpoints, according to an illustrative embodiment of the invention.
- Network 100 includes eight nodes 102 a - 102 h (each generally a node 102 ), four paths 106 a - 106 d (each generally a path 106 ), and a shared link 104 .
- Two nodes 102 are origin endpoints ( 102 a and 102 b ), and one node 102 is a destination endpoint ( 102 h ).
- An endpoint pair consists of an origin endpoint and a destination endpoint.
- the remaining nodes are routers (nodes 102 c - 102 g ) or intermediate nodes.
- Path 106 a originates from origin endpoint, node 102 a , and traverses through nodes 102 c and 102 e before reaching its destination endpoint, node 102 h .
- Path 106 b originates from origin endpoint, node 102 a , and traverses through nodes 102 d and 102 f before reaching its destination endpoint, node 102 h .
- paths 106 c and 106 d originate from origin endpoint, node 102 b , traverse through nodes 102 f or 102 g , respectively, and end at their destination endpoint, node 102 h.
- Network 100 illustrates a portion of an internet backbone network, which is an infrastructure that interconnects various networks and provides possible paths for transmitting data between two endpoint pairs.
- packets of data flow in the form of traffic along the paths 106 from an origin endpoint, e.g., nodes 102 a or 102 b (where traffic is egress, or outgoing) to a destination endpoint, e.g., node 102 h (where traffic is ingress, or incoming).
- Each path consists of multiple network links, which are direct connections between two nodes: an origin endpoint and a router, two routers, or a router and a destination endpoint.
- Links may be formed over fiber optic cable connections, over wireless connections, or using any other transmission mechanism that is capable of transferring data from one node to another node. Paths may also share links. For example, in network 100 , link 104 (defined between nodes 102 f and 102 h ) is shared by paths 106 b and 106 c.
- edge servers which are caching proxy servers, meaning that they store local copies of content that is otherwise stored in a central location. If a user located far from the central location wishes to access content that is centrally stored but is located near an edge server that stores the same content as a local copy, the user would access the local copy. In this way, edge servers are used to improve bandwidth and latency to users while lessening traffic congestion at the central location.
- edge servers that receive requests for access to content that is not stored locally may be configured to check the central servers for the original content as well as other edge servers for local copies.
- a user located near an edge server at node 102 h wishes to access data that is not stored on the edge server at node 102 h , but is stored on an edge server at node 102 a .
- data packets are sent over paths 106 a or 106 b from node 102 a to node 102 h such that the user may access the desired data.
- Each packet of data is marked with the address of its destination. Before the packets reach their destination at node 102 h , the packets need to traverse multiple links connected by routers at intermediate nodes 102 c - 102 g . These routers' functions include reading the destination address of each incoming packet and forwarding the packet to another router or directly to its destination. Thus, network links carry traffic of data packets to their destinations. Each link is associated with a maximum flow capacity, or the maximum amount of data the link can transfer within a unit of time.
- FIG. 1B is a block diagram of a system 101 for measuring the amount of traffic flow that traverses a link in the network 100 of FIG. 1A , according to an illustrative embodiment of the invention.
- System 101 includes network 100 , two origin servers 108 a and 108 b , a destination server 108 c , and a computing server 110 .
- Origin servers 108 a and 108 b monitor egress traffic that originates from origin endpoints, nodes 102 a and 102 b , respectively, while destination server 108 c monitors ingress traffic that arrives at destination endpoint, node 102 h .
- All three origin and destination servers 108 send data indicative of the traffic flows to computing server 110 , which aggregates the volume data associated with the traffic for each origin and destination endpoint. Based on the aggregate data, computing server 110 computes the amount of traffic flow on the shared link 104 .
- computing server 110 computes the amount of traffic flow on the shared link 104 .
- the logic corresponding to methods described herein may be implemented in hardware, software, firmware, or a combination thereof.
- the logic When the logic is implemented in software, the logic may be stored in a computer readable medium, and the program instructions may be executed by one or more processors incorporated into the various computing devices described below.
- Origin servers 108 a and 108 b are configured to monitor the traffic flows of egress data streams at origin endpoints, nodes 102 a and 102 b , respectively.
- Each origin server 108 includes a processor 109 , memory 111 , and a network interface card 113 .
- the network interface card 113 provides an interface with network 100 such that the origin servers 108 a and 108 b can monitor egress traffic at origin endpoints, nodes 102 a and 102 b , respectively, and communicate with the network.
- Origin servers 108 a and 108 b are further configured to monitor the destinations of the egress traffic as well as the relative proportions of paths that the egress traffic traverse to reach their destinations.
- destination server 108 c is configured to monitor the traffic flows of ingress data streams at a destination endpoint, node 102 h .
- Destination server 108 c includes a processor 109 , memory 111 , and a network interface card 113 .
- the logic is implemented in software, the logic is stored in the memory 111 , and the program instructions are executed by processor 109 .
- the network interface card 113 provides an interface with network 100 such that the destination servers 108 h can monitor ingress traffic at a destination endpoint, node 102 h , and communicate with the network.
- Destination server 108 c is further configured to monitor the origins of the ingress traffic as well as the relative proportions of paths that the ingress traffic has traversed to reach its destination at node 102 h.
- Origin servers 108 a and 108 b and destination server 108 c operate independently of one another.
- Computing server 110 is configured to receive traffic flow data from origin servers 108 a and 108 b and destination server 108 c and to compute the amount of traffic flow on a shared link based on the received data.
- the received data from origin servers 108 a and 108 b includes the amount of egress traffic, the destinations of egress traffic, the paths to be taken by the egress traffic, and the relative proportions of the egress traffic that will traverse the paths.
- the received data from destination server 108 c includes the amount of ingress traffic, the origins of ingress traffic, the paths taken by the ingress traffic, and the relative proportions of the ingress traffic that traversed the paths. From the received data, the computing server 110 computes the amount of traffic flow on a link that is shared by one or more paths by summing the aggregate amount of traffic flow on each path that traverses the link.
- U AH denote the total amount of traffic flowing from an origin endpoint, node 102 a , to a destination endpoint, node 102 h
- U BH denote the total amount of traffic flowing from an origin endpoint, node 102 b , to a destination endpoint, node 102 h
- the fractions of U AH and U BH traffic that traverse each path are known. For example,
- U AH is the amount of traffic flow that traverses path 106 a .
- computing server 110 uses the following equation to determine the amount of traffic flow on a link:
- V l ⁇ e ⁇ ⁇ p ⁇ f ep ⁇ U e ( 1 )
- V l corresponds to the amount of traffic flow on the link l
- U e corresponds to the total amount of traffic flow between an endpoint pair e (consisting of an origin endpoint and a destination endpoint)
- f ep corresponds to the proportion of traffic flow that traverses the path p in the plurality of paths between the endpoint pair e that crosses the link l. That is, computing server 110 computes the total amount of traffic flow on a link to be the sum of the amount of traffic flow of each path that shares the link.
- Network 100 contains two origin endpoints, nodes 102 a and 102 b , one destination endpoint, node 102 h , and four paths connecting the origin endpoints to the destination endpoint.
- links may be bi-directional, meaning that traffic may flow in either direction such that an endpoint may be both an origin endpoint and a destination endpoint for different data streams.
- nodes 102 c - 102 g may also be origin and/or destination endpoints for other paths that are not shown in FIG. 1A .
- a single node is an origin endpoint for one set of paths and is also a destination endpoint for another set of paths.
- a single server 108 may serve as both origin and destination server for the node.
- the node may have both an origin server and a destination server.
- servers 108 send the traffic flow data directly to computing server 110 , and computing server 110 aggregates the data and performs the computation of the amount of traffic flow on a shared link. In other embodiments, servers 108 perform the aggregation themselves and send the aggregate traffic flow data to computing server 110 . In this case, computing server 110 receives aggregated data and performs the computation of the amount of traffic flow on a shared link. In some embodiments, servers 108 send the traffic flow data to one or more intermediate devices, which then forward the data to computing server 110 . In this case, the intermediate device may perform the aggregation of data.
- the traffic flow on a link is aggregated according to user or computing application. For example, some applications may be ranked according to priority, and high priority applications are those with high quality of service requirements. In order to function properly, some applications are inelastic, meaning they require a minimum bandwidth or a maximum latency. Examples of such applications include video or audio streaming, videoconferencing, or Voice over IP. In contrast, elastic applications, such as file transfer applications, use however much or little bandwidth is available. Inelastic applications may be assigned higher priority than elastic applications.
- users may also be ranked according to priority. For example, some users pay an extra fee to get higher quality of service compared to other users. This fee may be in the form of a subscription with an internet service provider or another financial relationship between a user and an internet service provider. Ranking users or applications by priority is beneficial when aggregating the traffic flow so that computing server 110 may determine the possibility of rerouting an amount of traffic flow for a low-priority user or application before considering rerouting traffic flow for users or applications with higher priority.
- an end-to-end neutral network a network that abides by the principle of network neutrality
- the tiers of priority for certain users are limited, such that internet service providers or governments are not allowed to restrict users' access to the Internet.
- endpoint pairs may also be ranked according to priority. For example, the endpoint pair corresponding to nodes 102 a and 102 h may be given a higher priority than the endpoint pair corresponding to nodes 102 b and 102 h . In this case, paths 106 a and 106 b have higher priority than paths 106 c and 106 d . Similar to rerouting schemes when applications or users are ranked according to priority, traffic between a low-priority endpoint pair may be rerouted before rerouting traffic between an endpoint pair with higher priority.
- FIG. 2 is a simulated screenshot 200 of a user interface that displays the amount of traffic on a link, according to an illustrative embodiment of the invention.
- Display 200 includes a table 220 that lists the traffic flow in Mbits/second (Mb/s) by link, a table 222 that lists the traffic flow by application for a link, and a table 224 that lists traffic flow by user for an application over a link.
- Mb/s Mbits/second
- table 220 displays a high level list of the aggregate data that traverses a link defined by an endpoint pair as shown in FIG. 1A .
- the left column of table 220 indicates the endpoint pair that defines the link. For example, 6.3 Mb/s of traffic traverses the link defined by FH, or from node 102 f to node 102 h.
- Table 222 displays the different flow rates of traffic that traverses link FH for different applications. For example, streaming video data uses 2.5 Mb/s, and Voice over IP uses 0.1 Mb/s.
- Computing server 110 aggregates ingress and/or egress data separately for different applications.
- One reason for separate aggregation by application is to prioritize certain applications differently than others. For example, as described in relation to FIG. 1B , video streaming and Voice over IP require continuous un-interrupted data streams with minimal latency.
- file transfer applications may make use of however much bandwidth is available, and web browsing applications are bursty, meaning the data rate is volatile with time, and the ratio of peak to average data rate is high. Because different applications have different requirements, it is beneficial to aggregate traffic flows separately in order to ensure that the needs of different applications are met.
- table 224 displays the different flow rates of traffic that traverses link FH for different users for an application. As shown in table 224 , four users are streaming video on link FH. Similar to applications, users may also be ranked by priority. For example, some users may pay an extra fee in order to get higher priority over other users.
- the links are bidirectional, so two results may be shown for links FH (data sent from node 102 f to node 102 h ) and HF (data sent from node 102 h to node 102 f ).
- the display 200 of the user interface is supported by a database that stores the aggregate link data by application and user.
- computing server 110 stores the data shown in the user interface of display 200 in a data structure that is stored in the memory 111 of the computing server 110 .
- computing server 110 displays this information on a monitor, while in other embodiments, computing server 110 sends the data to an external device that displays the tables.
- Display 200 shows an example of what information the computing server 110 displays in a graphical user interface once the amount of traffic on a link has been computed. It is to be understood by one of ordinary skill in the art, based on the disclosure and teachings herein, that various other tables that display information indicative of the type of traffic flow that traverse a link may also be displayed.
- FIG. 3 shows a flowchart of a method 300 for measuring the amount of traffic flow that traverses a link in a network, according to an illustrative embodiment of the invention.
- the method 300 includes the steps of generating aggregate egress traffic at each endpoint (step 330 ), generating aggregate ingress traffic at each endpoint (step 332 ), obtaining proportions of traffic flow for each set of paths between endpoint pairs (step 334 ), and computing the total amount of traffic that traverses a link (step 336 ).
- computing server 110 generates aggregate egress traffic at each origin endpoint (step 330 ). Origin servers 108 a and 108 b send the outgoing data streams to the computing server 110 , which performs the aggregation by summing the amount of outgoing data over a time interval.
- computing server 110 generates aggregate ingress traffic at each destination endpoint (step 332 ).
- Destination server 108 c sends the incoming data streams to the computing server 110 , which performs the aggregation by summing the amount of incoming data over a time interval.
- computing server 110 obtains the proportions of traffic that traverses each path from the origin endpoint to the destination endpoint (step 334 ). For a given origin-destination endpoint pair, the proportions sum to one. Origin servers 108 a and 108 b monitor the destinations of the various egress data streams as well as these proportions, and destination servers 108 c monitor the origins of the various ingress data streams as well as these proportions. In network 100 , all of the egress traffic leaving origin endpoint, node 102 a , has a destination at destination endpoint, node 102 h , but in general egress traffic leaving an origin endpoint may have multiple destination endpoints.
- computing server 110 computes the total amount of traffic that traverses a link (step 336 ) by summing all the traffic that traverses each path that contains the link.
- Computing server 110 uses Equation 1 to perform this computation, summing the product of the aggregate traffic multiplied by the relative proportion of traffic flow across the set of paths for all endpoint pairs.
- computing server 110 performs this computation separately for different users or applications, or for different types of users or applications. For example, some users or applications have different levels of priority, and method 300 may be performed separately for different priority levels.
- Steps 330 , 332 , and 334 may be performed in any order (or two or all of the steps may be performed simultaneously) and are shown sequentially in method 300 for illustrative purposes only.
- the system is self-contained, meaning that for each origin server (corresponding to an origin endpoint) that sends data to the computing server 110 , the computing server 110 also receives data from all destination servers that correspond to the destinations of the data packets leaving the origin endpoint. Similarly, for each destination server (corresponding to a destination endpoint) that sends data to the computing server 110 , in a self-contained system, the computing server 110 also receives data from all origin servers that correspond to the origins of the data packets arriving at the destination endpoint.
- a self-contained system includes all possible origin and destination endpoint pairs, and the computing server 110 receives data regarding each endpoint pair.
- the system is not self-contained, meaning that the computing server 110 only receives data regarding a subset of a self-contained network.
- an origin server monitoring egress traffic at an origin endpoint sends data to the computing server 110 .
- the origin endpoint may have two data streams with different destination endpoints.
- the first destination endpoint's ingress traffic is monitored by a destination server that sends data to the computing server 110 .
- the other destination endpoint's traffic is either not monitored by a destination server, or its destination server does not send data to the computing server 110 .
- two separate entities may monitor the ingress traffic at the two destination endpoints, but are not willing to share this information with each other.
- the data monitored by origin servers and destination server is not redundant, requiring both steps 330 and 332 to be performed in order to more accurately perform the computation in step 336 .
- FIG. 4 is a flowchart of a method 400 for adjusting the traffic on the paths crossing a link if the measured amount of traffic flow exceeds a threshold, according to an illustrative embodiment of the invention.
- Method 400 includes the steps of computing the total amount of traffic that traverses a link (step 440 ), determining the flow capacity of the link (step 442 ), and when determining that the computed traffic exceeds a threshold percentage of the flow capacity (decision 444 ), adjusting the traffic on the paths crossing the link (step 446 ).
- the total amount of traffic that traverses a link is computed (step 440 ).
- Computing server 110 follows the method of 300 or a similar method as described in relation to FIG. 3 to perform this computation.
- the computing server 110 determines the flow capacity of the link (step 442 ).
- the link capacity is the maximum amount of traffic that may traverse a link and is measured in bandwidth, or Mb/s.
- Link capacity may be defined in several different ways, including maximum theoretical throughput, maximum achievable throughput, peak measured throughput, or maximum sustained throughput.
- the flow capacity of the link is determined by a server 108 or by an intermediate device and then sent to the computing server 110 .
- the computing server 110 determines whether the computed traffic in step 440 exceeds a threshold percentage of the link capacity determined in step 442 .
- the threshold percentage may be the same value for different links, or a different threshold percentage may be used for different links.
- the computing server 110 determines that the computed traffic exceeds a threshold percentage of the link capacity, the computing server 110 adjusts the amount of traffic on the paths crossing the link (step 446 ).
- the threshold percentage may be 80%, such that if the computed amount of traffic exceeds 80% of the link's capacity, the computing server 110 adjusts the amount of traffic on the paths crossing the link.
- One way the computing server 110 adjusts the amount of traffic on the paths crossing the link is by reducing egress traffic from at least one origin endpoint that has a path crossing the link.
- the computing server 110 modifies at least one of the paths that crosses the link to an alternate path that does not cross the link, thereby shifting the relative proportion of traffic that flows over at least one path that traverses the link to an alternate path and decreasing the amount of traffic on the paths crossing the link.
- traffic is ranked by priority. For example, some applications or users may have higher priority than other applications or users. As another example, certain origin-destination endpoint pairs may have higher priority than other endpoint pairs. In this case, the proportion of low-priority traffic that traverses the path is shifted to an alternate path before shifting the relative proportions of traffic with higher priority.
- computing server 110 determines that the computed traffic does not exceed a threshold percentage of the link capacity (decision 444 )
- the computing server 110 returns to step 440 to update the computation of the total traffic that traverses the link.
- computing server 110 performs the computation of the total traffic on multiple links simultaneously, such that at any given time, computing server 110 could be in different stages of method 400 for different links.
Abstract
Description
is the amount of traffic flow that traverses
traverses
traverses
traverses
In this example, the two
where Vl corresponds to the amount of traffic flow on the link l, Ue corresponds to the total amount of traffic flow between an endpoint pair e (consisting of an origin endpoint and a destination endpoint), and fep corresponds to the proportion of traffic flow that traverses the path p in the plurality of paths between the endpoint pair e that crosses the link l. That is,
Claims (18)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/755,277 US8964546B1 (en) | 2012-01-31 | 2013-01-31 | Indirect measurement of user traffic on links |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261592771P | 2012-01-31 | 2012-01-31 | |
US13/755,277 US8964546B1 (en) | 2012-01-31 | 2013-01-31 | Indirect measurement of user traffic on links |
Publications (1)
Publication Number | Publication Date |
---|---|
US8964546B1 true US8964546B1 (en) | 2015-02-24 |
Family
ID=52473011
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/755,277 Active 2033-08-16 US8964546B1 (en) | 2012-01-31 | 2013-01-31 | Indirect measurement of user traffic on links |
Country Status (1)
Country | Link |
---|---|
US (1) | US8964546B1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9553794B1 (en) * | 2013-01-10 | 2017-01-24 | Google Inc. | Traffic engineering for network usage optimization |
US9577925B1 (en) * | 2013-07-11 | 2017-02-21 | Juniper Networks, Inc. | Automated path re-optimization |
US10142015B2 (en) | 2015-02-16 | 2018-11-27 | Alibaba Group Holding Limited | Method and apparatus for detecting shared risk link groups |
US10567289B2 (en) * | 2007-10-17 | 2020-02-18 | Dispersive Networks, Inc. | Virtual dispersive networking systems and methods |
US11562042B2 (en) * | 2019-02-15 | 2023-01-24 | Guizhou Baishancloud Technology Co., Ltd. | Intelligent hotspot scattering method, apparatus, storage medium, and computer device |
CN117714360A (en) * | 2024-02-07 | 2024-03-15 | 腾讯科技（深圳）有限公司 | Data routing method, device, electronic equipment, storage medium and program product |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010019554A1 (en) * | 2000-03-06 | 2001-09-06 | Yuji Nomura | Label switch network system |
US20060252429A1 (en) * | 2005-05-04 | 2006-11-09 | Lucent Technologies, Inc. | Flow-based call admission control for wireless communication systems |
US8108495B1 (en) * | 2009-04-30 | 2012-01-31 | Palo Alto Networks, Inc. | Managing network devices |
US20130114409A1 (en) * | 2010-04-14 | 2013-05-09 | Telefonaktiebolaget L M Ericsson (Publ) | Link advertisement for path computation in a communications network |
-
2013
- 2013-01-31 US US13/755,277 patent/US8964546B1/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010019554A1 (en) * | 2000-03-06 | 2001-09-06 | Yuji Nomura | Label switch network system |
US20060252429A1 (en) * | 2005-05-04 | 2006-11-09 | Lucent Technologies, Inc. | Flow-based call admission control for wireless communication systems |
US8108495B1 (en) * | 2009-04-30 | 2012-01-31 | Palo Alto Networks, Inc. | Managing network devices |
US20130114409A1 (en) * | 2010-04-14 | 2013-05-09 | Telefonaktiebolaget L M Ericsson (Publ) | Link advertisement for path computation in a communications network |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10567289B2 (en) * | 2007-10-17 | 2020-02-18 | Dispersive Networks, Inc. | Virtual dispersive networking systems and methods |
US9553794B1 (en) * | 2013-01-10 | 2017-01-24 | Google Inc. | Traffic engineering for network usage optimization |
US9577925B1 (en) * | 2013-07-11 | 2017-02-21 | Juniper Networks, Inc. | Automated path re-optimization |
US10142015B2 (en) | 2015-02-16 | 2018-11-27 | Alibaba Group Holding Limited | Method and apparatus for detecting shared risk link groups |
US11562042B2 (en) * | 2019-02-15 | 2023-01-24 | Guizhou Baishancloud Technology Co., Ltd. | Intelligent hotspot scattering method, apparatus, storage medium, and computer device |
CN117714360A (en) * | 2024-02-07 | 2024-03-15 | 腾讯科技（深圳）有限公司 | Data routing method, device, electronic equipment, storage medium and program product |
CN117714360B (en) * | 2024-02-07 | 2024-04-26 | 腾讯科技（深圳）有限公司 | Data routing method, device, electronic equipment, storage medium and program product |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8964546B1 (en) | Indirect measurement of user traffic on links | |
EP1929711B1 (en) | Method and apparatus for maximizing data transmission capacity of a mesh network | |
EP3136651B1 (en) | Network management | |
US7917650B2 (en) | Load balancing techniques for inter-domain traffic engineering | |
JP4357537B2 (en) | Distribution route control device | |
US7929440B2 (en) | Systems and methods for capacity planning using classified traffic | |
US20080101227A1 (en) | QoS ROUTING METHOD AND QoS ROUTING APPARATUS | |
US6658479B1 (en) | Load-balanced anycasting and routing in a network | |
US20140086055A1 (en) | Load balancing traffic in a mpls network | |
US20050188073A1 (en) | Transmission system, delivery path controller, load information collecting device, and delivery path controlling method | |
US8804500B2 (en) | Management of network capacity to mitigate degradation of network services during maintenance | |
US10873529B2 (en) | Method and apparatus for low latency data center network | |
US9077479B2 (en) | Method and system for adjusting network interface metrics | |
US20130136020A1 (en) | Method for measurement of asymmetric network capacities | |
Sahhaf et al. | Adaptive and reliable multipath provisioning for media transfer in SDN-based overlay networks | |
US20110078237A1 (en) | Server, network device, client, and network system | |
US9397894B2 (en) | Managing quality of experience for media transmissions | |
US20200162378A1 (en) | Method And System For Accelerating Interactive-Streaming-Based Applications Via Cloud Overlay Networks | |
EP2296324A1 (en) | Distributed flow mechanism for peer-to-peer streaming | |
CN112825512A (en) | Load balancing method and device | |
JP2006197473A (en) | Node | |
JPWO2007026604A1 (en) | Multicast node device, multicast transfer method, and program | |
KR20130048442A (en) | Method and system for providing content using multiple channel | |
SE526346C2 (en) | Forwarding quality control method in data network, involves detecting potentially overload paths or individual out-interfaces to select nodes for measurement, and measuring quality metrics of selected nodes | |
JP2005318222A (en) | System and method for packet transmission |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:JAIN, SUSHANT;KUMAR, ALOK;ZHANG-SHEN, RUI;AND OTHERS;SIGNING DATES FROM 20130118 TO 20130130;REEL/FRAME:029734/0750 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE TO CORRECT INVENTOR MATTHEW JONATHAN HOLLIMAN'S NAME FROM MICHAEL JONATHAN HOLLIMAN TO MATTHEW JONATHAN HOLLIMAN PREVIOUSLY RECORDED ON REEL 029734 FRAME 0750. ASSIGNOR(S) HEREBY CONFIRMS THE MICHAEL JONATHAN HOLLIMAN SHOULD BE MATTHEW JONATHAN HOLLIMAN;ASSIGNORS:JAIN, SUSHANT;KUMAR, ALOK;ZHANG-SHEN, RUI;AND OTHERS;SIGNING DATES FROM 20130118 TO 20130130;REEL/FRAME:030205/0665 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |