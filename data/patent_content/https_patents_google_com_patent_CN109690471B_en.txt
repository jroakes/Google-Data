CN109690471B - Media rendering using orientation metadata - Google Patents
Media rendering using orientation metadata Download PDFInfo
- Publication number
- CN109690471B CN109690471B CN201680088676.8A CN201680088676A CN109690471B CN 109690471 B CN109690471 B CN 109690471B CN 201680088676 A CN201680088676 A CN 201680088676A CN 109690471 B CN109690471 B CN 109690471B
- Authority
- CN
- China
- Prior art keywords
- frame
- video
- media
- implementations
- orientation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000009877 rendering Methods 0.000 title description 2
- 238000000034 method Methods 0.000 claims abstract description 69
- 230000008859 change Effects 0.000 claims description 35
- 230000004044 response Effects 0.000 abstract description 3
- 230000033001 locomotion Effects 0.000 description 33
- 230000003068 static effect Effects 0.000 description 24
- 238000001514 detection method Methods 0.000 description 18
- 238000010586 diagram Methods 0.000 description 16
- 238000010191 image analysis Methods 0.000 description 13
- 238000012015 optical character recognition Methods 0.000 description 13
- 238000004458 analytical method Methods 0.000 description 11
- 238000012731 temporal analysis Methods 0.000 description 10
- 238000004590 computer program Methods 0.000 description 9
- 230000001815 facial effect Effects 0.000 description 9
- 230000003287 optical effect Effects 0.000 description 9
- 230000008569 process Effects 0.000 description 8
- 239000013598 vector Substances 0.000 description 8
- 230000000007 visual effect Effects 0.000 description 7
- 239000003086 colorant Substances 0.000 description 6
- 241001465754 Metazoa Species 0.000 description 5
- 238000006243 chemical reaction Methods 0.000 description 5
- 238000007781 pre-processing Methods 0.000 description 5
- 238000013500 data storage Methods 0.000 description 4
- 230000004927 fusion Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 238000013135 deep learning Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/36—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators characterised by the display of a graphic pattern, e.g. using an all-points-addressable [APA] memory
- G09G5/363—Graphics controllers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/14—Digital output to display device ; Cooperation and interconnection of the display device with other functional units
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T3/00—Geometric image transformation in the plane of the image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2210/00—Indexing scheme for image generation or computer graphics
- G06T2210/22—Cropping
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/0442—Handling or displaying different aspect ratios, or changing the aspect ratio
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/045—Zooming at least part of an image, i.e. enlarging it or shrinking it
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/0492—Change of orientation of the displayed image, e.g. upside-down, mirrored
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/14—Solving problems related to the presentation of information to be displayed
Abstract
Systems and methods for cropping specifically oriented media using a computing device are described. In some implementations, a method may first include receiving, by a video preprocessor of an apparatus, a first media frame in a first orientation. A first region within the first frame including the first feature may be identified by an image analyzer. A clipping calculator of the device may generate a score for the first region based on the characteristic of the first feature and determine that the score for the first region exceeds a threshold. In response to determining that the score of the first region exceeds the threshold, the image processor of the device may then crop the first frame of the video to include the first region within a predetermined display region that includes a subset of the first frame in the second orientation.
Description
Background
In a network environment, such as the internet or other network, a first-party content provider may provide publicly presented information on resources, such as web pages, documents, applications, and/or other resources. The first party content may include text, video and/or audio information provided by the first party content provider via the resource server for presentation on the client device over the internet. Videos and similar media recorded at a wide aspect ratio that may be designed to be viewed on a desktop or in landscape orientation cannot be displayed directly full screen onto a mobile device held in a vertical or portrait orientation and typically cropped to the center, either losing detail at the left and right edges of the video, or enclosed with black bars at the top and bottom, reducing the display size of the video. Vertically oriented media is a popular format for viewing and displaying media in many applications. Because many videos and other media are recorded in wide aspect ratio layouts only, the layouts are in large inventory, however, vertical layouts are increasingly required by distributor requirements.
Disclosure of Invention
One implementation relates to a method comprising: receiving, by a mobile device, a first frame of a video in a first orientation; receiving, by a mobile device, metadata associated with a first frame of a video; extracting, by a video preprocessor of the mobile device from the metadata, an identification of a first region of a first frame of the video; cropping, by a video preprocessor of a mobile device, a first frame of a video to a cropped size centered on an identified first region of the first frame of the video, the cropped size based on a first aspect ratio, a resolution, and a second orientation to display the first frame of the video; and displaying the cropped first frame of the video in the second orientation on the display of the mobile device.
Cropping the first frame of the video may further comprise: calculating a frame boundary based on the first aspect ratio, the resolution, and the second orientation for application to a first frame of the video; and based on the calculated frame boundary, identifying cropping coordinates of a rectangle centered on the identified first region of the first frame of the video.
Cropping the first frame of the video may also include identifying cropping coordinates of a rectangle bounding all of the first regions.
The region may be associated with a plurality of regions of a first frame of the video, and cropping the first frame of the video may further include: calculating a frame boundary to apply to a first frame of the video based on the first aspect ratio, the resolution, and the second orientation; and based on the calculated frame boundary, identifying cropping coordinates of a rectangle centered on the identified first region of the first frame of the video.
The method may further comprise: determining that the clipping coordinates of the rectangle are insufficient to define all of the plurality of regions; and wherein identifying the cropping coordinates of the rectangle centered on the one of the plurality of regions may include including the text image within the cropping coordinates of the rectangle.
The method may further comprise: receiving, by the mobile device, a second frame of the video in the first orientation; extracting, by a video preprocessor of the mobile device from the metadata, region data associated with a second region of a second frame of the video; cropping, by a video preprocessor of the mobile device, the second frame of the video to a cropped size centered on the identified first region of the first frame and further based on the first aspect ratio, the resolution, and the first orientation to display the second frame of the video; and displaying the cropped second frame of the video in the first orientation on the display of the mobile device.
The metadata may include one or more regions in the first frame of the video having associated scores. The or each score may be associated with at least one feature in the respective region. The or each score is generated based on one or more characteristics of at least one feature. One or more regions of a first frame of the video may be associated with a score that exceeds a predetermined threshold.
The region identified by the metadata may be generated by receiving, by a video pre-processor of the apparatus, a first media (e.g., video) frame in a first orientation. A first region within the first frame including the first feature may be identified by an image analyzer. A clipping calculator of the device may generate a score for the first region based on the characteristic of the first feature and determine that the score for the first region exceeds a threshold. Other implementations relate to a system for cropping specifically oriented media using a computing device. The system may include one or more of the one or more processors of the device, a network interface electrically connected to the one or more processors, and a computer storage device electrically connected to the one or more processors that stores instructions. The instructions, when executed by the one or more processors, may cause the one or more processors to perform operations comprising the above-described methods.
Other implementations also relate to computer-readable storage devices storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations may include operations comprising the above-described methods.
Drawings
The details of one or more embodiments are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the disclosure will become apparent from the description, the drawings, and the claims, wherein:
FIG. 1 is a block diagram depicting an implementation of an environment for automatically transitioning media from one orientation to another.
Fig. 2 is a block diagram depicting software and/or hardware modules configured for media pre-processing, media analysis, and cropping of received media.
FIG. 3 is a flow diagram depicting an implementation of a method of cropping a media frame.
FIG. 4 is a flow diagram depicting an implementation of a method of cropping a media frame by determining a score for each of a plurality of regions.
FIG. 5 is a flow diagram depicting an implementation of a method of generating or updating a score based on movement of a feature.
FIG. 6 is a flow diagram depicting an implementation of a method of cropping media frames using received metadata.
FIG. 7 is a flow diagram depicting an implementation of a method of adjusting crop based on changes in orientation.
FIG. 8 is a block diagram depicting the general architecture of a computing system that may be used to implement the various elements and methods of the systems described and illustrated herein.
It will be appreciated that some or all of the figures are schematic representations for purposes of illustration. The drawings are provided for the purpose of illustrating one or more implementations and are to be clearly understood as not being limiting the scope or meaning of the claims.
Detailed Description
Various concepts related to, and implementations of, methods, apparatus, and systems for providing information over a computer network are described in greater detail below. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways, as the concepts described are not limited to any particular one of the implementations. Various implementations and applications are provided primarily for purposes of illustration.
Implementations disclosed herein relate to automatically converting video in landscape (landscape) mode to a portrait-compliant (portrait) mode or vice versa, while maintaining full screen, which provides improved cropped video compared to center cropping or adding padding to the top/bottom. The video may be modified in some implementations so as to improve the display on the device, and may be adapted to the device in some implementations. The conversion may include detecting important portions (e.g., features) of the image or video for each frame. Important regions, images or video based on identification may be intelligently cropped or populated to preserve important features while discarding unimportant regions, static boundaries, and the like. The detected features may include face tracking, object detection and/or recognition, text detection, detection of dominant colors (dominant colors), motion analysis, scene change detection, and image saliency. Detection and identification may use deep learning based methods and algorithms. Text recognition may use Optical Character Recognition (OCR). The detection of the features allows for an optimal clipping path. Other aspects may include filling the image to match background color, and removing and/or reformatting any borders to fit the new display mode. Although discussed primarily in terms of video, in many implementations the system may be applied to individual images or frames.
Fig. 1 is a block diagram of an implementation of an environment 100 that automatically transitions video from one orientation to another orientation over a network 106. Network 106 may comprise a Local Area Network (LAN), a Wide Area Network (WAN), a telephone network such as the Public Switched Telephone Network (PSTN), a wireless link, an intranet, the internet, or a combination thereof. The environment 100 also includes a mobile device 102. In some implementations, the mobile device 102 includes a processor 122, a data store 124, a network interface 126, a display 128, an input/output module 130, a sensor module 132, and a media module 134. Sensor module 132 may be configured to include sensors (e.g., accelerometers and/or magnetometers) that detect orientation of the computing device, as well as other similar sensors included in many mobile devices. The processor 122 may comprise a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), the like, or a combination thereof. Data storage 124 may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing a processor with program instructions. The memory may comprise a floppy disk, compact disk read only memory (CD-ROM), Digital Versatile Disk (DVD), magnetic disk, memory chip, Read Only Memory (ROM), Random Access Memory (RAM), Electrically Erasable Programmable Read Only Memory (EEPROM), Erasable Programmable Read Only Memory (EPROM), flash memory, optical media, or the processor 122 may be removable fromAny other suitable memory to read instructions. The instructions may comprise code in any suitable computer-programming language, such as, but not limited to C, C + +, C #, or,
The mobile device 102 may include one or more devices, such as a computer, laptop, smartphone, tablet, personal digital assistant, and the like, configured to communicate with other devices via the network 106. The device may be any form of portable electronic device that includes a data processor and memory. The data store 124 may store machine instructions that, when executed by a processor, cause the processor to perform one or more of the operations described herein. The data store 124 can also store data to enable presentation of one or more resources, content items on a computing device. The processor may comprise a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), the like, or a combination thereof. Data storage 124 may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing a processor with program instructions. Data storage 124 may comprise a floppy disk, a compact disc read only memory (CD-ROM), a Digital Versatile Disc (DVD), a magnetic disk, a memory chip, a Read Only Memory (ROM), a Random Access Memory (RAM), an Electrically Erasable Programmable Read Only Memory (EEPROM), an Erasable Programmable Read Only Memory (EPROM), a flash memory, an optical medium, or any other suitable memory from which a processor may read instructions. The instructions may comprise code in any suitable computer programming language, such as but not limited to
The mobile device 102 may execute a software application, such as a web browser or other application, to retrieve (retrieve) content from other computing devices over the network 106. Such an application may be configured to retrieve first-party content from the media server system 104. In some cases, the application running on the mobile device 102 may itself be first party content (e.g., a game, a media player, etc.). In one implementation, the mobile device 102 may execute a web browser application that provides a browser window on a display of a client device. A web browser application providing a browser window may operate by receiving an input of a Uniform Resource Locator (URL), such as a network address, from an input device, such as a pointing device, keyboard, touch screen, or other form of input device. In response, one or more processors of the client device executing instructions from the web browser application may request data from other devices connected to the network 106 referenced by the URL address (e.g., the media server system 104). The other device may then provide the web page data and/or other data to the mobile device 102 that causes the visual indicia to be displayed by the display of the mobile device 102. Accordingly, the browser window displays retrieved first-party content, such as web pages from various websites, to facilitate user interaction with the first-party content.
In some implementations, the media module 134 of the mobile device 102 is configured to receive a plurality of media frames and associated metadata. Media may be received through network interface 126 and stored in data store 124. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received through network interface 146. In some implementations, the media module 134 is configured to identify regions in the frame based on the received metadata. In some implementations, the media module 134 is configured to crop the media frame based on the region. In some implementations, the cropped area is also based on one or more media frames before and/or after the media frame.
In some implementations, the media module 134 of the mobile device 102 is configured to receive an indication of a change in orientation from one or more sensor modules 132. In some implementations, the media module 134 is configured to dynamically adjust the cropping of the playing media based on the orientation changes.
The media server system 104 may include a processor 142, a data store 144, a network interface 146, a content selection module 148, a media cropping module 150, a metadata module 152, and a media content database 154. In some implementations, the content selection module 148 of the media server system 104 is configured to select media from the media content database 154. In some implementations, the media cropping module 150 is configured to pre-process media, analyze features and/or objects of the media, and crop the media based on the analysis of the features and/or objects. In some implementations, the metadata module 152 is configured to extract data based on a clipping path that preprocesses the media, analyzes features and/or objects of the media, and determines a target aspect ratio or resolution. Although shown on the media server system 104, in many implementations, the media clipping module 150 may be executed on one or more mobile devices 102.
The media server system is shown as containing a media cropping module 150. In some implementations, the media cropping module 150 is configured to pre-process media, analyze features and/or objects of the media, and crop the media based on the analysis of the features and/or objects. In some implementations, the media cropping module 150 is configured to determine whether cropping is needed based on whether one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame being analyzed. In some implementations, the media cropping module 150 is configured to crop the media frame only if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media cropping module 150 is configured to crop the media to match a target aspect ratio or to match a target resolution. The media cropping module 150 may be configured to add additional padding to one or more sides of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the media cropping module 150 may be configured to base the cropped area also on one or more media frames before and/or after the current media frame being cropped. In some implementations, the media cropping module 150 is configured to include one or more regions that exceed a threshold. In some implementations, the media clipping module 150 is configured to: in cropping the media frame, when the media cropping module 150 determines the regions to include, at least one or more of the regions that include a score that exceeds a threshold are considered.
In some implementations, the metadata module 152 is configured to extract data based on pre-processing the media, analyzing features and/or objects of the media, and determining a clipping path for a target aspect ratio or resolution. In some implementations, the metadata module 152 is configured to receive the metadata as part of a media file containing a plurality of media frames. In some implementations, the metadata module 152 is configured to receive the metadata independently, along with identifiers or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata module 152 is configured to analyze the metadata to determine portions of the data that are relevant to regions associated with one or more media frames associated with the media. In some implementations, the metadata module 152 is configured to extract boundary information for one or more regions for each of a plurality of media frames contained in the metadata. In some implementations, the metadata module 152 is configured to extract a location within each of the plurality of media frames of one or more features. Features may include objects such as cars, buildings, people, animals, street signs, etc., text, boundaries of media frames, consistent color fill of one or more sides of media frames, etc. In some implementations, the metadata module 152 is configured to identify a plurality of features and/or regions of one or more of the plurality of media frames. In some implementations, the metadata module 152 is configured to associate the received metadata with a target aspect ratio or a target resolution.
Fig. 2 is a block diagram of software and/or hardware modules for media pre-processing, media analysis, and cropping received media. In some implementations, the pre-processing module 210 is configured to pre-process the media and down-convert the media using a down conversion (down convert) module 212, a down conversion module 214, a down conversion module 216, a down conversion module 218, and/or the like. In some implementations, the pre-processing module 210 is configured to send the resulting output to one or more of the temporal analysis module 220 and the image analysis module 230. The temporal analysis module 220 may include a scene change module 222 and a static boundary module 224. The image analysis 230 may include an OCR module 232, an object detection module 234, a face tracking module 236, a motion analysis module 238, and an entropy module 240. The temporal analysis module 220 and the image analysis module 230 may be configured to send their data results to a signal fusion (fusing) calculator 250 and a cropping calculator 252. Although shown separately, in many implementations, the temporal analysis and image analysis modules may be components of the same analyzer system or module. Similarly, the components shown within the temporal analysis and image analysis modules may be separate from the temporal analysis or image analysis modules, or may be provided by other modules.
In some implementations, the temporal analysis module 220 may include an application, applet, service, server, daemon, routine, or other executable logic to analyze a series of images, such as images of a video. Temporal analysis module 220 may include a scene change module 222 configured to analyze a plurality of media frames to determine a scene change. The scene change module 222 may include an application, applet, service, server, daemon, routine, or other executable logic to identify differences between successive images that indicate a scene change or significant interruption in a video. In some implementations, the scene change module 222 is configured to determine scene changes by using keypoint detection to analyze when there is a large change in keypoints that indicates a scene break or scene change. In some implementations, the scene change module 222 is configured to compare all pixels in one frame to pixels in successive frames and, when considered part of the optical flow, indicate a scene change if more than a certain threshold of pixels are different. In some implementations, the scene change module 222 is configured to calculate motion vectors between multiple media frames, and the absence of consecutive motion vectors between consecutive frames indicates a scene change. Features may then be identified within a particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In some implementations, the scene change module 222 is configured to track information of the location of particular features within the plurality of media frames, and such information is also used to determine the location of the cropped media frames based on the region.
In some implementations, the temporal analysis module 220 includes a static boundary module 224 configured to analyze a plurality of media frames to determine whether a static boundary exists and a location of the static boundary. The static boundary module 224 may include an application, applet, service, server, daemon, routine, or other executable logic to identify a static boundary that indicates a boundary on at least one edge of a frame that remains substantially unchanged between successive images. In some implementations, the static boundary module 224 is configured to receive a plurality of media frames and to analyze the plurality of media frames to find a static boundary along edges of the plurality of frames. In some implementations, the static boundary module 224 is configured to locate the boundary by selecting one or more random pixels and compare the line of pixels vertically and/or horizontally to the random pixels to determine whether there is a continuous line of pixels that is a color close to the randomly selected pixels. Such a line of pixels may extend across the entire image, in some implementations, or be a portion of the image (e.g., a quarter of the image). In some implementations, the static boundary module 224 is configured to locate a boundary that is static from one frame to the next and contains pixels that are relatively consistent in color. In some implementations, the static boundary module 224 is configured to locate a boundary that is static from one frame to the next and contains pixels that are relatively uniform in color, but also contains some additional static information such as text with different colors embedded in the boundary. Once the boundary is located, whether or not it contains embedded text, it may be processed as an image during processing of the crop.
In some implementations, the image analysis module 230 includes an optical character recognition, OCR module 232 configured to detect text embedded in the image data. The image data may be one or more frames of media such as video. OCR module 232 may include an application, applet, service, server, daemon, routine, or other executable logic to identify text embedded in image data of one or more media frames. In some implementations, the OCR module 232 may compare a predetermined vector or bitmap corresponding to a letter to a portion of an image, such as via a sliding window. In some implementations, the OCR module 232 may select a reference image (e.g., letter) based on previous letters (e.g., according to a text prediction system), which may improve efficiency.
In some implementations, the image analysis module 230 includes an object detection module 234 configured as a neural network trained on different objects using, for example, tens, hundreds, or thousands of reference images of the objects. The object detection module 234 comprises an application, applet, service, server, daemon, routine, or other executable logic to identify visual objects (i.e., data that creates a visual representation of an object when displayed) in one or more media frames. The neural network may identify similar elements in an image of the object and create a classification of the elements representing the object, which may then be used to identify the object in the new image. The image analysis module 230 may generate a bounding box around the identified object such that the bounding box may be tracked image-by-image.
In some implementations, the image analysis module 230 includes a face tracking module 236 configured to receive a plurality of media frames and analyze the plurality of media frames to detect facial features, e.g., via an eigenface (eigenface) or similar structure. The face tracking module 236 may include an application, applet, service, server, daemon, routine, or other executable logic to identify similarities between one or more consecutive frames of media that, when displayed, create a visual representation of one or more faces and relative motion of the one or more faces. Face tracking may then be accomplished by tracking the facial features to match the facial features in each of the plurality of media frames.
In some implementations, the image analysis module 230 includes a motion analysis module 238 configured to analyze motion of objects detected in a plurality of media frames and to calculate motion vectors between the plurality of media frames. The motion analysis module 238 may include an application, applet, service, server, daemon, routine, or other executable logic to identify similarities between one or more consecutive frames of media that, when displayed, create a visual representation of one or more objects and relative motion of the one or more objects. In some implementations, the motion analysis module 238 is configured to calculate a global motion vector from a difference of pixels in a region of the first media frame to pixels of the second media frame.
In some implementations, the image analysis module 230 includes an entropy module 240 configured to analyze the entropy of each of a plurality of media frames and calculate the difference in entropy (i.e., measure the amount of change or difference that has occurred from one frame to another) to determine key frames. The entropy module 240 may include an application, applet, service, server, daemon, routine, or other executable logic to analyze the entropy of one or more media frames. In some implementations, the entropy module 240 is configured to analyze entropy between identified regions of the media frame to calculate differences in entropy to determine key regions. In some implementations, the entropy module 240 is configured to extract values from multiple media frames that characterize the randomness of motion vectors associated with regions in the frames, allowing multiple media segments to be segmented into different events (e.g., scene changes in video).
In some implementations, the signal fusion calculator module 250 is configured to combine data from the temporal analysis module 220 and the image analysis module 230 and determine important objects and/or features of the entire scene including the plurality of media frames. The merged data may then be used by the clipping calculator module 252 to clip the plurality of media frames to regenerate the media. In some implementations, the media is re-rendered into a video of a target aspect ratio. In some implementations, the signal fusion calculator module 250 is configured to assign weights to different outputs of the analyzer. The signal fusion calculator module 250 may normalize (normalize) the different outputs by a specified range to a value that has been determined by a deep learning method.
Method for automatically cutting media
Fig. 3 is a flow diagram of an implementation of a method 300 of cropping a media frame. In some implementations, the method 300 is implemented by the processor 142 in the media server system 104 executing instructions stored on the data store 144 and may use media extracted from the media content database 154. Briefly, the method 300 includes receiving a media frame at 302 and identifying a region in the frame that includes a feature at 304. If additional regions are identified at 306, the method returns to identifying regions in the frame that include features at 304. If there are regions that cannot be identified at 306, the method continues to crop the media frame based on the identified one or more regions at 308.
Still referring to fig. 3 and in more detail, the method 300 begins when a media frame is received at 302. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received through network interface 146. In some implementations, the media frames are part of a stored media list, and each media is processed in turn. In some implementations, it is first determined whether the media needs to be trimmed and/or processed. The determination may be accomplished by comparing the stored dimensions, aspect ratio, resolution, etc. of the stored media to target values.
At 304, regions in the frame that include features are identified. In some implementations, the features are identified by analyzing the frames using facial recognition. In some implementations, the features are identified by analyzing frames of text using optical character recognition. In some implementations, the features are identified by analyzing frames of objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, the features are identified by analyzing frames, and/or padding of the boundary (e.g., a boundary with a consistent or near consistent color at one or more edges of the frame). In some implementations, the frames are analyzed to identify a plurality of features. The features may be of different types (e.g., face, text, object, etc.). If additional regions are identified at 306, the method returns to identifying additional regions in the frame that include features at 304.
If additional regions cannot be identified at 306, the media frame is cropped based on the one or more regions at 308. In some implementations, the media frame is only cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, the media frame is only cropped if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. Additional padding may be added to one or more sides of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped area is also based on one or more media frames before and/or after the media frame.
In some implementations, some padding may be added during cropping to meet the target aspect ratio. In some implementations, if static boundaries exist on one or more edges of a media frame, they may be moved or reformatted to form and/or become part of the padding.
In some implementations, a plurality of media frames are received and analyzed to determine a scene change. Keypoint detection may be used to analyze when there is a large change in keypoints indicating a scene break or scene change. In some implementations, a comparison of all pixels in one frame is compared to pixels in successive frames and, when considered part of the optical flow, if more than a certain threshold of pixels are different, they indicate a scene change, indicating a scene change. In some implementations, motion vectors are calculated between multiple media frames, and the absence of a consecutive motion vector between consecutive frames indicates a scene change. Features may then be identified within a particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In implementations, the information that tracks the location of the particular feature within the plurality of media frames is also used to determine the location of the cropped media frame based on the region.
In some implementations, a plurality of media frames are received and analyzed to identify facial features. Face tracking may then be accomplished by tracking the facial features to match the facial features in each of the plurality of media frames.
In some implementations, a plurality of media frames are received and analyzed to find a static boundary along an edge of the plurality of frames. In some implementations, to locate the boundary, random pixels are selected and lines of pixels are compared vertically and/or horizontally to the random pixels to determine whether there are consecutive lines of pixels that are close in color to the randomly selected pixels. In some implementations, a boundary is located that is static from one frame to the next and contains pixels that are relatively consistent in color. In some implementations, a boundary is located that is static from one frame to the next and contains pixels that are relatively uniform in color, but also contains some additional static information such as text with different colors embedded in the boundary. Once the boundary is located, whether or not it contains embedded text, it may be processed as an image during processing of the crop.
Fig. 4 is a flow diagram of an implementation of a method 400 of cropping a media frame by determining a score for each of a plurality of regions. In some implementations, the method 300 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data store 144 and may use media extracted from the media content database 154. Briefly, the method 400 includes receiving a media frame at 402 and identifying a region in the frame that includes a feature at 404. If additional regions are identified at 406, the method determines scores for the identified regions based on the respective characteristics at 408, and returns to identifying regions in the frame that include features at 404. If regions cannot be identified at 406, the method continues with determining that a score of one or more of the identified regions exceeds a threshold at 410, and cropping the media frame to include each region having a score that exceeds the threshold at 412.
Referring to fig. 4 and in more detail, the method 400 begins when a media frame is received at 402. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received through network interface 146.
At 404, regions in the frame that include features are identified. In some implementations, one or more features in the region are identified by using facial recognition analysis frames. In some implementations, one or more features in the region are identified by analyzing frames of text using optical character recognition. In some implementations, one or more features in a region are identified by analyzing frames of objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, one or more features in a region are identified by analyzing frames, and/or padding of a boundary (e.g., a boundary with a consistent or near consistent color at one or more edges of a frame). In some implementations, each region is further analyzed to potentially identify a plurality of features in one or more regions. The features of each of the plurality of regions may be of different types (e.g., face, text, object, etc.). If additional regions are identified at 406, the method determines scores for the identified regions based on the respective characteristics at 408, and returns to identifying additional regions in the frame that include features at 404.
At 408, a score for the identified region is determined based on the respective characteristic. In some implementations, the score is based on a type of feature located in or at least partially in the region. In some implementations, the scores are weighted based on the types of features located in the regions. Some characteristics on which the score may be based may include: feature size in a region, feature type in a region, motion of features in a region, relative motion of features in a region, an amount of blur associated with features in a region, and so forth. In some implementations, a score is assigned to a feature rather than a region containing the feature. In some implementations, determining the score for each of the plurality of regions includes determining a ranking of the plurality of regions by determining at least a top-ranked region of the plurality of regions. In some implementations, determining the score for each of the plurality of regions includes ranking each of the plurality of regions from high to low, where higher ranked regions are more likely to be included in any clipping of the media frame.
If additional regions cannot be identified at 406, the method determines that the score for one or more regions exceeds a threshold at 410. In some implementations, the score for each of the plurality of regions includes a value for comparison. In some implementations, when cropping a media frame, the score for a region must exceed a threshold before the region is considered. In some implementations, only the region with the highest score is preferentially included when cropping the media frame. In some implementations, multiple regions are preferentially included based on their respective scores when cropping a media frame. In some implementations, it is determined which combination of regions results in a maximized score, where all regions can fit inside the region of the cropped media frame.
If additional regions are identified at 406, the media frame is cropped to include one or more regions having associated scores that exceed a threshold at 412. In some implementations, only regions of the plurality of regions that have a score that exceeds a threshold are considered when determining the regions to include when cropping the media frame. In some implementations, the media frame is only cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, the media frame is cropped only if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. Additional padding may be added to one or more sides of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped area is also based on one or more media frames before and/or after the media frame.
FIG. 5 is a flow diagram of an implementation of a method 500 of generating or updating a score based on movement of a feature. In some implementations, the method 500 is implemented by the processor 142 in the media server system 104 executing instructions stored on the data store 144 and may use media extracted from the media content database 154. Briefly, the method 500 includes receiving a plurality of media frames at 502 and identifying regions in each of the plurality of frames that include the same features at 504. If additional regions are identified at 506, the method continues by determining an amount of movement of the feature from the region at 508, generating or updating a score based on the movement of the feature at 510, and then returning to 504 to identify additional regions in each of the plurality of frames that include the same feature. If the additional region cannot be identified at 506, the method stops.
Still referring to fig. 5 and in more detail, the method 500 begins when a plurality of media frames are received at 502. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, a plurality of media frames are received as part of the streaming media data. Streaming media may be received through network interface 146.
At 504, regions in each of the plurality of frames that include the same feature are identified. In some implementations, identifying the features as being the same includes comparing characteristics of the features. The characteristics of the features may include object attributes, color values, sizes, and the like. In some implementations, identifying the feature as the same feature is also based on a proximity of a region bounding the feature between frames in a plurality of frames that precede and follow the frame.
If additional regions are identified at 506, an amount of movement of features from the regions is determined at 508. In some implementations, the amount of movement of the feature from the region is determined by the absolute position of the feature within each of the plurality of frames. In some implementations, the amount of movement of the feature from the region is determined by the relative position of the feature within each of the plurality of frames when compared to one or more of the preceding and following frames. In some implementations, the amount of movement is determined by an increase or decrease in the size of the feature between one or more of the plurality of frames. Combinations of different methods of determining the amount of movement of a feature may be used to determine the amount of movement between two or more of the plurality of frames.
At 501, a score for a region is generated or updated based on the amount of movement. In some implementations, the score is adjusted based on an amount of movement of the feature between two or more of the received plurality of frames or based on an amount of movement of the feature between two or more of the received plurality of frames. In some implementations, the adjustment of the score is accomplished by: the existing scores of regions of the frame containing the one or more features are weighted based on the determined amount of movement of the one or more features between the plurality of frames. In some implementations, the score is assigned to a feature rather than a region containing the feature. In some implementations, determining the score for each of the plurality of regions includes determining a ranking of the plurality of regions by determining at least a top-ranked region of the plurality of regions. In some implementations, determining the score for each of the plurality of regions includes ranking each of the plurality of regions from high to low, where higher ranked regions are more likely to be included in any clipping of the media frame.
Use of metadata
The transformation of the media may be done on different computing systems, which includes detecting important portions (e.g., features) of the image or video of each frame, and intelligent cropping or padding to preserve important features while discarding unimportant areas, static boundaries, and the like. In some implementations, the detection of portions of images, videos, or other media may be done on a server system and used to create metadata that associates regions or bounds containing features with media frames. Important regions, images or video based on identification may be intelligently cropped or populated to preserve important features on other devices such as mobile devices while discarding unimportant regions, static boundaries, and the like. The detected features may include face tracking, object detection and/or recognition, text detection, detection of dominant colors, motion analysis, scene change detection, and image saliency. Detection and identification may use deep learning based methods and algorithms. Text recognition may use Optical Character Recognition (OCR). Detecting features to be placed in the metadata allows an optimal clipping path to be performed on the mobile device. Other aspects of the invention may include filling in the image to match background colors, and removing and/or reformatting any borders to fit the new display mode. Although media is discussed primarily in terms of video, in many implementations the system may be applied to individual images or frames.
Fig. 6 is a flow diagram of an implementation of a method 600 of cropping media frames using received metadata. In some implementations, the method 600 is implemented by the processor 122 in the mobile device 102 executing instructions stored on the data store 124. Briefly, the method 600 includes receiving a media frame at 602, receiving metadata associated with the media at 604, identifying a region in the frame based on the received metadata at 606, cropping the media frame based on the region at 608, and receiving a next media frame at 610.
Referring to fig. 6 and in more detail, the method 600 begins when a media frame is received at 602. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. Media may be received through network interface 126 and stored in data store 124. In some implementations, the media frames are received as part of streaming media data. Streaming media may be received through network interface 146.
Metadata associated with the media is received at 604. In some implementations, the metadata is received as part of a media file containing a plurality of media frames. In some implementations, the metadata is received independently, along with identifiers or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata includes data related to a region associated with one or more of the plurality of media frames. In some implementations, the boundary information for the one or more regions of each of the plurality of media frames is included in the metadata. In some implementations, the locations of the plurality of media frames in the plurality of features are included in the metadata. Features may include objects such as cars, buildings, people, animals, street signs, etc., text, boundaries of media frames, consistent color fill of one or more sides of media frames, etc. In some implementations, the metadata can identify a plurality of features and/or regions of one or more of the plurality of media frames. In some implementations, the metadata is associated with a target aspect ratio or a target resolution. The metadata may identify one or more regions of the media frame in some implementations. Each of the one or more identified regions may be a region determined to have a score that exceeds a threshold. The score may be determined by a clipping calculator as described above.
At 606, based on the received metadata, regions in the frame are identified. In some implementations, a region in the frame is retrieved from the metadata and includes a feature identified using facial recognition. In some implementations, a region in a frame is retrieved from the metadata and includes a feature identified by analyzing the frame of text using optical character recognition. In some implementations, a region in a frame is retrieved from the metadata and includes features identified by analyzing the frame of an object (e.g., a car, a building, a person, an animal, a street sign, etc.) using object recognition. In some implementations, regions in a frame are retrieved from the metadata and include features identified by analyzing frames, and/or padding of the boundary (e.g., a boundary with a consistent or near consistent color at one or more edges of the frame). In some implementations, a region in the frame is retrieved from the metadata, and the region in the frame includes a plurality of features. The features may be of different types (e.g., face, text, object, etc.). In some implementations, multiple regions may be retrieved from the metadata of the media frame. In some implementations, multiple media frames may be retrieved and metadata associated with the multiple media frames.
At 608, the media frame is cropped based on the region. In some implementations, the media frame is only cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, the media frame is cropped only if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. The target aspect ratio or target resolution may vary depending on the orientation of the mobile device 102 displaying the media frame. Additional padding may be added to one or more sides of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped area is also based on one or more of the media frames before and/or after the media frame.
The next media frame is received at 610 until no more frames are available. The next media frame may be received through the network interface 126 and stored in the data store 124. In some implementations, the next media frame is received as part of the streaming media data. Streaming media may be received through network interface 146. As long as there are more frames available, the method may continue, again proceeding to identifying regions in the next frame based on the received metadata.
FIG. 7 is a flow diagram of an implementation of a method 700 of adjusting cropping based on a change in orientation. In some implementations, the method 700 is implemented by the processor 122 of the mobile device 102. The processor 122 executes instructions stored on the data storage 124 and receives data from one or more sensor modules 132. Briefly, the method 700 includes receiving an indication of a change in orientation at 702, identifying a resolution of a new orientation at 704, and dynamically adjusting a cropping of the playing media based on the new orientation at 706.
Still referring to fig. 7 and in more detail, the method 700 begins when an indication of a change in orientation is received at 702. In some implementations, an indication of a change in orientation is received from the sensor module 132 (e.g., an accelerometer and/or magnetometer). In some implementations, the change in orientation or the detection of the orientation occurs before the display of the media. In some implementations, the change in orientation occurs during the display of the media, and the change in the display of the media occurs in real-time after the change in orientation is detected.
The resolution and/or aspect ratio of the new orientation is identified at 704. In some implementations, the resolution and/or aspect ratio is predetermined by the application displaying the media. The resolution and/or aspect ratio may have predetermined values for the landscape and portrait orientations. In some implementations, orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of unused display space. In some implementations, orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of fill required to adapt the displayed media into the available display space.
At 706, the cropping of the playing media is dynamically adjusted based on the new orientation. In some implementations, the change in orientation occurs during the display of the media, and the change in the display of the media occurs in real-time after the change in orientation is detected. In some implementations, the media frame(s) remain the same, but the cropping is changed based on the received metadata to accommodate the new resolution and/or aspect ratio.
Fig. 8 is a block diagram of the general architecture of a computing system 800 that may be used to implement the mobile device 102, the media server system 104, and the like. Computing system 800 includes a bus 805 or other communication mechanism for communicating information, and a processor 810 coupled with bus 805 for processing information. Computing system 800 may also include one or more processors 810 coupled to bus 805 to process information. Computing system 800 also includes main memory 815, such as a RAM or other dynamic storage device, coupled to bus 805 for storing information and instructions to be executed by processor 810. Main memory 815 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 810. Computing system 800 may also include a ROM 802 or other static storage device coupled to bus 805 for storing static information and instructions for processor 810. A storage device 825, such as a solid state disk, magnetic disk or optical disk, is coupled to bus 805 for persistently storing information and instructions. Computing system 800 also includes, but is not limited to, digital computers such as laptop computers, desktop computers, workstations, personal digital assistants, servers, blade servers, mainframes, cellular telephones, smart phones, mobile computing devices (e.g., notepads, e-readers, etc.), and the like.
According to various implementations, the processes and/or methods described herein may be implemented by the computing system 800 in response to the processor 810 executing an arrangement of instructions contained in main memory 815. Such instructions may be read into main memory 815 from another computer-readable medium, such as storage device 825. Execution of the arrangement of instructions contained in main memory 815 causes the computing system 800 to perform the illustrative processes and/or method steps described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 815. In alternative implementations, hard-wired circuitry may be used in place of or in combination with software instructions to implement the illustrative implementations. Thus, implementations are not limited to any specific combination of hardware circuitry and software.
Although an implementation of computing system 800 has been described in fig. 8, implementations and functional operations of the subject matter described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Implementations and operations of the subject matter described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware embodied in tangible media, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium may be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Furthermore, although a computer storage medium is not a propagated signal, a computer storage medium can be the source and destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be contained in one or more separate components or media, such as multiple CDs, diskettes, or other storage devices. Accordingly, computer storage media are tangible and non-transitory.
The operations described in this specification may be performed by a data processing apparatus on data stored on one or more computer readable storage media or data received from other sources.
The terms "data processing apparatus," "computing device," or "processing circuitry" encompass all kinds of apparatus, devices, and machines that process data, which in some implementations include a programmable processor, a computer, system(s) on a chip, portions of a programmed processor, or a combination of the foregoing. The device may contain special purpose logic circuitry, e.g., an FPGA or an ASIC. In addition to hardware, the apparatus can also include code that creates an execution environment for the associated computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The devices and execution environments may implement a variety of different computing model infrastructures, such as web services, distributed computing infrastructures, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
In some implementations, processors suitable for the execution of a computer program include both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor that acts in accordance with instructions and one or more memory devices that store instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Furthermore, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Suitable means for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including in some implementations semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices), magnetic disks (e.g., internal hard disk or removable disk), magneto-optical disks, and CD-ROM and DVD disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, implementations of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; in some implementations, the feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user may be received in any form, including acoustic, speech, or tactile input.
While this specification contains many implementation-specific details, these should not be construed as limitations on the scope of protection claimed, but rather as specific features describing particular implementations. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are, for brevity, described in the context of a single implementation, may also be implemented in multiple implementations separately or in suitable subcombinations. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some environments, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products embodied on tangible media.
References to "or" may be understood to be inclusive such that any term described using "or" may refer to any term in the singular, more than one, and all of the described terms.
Thus, particular implementations of the subject matter have been described. Other implementations are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
The claims should not be read as limited to the described order or elements unless stated to that effect. It will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the appended claims. All implementations that come within the spirit and scope of the following claims and equivalents thereto are claimed.
Claims (10)
1. A method, comprising:
receiving, by a mobile device, a first frame of a video in a first orientation;
receiving, by the mobile device, metadata associated with a first frame of the video;
extracting, by a video preprocessor of the mobile device, an identification of a first region of a first frame of the video from the metadata;
cropping, by a video preprocessor of the mobile device, a first frame of the video to a first cropped size centered over the identified first region of the first frame of the video, the first cropped size based on a first aspect ratio, a first resolution, and a second orientation that is different from the first orientation to display the first frame of the video; and
displaying, on a display screen of the mobile device, a cropped first frame of the video in the second orientation;
receiving an indication of a change in orientation from the second orientation to the first orientation;
receiving, by the mobile device, a second frame of the video in the first orientation;
extracting, by a video preprocessor of the mobile device, an identification of a second region of a second frame of the video from the metadata;
cropping, by a video preprocessor of the mobile device, a second frame of the video to a second cropped size centered over the identified second region of the second frame, the second cropped size based on a second aspect ratio, a second resolution, and a first orientation to display the second frame of the video; and
displaying a cropped second frame of the video in the first orientation on a display screen of the mobile device.
2. The method of claim 1, wherein cropping a first frame of the video further comprises:
calculating a frame boundary based on the first aspect ratio, the resolution, and the second orientation for application to a first frame of the video; and
based on the calculated frame boundary, crop coordinates of a rectangle centered on the identified first region of the first frame of the video are identified.
3. The method of claim 2, wherein cropping a first frame of the video further comprises identifying cropping coordinates of a rectangle bounding all first regions.
4. The method of any of the preceding claims, wherein the region data is associated with a plurality of regions of a first frame of the video, and cropping the first frame of the video further comprises:
calculating a frame boundary based on the first aspect ratio, the resolution, and the second orientation for application to a first frame of the video; and is
Based on the calculated frame boundary, crop coordinates of a rectangle centered on one of a plurality of regions of a first frame of the video are identified.
5. The method of claim 4, further comprising:
determining that clipping coordinates of the rectangle are insufficient to define all of the plurality of regions; and is
Wherein identifying the cropping coordinates of the rectangle centered on one of the plurality of regions comprises including a text image within the cropping coordinates of the rectangle.
6. The method of any preceding claim, wherein the metadata comprises one or more regions of a first frame of the video having an associated score.
7. A method according to claim 6, wherein the or each score is associated with at least one feature of the respective region.
8. A method according to claim 7, wherein the or each score is a score generated based on one or more characteristics of the at least one feature.
9. A system, comprising:
one or more processors of a mobile device;
a network interface electrically connected to the one or more processors; and
computer storage electrically connected to the one or more processors and storing instructions that, when executed by the one or more processors, cause the one or more processors to perform a method according to any one of the preceding claims.
10. A non-transitory computer-readable storage medium storing instructions executable by one or more processors to perform the method of any one of claims 1-8.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2016/062606 WO2018093372A1 (en) | 2016-11-17 | 2016-11-17 | Media rendering with orientation metadata |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109690471A CN109690471A (en) | 2019-04-26 |
CN109690471B true CN109690471B (en) | 2022-05-31 |
Family
ID=57543165
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680088676.8A Active CN109690471B (en) | 2016-11-17 | 2016-11-17 | Media rendering using orientation metadata |
Country Status (4)
Country | Link |
---|---|
US (2) | US10885879B2 (en) |
EP (1) | EP3482286A1 (en) |
CN (1) | CN109690471B (en) |
WO (2) | WO2018093372A1 (en) |
Families Citing this family (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109690471B (en) | 2016-11-17 | 2022-05-31 | 谷歌有限责任公司 | Media rendering using orientation metadata |
KR102641163B1 (en) * | 2018-11-29 | 2024-02-28 | 삼성전자주식회사 | Image processing apparatus and image processing method thereof |
US11665312B1 (en) * | 2018-12-27 | 2023-05-30 | Snap Inc. | Video reformatting recommendation |
US10887542B1 (en) | 2018-12-27 | 2021-01-05 | Snap Inc. | Video reformatting system |
US10929979B1 (en) * | 2018-12-28 | 2021-02-23 | Facebook, Inc. | Systems and methods for processing content |
CN109857907B (en) * | 2019-02-25 | 2021-11-30 | 百度在线网络技术（北京）有限公司 | Video positioning method and device |
CN111062852B (en) * | 2019-12-16 | 2023-10-17 | 阿波罗智联(北京)科技有限公司 | Map rendering method and device, electronic equipment and storage medium |
CN112218160A (en) * | 2020-10-12 | 2021-01-12 | 北京达佳互联信息技术有限公司 | Video conversion method and device, video conversion equipment and storage medium |
CN112423021B (en) * | 2020-11-18 | 2022-12-06 | 北京有竹居网络技术有限公司 | Video processing method and device, readable medium and electronic equipment |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102428493A (en) * | 2009-05-20 | 2012-04-25 | 高通股份有限公司 | Method and apparatus for content boundary detection and scaling |
CN105144691A (en) * | 2013-03-08 | 2015-12-09 | 汤姆逊许可公司 | Method and system for stabilization and reframing |
CN105224165A (en) * | 2014-06-30 | 2016-01-06 | 英特尔公司 | For the fuzzy graph image of calculation element upper part promotes dynamic and effective pretrigger cutting |
CN105580013A (en) * | 2013-09-16 | 2016-05-11 | 汤姆逊许可公司 | Browsing videos by searching multiple user comments and overlaying those into the content |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2004084535A2 (en) * | 2003-03-14 | 2004-09-30 | Starz Encore Group Llc | Video aspect ratio manipulation |
US7760956B2 (en) * | 2005-05-12 | 2010-07-20 | Hewlett-Packard Development Company, L.P. | System and method for producing a page using frames of a video stream |
KR101464572B1 (en) * | 2008-03-20 | 2014-11-24 | 인스티튜트 퓌어 룬트퐁크테크닉 게엠베하 | A method of adapting video images to small screen sizes |
US20090295787A1 (en) * | 2008-06-02 | 2009-12-03 | Amlogic, Inc. | Methods for Displaying Objects of Interest on a Digital Display Device |
US20100110210A1 (en) * | 2008-11-06 | 2010-05-06 | Prentice Wayne E | Method and means of recording format independent cropping information |
US20100138775A1 (en) * | 2008-11-28 | 2010-06-03 | Sharon Kohen | Method, device and system, for extracting dynamic content from a running computer application |
CN102541494B (en) * | 2010-12-30 | 2016-01-06 | 中国科学院声学研究所 | A kind of video size converting system towards display terminal and method |
US20130069980A1 (en) * | 2011-09-15 | 2013-03-21 | Beau R. Hartshorne | Dynamically Cropping Images |
US9516229B2 (en) * | 2012-11-27 | 2016-12-06 | Qualcomm Incorporated | System and method for adjusting orientation of captured video |
US9741150B2 (en) * | 2013-07-25 | 2017-08-22 | Duelight Llc | Systems and methods for displaying representative images |
US10078917B1 (en) * | 2015-06-26 | 2018-09-18 | Lucasfilm Entertainment Company Ltd. | Augmented reality simulation |
US11282165B2 (en) * | 2016-02-26 | 2022-03-22 | Netflix, Inc. | Dynamically cropping digital content for display in any aspect ratio |
CN109690471B (en) | 2016-11-17 | 2022-05-31 | 谷歌有限责任公司 | Media rendering using orientation metadata |
WO2018106213A1 (en) * | 2016-12-05 | 2018-06-14 | Google Llc | Method for converting landscape video to portrait mobile layout |
-
2016
- 2016-11-17 CN CN201680088676.8A patent/CN109690471B/en active Active
- 2016-11-17 EP EP16810147.5A patent/EP3482286A1/en not_active Withdrawn
- 2016-11-17 WO PCT/US2016/062606 patent/WO2018093372A1/en unknown
-
2017
- 2017-11-16 US US16/325,366 patent/US10885879B2/en active Active
- 2017-11-16 WO PCT/US2017/062022 patent/WO2018094052A1/en active Application Filing
-
2021
- 2021-01-04 US US17/140,632 patent/US11322117B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102428493A (en) * | 2009-05-20 | 2012-04-25 | 高通股份有限公司 | Method and apparatus for content boundary detection and scaling |
CN105144691A (en) * | 2013-03-08 | 2015-12-09 | 汤姆逊许可公司 | Method and system for stabilization and reframing |
CN105580013A (en) * | 2013-09-16 | 2016-05-11 | 汤姆逊许可公司 | Browsing videos by searching multiple user comments and overlaying those into the content |
CN105224165A (en) * | 2014-06-30 | 2016-01-06 | 英特尔公司 | For the fuzzy graph image of calculation element upper part promotes dynamic and effective pretrigger cutting |
Also Published As
Publication number | Publication date |
---|---|
US20210125582A1 (en) | 2021-04-29 |
WO2018093372A1 (en) | 2018-05-24 |
WO2018094052A1 (en) | 2018-05-24 |
US20190266980A1 (en) | 2019-08-29 |
US10885879B2 (en) | 2021-01-05 |
EP3482286A1 (en) | 2019-05-15 |
US11322117B2 (en) | 2022-05-03 |
CN109690471A (en) | 2019-04-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109791600B (en) | Method for converting horizontal screen video into vertical screen mobile layout | |
CN109690471B (en) | Media rendering using orientation metadata | |
US11978238B2 (en) | Method for converting landscape video to portrait mobile layout using a selection interface | |
US10424341B2 (en) | Dynamic video summarization | |
US9436883B2 (en) | Collaborative text detection and recognition | |
US8933938B2 (en) | Generating simulated eye movement traces for visual displays | |
CA2918840C (en) | Presenting fixed format documents in reflowed format | |
KR20160132842A (en) | Detecting and extracting image document components to create flow document | |
US11875512B2 (en) | Attributionally robust training for weakly supervised localization and segmentation | |
CN112215171B (en) | Target detection method, device, equipment and computer readable storage medium | |
EP3175375A1 (en) | Image based search to identify objects in documents | |
KR20220097945A (en) | Non-Closed Video Overlays | |
US11526652B1 (en) | Automated optimization of displayed electronic content imagery | |
CN116719964A (en) | Picture generation method, electronic device and computer readable storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |