US20160283841A1 - Convolutional neural networks - Google Patents
Convolutional neural networks Download PDFInfo
- Publication number
- US20160283841A1 US20160283841A1 US14/805,704 US201514805704A US2016283841A1 US 20160283841 A1 US20160283841 A1 US 20160283841A1 US 201514805704 A US201514805704 A US 201514805704A US 2016283841 A1 US2016283841 A1 US 2016283841A1
- Authority
- US
- United States
- Prior art keywords
- neural network
- input values
- output
- filter
- cnn
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000013527 convolutional neural network Methods 0.000 title claims abstract description 178
- 238000001514 detection method Methods 0.000 claims abstract description 36
- 238000000034 method Methods 0.000 claims abstract description 35
- 238000012549 training Methods 0.000 claims abstract description 15
- 238000013528 artificial neural network Methods 0.000 claims description 31
- 238000011176 pooling Methods 0.000 claims description 27
- 230000009471 action Effects 0.000 claims description 21
- 230000005236 sound signal Effects 0.000 claims description 21
- 238000012545 processing Methods 0.000 claims description 20
- 239000011159 matrix material Substances 0.000 claims description 16
- 239000013598 vector Substances 0.000 claims description 14
- 238000004891 communication Methods 0.000 claims description 6
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000008569 process Effects 0.000 description 22
- 238000000605 extraction Methods 0.000 description 11
- 238000010586 diagram Methods 0.000 description 7
- 239000000945 filler Substances 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003044 adaptive effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000037433 frameshift Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
Definitions
- This specification generally relates to speech recognition systems.
- DNN deep neural networks
- a keyword detection system may use a convolutional neural network (CNN) for KWS, instead of a DNN, for small and large vocabulary tasks with fewer parameters, multiplication operations (multiplies), or both.
- CNN convolutional neural network
- a keyword detection system may use a CNN architecture that strides the filter in frequency to allow the system to remain within the computational constraints, e.g., to limit the number of multiplication operations used by the CNN to determine output.
- a keyword detection system may use a CNN architecture that pools in time to remain within computational constraints, e.g., to limit the number of parameters used by the CNN. For instance, when a keyword detection system is limited in a total number of parameters used, the keyword detection system may use a CNN and pool in time..
- a keyword detection system may use a CNN architecture that pools in time and frequency to improve KWS performance.
- the system may use a single CNN layer, e.g., a single convolutional block, when pooling in both time and frequency.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of training, by a keyword detection system, a convolutional neural network for keyword detection by providing a two-dimensional set of input values to the convolutional neural network, the input values including a first dimension in time and a second dimension in frequency, and performing convolutional multiplication on the two-dimensional set of input values for a filter using a frequency stride greater than one to generate a feature map.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- Performing convolutional multiplication on the two-dimensional set of input values for the filter using the frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride of four to generate the feature map.
- Performing convolutional multiplication on the two-dimensional set of input values for the filter using the frequency stride greater than one to generate the feature map may include performing convolutional multiplication across the entire first dimension in time on the two-dimensional set of input values for the filter.
- performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a time stride of one.
- Performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter, the filter comprising a frequency size of eight and a time size of thirty-two.
- Performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for n different filters using a frequency stride greater than one for each of the n different filters to generate n different feature maps, each of the feature maps generated using a corresponding filter.
- Performing convolutional multiplication on the two-dimensional set of input values for the n different filters using a frequency stride greater than one for each of the n different filters to generate the n different feature maps may include performing convolutional multiplication on the two-dimensional set of input values for the n different filters, each of the n different filters having a size that is the same as the sizes of the other filters.
- the convolutional neural network may be a layer in a neural network that includes a second, different convolutional neural network layer, a linear low rank layer, a deep neural network layer, and a softmax layer.
- Training, by the keyword detection system, a convolutional neural network for keyword detection may include generating, by the second, different convolutional neural network, second output using the feature map, generating, by the linear low rank layer, a third output using the second output, generating, by the deep neural network, a fourth output using the third output, and generating, by the softmax layer, a final output of the neural network using the fourth output.
- the feature map may be a matrix.
- the second output may include a matrix.
- Generating, by the linear low rank layer, a third output using the second output may include creating a vector from the second output, and generating the third output using the vector.
- the method may include updating the neural network using an accuracy of the final output.
- training, by the keyword detection system, a convolutional neural network for keyword detection may include updating a set of weight values for the filter using the feature map without performing a pooling operation.
- the method may include providing the convolutional neural network to a device for keyword detection.
- the method may include using the convolutional neural network for keyword detection by receiving an audio signal encoding an utterance, analyzing the audio signal to identify a command included in the utterance, and performing an action that corresponds to the command.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of training, by a keyword detection system, a convolutional neural network for keyword detection by providing a two-dimensional set of input values to the convolutional neural network, the input values including a first dimension in time and a second dimension in frequency, performing convolutional multiplication on the two-dimensional set of input values for a filter to generate a feature map, and determining a value for a region of the feature map, the region including a time pooling dimension greater than one.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- performing convolutional multiplication on the two-dimensional set of input values for the filter to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride of one and a time stride of one.
- Performing convolutional multiplication on the two-dimensional set of input values for the filter to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter, the filter comprising a frequency size of eight and a time size of twenty-one.
- Determining the value for the region of the feature map may include determining a value for a region of each of the feature maps, each of the regions having a time pooling dimension that is the same as the time pooling dimension of the other regions.
- the convolutional neural network may be a layer in a neural network that includes a second, different convolutional neural network layer, a linear low rank layer, a deep neural network layer, and a softmax layer.
- Training, by the keyword detection system, a convolutional neural network for keyword detection may include generating, by the second, different convolutional neural network, second output using the value, generating, by the linear low rank layer, a third output using the second output, generating, by the deep neural network, a fourth output using the third output, and generating, by the softmax layer, a final output of the neural network using the fourth output.
- a keyword spotting system that uses a CNN, e.g., to identify voice commands to wake up and have basic spoken interactions with a device, has a low latency, is power efficient, is flexible, e.g., a user experience designer or user can choose the keywords, is speaker adaptive, e.g., other speakers cannot activate the device, or two or more of these.
- a keyword spotting system that uses a CNN has a lower percentage of false alarms, false rejects, or both, compared to another keyword spotting system that only uses a DNN, e.g., when the CNN has either the same number of matrix multiplication operations or parameters as the DNN system.
- a keyword spotting system that uses a CNN has a small neural network model, e.g., less than about 250 kb, to allow the model to be stored in a small memory, e.g., a digital signal processor (DSP) memory.
- DSP digital signal processor
- a keyword spotting system that uses a CNN has a ten percent relative improvement over a different keyword spotting system that uses only a DNN, e.g., while fitting into the constraints of each system.
- FIG. 1 is a block diagram of an example keyword spotting system 100 that includes a convolutional neural network and a deep neural network.
- FIG. 2 represents an example CNN.
- FIGS. 3 and 4 are flow diagrams of processes for determining an output for a convolutional neural network.
- FIG. 5 is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this document.
- FIG. 1 is a block diagram of an example keyword spotting (KWS) system 100 that includes a convolutional neural network (CNN) and a deep neural network (DNS).
- the KWS system 100 may include a feature extraction module 110 , a CNN 120 , a DNN 140 , and a posterior handling module 160 .
- the KWS system 100 is trained to determine whether an audio signal encodes an utterance of a particular phrase, such as “answer call.”
- the KWS system 100 may send a signal to another system, e.g., an application executing on a device, such as a mobile device, indicating that the utterance “answer call” was identified.
- the application may perform an action, such as answering an incoming call on the mobile device.
- the KWS system 100 determines that an audio signal encodes an utterance for which the KWS system 100 is not trained, the KWS system 100 performs no action.
- the KWS system 100 may trigger or perform different actions for different keywords or key phrases for which the KWS system 100 is trained. For instance, when the KWS system 100 detects the utterance “answer call,” the KWS system 100 may trigger a phone application, e.g., to answer an incoming telephone call. When the KWS system 100 detects the utterance “give me directions to,” the KWS system 100 may cause a map application to launch.
- the feature extraction module 110 may compute dimensional log-mel filterbank features over a particular time period using a time frame shift. At every frame, the feature extraction module 110 may stack a particular number of frames to the left and different number of frames to the right. A single frame or a stack of frames is used as input to the CNN 120 .
- the CNN 120 receives the input and, as described in more detail below, generates feature maps as output.
- the CNN 120 or another component of the KWS system 100 , may convert the feature maps to a vector and provide the vector to a linear low-rank layer 130 .
- the linear low-rank layer 130 processes the vector and provides output to the DNN 140 .
- the linear low-rank layer 130 is a first layer in the DNN 140 .
- the DNN 140 may include one or more hidden layers, e.g., two or three hidden layers. Each hidden layer may include 128 hidden units. Each hidden layer in the DNN 140 may use a rectified linear unit (ReLU) nonlinearity or any other appropriate algorithm to compute output.
- the DNN 140 may process the output received from the linear low-rank layer 130 using the hidden layers and provide updated output to a softmax layer 150 . In some examples, the softmax layer 150 is part of the DNN 140 .
- the softmax layer 150 may contain one output target for each of the words in the keyword or key phrase to be detected, e.g., the phrase “answer call,” and a single additional output target which may represent all frames that do not belong to any of the words in the keyword, as represented as ‘filler’.
- the neural network weights, e.g., of the DNN 140 may be trained to optimize a cross-entropy criterion using distributed asynchronous gradient descent.
- the KWS system 100 detects multiple keywords or key phrases
- the KWS system 100 includes an output target for each word in each of the keywords and key phrases.
- the softmax layer 150 has six output targets, one for each of the words.
- the softmax layer 150 may optionally have an additional output target for filler, increasing the total number of output targets to seven.
- the DNN 140 generates an output, using the vector from the CNN 120 that was processed by the linear low-rank layer 130 , and provides the output to the softmax layer 150 .
- the softmax layer 150 processes the output and provides updated output to the posterior handling module 160 .
- the posterior handling module 160 may generate individual frame-level posterior scores using the output from the DNN 140 , e.g., from the softmax layer 150 .
- the posterior handling module 160 may combine the frame-level posterior scores into a single score corresponding to the key phrase. For example, the posterior handling module 160 may combine the scores from consecutive frames for “answer” and “call” to determine whether an audio signal encoded the utterance “answer call.”
- the CNN 120 may generate output using the corresponding frame from the feature extraction module 110 as input.
- the CNN 120 provides the output to the DNN 140 , which includes the linear low-rank layer 130 and the softmax layer 150 , and the DNN 140 generates second output and provides the second output to the posterior handling module 160 .
- the KWS system 100 may include multiple CNN layers in the CNN 120 , multiple DNN layers in the DNN 140 , or both.
- the KWS system 100 may include two CNN layers, between the feature extraction module 110 and the linear low-rank layer 130 .
- the KWS system 100 may include two DNN layers, between the linear low-rank layer 130 and the softmax layer 150 .
- each of these layers may have different attributes, e.g., a different number of multiplies, a different number of parameters, a different number of units in each layer, or two or more of these.
- a first DNN layer may have 4.1 K parameters and a second DNN layer may have 16.4K parameters.
- FIG. 2 represents an example CNN 200 , e.g., for a KWS system.
- the CNN 200 receives an input signal V ⁇ t ⁇ f , where t is the input feature dimension in time of the input signal and f is the input feature dimension in frequency of the input signal.
- the CNN 200 receives the input signal from a feature extraction module and uses the input signal to create a matrix of input values for an input layer 202 .
- the input signal includes frequency values across time.
- the CNN 200 may receive forty frequency value for each time frame and frequency values for thirty-two different time frames. The CNN 200 combines these values to create a matrix of input values for the input layer 202 .
- the CNN 200 includes a weight matrix W ⁇ (m ⁇ r) ⁇ n .
- the weight matrix W spans across a time-frequency area, of the input layer 202 , of size m ⁇ r, e.g., defined by a filter, where m ⁇ t and r ⁇ f.
- the CNN 200 convolves the time-frequency area of the input layer 202 , defined by the filter, with the weight matrix W to produce n feature maps 204 , each of which have a size that satisfies Equation 1 below.
- the CNN 200 uses the filter to stride the input layer 202 , e.g., to move the filter across the input layer 202 , by a non-zero amount s in time and v in frequency. For example, when the CNN 200 first convolves the weight matrix W with the input layer 202 , the CNN 200 selects an area, defined by the filter, starting at the coordinates 0,0 to determine a first value for a feature map. With a filter size of m ⁇ r, the area is defined by 0,0 and m,r. The CNN 200 may then select a second area, defined by the filter, starting at the coordinates m,0 and convolve the area defined by m,0 and 2m,r to determine a second value for the feature map. The CNN 200 may continue with this process to define all (t ⁇ m+1)/s ⁇ (f ⁇ r+1)/v values of the feature map.
- the CNN 200 uses weight sharing, e.g., to model local correlations in the input signal. For instance, the CNN 200 uses the same weights for each filter applied to the input layer 202 to generate the values for a single feature map. The CNN 200 may use a different set of weights to generate each of the features maps, while the same set of weights is used to generate each value in a particular feature map.
- the weight matrix W has n hidden units to allow the CNN 200 to generate n feature maps 204 .
- the CNN 200 uses each of the n hidden units, or n different filters, to determine different properties of the input layer. For example, one hidden unit may focus on tone and another hidden unit may focus on pitch when the input layer 202 models speech.
- the sizes of each of the n different filters, and the corresponding matrices in the weight matrix W are the same.
- each of the n feature maps 204 has the same size as the other feature maps 204 .
- the CNN 200 may use subsampling to pool values from a particular feature map with other values from the particular feature map, e.g., to reduce the time-frequency space. For instance, the CNN may generate a pooling layer 206 , e.g., a max-pooling layer, to remove variability in the time-frequency space, e.g., that exists due to different speaking styles, channel distortions, etc.
- a pooling layer 206 e.g., a max-pooling layer
- the CNN 200 may use a pooling size of p ⁇ q to create n reduced feature maps in the pooling layer 206 from the n feature maps 204 .
- each of the n reduced feature maps has a time-frequency space dimension that satisfies Equation 2 below.
- some of the n reduced feature maps have a different size.
- the CNN 200 may use the n reduced feature maps in the pooling layer 206 to determine properties of the input signal, or provide the n reduced features maps, or values from those features maps, to another neural network layer, to name a few examples.
- the CNN 200 may represent each of the n reduced features maps as a matrix.
- the CNN 200 may generate a vector from each matrix, by concatenating each of the rows in the matrix, e.g., so that a row representing a particular instance in time is concatenated with an adjacent row representing a subsequent instance in time, and provide the vector as input to another layer in a neural network.
- the CNN 200 may generate a vector for each of the n reduced features maps and concatenate each of those vectors to create a final vector and provide the final vector to another layer in a neural network, e.g., a DNN layer or a linear low-rank layer.
- a neural network e.g., a DNN layer or a linear low-rank layer.
- a convolutional architecture may include two convolutional layers, e.g., as part of a keyword spotting (KWS) system.
- the keyword spotting system with two convolutional layers may be less sensitive to the filter size in time compared to other keyword spotting systems.
- a typical architecture CNN architecture can be as shown in Table 1 as cnn-trad-fpool3.
- the cnn-trad-fpool3 architecture may have two CNN layers, one linear low-rank layer, one DNN layer, and one softmax layer.
- a KWS system may use a CNN architecture to limit the number of multiplies performed by the KWS system.
- the KWS system may provide the output of this CNN layer to a linear low-rank layer, which processes the output from the CNN.
- the KWS system then processes the output from the CNN with two DNN layers.
- Table 3 illustrates that striding the filter by v>1 the number of multiplies, e.g., multiplication operations performed by the CNN, is reduced. To maintain accuracy of the CNN, and the number of hidden units n in the CNN may be increased.
- a KWS system that includes a CNN architecture indicated in Table 3 may include a linear low-rank layer, one or more DNN layers, and a softmax layer.
- a KWS system may increase a number of CNN parameters when the number of CNN multiplies is fixed. For instance, a KWS system may limit a CNN model size to include at most 250K parameters and not limit the number of CNN multiplies.
- a KWS system may limit the number of parameters of a CNN model while increasing the number of feature maps to improve CNN performance.
- the KWS system may sample, e.g., pool, in time and frequency to increase feature maps while keeping parameters fixed.
- a KWS system may include a CNN architecture that strides a time filter in convolution by an amount of s>1.
- Table 4 shows three CNN architectures: cnn-tstride2, cnn-tstride4 and cnn-tstride8, that include changes to the time filter stride.
- the number of feature maps n in a CNN may be increased by increasing the time filter stride such that the number of parameters in the CNN remains constant.
- subsampling in time might not degrade performance, as increasing the number of feature maps may offset the change to the stride in time, e.g., and may improve performance.
- a CNN that pools in time may increase the number of feature maps n while keeping the total number of parameters for the CNN constant.
- FIG. 3 is a flow diagram of a process 300 for determining an output for a convolutional neural network (CNN).
- the process 300 can be used by the KWS system 100 or the CNN 200 or another system that includes the KWS system 100 , the CNN 200 , or both.
- the KWS system provides a two-dimensional set of input values to a convolutional neural network, the input values including a first dimension in time and a second dimension in frequency ( 302 ).
- the KWS system includes a feature extraction model that receives an audio signal encoding an utterance and generates frames representing features of the utterance at specific points in time.
- the feature extraction module provides multiple frames to the convolutional neural network. The number of frames the features extraction module sends to the convolutional neural network is determined by the first dimension in time.
- the KWS system performs convolutional multiplication on the two-dimensional set of input values for a filter using a frequency stride greater than one to generate a feature map ( 304 ).
- the CNN uses a filter with a frequency stride greater than one to generate the feature map.
- the CNN may use the same size filter and the same frequency stride to generate a plurality of feature maps while using different weights, or sets of weights, for each of the feature maps.
- Each of the feature maps may represent a different feature of the frames included in the set of input values.
- the KWS system updates a neural network that includes the convolutional neural network using an accuracy of a final output ( 306 ).
- the CNN provides an output to another layer in the KWS system.
- the output may contain values from the feature maps or values based on the feature maps.
- the other layer processes the output, potentially providing further output to additional layers in the KWS system.
- a final output of the layers e.g., determined by a softmax layer in the neural network, is provided to a posterior handling module and the posterior handling module determines a classification the utterance encoded in the audio signal.
- the posterior handling module may determine whether the utterance is a keyword, key phrase, or other content, e.g., filler.
- the KWS system determines an accuracy of the determination made by the posterior handling module and updates the neural network accordingly. For example, the KWS system may update one or more CNN layers included in the KWS system, one or more DNN layers included in the KWS system, or both. The KWS system may use any appropriate method to update the neural network and the layers included in the neural network.
- a system provides the convolutional neural network to a device for keyword detection ( 308 ). For instance, after training, a server may provide the convolutional neural network to a mobile device for use during keyword detection.
- the KWS system receives an audio signal encoding an utterance ( 310 ).
- a microphone included in the mobile device records the audio signal.
- the mobile device provides the audio signal to the KWS system, and the included CNN, to determine whether the audio signal encodes an utterance that includes a keyword or a key phrase.
- the KWS system analyzes the audio signal to identify a command included in the utterance ( 312 ). For instance, the KWS system uses the CNN to determine whether the utterance encoded in the audio signal includes a command, e.g., a keyword or a key phrase, or does not include a command. One example of a command includes “answer call.” For instance, the KWS system is trained to detect particular keywords, key phrases, or both, and classify any other words or phrases as filler by comparing frames from an utterance and corresponding audio features with representative audio features for the particular keywords and key phrases. The KWS system uses the CNN and potentially other layers in a neural network to compare the audio features for the utterance to the representative audio features for the particular keywords and key phrases.
- a system performs an action that corresponds to the command ( 314 ). For example, when the KWS system determines that the utterance is a command, e.g., the command “answer call,” the KWS system provides the mobile device with a message that indicates the command. The mobile device may provide the message to an application associated with the command, such as a phone application when the command is “answer call.” The application may then perform an action, such as connecting a telephone call, that corresponds to the command.
- an application associated with the command such as a phone application when the command is “answer call.”
- the application may then perform an action, such as connecting a telephone call, that corresponds to the command.
- the process 300 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps.
- the KWS system may perform steps 302 , 304 , and 306 , one or more times, without performing the other steps in the process 300 .
- a system may perform steps 310 , 312 , and 314 , or a subset of these steps, without performing the other steps in the process 300 .
- a mobile device may receive the CNN, receive an audio signal encoding an utterance, analyze the audio signal using the CNN, and other layers included in a KWS system, and determine that the utterance is not a keyword or a key phrase, and perform steps 310 and 312 again multiple times, e.g., for other utterances. The mobile device may then determine that another utterance is a keyword or a key phrase and perform an action in response to the determination.
- FIG. 4 is a flow diagram of a process 400 for determining an output for a convolutional neural network (CNN).
- CNN convolutional neural network
- the process 400 can be used by the CNN 200 .
- the CNN receives a two-dimensional set of input values, the input values including a first dimension in time and a second dimension in frequency ( 402 ).
- the KWS system includes a feature extraction model that receives an audio signal encoding an utterance and generates frames representing features of the utterance at specific points in time.
- the feature extraction module provides multiple frames, determined by the first dimension in time, to the CNN.
- the CNN performs convolutional multiplication on the two-dimensional set of input values for a filter to generate a feature map ( 404 ).
- the CNN uses a filter to generate the feature map.
- the CNN may use the same size filter and the same frequency stride and time stride to generate a plurality of feature maps while using different weights, or sets of weights, for each of the feature maps.
- Each of the feature maps may represent a different feature of the frames included in the set of input values, and the corresponding audio signal.
- the value for the region may be a maximum value, an average, or any other appropriate value for the region.
- the process 400 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps.
- the process 400 may include one or more of steps 306 through 314 .
- a KWS system may perform multiple iterations of steps 402 , 404 , 406 , and 306 during training of a CNN.
- a mobile device may perform steps 310 , 312 , and 314 one or more times using a CNN trained by the process 400 .
- the mobile device may receive the CNN, trained by the process 400 , from a server.
- the mobile device may use the CNN as part of a KWS system to determine whether utterances encoded in audio signals include keywords or key phrases for which the CNN, and the KWS system, were trained.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be or further include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received from the user device at the server.
- FIG. 5 shows a schematic diagram of a generic computer system 500 .
- the system 500 can be used for the operations described in association with any of the computer-implement methods described previously, according to one implementation.
- the system 500 includes a processor 510 , a memory 520 , a storage device 530 , and an input/output device 540 .
- Each of the components 510 , 520 , 530 , and 540 are interconnected using a system bus 550 .
- the processor 510 is capable of processing instructions for execution within the system 500 .
- the processor 510 is a single-threaded processor.
- the processor 510 is a multi-threaded processor.
- the processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530 to display graphical information for a user interface on the input/output device 540 .
- the memory 520 stores information within the system 500 .
- the memory 520 is a computer-readable medium.
- the memory 520 is a volatile memory unit.
- the memory 520 is a non-volatile memory unit.
- the storage device 530 is capable of providing mass storage for the system 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device.
- the input/output device 540 provides input/output operations for the system 500 .
- the input/output device 540 includes a keyboard and/or pointing device.
- the input/output device 540 includes a display unit for displaying graphical user interfaces.
Abstract
Description
- This application claims priority to U.S. Provisional Application No. 62/139,139, filed on Mar. 27, 2015, the contents of which are incorporated herein by reference.
- This specification generally relates to speech recognition systems.
- With the development of mobile devices, some speech-related technologies are becoming popular. For example, search by voice services may utilize speech recognition to interact with mobile devices. Some existing keyword spotting (KWS) systems on mobile devices use deep neural networks (DNN), which are trained to predict sub-keyword targets. For instance, DNNs may be used for keyword spotting since DNN models may be adjusted by changing the number of parameters in the networks.
- In some implementations, a keyword detection system may use a convolutional neural network (CNN) for KWS, instead of a DNN, for small and large vocabulary tasks with fewer parameters, multiplication operations (multiplies), or both. In some implementations, a keyword detection system may use a CNN architecture that strides the filter in frequency to allow the system to remain within the computational constraints, e.g., to limit the number of multiplication operations used by the CNN to determine output. In some implementations, a keyword detection system may use a CNN architecture that pools in time to remain within computational constraints, e.g., to limit the number of parameters used by the CNN. For instance, when a keyword detection system is limited in a total number of parameters used, the keyword detection system may use a CNN and pool in time.. In some implementations, a keyword detection system may use a CNN architecture that pools in time and frequency to improve KWS performance. The system may use a single CNN layer, e.g., a single convolutional block, when pooling in both time and frequency.
- In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of training, by a keyword detection system, a convolutional neural network for keyword detection by providing a two-dimensional set of input values to the convolutional neural network, the input values including a first dimension in time and a second dimension in frequency, and performing convolutional multiplication on the two-dimensional set of input values for a filter using a frequency stride greater than one to generate a feature map. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination. Performing convolutional multiplication on the two-dimensional set of input values for the filter using the frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride of four to generate the feature map. Performing convolutional multiplication on the two-dimensional set of input values for the filter using the frequency stride greater than one to generate the feature map may include performing convolutional multiplication across the entire first dimension in time on the two-dimensional set of input values for the filter.
- In some implementations, performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a time stride of one. Performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter, the filter comprising a frequency size of eight and a time size of thirty-two. Performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride greater than one to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for n different filters using a frequency stride greater than one for each of the n different filters to generate n different feature maps, each of the feature maps generated using a corresponding filter. Performing convolutional multiplication on the two-dimensional set of input values for the n different filters using a frequency stride greater than one for each of the n different filters to generate the n different feature maps may include performing convolutional multiplication on the two-dimensional set of input values for the n different filters, each of the n different filters having a size that is the same as the sizes of the other filters.
- In some implementations, the convolutional neural network may be a layer in a neural network that includes a second, different convolutional neural network layer, a linear low rank layer, a deep neural network layer, and a softmax layer. Training, by the keyword detection system, a convolutional neural network for keyword detection may include generating, by the second, different convolutional neural network, second output using the feature map, generating, by the linear low rank layer, a third output using the second output, generating, by the deep neural network, a fourth output using the third output, and generating, by the softmax layer, a final output of the neural network using the fourth output. The feature map may be a matrix. The second output may include a matrix. Generating, by the linear low rank layer, a third output using the second output may include creating a vector from the second output, and generating the third output using the vector. The method may include updating the neural network using an accuracy of the final output.
- In some implementations, training, by the keyword detection system, a convolutional neural network for keyword detection may include updating a set of weight values for the filter using the feature map without performing a pooling operation. The method may include providing the convolutional neural network to a device for keyword detection. The method may include using the convolutional neural network for keyword detection by receiving an audio signal encoding an utterance, analyzing the audio signal to identify a command included in the utterance, and performing an action that corresponds to the command.
- In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of training, by a keyword detection system, a convolutional neural network for keyword detection by providing a two-dimensional set of input values to the convolutional neural network, the input values including a first dimension in time and a second dimension in frequency, performing convolutional multiplication on the two-dimensional set of input values for a filter to generate a feature map, and determining a value for a region of the feature map, the region including a time pooling dimension greater than one. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination. Determining the value for the region of the feature map, the region including a time pooling dimension greater than one may include determining the value for the region of the feature map, the region including a time pooling dimension of two. Determining the value for the region of the feature map may include determining the value for the region, the region including a frequency pooling dimension of three. Determining the value for the region of the feature map may include determining a maximum value for the region.
- In some implementations, performing convolutional multiplication on the two-dimensional set of input values for the filter to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter using a frequency stride of one and a time stride of one. Performing convolutional multiplication on the two-dimensional set of input values for the filter to generate the feature map may include performing convolutional multiplication on the two-dimensional set of input values for the filter, the filter comprising a frequency size of eight and a time size of twenty-one. Determining the value for the region of the feature map may include determining a value for a region of each of the feature maps, each of the regions having a time pooling dimension that is the same as the time pooling dimension of the other regions. The convolutional neural network may be a layer in a neural network that includes a second, different convolutional neural network layer, a linear low rank layer, a deep neural network layer, and a softmax layer. Training, by the keyword detection system, a convolutional neural network for keyword detection may include generating, by the second, different convolutional neural network, second output using the value, generating, by the linear low rank layer, a third output using the second output, generating, by the deep neural network, a fourth output using the third output, and generating, by the softmax layer, a final output of the neural network using the fourth output.
- The subject matter described in this specification can be implemented in particular embodiments and may result in one or more of the following advantages. In some implementations, a keyword spotting system that uses a CNN, e.g., to identify voice commands to wake up and have basic spoken interactions with a device, has a low latency, is power efficient, is flexible, e.g., a user experience designer or user can choose the keywords, is speaker adaptive, e.g., other speakers cannot activate the device, or two or more of these. In some implementations, a keyword spotting system that uses a CNN has a lower percentage of false alarms, false rejects, or both, compared to another keyword spotting system that only uses a DNN, e.g., when the CNN has either the same number of matrix multiplication operations or parameters as the DNN system. In some implementations, a keyword spotting system that uses a CNN has a small neural network model, e.g., less than about 250 kb, to allow the model to be stored in a small memory, e.g., a digital signal processor (DSP) memory. In some implementations, a keyword spotting system that uses a CNN has a ten percent relative improvement over a different keyword spotting system that uses only a DNN, e.g., while fitting into the constraints of each system.
- The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 is a block diagram of an examplekeyword spotting system 100 that includes a convolutional neural network and a deep neural network. -
FIG. 2 represents an example CNN. -
FIGS. 3 and 4 are flow diagrams of processes for determining an output for a convolutional neural network. -
FIG. 5 is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this document. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 is a block diagram of an example keyword spotting (KWS)system 100 that includes a convolutional neural network (CNN) and a deep neural network (DNS). In general, the KWSsystem 100 may include afeature extraction module 110, a CNN 120, a DNN 140, and aposterior handling module 160. - The KWS
system 100 is trained to determine whether an audio signal encodes an utterance of a particular phrase, such as “answer call.” When theKWS system 100 determines that an audio signal encodes the utterance “answer call,” theKWS system 100 may send a signal to another system, e.g., an application executing on a device, such as a mobile device, indicating that the utterance “answer call” was identified. In response, the application may perform an action, such as answering an incoming call on the mobile device. When theKWS system 100 determines that an audio signal encodes an utterance for which theKWS system 100 is not trained, theKWS system 100 performs no action. - The
KWS system 100 may trigger or perform different actions for different keywords or key phrases for which theKWS system 100 is trained. For instance, when theKWS system 100 detects the utterance “answer call,” theKWS system 100 may trigger a phone application, e.g., to answer an incoming telephone call. When theKWS system 100 detects the utterance “give me directions to,” theKWS system 100 may cause a map application to launch. - The
feature extraction module 110 may compute dimensional log-mel filterbank features over a particular time period using a time frame shift. At every frame, thefeature extraction module 110 may stack a particular number of frames to the left and different number of frames to the right. A single frame or a stack of frames is used as input to theCNN 120. - The
CNN 120 receives the input and, as described in more detail below, generates feature maps as output. TheCNN 120, or another component of theKWS system 100, may convert the feature maps to a vector and provide the vector to a linear low-rank layer 130. The linear low-rank layer 130 processes the vector and provides output to theDNN 140. In some examples, the linear low-rank layer 130 is a first layer in theDNN 140. - The
DNN 140 may include one or more hidden layers, e.g., two or three hidden layers. Each hidden layer may include 128 hidden units. Each hidden layer in theDNN 140 may use a rectified linear unit (ReLU) nonlinearity or any other appropriate algorithm to compute output. TheDNN 140 may process the output received from the linear low-rank layer 130 using the hidden layers and provide updated output to asoftmax layer 150. In some examples, thesoftmax layer 150 is part of theDNN 140. - The
softmax layer 150 may contain one output target for each of the words in the keyword or key phrase to be detected, e.g., the phrase “answer call,” and a single additional output target which may represent all frames that do not belong to any of the words in the keyword, as represented as ‘filler’. The neural network weights, e.g., of theDNN 140, may be trained to optimize a cross-entropy criterion using distributed asynchronous gradient descent. - In examples when the
KWS system 100 detects multiple keywords or key phrases, theKWS system 100 includes an output target for each word in each of the keywords and key phrases. For example, when theKWS system 100 is trained to detect only the key phrases “answer call” and “give me directions to,” thesoftmax layer 150 has six output targets, one for each of the words. Thesoftmax layer 150 may optionally have an additional output target for filler, increasing the total number of output targets to seven. - The
DNN 140 generates an output, using the vector from theCNN 120 that was processed by the linear low-rank layer 130, and provides the output to thesoftmax layer 150. Thesoftmax layer 150 processes the output and provides updated output to theposterior handling module 160. - The
posterior handling module 160 may generate individual frame-level posterior scores using the output from theDNN 140, e.g., from thesoftmax layer 150. Theposterior handling module 160 may combine the frame-level posterior scores into a single score corresponding to the key phrase. For example, theposterior handling module 160 may combine the scores from consecutive frames for “answer” and “call” to determine whether an audio signal encoded the utterance “answer call.” - For instance, for each frame, the
CNN 120 may generate output using the corresponding frame from thefeature extraction module 110 as input. TheCNN 120 provides the output to theDNN 140, which includes the linear low-rank layer 130 and thesoftmax layer 150, and theDNN 140 generates second output and provides the second output to theposterior handling module 160. - The
KWS system 100 may include multiple CNN layers in theCNN 120, multiple DNN layers in theDNN 140, or both. For instance, theKWS system 100 may include two CNN layers, between thefeature extraction module 110 and the linear low-rank layer 130. In some examples, theKWS system 100 may include two DNN layers, between the linear low-rank layer 130 and thesoftmax layer 150. - When the
KWS system 100 includes multiple layers of a particular type, each of these layers may have different attributes, e.g., a different number of multiplies, a different number of parameters, a different number of units in each layer, or two or more of these. For example, a first DNN layer may have 4.1 K parameters and a second DNN layer may have 16.4K parameters. -
FIG. 2 represents anexample CNN 200, e.g., for a KWS system. TheCNN 200 receives an input signal V∈CNN 200 receives the input signal from a feature extraction module and uses the input signal to create a matrix of input values for aninput layer 202. - The input signal includes frequency values across time. For instance, the
CNN 200 may receive forty frequency value for each time frame and frequency values for thirty-two different time frames. TheCNN 200 combines these values to create a matrix of input values for theinput layer 202. - The
CNN 200 includes a weight matrix W∈input layer 202, of size m×r, e.g., defined by a filter, where m≦t and r≦f. TheCNN 200 convolves the time-frequency area of theinput layer 202, defined by the filter, with the weight matrix W to produce n feature maps 204, each of which have a size that satisfiesEquation 1 below. -
- The
CNN 200 uses the filter to stride theinput layer 202, e.g., to move the filter across theinput layer 202, by a non-zero amount s in time and v in frequency. For example, when theCNN 200 first convolves the weight matrix W with theinput layer 202, theCNN 200 selects an area, defined by the filter, starting at thecoordinates CNN 200 may then select a second area, defined by the filter, starting at the coordinates m,0 and convolve the area defined by m,0 and 2m,r to determine a second value for the feature map. TheCNN 200 may continue with this process to define all (t−m+1)/s×(f−r+1)/v values of the feature map. - The
CNN 200 uses weight sharing, e.g., to model local correlations in the input signal. For instance, theCNN 200 uses the same weights for each filter applied to theinput layer 202 to generate the values for a single feature map. TheCNN 200 may use a different set of weights to generate each of the features maps, while the same set of weights is used to generate each value in a particular feature map. - The weight matrix W has n hidden units to allow the
CNN 200 to generate n feature maps 204. For instance, theCNN 200 uses each of the n hidden units, or n different filters, to determine different properties of the input layer. For example, one hidden unit may focus on tone and another hidden unit may focus on pitch when theinput layer 202 models speech. In some examples, the sizes of each of the n different filters, and the corresponding matrices in the weight matrix W, are the same. In some examples, each of the n feature maps 204 has the same size as the other feature maps 204. - After performing convolution for each of the n hidden units to generate the n feature maps 204, the
CNN 200 may use subsampling to pool values from a particular feature map with other values from the particular feature map, e.g., to reduce the time-frequency space. For instance, the CNN may generate apooling layer 206, e.g., a max-pooling layer, to remove variability in the time-frequency space, e.g., that exists due to different speaking styles, channel distortions, etc. - The
CNN 200 may use a pooling size of p×q to create n reduced feature maps in thepooling layer 206 from the n feature maps 204. In some examples, each of the n reduced feature maps has a time-frequency space dimension that satisfies Equation 2 below. In some examples, some of the n reduced feature maps have a different size. -
- The
CNN 200 may use the n reduced feature maps in thepooling layer 206 to determine properties of the input signal, or provide the n reduced features maps, or values from those features maps, to another neural network layer, to name a few examples. For instance, theCNN 200 may represent each of the n reduced features maps as a matrix. TheCNN 200 may generate a vector from each matrix, by concatenating each of the rows in the matrix, e.g., so that a row representing a particular instance in time is concatenated with an adjacent row representing a subsequent instance in time, and provide the vector as input to another layer in a neural network. In some examples, theCNN 200 may generate a vector for each of the n reduced features maps and concatenate each of those vectors to create a final vector and provide the final vector to another layer in a neural network, e.g., a DNN layer or a linear low-rank layer. - In some implementations, a convolutional architecture may include two convolutional layers, e.g., as part of a keyword spotting (KWS) system. For example, if the log-mel input into the CNN is t×f=32×40, e.g., the
input layer 202 has t=30 and f=40, then a first CNN layer may have a filter size in frequency of r=9 and a filter size in time of m=20, e.g., a filter size in time which spans two thirds of the overall input size in time. For instance, the keyword spotting system with two convolutional layers may be less sensitive to the filter size in time compared to other keyword spotting systems. - The first CNN layer may perform convolutional multiplication by striding the filter by s=1 and v=1 across both time and frequency. In some examples, the first CNN layer may perform non-overlapping max-pooling in frequency, e.g., with a pooling region of q=3. The second CNN layer may have a filter size of r=4 in frequency and m=10 in time. The second CNN layer might not use max-pooling, or any pooling.
- In some examples, when the number of parameters for a keyword spotting system must be below 250K, a typical architecture CNN architecture can be as shown in Table 1 as cnn-trad-fpool3. The cnn-trad-fpool3 architecture may have two CNN layers, one linear low-rank layer, one DNN layer, and one softmax layer.
-
TABLE 1 CNN Architecture for cnn-trad-fpool3 Type m r n p q Parameters Multiplies CNN 20 8 64 1 3 10.2K 4.4M CNN 10 4 64 1 1 164.8K 5.2M Linear — — 32 — — 65.5K 65.5K DNN — — 128 — — 4.1K 4.1K Softmax — — 4 — — 0.5K 0.5K Total — — — — — 244.2K 9.7M - In some implementations, a KWS system may use a CNN architecture to limit the number of multiplies performed by the KWS system. For example, the CNN architecture may have one convolutional layer and have the time filter span all of time, e.g., m=32. The KWS system may provide the output of this CNN layer to a linear low-rank layer, which processes the output from the CNN. The KWS system then processes the output from the CNN with two DNN layers. Table 2 includes one example of the KWS system with a CNN layer, cnn-one-fpool3. The filter sizes s=1 and v=1 for CNN layer are omitted from Table 2.
-
TABLE 2 CNN Architecture for cnn-one-fpool3 Type m r n p q Parameters Multiplies CNN 32 8 54 1 3 13.8K 4.6K Linear — — 32 — — 19.8K 19.8K DNN — — 128 — — 4.1K 4.1K DNN — — 128 — — 16.4K 16.4K Softmax — — 4 — — 0.5K 0.5K Total — — 4 — — 53.8K 5.0K - In some implementations, when a KWS system pools in frequency q=3, the KWS system strides the filter by v=1. In some examples, this may increase multiplies, e.g., multiplication operations performed by the KWS system. To reduce the number of multiplies, a KWS system may stride the filter by v>1. Table 3 represents example CNN architectures, cnn-one-fstride4 and cnn-one-fstride8, which use a frequency filters of size r=8 and stride the filter by v=4, e.g., 50% overlap between adjacent filters, and by v=8, e.g., no overlap between adjacent filters, respectively. Table 3 illustrates that striding the filter by v>1 the number of multiplies, e.g., multiplication operations performed by the CNN, is reduced. To maintain accuracy of the CNN, and the number of hidden units n in the CNN may be increased. A KWS system that includes a CNN architecture indicated in Table 3 may include a linear low-rank layer, one or more DNN layers, and a softmax layer.
-
TABLE 3 CNN Architecture for (a) cnn-one-fstride4 and (b) cnn-one-fstride8 Model m r n s v Parameters Multiplies CNN (a) 32 8 186 1 4 47.6K 4.3K CNN (b) 32 8 336 1 8 86.6K 4.3K - In some implementations, a KWS system may increase a number of CNN parameters when the number of CNN multiplies is fixed. For instance, a KWS system may limit a CNN model size to include at most 250K parameters and not limit the number of CNN multiplies.
- In some examples, a KWS system may limit the number of parameters of a CNN model while increasing the number of feature maps to improve CNN performance. In these examples, the KWS system may sample, e.g., pool, in time and frequency to increase feature maps while keeping parameters fixed.
- In some implementations, a KWS system may include a CNN architecture that strides a time filter in convolution by an amount of s>1. For example, Table 4 shows three CNN architectures: cnn-tstride2, cnn-tstride4 and cnn-tstride8, that include changes to the time filter stride. The number of feature maps n in a CNN may be increased by increasing the time filter stride such that the number of parameters in the CNN remains constant. Here the frequency stride v=1 and the pool in time p=1. In some implementations, subsampling in time might not degrade performance, as increasing the number of feature maps may offset the change to the stride in time, e.g., and may improve performance.
-
TABLE 4 CNN Architectures for striding in time Model Type m r n s q Parameters cnn- CNN 16 8 78 2 3 10.0K stride2 CNN 9 4 78 1 1 219.0K Linear — — 32 — — 20.0K cnn- CNN 16 8 100 4 3 12.8K stride4 CNN 5 4 78 1 1 200.0K Linear — — 32 — — 25.6K cnn- CNN 16 8 126 8 3 16.1K stride8 CNN 5 4 78 1 1 190.5K Linear — — 32 — — 32.2K - In some implementations, a KWS system may use a CNN that pools, e.g., samples, in time, by a non-overlapping amount. For instance, the CNN may pool in time by p=2 or p=4. The CNN may have a frequency stride of v=1, a time stride of s=1, or both. Table 5 represents configurations of CNN architectures, cnn-tpool2 and cnn-tpool3, where the pooling in time p is varied, e.g., where p=2 and p=3 respectively. In some examples, a CNN that pools in time may increase the number of feature maps n while keeping the total number of parameters for the CNN constant.
-
TABLE 5 CNN Architectures for pooling in time Model Type m r n p q Parameters cnn- CNN 21 8 94 2 3 5.6M tpool2 CNN 6 4 94 1 1 1.8M Linear — — 32 — — 65.5K cnn- CNN 15 8 94 3 3 7.1M tpool3 CNN 6 4 94 1 1 1.6M Linear — — 32 — — 65.5K -
FIG. 3 is a flow diagram of a process 300 for determining an output for a convolutional neural network (CNN). For example, the process 300 can be used by theKWS system 100 or theCNN 200 or another system that includes theKWS system 100, theCNN 200, or both. - The KWS system provides a two-dimensional set of input values to a convolutional neural network, the input values including a first dimension in time and a second dimension in frequency (302). For example, the KWS system includes a feature extraction model that receives an audio signal encoding an utterance and generates frames representing features of the utterance at specific points in time. The feature extraction module provides multiple frames to the convolutional neural network. The number of frames the features extraction module sends to the convolutional neural network is determined by the first dimension in time.
- The KWS system performs convolutional multiplication on the two-dimensional set of input values for a filter using a frequency stride greater than one to generate a feature map (304). For instance, the CNN uses a filter with a frequency stride greater than one to generate the feature map. The CNN may use the same size filter and the same frequency stride to generate a plurality of feature maps while using different weights, or sets of weights, for each of the feature maps. Each of the feature maps may represent a different feature of the frames included in the set of input values.
- The KWS system updates a neural network that includes the convolutional neural network using an accuracy of a final output (306). For example, the CNN provides an output to another layer in the KWS system. The output may contain values from the feature maps or values based on the feature maps. The other layer processes the output, potentially providing further output to additional layers in the KWS system. A final output of the layers, e.g., determined by a softmax layer in the neural network, is provided to a posterior handling module and the posterior handling module determines a classification the utterance encoded in the audio signal. For instance, the posterior handling module may determine whether the utterance is a keyword, key phrase, or other content, e.g., filler.
- During training, the KWS system determines an accuracy of the determination made by the posterior handling module and updates the neural network accordingly. For example, the KWS system may update one or more CNN layers included in the KWS system, one or more DNN layers included in the KWS system, or both. The KWS system may use any appropriate method to update the neural network and the layers included in the neural network.
- A system provides the convolutional neural network to a device for keyword detection (308). For instance, after training, a server may provide the convolutional neural network to a mobile device for use during keyword detection.
- The KWS system receives an audio signal encoding an utterance (310). For example, a microphone included in the mobile device records the audio signal. The mobile device provides the audio signal to the KWS system, and the included CNN, to determine whether the audio signal encodes an utterance that includes a keyword or a key phrase.
- The KWS system analyzes the audio signal to identify a command included in the utterance (312). For instance, the KWS system uses the CNN to determine whether the utterance encoded in the audio signal includes a command, e.g., a keyword or a key phrase, or does not include a command. One example of a command includes “answer call.” For instance, the KWS system is trained to detect particular keywords, key phrases, or both, and classify any other words or phrases as filler by comparing frames from an utterance and corresponding audio features with representative audio features for the particular keywords and key phrases. The KWS system uses the CNN and potentially other layers in a neural network to compare the audio features for the utterance to the representative audio features for the particular keywords and key phrases.
- A system performs an action that corresponds to the command (314). For example, when the KWS system determines that the utterance is a command, e.g., the command “answer call,” the KWS system provides the mobile device with a message that indicates the command. The mobile device may provide the message to an application associated with the command, such as a phone application when the command is “answer call.” The application may then perform an action, such as connecting a telephone call, that corresponds to the command.
- In some implementations, the process 300 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps. For example, the KWS system may perform
steps - In some examples, a system may perform
steps steps -
FIG. 4 is a flow diagram of aprocess 400 for determining an output for a convolutional neural network (CNN). For example, theprocess 400 can be used by theCNN 200. - The CNN receives a two-dimensional set of input values, the input values including a first dimension in time and a second dimension in frequency (402). For example, the KWS system includes a feature extraction model that receives an audio signal encoding an utterance and generates frames representing features of the utterance at specific points in time. The feature extraction module provides multiple frames, determined by the first dimension in time, to the CNN.
- The CNN performs convolutional multiplication on the two-dimensional set of input values for a filter to generate a feature map (404). For instance, the CNN uses a filter to generate the feature map. The filter may have a time dimension of m=21, a frequency dimension of r=8, or both. The CNN may use the same size filter and the same frequency stride and time stride to generate a plurality of feature maps while using different weights, or sets of weights, for each of the feature maps. Each of the feature maps may represent a different feature of the frames included in the set of input values, and the corresponding audio signal.
- The CNN determines a value for a region of the feature map, the region including a time pooling dimension greater than one (406). For instance, the CNN may determine a region in a feature map with a time pooling dimension of p=2, a frequency pooling dimension of q=3, or both. The value for the region may be a maximum value, an average, or any other appropriate value for the region.
- In some implementations, the
process 400 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps. For example, theprocess 400 may include one or more ofsteps 306 through 314. In some examples, a KWS system may perform multiple iterations ofsteps - In some implementations, a mobile device may perform
steps process 400. For instance, the mobile device may receive the CNN, trained by theprocess 400, from a server. The mobile device may use the CNN as part of a KWS system to determine whether utterances encoded in audio signals include keywords or key phrases for which the CNN, and the KWS system, were trained. - Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received from the user device at the server.
- An example of one such type of computer is shown in
FIG. 5 , which shows a schematic diagram of a generic computer system 500. The system 500 can be used for the operations described in association with any of the computer-implement methods described previously, according to one implementation. The system 500 includes aprocessor 510, amemory 520, astorage device 530, and an input/output device 540. Each of thecomponents system bus 550. Theprocessor 510 is capable of processing instructions for execution within the system 500. In one implementation, theprocessor 510 is a single-threaded processor. In another implementation, theprocessor 510 is a multi-threaded processor. Theprocessor 510 is capable of processing instructions stored in thememory 520 or on thestorage device 530 to display graphical information for a user interface on the input/output device 540. - The
memory 520 stores information within the system 500. In one implementation, thememory 520 is a computer-readable medium. In one implementation, thememory 520 is a volatile memory unit. In another implementation, thememory 520 is a non-volatile memory unit. - The
storage device 530 is capable of providing mass storage for the system 500. In one implementation, thestorage device 530 is a computer-readable medium. In various different implementations, thestorage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device. - The input/output device 540 provides input/output operations for the system 500. In one implementation, the input/output device 540 includes a keyboard and/or pointing device. In another implementation, the input/output device 540 includes a display unit for displaying graphical user interfaces.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments.
- Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/805,704 US10762894B2 (en) | 2015-03-27 | 2015-07-22 | Convolutional neural networks |
US16/654,041 US20200051551A1 (en) | 2015-03-27 | 2019-10-16 | Convolutional neural networks |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562139139P | 2015-03-27 | 2015-03-27 | |
US14/805,704 US10762894B2 (en) | 2015-03-27 | 2015-07-22 | Convolutional neural networks |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/654,041 Continuation US20200051551A1 (en) | 2015-03-27 | 2019-10-16 | Convolutional neural networks |
Publications (2)
Publication Number | Publication Date |
---|---|
US20160283841A1 true US20160283841A1 (en) | 2016-09-29 |
US10762894B2 US10762894B2 (en) | 2020-09-01 |
Family
ID=56975462
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/805,704 Active 2038-03-15 US10762894B2 (en) | 2015-03-27 | 2015-07-22 | Convolutional neural networks |
US16/654,041 Pending US20200051551A1 (en) | 2015-03-27 | 2019-10-16 | Convolutional neural networks |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/654,041 Pending US20200051551A1 (en) | 2015-03-27 | 2019-10-16 | Convolutional neural networks |
Country Status (1)
Country | Link |
---|---|
US (2) | US10762894B2 (en) |
Cited By (94)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107203134A (en) * | 2017-06-02 | 2017-09-26 | 浙江零跑科技有限公司 | A kind of front truck follower method based on depth convolutional neural networks |
CN107437110A (en) * | 2017-07-11 | 2017-12-05 | 中国科学院自动化研究所 | The piecemeal convolution optimization method and device of convolutional neural networks |
US20180018539A1 (en) * | 2016-07-12 | 2018-01-18 | Beihang University | Ranking convolutional neural network constructing method and image processing method and apparatus thereof |
CN107844795A (en) * | 2017-11-18 | 2018-03-27 | 中国人民解放军陆军工程大学 | Convolutional neural networks feature extracting method based on principal component analysis |
CN107993651A (en) * | 2017-12-29 | 2018-05-04 | 深圳和而泰数据资源与云技术有限公司 | A kind of audio recognition method, device, electronic equipment and storage medium |
CN108010514A (en) * | 2017-11-20 | 2018-05-08 | 四川大学 | A kind of method of speech classification based on deep neural network |
CN108010515A (en) * | 2017-11-21 | 2018-05-08 | 清华大学 | A kind of speech terminals detection and awakening method and device |
CN108073983A (en) * | 2016-11-10 | 2018-05-25 | 谷歌有限责任公司 | Core is performed within hardware to cross over |
CN108304798A (en) * | 2018-01-30 | 2018-07-20 | 北京同方软件股份有限公司 | The event video detecting method of order in the street based on deep learning and Movement consistency |
CN108460772A (en) * | 2018-02-13 | 2018-08-28 | 国家计算机网络与信息安全管理中心 | Harassing of advertisement facsimile signal detecting system based on convolutional neural networks and method |
WO2018174438A1 (en) * | 2017-03-23 | 2018-09-27 | Samsung Electronics Co., Ltd. | Electronic apparatus for operating machine learning and method for operating machine learning |
CN108833720A (en) * | 2018-05-04 | 2018-11-16 | 北京邮电大学 | Fraudulent call number identification method and system |
US20190080683A1 (en) * | 2017-09-11 | 2019-03-14 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for recognizing text segmentation position |
US10304440B1 (en) * | 2015-07-10 | 2019-05-28 | Amazon Technologies, Inc. | Keyword spotting using multi-task configuration |
US10460722B1 (en) * | 2017-06-30 | 2019-10-29 | Amazon Technologies, Inc. | Acoustic trigger detection |
US10460729B1 (en) * | 2017-06-30 | 2019-10-29 | Amazon Technologies, Inc. | Binary target acoustic trigger detecton |
CN110595811A (en) * | 2019-09-11 | 2019-12-20 | 浙江工业大学之江学院 | Method for constructing health state characteristic diagram of mechanical equipment |
WO2020010639A1 (en) * | 2018-07-13 | 2020-01-16 | 华为技术有限公司 | Convolution method and device for neural network |
US10579924B1 (en) * | 2018-09-17 | 2020-03-03 | StradVision, Inc. | Learning method, learning device with multi-feeding layers and testing method, testing device using the same |
CN111357018A (en) * | 2017-11-20 | 2020-06-30 | 谷歌有限责任公司 | Image segmentation using neural networks |
CN111737193A (en) * | 2020-08-03 | 2020-10-02 | 深圳鲲云信息科技有限公司 | Data storage method, device, equipment and storage medium |
EP3726856A1 (en) * | 2019-04-17 | 2020-10-21 | Oticon A/s | A hearing device comprising a keyword detector and an own voice detector and/or a transmitter |
CN111933124A (en) * | 2020-09-18 | 2020-11-13 | 电子科技大学 | Keyword detection method capable of supporting self-defined awakening words |
CN112041856A (en) * | 2018-03-01 | 2020-12-04 | 皇家飞利浦有限公司 | Cross-modal neural network for prediction |
US20200395036A1 (en) * | 2018-02-23 | 2020-12-17 | Nippon Telegraph And Telephone Corporation | Sound signal model learning device, sound signal analysis device, method and program |
US20210049473A1 (en) * | 2019-08-14 | 2021-02-18 | The Board Of Trustees Of The Leland Stanford Junior University | Systems and Methods for Robust Federated Training of Neural Networks |
US20210055778A1 (en) * | 2017-12-29 | 2021-02-25 | Fluent.Ai Inc. | A low-power keyword spotting system |
US10956748B2 (en) * | 2017-09-15 | 2021-03-23 | Tencent Technology (Shenzhen) Company Limited | Video classification method, information processing method, and server |
US11005995B2 (en) * | 2018-12-13 | 2021-05-11 | Nice Ltd. | System and method for performing agent behavioral analytics |
US11030394B1 (en) * | 2017-05-04 | 2021-06-08 | Amazon Technologies, Inc. | Neural models for keyphrase extraction |
CN112997479A (en) * | 2018-11-15 | 2021-06-18 | Oppo广东移动通信有限公司 | Method, system and computer readable medium for processing images across a phase jump connection |
US11062725B2 (en) * | 2016-09-07 | 2021-07-13 | Google Llc | Multichannel speech recognition using neural networks |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11175888B2 (en) | 2017-09-29 | 2021-11-16 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11184704B2 (en) | 2016-02-22 | 2021-11-23 | Sonos, Inc. | Music service selection |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11200889B2 (en) * | 2018-11-15 | 2021-12-14 | Sonos, Inc. | Dilated convolutions and gating for efficient keyword spotting |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11238337B2 (en) * | 2016-08-22 | 2022-02-01 | Applied Brain Research Inc. | Methods and systems for implementing dynamic neural networks |
US11302326B2 (en) | 2017-09-28 | 2022-04-12 | Sonos, Inc. | Tone interference cancellation |
US11308961B2 (en) | 2016-10-19 | 2022-04-19 | Sonos, Inc. | Arbitration-based voice recognition |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US11343614B2 (en) | 2018-01-31 | 2022-05-24 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US11354572B2 (en) | 2018-12-05 | 2022-06-07 | International Business Machines Corporation | Multi-variables processing neurons and unsupervised multi-timescale learning for spiking neural networks |
US11354092B2 (en) | 2019-07-31 | 2022-06-07 | Sonos, Inc. | Noise classification for event detection |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11380322B2 (en) | 2017-08-07 | 2022-07-05 | Sonos, Inc. | Wake-word detection suppression |
US11405430B2 (en) | 2016-02-22 | 2022-08-02 | Sonos, Inc. | Networked microphone device control |
US11432030B2 (en) | 2018-09-14 | 2022-08-30 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US11451908B2 (en) | 2017-12-10 | 2022-09-20 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11482978B2 (en) | 2018-08-28 | 2022-10-25 | Sonos, Inc. | Audio notifications |
US11501773B2 (en) | 2019-06-12 | 2022-11-15 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11501795B2 (en) | 2018-09-29 | 2022-11-15 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11500611B2 (en) | 2017-09-08 | 2022-11-15 | Sonos, Inc. | Dynamic computation of system response volume |
US11513763B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Audio response playback |
US11514898B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Voice control of a media playback system |
US11516610B2 (en) | 2016-09-30 | 2022-11-29 | Sonos, Inc. | Orientation-based playback device microphone selection |
US11521599B1 (en) * | 2019-09-20 | 2022-12-06 | Amazon Technologies, Inc. | Wakeword detection using a neural network |
US11531520B2 (en) | 2016-08-05 | 2022-12-20 | Sonos, Inc. | Playback device supporting concurrent voice assistants |
US11538460B2 (en) | 2018-12-13 | 2022-12-27 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US11538451B2 (en) | 2017-09-28 | 2022-12-27 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US11540047B2 (en) | 2018-12-20 | 2022-12-27 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11545169B2 (en) | 2016-06-09 | 2023-01-03 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US11551669B2 (en) | 2019-07-31 | 2023-01-10 | Sonos, Inc. | Locally distributed keyword detection |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US11556306B2 (en) | 2016-02-22 | 2023-01-17 | Sonos, Inc. | Voice controlled media playback system |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11563842B2 (en) | 2018-08-28 | 2023-01-24 | Sonos, Inc. | Do not disturb feature for audio notifications |
US11641559B2 (en) | 2016-09-27 | 2023-05-02 | Sonos, Inc. | Audio playback settings for voice interaction |
US11646045B2 (en) | 2017-09-27 | 2023-05-09 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US11646023B2 (en) | 2019-02-08 | 2023-05-09 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11664023B2 (en) | 2016-07-15 | 2023-05-30 | Sonos, Inc. | Voice detection by multiple devices |
US11676590B2 (en) | 2017-12-11 | 2023-06-13 | Sonos, Inc. | Home graph |
US11696074B2 (en) | 2018-06-28 | 2023-07-04 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11710487B2 (en) | 2019-07-31 | 2023-07-25 | Sonos, Inc. | Locally distributed keyword detection |
US11715489B2 (en) | 2018-05-18 | 2023-08-01 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US11727936B2 (en) | 2018-09-25 | 2023-08-15 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11726742B2 (en) | 2016-02-22 | 2023-08-15 | Sonos, Inc. | Handling of loss of pairing between networked devices |
US11790911B2 (en) | 2018-09-28 | 2023-10-17 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11790937B2 (en) | 2018-09-21 | 2023-10-17 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11792590B2 (en) | 2018-05-25 | 2023-10-17 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US11798553B2 (en) | 2019-05-03 | 2023-10-24 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11861452B1 (en) * | 2019-06-17 | 2024-01-02 | Cadence Design Systems, Inc. | Quantized softmax layer for neural networks |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11979960B2 (en) | 2016-07-15 | 2024-05-07 | Sonos, Inc. | Contextualization of voice inputs |
US11983463B2 (en) | 2021-10-04 | 2024-05-14 | Sonos, Inc. | Metadata exchange involving a networked playback system and a networked microphone system |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10735023B2 (en) | 2017-02-24 | 2020-08-04 | Texas Instruments Incorporated | Matrix compression accelerator system and method |
US11086967B2 (en) * | 2017-03-01 | 2021-08-10 | Texas Instruments Incorporated | Implementing fundamental computational primitives using a matrix multiplication accelerator (MMA) |
KR102345409B1 (en) | 2019-08-29 | 2021-12-30 | 주식회사 하이퍼커넥트 | Processor Accelerating Convolutional Computation in Convolutional Neural Network AND OPERATING METHOD FOR THE SAME |
US11854536B2 (en) * | 2019-09-06 | 2023-12-26 | Hyperconnect Inc. | Keyword spotting apparatus, method, and computer-readable recording medium thereof |
EP4094194A1 (en) | 2020-01-23 | 2022-11-30 | Umnai Limited | An explainable neural net architecture for multidimensional data |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6041299A (en) * | 1997-03-11 | 2000-03-21 | Atr Interpreting Telecommunications Research Laboratories | Apparatus for calculating a posterior probability of phoneme symbol, and speech recognition apparatus |
US20070211064A1 (en) * | 2004-04-30 | 2007-09-13 | Microsoft Corporation | Processing machine learning techniques using a graphics processing unit |
US8676728B1 (en) * | 2011-03-30 | 2014-03-18 | Rawles Llc | Sound localization with artificial neural network |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8463025B2 (en) | 2011-04-26 | 2013-06-11 | Nec Laboratories America, Inc. | Distributed artificial intelligence services on a cell phone |
US9811775B2 (en) | 2012-12-24 | 2017-11-07 | Google Inc. | Parallelizing neural networks during training |
US20150032449A1 (en) | 2013-07-26 | 2015-01-29 | Nuance Communications, Inc. | Method and Apparatus for Using Convolutional Neural Networks in Speech Recognition |
-
2015
- 2015-07-22 US US14/805,704 patent/US10762894B2/en active Active
-
2019
- 2019-10-16 US US16/654,041 patent/US20200051551A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6041299A (en) * | 1997-03-11 | 2000-03-21 | Atr Interpreting Telecommunications Research Laboratories | Apparatus for calculating a posterior probability of phoneme symbol, and speech recognition apparatus |
US20070211064A1 (en) * | 2004-04-30 | 2007-09-13 | Microsoft Corporation | Processing machine learning techniques using a graphics processing unit |
US8676728B1 (en) * | 2011-03-30 | 2014-03-18 | Rawles Llc | Sound localization with artificial neural network |
Non-Patent Citations (6)
Title |
---|
Chang, Shuo-Yiin and Nelson Morgan. "Robust CNN-based Speech Recognition with Gabor Filter Kernels" 2014 [ONLINE] Downloaded 1/9/2018 https://mazsola.iit.uni-miskolc.hu/~czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS140259.PDF * |
He, Kaiming and JIan Sun. "Convolutional Neural Netowrks at Constrianed Time Cost" 4 December 2014 [ONLINE] Downlaoded 1/9/2018 https://arxiv.org/pdf/1412.1710.pdf * |
Jaderberg, Max et al "Speeding up Convolutional Neural Networks with Low Rank Expansions" 2014 [ONLINE] Downloaded 1/9/2018 https://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14b/jaderberg14b.pdf * |
Szegedy, Christian et al. "Intriguing properties of Neural Networks" Feb 2014 [ONLINE] Downloaded 7/10/2018 https://arxiv.org/pdf/1312.6199.pdf * |
UFLDL Tutorial "Pooling" Online 2011 [ONLINE] Downlaoded 1/9/2018 http://ufldl.stanford.edu/tutorial/supervised/Pooling/ * |
Wheeler, David. "Voice recognition will always be stupid" 2013 [ONLINE] Downloaded 1/9/2018 http://www.cnn.com/2013/08/20/opinion/wheeler-voice-recognition/index.html * |
Cited By (133)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10304440B1 (en) * | 2015-07-10 | 2019-05-28 | Amazon Technologies, Inc. | Keyword spotting using multi-task configuration |
US11863593B2 (en) | 2016-02-22 | 2024-01-02 | Sonos, Inc. | Networked microphone device control |
US11832068B2 (en) | 2016-02-22 | 2023-11-28 | Sonos, Inc. | Music service selection |
US11513763B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Audio response playback |
US11405430B2 (en) | 2016-02-22 | 2022-08-02 | Sonos, Inc. | Networked microphone device control |
US11212612B2 (en) | 2016-02-22 | 2021-12-28 | Sonos, Inc. | Voice control of a media playback system |
US11750969B2 (en) | 2016-02-22 | 2023-09-05 | Sonos, Inc. | Default playback device designation |
US11726742B2 (en) | 2016-02-22 | 2023-08-15 | Sonos, Inc. | Handling of loss of pairing between networked devices |
US11514898B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Voice control of a media playback system |
US11184704B2 (en) | 2016-02-22 | 2021-11-23 | Sonos, Inc. | Music service selection |
US11736860B2 (en) | 2016-02-22 | 2023-08-22 | Sonos, Inc. | Voice control of a media playback system |
US11556306B2 (en) | 2016-02-22 | 2023-01-17 | Sonos, Inc. | Voice controlled media playback system |
US11545169B2 (en) | 2016-06-09 | 2023-01-03 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10504209B2 (en) * | 2016-07-12 | 2019-12-10 | Beihang University | Image dehazing method |
US20180018539A1 (en) * | 2016-07-12 | 2018-01-18 | Beihang University | Ranking convolutional neural network constructing method and image processing method and apparatus thereof |
US11979960B2 (en) | 2016-07-15 | 2024-05-07 | Sonos, Inc. | Contextualization of voice inputs |
US11664023B2 (en) | 2016-07-15 | 2023-05-30 | Sonos, Inc. | Voice detection by multiple devices |
US11531520B2 (en) | 2016-08-05 | 2022-12-20 | Sonos, Inc. | Playback device supporting concurrent voice assistants |
US11238337B2 (en) * | 2016-08-22 | 2022-02-01 | Applied Brain Research Inc. | Methods and systems for implementing dynamic neural networks |
US11783849B2 (en) | 2016-09-07 | 2023-10-10 | Google Llc | Enhanced multi-channel acoustic models |
US11062725B2 (en) * | 2016-09-07 | 2021-07-13 | Google Llc | Multichannel speech recognition using neural networks |
US11641559B2 (en) | 2016-09-27 | 2023-05-02 | Sonos, Inc. | Audio playback settings for voice interaction |
US11516610B2 (en) | 2016-09-30 | 2022-11-29 | Sonos, Inc. | Orientation-based playback device microphone selection |
US11727933B2 (en) | 2016-10-19 | 2023-08-15 | Sonos, Inc. | Arbitration-based voice recognition |
US11308961B2 (en) | 2016-10-19 | 2022-04-19 | Sonos, Inc. | Arbitration-based voice recognition |
US11816532B2 (en) | 2016-11-10 | 2023-11-14 | Google Llc | Performing kernel striding in hardware |
CN108073983A (en) * | 2016-11-10 | 2018-05-25 | 谷歌有限责任公司 | Core is performed within hardware to cross over |
WO2018174438A1 (en) * | 2017-03-23 | 2018-09-27 | Samsung Electronics Co., Ltd. | Electronic apparatus for operating machine learning and method for operating machine learning |
KR102414583B1 (en) | 2017-03-23 | 2022-06-29 | 삼성전자주식회사 | Electronic apparatus for operating machine learning and method for operating machine learning |
US11907826B2 (en) | 2017-03-23 | 2024-02-20 | Samsung Electronics Co., Ltd | Electronic apparatus for operating machine learning and method for operating machine learning |
EP3529752A4 (en) * | 2017-03-23 | 2020-01-08 | Samsung Electronics Co., Ltd. | Electronic apparatus for operating machine learning and method for operating machine learning |
CN110494867A (en) * | 2017-03-23 | 2019-11-22 | 三星电子株式会社 | Method for operating the electronic device of machine learning and for operating machine learning |
KR20180107869A (en) * | 2017-03-23 | 2018-10-04 | 삼성전자주식회사 | Electronic apparatus for operating machine learning and method for operating machine learning |
US11030394B1 (en) * | 2017-05-04 | 2021-06-08 | Amazon Technologies, Inc. | Neural models for keyphrase extraction |
CN107203134A (en) * | 2017-06-02 | 2017-09-26 | 浙江零跑科技有限公司 | A kind of front truck follower method based on depth convolutional neural networks |
US10460729B1 (en) * | 2017-06-30 | 2019-10-29 | Amazon Technologies, Inc. | Binary target acoustic trigger detecton |
US10460722B1 (en) * | 2017-06-30 | 2019-10-29 | Amazon Technologies, Inc. | Acoustic trigger detection |
CN107437110A (en) * | 2017-07-11 | 2017-12-05 | 中国科学院自动化研究所 | The piecemeal convolution optimization method and device of convolutional neural networks |
US11900937B2 (en) | 2017-08-07 | 2024-02-13 | Sonos, Inc. | Wake-word detection suppression |
US11380322B2 (en) | 2017-08-07 | 2022-07-05 | Sonos, Inc. | Wake-word detection suppression |
US11500611B2 (en) | 2017-09-08 | 2022-11-15 | Sonos, Inc. | Dynamic computation of system response volume |
US11004448B2 (en) * | 2017-09-11 | 2021-05-11 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for recognizing text segmentation position |
US20190080683A1 (en) * | 2017-09-11 | 2019-03-14 | Baidu Online Network Technology (Beijing) Co., Ltd. | Method and device for recognizing text segmentation position |
US10956748B2 (en) * | 2017-09-15 | 2021-03-23 | Tencent Technology (Shenzhen) Company Limited | Video classification method, information processing method, and server |
US11646045B2 (en) | 2017-09-27 | 2023-05-09 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US11538451B2 (en) | 2017-09-28 | 2022-12-27 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US11769505B2 (en) | 2017-09-28 | 2023-09-26 | Sonos, Inc. | Echo of tone interferance cancellation using two acoustic echo cancellers |
US11302326B2 (en) | 2017-09-28 | 2022-04-12 | Sonos, Inc. | Tone interference cancellation |
US11893308B2 (en) | 2017-09-29 | 2024-02-06 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11175888B2 (en) | 2017-09-29 | 2021-11-16 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11288039B2 (en) | 2017-09-29 | 2022-03-29 | Sonos, Inc. | Media playback system with concurrent voice assistance |
CN107844795A (en) * | 2017-11-18 | 2018-03-27 | 中国人民解放军陆军工程大学 | Convolutional neural networks feature extracting method based on principal component analysis |
CN108010514A (en) * | 2017-11-20 | 2018-05-08 | 四川大学 | A kind of method of speech classification based on deep neural network |
CN111357018A (en) * | 2017-11-20 | 2020-06-30 | 谷歌有限责任公司 | Image segmentation using neural networks |
CN108010515A (en) * | 2017-11-21 | 2018-05-08 | 清华大学 | A kind of speech terminals detection and awakening method and device |
US11451908B2 (en) | 2017-12-10 | 2022-09-20 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US11676590B2 (en) | 2017-12-11 | 2023-06-13 | Sonos, Inc. | Home graph |
US20210055778A1 (en) * | 2017-12-29 | 2021-02-25 | Fluent.Ai Inc. | A low-power keyword spotting system |
CN107993651A (en) * | 2017-12-29 | 2018-05-04 | 深圳和而泰数据资源与云技术有限公司 | A kind of audio recognition method, device, electronic equipment and storage medium |
CN108304798A (en) * | 2018-01-30 | 2018-07-20 | 北京同方软件股份有限公司 | The event video detecting method of order in the street based on deep learning and Movement consistency |
US11343614B2 (en) | 2018-01-31 | 2022-05-24 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US11689858B2 (en) | 2018-01-31 | 2023-06-27 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
CN108460772A (en) * | 2018-02-13 | 2018-08-28 | 国家计算机网络与信息安全管理中心 | Harassing of advertisement facsimile signal detecting system based on convolutional neural networks and method |
US11935553B2 (en) * | 2018-02-23 | 2024-03-19 | Nippon Telegraph And Telephone Corporation | Sound signal model learning device, sound signal analysis device, method and program |
US20200395036A1 (en) * | 2018-02-23 | 2020-12-17 | Nippon Telegraph And Telephone Corporation | Sound signal model learning device, sound signal analysis device, method and program |
CN112041856A (en) * | 2018-03-01 | 2020-12-04 | 皇家飞利浦有限公司 | Cross-modal neural network for prediction |
CN108833720A (en) * | 2018-05-04 | 2018-11-16 | 北京邮电大学 | Fraudulent call number identification method and system |
US11797263B2 (en) | 2018-05-10 | 2023-10-24 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11715489B2 (en) | 2018-05-18 | 2023-08-01 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US11792590B2 (en) | 2018-05-25 | 2023-10-17 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US11696074B2 (en) | 2018-06-28 | 2023-07-04 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
WO2020010639A1 (en) * | 2018-07-13 | 2020-01-16 | 华为技术有限公司 | Convolution method and device for neural network |
US11482978B2 (en) | 2018-08-28 | 2022-10-25 | Sonos, Inc. | Audio notifications |
US11563842B2 (en) | 2018-08-28 | 2023-01-24 | Sonos, Inc. | Do not disturb feature for audio notifications |
US11778259B2 (en) | 2018-09-14 | 2023-10-03 | Sonos, Inc. | Networked devices, systems and methods for associating playback devices based on sound codes |
US11432030B2 (en) | 2018-09-14 | 2022-08-30 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US10579924B1 (en) * | 2018-09-17 | 2020-03-03 | StradVision, Inc. | Learning method, learning device with multi-feeding layers and testing method, testing device using the same |
KR102313604B1 (en) * | 2018-09-17 | 2021-10-19 | 주식회사 스트라드비젼 | Learning method, learning device with multi feeding layers and test method, test device using the same |
KR20200031992A (en) * | 2018-09-17 | 2020-03-25 | 주식회사 스트라드비젼 | Learning method, learning device with multi feeding layers and test method, test device using the same |
US11790937B2 (en) | 2018-09-21 | 2023-10-17 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11727936B2 (en) | 2018-09-25 | 2023-08-15 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11790911B2 (en) | 2018-09-28 | 2023-10-17 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11501795B2 (en) | 2018-09-29 | 2022-11-15 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
CN112997479A (en) * | 2018-11-15 | 2021-06-18 | Oppo广东移动通信有限公司 | Method, system and computer readable medium for processing images across a phase jump connection |
US20220277736A1 (en) * | 2018-11-15 | 2022-09-01 | Sonos Vox France Sas | Dilated convolutions and gating for efficient keyword spotting |
US11200889B2 (en) * | 2018-11-15 | 2021-12-14 | Sonos, Inc. | Dilated convolutions and gating for efficient keyword spotting |
US11741948B2 (en) * | 2018-11-15 | 2023-08-29 | Sonos Vox France Sas | Dilated convolutions and gating for efficient keyword spotting |
US11354572B2 (en) | 2018-12-05 | 2022-06-07 | International Business Machines Corporation | Multi-variables processing neurons and unsupervised multi-timescale learning for spiking neural networks |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11557294B2 (en) | 2018-12-07 | 2023-01-17 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11005995B2 (en) * | 2018-12-13 | 2021-05-11 | Nice Ltd. | System and method for performing agent behavioral analytics |
US11538460B2 (en) | 2018-12-13 | 2022-12-27 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US11540047B2 (en) | 2018-12-20 | 2022-12-27 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US11646023B2 (en) | 2019-02-08 | 2023-05-09 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
CN111836178A (en) * | 2019-04-17 | 2020-10-27 | 奥迪康有限公司 | Hearing device comprising a keyword detector and a self-voice detector and/or transmitter |
US11546707B2 (en) | 2019-04-17 | 2023-01-03 | Oticon A/S | Hearing device comprising a keyword detector and an own voice detector and/or a transmitter |
EP3726856B1 (en) | 2019-04-17 | 2022-11-16 | Oticon A/s | A hearing device comprising a keyword detector and an own voice detector |
US11968501B2 (en) | 2019-04-17 | 2024-04-23 | Oticon A/S | Hearing device comprising a transmitter |
EP3726856A1 (en) * | 2019-04-17 | 2020-10-21 | Oticon A/s | A hearing device comprising a keyword detector and an own voice detector and/or a transmitter |
US11798553B2 (en) | 2019-05-03 | 2023-10-24 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11501773B2 (en) | 2019-06-12 | 2022-11-15 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11854547B2 (en) | 2019-06-12 | 2023-12-26 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11861452B1 (en) * | 2019-06-17 | 2024-01-02 | Cadence Design Systems, Inc. | Quantized softmax layer for neural networks |
US11714600B2 (en) | 2019-07-31 | 2023-08-01 | Sonos, Inc. | Noise classification for event detection |
US11710487B2 (en) | 2019-07-31 | 2023-07-25 | Sonos, Inc. | Locally distributed keyword detection |
US11354092B2 (en) | 2019-07-31 | 2022-06-07 | Sonos, Inc. | Noise classification for event detection |
US11551669B2 (en) | 2019-07-31 | 2023-01-10 | Sonos, Inc. | Locally distributed keyword detection |
US20210049473A1 (en) * | 2019-08-14 | 2021-02-18 | The Board Of Trustees Of The Leland Stanford Junior University | Systems and Methods for Robust Federated Training of Neural Networks |
CN110595811A (en) * | 2019-09-11 | 2019-12-20 | 浙江工业大学之江学院 | Method for constructing health state characteristic diagram of mechanical equipment |
US11521599B1 (en) * | 2019-09-20 | 2022-12-06 | Amazon Technologies, Inc. | Wakeword detection using a neural network |
US11862161B2 (en) | 2019-10-22 | 2024-01-02 | Sonos, Inc. | VAS toggle based on device orientation |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11869503B2 (en) | 2019-12-20 | 2024-01-09 | Sonos, Inc. | Offline voice control |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11961519B2 (en) | 2020-02-07 | 2024-04-16 | Sonos, Inc. | Localized wakeword verification |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11694689B2 (en) | 2020-05-20 | 2023-07-04 | Sonos, Inc. | Input detection windowing |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
CN111737193A (en) * | 2020-08-03 | 2020-10-02 | 深圳鲲云信息科技有限公司 | Data storage method, device, equipment and storage medium |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
CN111933124A (en) * | 2020-09-18 | 2020-11-13 | 电子科技大学 | Keyword detection method capable of supporting self-defined awakening words |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US11983463B2 (en) | 2021-10-04 | 2024-05-14 | Sonos, Inc. | Metadata exchange involving a networked playback system and a networked microphone system |
US11984123B2 (en) | 2021-11-11 | 2024-05-14 | Sonos, Inc. | Network device interaction by range |
Also Published As
Publication number | Publication date |
---|---|
US10762894B2 (en) | 2020-09-01 |
US20200051551A1 (en) | 2020-02-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20200051551A1 (en) | Convolutional neural networks | |
US11948066B2 (en) | Processing sequences using convolutional neural networks | |
US11393457B2 (en) | Complex linear projection for acoustic modeling | |
US11410660B2 (en) | Voice recognition system | |
US11195521B2 (en) | Generating target sequences from input sequences using partial conditioning | |
US10192556B2 (en) | Speech recognition with acoustic models | |
US11080599B2 (en) | Very deep convolutional neural networks for end-to-end speech recognition | |
Zhang et al. | Hello edge: Keyword spotting on microcontrollers | |
US9984683B2 (en) | Automatic speech recognition using multi-dimensional models | |
US8494850B2 (en) | Speech recognition using variable-length context | |
US9336771B2 (en) | Speech recognition using non-parametric models | |
US10885438B2 (en) | Self-stabilized deep neural network | |
US20200265327A1 (en) | Selecting answer spans from electronic documents using neural networks | |
CN109344246B (en) | Electronic questionnaire generating method, computer readable storage medium and terminal device | |
CN116895289A (en) | Training method of voice activity detection model, voice activity detection method and device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SAINATH, TARA N.;MARTIN, MARIA CAROLINA;SIGNING DATES FROM 20150706 TO 20150721;REEL/FRAME:036304/0879 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |