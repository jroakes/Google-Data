CN105210064A - Classifying resources using a deep network - Google Patents
Classifying resources using a deep network Download PDFInfo
- Publication number
- CN105210064A CN105210064A CN201480026906.9A CN201480026906A CN105210064A CN 105210064 A CN105210064 A CN 105210064A CN 201480026906 A CN201480026906 A CN 201480026906A CN 105210064 A CN105210064 A CN 105210064A
- Authority
- CN
- China
- Prior art keywords
- resource
- feature
- score
- search
- classification
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
- G06F16/353—Clustering; Classification into predefined classes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/042—Knowledge-based neural networks; Logical representations of neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Abstract
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for scoring concept terms using a deep network. One of the methods includes receiving an input comprising a plurality of features of a resource, wherein each feature is a value of a respective attribute of the resource; processing each of the features using a respective embedding function to generate one or more numeric values; processing the numeric values using one or more neural network layers to generate an alternative representation of the features, wherein processing the numeric values comprises applying one or more non-linear transformations to the numeric values; and processing the alternative representation of the input using a classifier to generate a respective category score for each category in a pre-determined set of categories, wherein each of the respective category scores measure a predicted likelihood that the resource belongs to the corresponding category.
Description
Background technology
Internet search engine is intended to identify the resource relevant to user's request, such as webpage, image, text document or content of multimedia, and presents the information about these resources in the mode the most useful to user.The inquiry that internet search engine is generally submitted in response to user returns the set of Search Results, wherein each Search Results identification one resource.
Summary of the invention
This instructions relates to use degree of depth network by resource classification in classification.
In general in 1, a kind of method performed by one or more computing machine comprises: receive the input comprising multiple features of resource, wherein each feature is the value of the respective attributes of resource; Use each in feature described in corresponding imbedding function process to generate one or more numerical value; Use one or more neural net layer (such as, the artificial neural network in analog neuron loop) to process numerical value to generate the replacing representation of the feature of resource, wherein, process floating point values comprises applies one or more nonlinear transformation to floating point values; And use the replacing representation of sorter process input to generate the respective classes score of each classification in predetermine class set, wherein, each the tolerance resource in respective classes score belongs to the prediction possibility of corresponding classification.
According to the aspect 2 of aspect 1, wherein, each feature specific to individual features type in imbedding function, and wherein, each in imbedding function receives the feature of respective type and converts to this feature application, and this Feature Mapping is become numeric representation according to one group of imbedding function parameter by this conversion.
According to any one aspect 3 in aspect 1 or 2, wherein, predetermine class set comprises SPAM information (spam) classification, and the category score of resource measures the prediction possibility that this resource is SPAM information resources.
According to any one aspect 4 in aspect 1 to 3, wherein, predetermine class set comprises for the respective classes of each in polytype SPAM information.
According to any one aspect 5 in aspect 1 to 4, wherein, predetermine class set comprises the respective classes for often kind of resource type in resource type group.
According to any one aspect 6 in aspect 1 to 5, also comprise: provide category score for the use when determining whether to carry out index to resource in search engine index to search system.
According to any one aspect 7 in aspect 1 to 6, also comprise: provide category score for generating Search Results in response to the search inquiry that receives and use during search results ranking to search system.
According to any one aspect 8 in aspect 1 to 7, wherein, numerical value comprises floating point values or numerical value is floating point values.
According to any one aspect 9 in aspect 1 to 8, wherein, numerical value comprises the round values of quantification or numerical value is the round values quantized, and wherein, the integer-valued coded representation floating point values of quantification.
The computer program that other embodiments of these aspects comprises corresponding computer system, device and is recorded on one or more computer memory device, each is configured to the action of manner of execution.
One or more system for computer can be configured to by means of having installation software on this system, firmware, hardware or their combination and perform specific operation or action, described software, firmware, hardware or they be combined in operation time described system is performed an action.One or more computer program can be configured to perform specific operation or action by means of comprising instruction, and described instruction makes this device perform an action when being performed by data processing equipment.
The specific embodiment that can realize the theme described in this instructions is one or more with what reach in following advantage.The degree of depth network comprising one or more hiding neural net layer can be effective to resource classification in classification.Such as, it is junk information or non-spam that resource can be effectively categorized as, and is classified as being the one in several dissimilar junk information, or is classified as the one in two or more resource types.Use degree of depth network by resource classification to classification can such as by providing information to search engine so that the information requirement allowing search engine to meet user in the following manner better obtains the relevant search result of greater number: such as by effectively detecting junk information resource and avoiding providing the Search Results that identifies those resources or by providing the Search Results identifying and belong to the resource of the classification of match user information requirement better to user.Use degree of depth network accurate, quick, reliable and efficient mode can be provided in resource classification to classification to perform search mission and authentication of users claiming about resource, such as resource belongs to particular category and has been wrongly classified as the user belonged to a different category claims.
Illustrate the one or more embodiments of the detail of the theme of this instructions in the the accompanying drawings and the following description.Other features of this theme, aspect and advantage will become clear from description, figure and claim.
Accompanying drawing explanation
Fig. 1 shows exemplary search system.
Fig. 2 is the block diagram of example resources categorizing system.
Fig. 3 is the process flow diagram for the instantiation procedure by resource classification.
Same reference number in the various figures and sign indicate same element.
Embodiment
Fig. 1 shows exemplary search system 114.Search system 114 is examples of the information retrieval system of the computer program be implemented as on the one or more computing machines in one or more position, can realize system described below, assembly and technology wherein.
User 102 is mutual by subscriber equipment 104 and search system 114.Subscriber equipment 104 generally will comprise the storer for storing instruction and data, such as random access memory (RAM) 106, and for performing the processor 108 of stored instruction.Storer can comprise ROM (read-only memory) and can write storer.Such as, subscriber equipment 104 can be the computing machine being coupled to search system 114 by data communication network 112, described data communication network 112 such as LAN (Local Area Network) (LAN) or wide area network (WAN), such as the Internet, or the combination of network, any one the comprised wireless link in described network.
In some implementations, search system 114 provides user interface to subscriber equipment 104, and user 102 is mutual with search system 114 by this subscriber equipment 104.Such as, search system 114 can provide the user interface of the form web page drawn by the web browser that subscriber equipment 104 runs.
User 102 can use subscriber equipment 104 to search system 114 submit Query 110.Search engine 130 in search system 114 performs search to identify the resource with match query.When user's 102 submit Query 110, by network 112, inquiry 110 is sent to search system 114.Search system 114 comprises index data base 122 and search engine 130.Search system 114 responds to inquiry 110 by generating Search Results 128, Search Results 128 is sent to subscriber equipment 104 for presenting to user 102 by network, such as, as the search result web page shown by the web browser run on subscriber equipment 104.
In this manual, term " database " will be generally used to refer to acts as meaning data acquisition: these data do not need with any ad hoc fashion structuring, or do not need to be structured, and it can be stored on the multiple memory devices in one or more position.Therefore, such as, index data base 122 can comprise multiple data acquisition, and wherein each data acquisition can be organized differently and access.Similarly, in this manual, term " engine " will be generally used to refer to generation and can perform the system based on software or the subsystem of one or more specific function.In general, engine is by one or more software module that the one or more computing machines be implemented as in one or more position are installed or assembly.In some cases, one or more computing machine will be exclusively used in particular engine; In other cases, multiple engine can be installed on identical one or more computing machines and to run on one or more computing machines that this is identical.
When search engine 130 receives inquiry 110, search engine 130 identifies the resource meeting inquiry 110.Search engine 130 generally will comprise the index engine 120 resource being carried out to index, stores the index data base 122 of index information and generates score and the ranking engine 152 sorted to resource according to the respective score of resource or other software for the resource meeting inquiry 110.
Search system 114 also comprises resource classification to the resource classification system 140 in predetermine class or can communicate with this resource classification system 140.Search system 114 can such as when determining whether to carry out index to resource in index data base 122 or when generating Search Results in response to inquiry 110, and any one in many ways uses the classification generated by resource classification system 140.Below with reference to Fig. 2 and Fig. 3 describe example resources categorizing system and for by resource classification to the instantiation procedure in predetermine class.
Fig. 2 is the block diagram of example resources categorizing system 200.Resource classification system 200 is examples of the system of the computer program be embodied as on the one or more computing machines in one or more position, can realize system described below, assembly and technology wherein.
Resource classification system 200 receives input and based on the output of the input generation forecast received.Specifically, this input is the set of resource characteristic, and resource classification system 200 is the phase reserved portion of each predetermine class in predetermine class set based on the output that the characteristic set received generates.The each score generated for each classification is the prediction of the possibility this resource being belonged to corresponding classification.
Such as, in some implementations, resource classification is SPAM information resources or non-search engine junk information resource by resource classification system 200, that is, the classification in predetermine class set is " junk information " classification and " non-spam " classification.
SPAM information resources are available to search system, such as be supplied to the search system 114 of Fig. 1 for the resource of carrying out index in index data base 122, this resource is handled by individual or lineup's (can be called junk information fabricator respectively or universally) institute, gives this resource and sorts as to the response of the one or more inquiry higher search engine that it can not have in proper situation.Such as, the content in resource can be made to seem relevant especially to specific geographical area, thus make the content in resource higher for the inquiry sequence pointing to this region, and in fact this content relates to is the enterprise such as not having business locations in this region.SPAM information also can comprise other forms of error message.In this manual, SPAM information also can be called as spam content, or when its implication be based on context clearly time, also can be called junk information for short.
In these implementations, resource classification system 200 can generate score, is somebody's turn to do be divided into resource to be junk information resource, that is, belongs to the prediction possibility of junk information classification.
In some other implementation, resource classification is or non-spam by resource classification system 200, or the one in various types of SPAM information, that is, the classification in predetermine class set is the corresponding classification of the junk information of " non-spam " classification and every type.Such as, the type of SPAM information can comprise: comprise the resource of content junk information, comprise the resource of link junk information, pretend junk information resource, etc.In these implementations, system can generate the score for each classification, and this score represents that resource belongs to such other prediction possibility.
In some other implementation, resource classification system 200 according to the resource type of predetermined group by resource classification.Such as, resource type can comprise any one in News Resources, blog resource, forum's resource, shopping resource, product resource etc.Depend on the resource type in the resource type of predetermined group, resource can be classified as belong to more than one predetermined group.Such as, if resource type comprises political resources and blog resource, then the blog about election or political wrangling can be classified as political resources and blog resource.In these implementations, the set of predetermine class comprises the respective classes of each resource type in group.
The feature of resource is the value of each Resource Properties characterizing resource in some way, and can comprise the feature of various features type, different in each comfortable classification of described various features type.The feature of particular type is the list of the one or more validity feature elements selected from the vocabulary of the possible characteristic element of this characteristic type, that is, mark the list of (token) or mark-be worth right list.Such as, mark can be the words in natural language (such as English), and the vocabulary of this characteristic type can be words known in natural language.The vocabulary of characteristic type can be overlapping or non-overlapped, and the list of given characteristic type can be orderly or unordered.
Such as, resource classification system 200 can receive the feature 220 of resource from characteristic thesaurus 202.Feature 220 is the features extracted from resource.Specifically, the feature of resource comprises the mark of the content from resource.Alternatively, mark is associated with label, wherein each label and to be marked at the specific part (such as, the link in the title of resource, the stem of resource, resource, etc.) of the resource wherein occurred corresponding.Further alternatively, the feature of resource can comprise the feature of other types, such as: any one in the age of the URL(uniform resource locator) (URL) of resource, the domain name of resource, resource, the length of resource.In some implementations, feature also comprises by identifying the data that the resource classification system 200 of the classification relevant to resource or entity type obtains.The feature of resource can also comprise the feature obtained from the other system or service of summarizing perhaps effectiveness in resource, and described other system or service are such as attempted the system of the most important term in recognition resource, identified the system of the entity relevant to resource, etc.
Resource classification system 200 uses the input feature vector received to predict output, that is, comprise the score vector of the phase reserved portion of each classification in category set.Output can be provided to search system, the search system 114 of such as Fig. 1, or can be stored in category score data repository 214 for searched system use after a while.Search system can in many ways in any one utilize generate score.In some implementations, search system is determining whether the category score using this resource when carrying out index to given resource in index data base.Such as, when score represents that resource is the possibility of SPAM information resources, search system can use this score in decision process, to make may be more that the resource of junk information is more impossible indexed in index data base.As another example, when score represents that resource is a kind of possibility in some dissimilar SPAM information, search system can determine that the resource one of described type to the score exceeding threshold score is not indexed in index data base.Higher score may imply that this resource more may comprise junk information.
In some other implementation, search system can utilize the score generated when generating the Search Results of ad hoc inquiry.Such as, when score represents that resource is the possibility of SPAM information resources, search system can use given resource must assign to determine whether providing Search Results for removing the Search Results identifying this resource before presenting to user, or in the order of Search Results, whether reduce the order of the Search Results identifying this resource.Similarly, when score represents that resource belongs to the possibility of one of the resource type of predetermined group, search system can use assign to lifting or reduction in the order of the Search Results generated in response to particular search query (such as, being confirmed as is the search inquiry finding particular type resource) identify the order of the Search Results of this resource.
In some other implementation, when score represents that resource belongs to the possibility of one of the resource type of predetermined group, search system can utilize when verifying and stating about the user of resource the score generated.Such as, search system can provide the option of the resource of the searched system mistake classification of identification to user.Such as, user can submit to and identify that the blog resource that the Search Results of product resource has been included in the statement in news search result list or has had a product review has been included in the statement can therefrom bought in shopping the Resources list of product.When receive state about the user of resource time, system can evaluate resource by the possibility of mis-classification and determine whether the classification of adjustresources time be utilized as the score that this resource generates.
Resource classification system 200 comprises degree of depth network 206 and sorter 212.Degree of depth Web vector graphic one group of machine learning algorithm, described machine learning algorithm is attempted using the framework that is made up of one or more nonlinear transformation to the structure in data or is abstractly carried out modeling or imitation.Specifically, degree of depth network is the machine learning system comprising one or more hidden layer between input layer and output layer.Every one deck in hidden layer applies corresponding nonlinear transformation according to one group of parameter to being received from the input of one deck before in degree of depth network, and the result of nonlinear transformation is supplied to the lower one deck in degree of depth network.Hidden layer provides abstraction level, thus adds the modeling ability of degree of depth network.Degree of depth network 206 comprises one group of imbedding function 208 and one or more hiding ANN network layers 210, and everyone artificial neural networks layer has corresponding one group of parameter.Each in imbedding function 208 receives the individual features of respective type, and according to one group of imbedding function parameter, to feature application, Feature Mapping is become the conversion of numeric representation.Such as, imbedding function 208 can apply conversion so that Feature Mapping is become floating point representation 222 to feature 220.In more detail imbedding function is described below with reference to Fig. 3.
The initiation layer of neural net layer 210 receives the numeric representation of the input feature vector generated by imbedding function as input, and each in neural net layer 210 applies one or more corresponding nonlinear transformation to generate the replacing representation of this input to floating point representation.Replacing representation is by representing to the initial value of input, the expression such as generated by imbedding function 208, applies one or more nonlinear transformation and the numeric representation of input that generates.Such as, neural net layer 210 can generate replacing representation 224 from the floating point representation 222 of the feature 220 being received from imbedding function 208.Each neural net layer is the set of artificial node, and described artificial node receives input, and according to one group of parameter, by coming to calculate from input to export to input application nonlinear transformation.Then, other neural net layers or other assemblies that are fed to degree of depth network is exported.Replace neural net layer 210 or except neural net layer 210, degree of depth network 206 can also comprise the layer of another group, its some or all floating point representations generated at imbedding function 208 are applied the linear or nonlinear transformation of continuous print.
Sorter 212 receives the replacing representation generated by degree of depth network 206, and the value of each field according to the value prediction category score vector of the classifier parameters of sorter 212.Each field in category score vector corresponds to the respective classes in category set.Such as, sorter 212 can generate category score vector 226 from replacing representation 224.Depend on the quantity of classification in implementation and category set, namely, for the quantity of the field of its generation forecast value, sorter 212 can be the binary classifier of such as logistic regression sorter, support vector machine classifier, Bayes classifier, softmax sorter etc., or can be multiclass or many labelings device that such as many-sorted logic returns sorter, multi-class support vector machine sorter, Bayes classifier etc.
Fig. 3 is the process flow diagram for the instantiation procedure 300 by resource classification.For simplicity, the one or more system for computer be described to by being arranged in one or more position perform by process 300.Such as, the resource classification system of the resource classification system 200 of such as Fig. 2 of suitably programming can implementation 300.
System obtains the feature (step 302) of resource.
System uses the imbedding function of the characteristic type being used for feature to process each feature (step 304) with the numeric representation of generating feature.Depend on characteristic type and implementation, the imbedding function for given characteristic type can be any one in multiple imbedding function.System by each resource is resolved by system acceptance to original input data determine the type of feature.As an example, if a part for original input data is the mark " example title " with correlation tag " title ", then system can resolve input data to determine one of resource characteristic and be " example title " and this feature is resource title feature type.As another example, if a part for the original input data of given resource is " URL:www.examplesite.com ", then system can resolve these input data to determine one of resource characteristic and be www.examplesite.com and this feature is URL characteristic type.
Such as, for the characteristic type that it is characterized in that single marking, imbedding function can be simple imbedding function.Single marking is mapped to floating point vector by simple imbedding function, that is, the vector of floating point values.Such as, simple imbedding function can based on the current parameter value of imbedding function such as stored in look-up table, will mark " cat " and be mapped to vector [0.1,0.5,0.2] and will mark " flat board " and be mapped to vector [0.3,0.9,0.0].
As another example, be likely the characteristic type of the list of two or more marks for its feature, imbedding function can be parallel imbedding function.Each mark in mark list is mapped to corresponding floating point vector by parallel imbedding function, and exports single vector, and this single vector is the serial connection of each floating point vector.Such as, for marking list { " Atlanta ", " hotel " } in order, " Atlanta " can be mapped to vector [0.1,0.2,0.3] and " hotel " is mapped to vector [0.4 by parallel imbedding function, 0.5,0.6], then [0.1,0.2 is exported, 0.3,0.4,0.5,0.6].In order to obtain corresponding floating point vector, parallel imbedding function can use single look-up table or multiple different look-up table.
As another example, be likely the characteristic type of the list of two or more marks for its feature, imbedding function can be combination imbedding function.Each mark in list is mapped to corresponding floating point vector by combination imbedding function, then each floating point vector is merged into single merging vector.Combination imbedding function can use linear function, such as, the summation of each floating point vector, average or weighted linear combination, such as, or use nonlinear function, component form maximizes (component-wisemaximum) or norm constraint linear combination (norm-constrainedlinearcombination) merges each floating point vector.In order to identify corresponding floating point vector, parallel imbedding function can use single look-up table or multiple different look-up table.Such as, for ordered list { " Atlanta ", " hotel " }, " Atlanta " can be mapped to vector [0.1 by parallel imbedding function, 0.2,0.3] and " hotel " is mapped to vector [0.4,0.5,0.6], then export two vectorial and, that is, [0.5,0.7,0.9].
As another example, be likely the characteristic type of the list of two or more marks for its feature, imbedding function can be mixing imbedding function.Each mark in mark list is mapped to corresponding floating point vector by mixing imbedding function, and generates initial vector, and this initial vector is the serial connection of each floating point vector.Then, each floating point vector is merged into merge vector and will merge vector and is connected in series with initial vector by mixing imbedding function.Such as, for ordered list { " Atlanta ", " hotel " }, mixing imbedding function can export the serial connection of the vector exported by parallel imbedding function and combination imbedding function, that is, [0.1,0.2,0.3,0.4,0.5,0.6,0.5,0.7,0.9].
Depend on implementation, system can utilize two different types of imbedding functions for two different characteristic types, and these two imbedding functions can be shared or do not share parameter.Such as, system can utilize combination imbedding function for fisrt feature type and utilize mixing imbedding function for second feature type.
If one or more in the feature of resource are not discrete, then, before use imbedding function processing feature, system uses hash function to carry out hash to each discrete feature.Then the feature of each hash can be divided in a subregion in the set of predetermined partition by system, and uses for the imbedding function process of this feature value corresponding with this subregion.In addition, if can not obtain special characteristic for given resource, then this system can by this Feature Mapping to predetermined value.
In some implementations, replace floating point values, given imbedding function can generate dissimilar numerical value.Such as, imbedding function can the round values of generating quantification, its coded representation floating point values.
System uses one or more neural net layer process numeric representation (step 306).One or more neural net layer comprises one or more layers nonlinear transformation, and wherein each conversion defines based on corresponding one group of parameter.In general, one or more neural net layer represents based on the floating point vector of the feature of input the replacing representation generating this input.Alternatively, then system can use sparse binary output layer, and such as, exporting in each position is the layer of the vector of 0 or 1, processes replacing representation.At RuslanSalakhutdinov & GeoffreyHinton, the example technique for using neural net layer process floating point representation is described in Semantichashing, InternationalJournalofApproximateReasoning50 (2009) 969-978.But, can use for utilizing neural net layer to representing the many different technology and mechanism that process.
System uses the replacing representation (step 308) of sorter process input to export for each class prediction in category set.Sorter predicts output based on the value of one group of parameter and replacing representation.Output for given classification is the prediction of the value to the variable corresponding with this classification, and such as, such other score, it represents that resource belongs to such other prediction possibility.In some implementations, system can use ranking function to replace sorter to process the replacing representation of input, to belong to the sequence of the prediction possibility prediction classification of each classification according to resource.
Implementation 300 can not know the score of the input of its desired output with prediction, that is, its score expecting the resource of classification is not known in prediction.Can also to the input in training data set, that is, known system tackles the input set of the output of its prediction, and implementation 300, so that training system, namely to determine the optimal value of the parameter of sorter and degree of depth network.Such as, can to the input selected from training data set repeatedly implementation 300, using as the part of backpropagation training technique of optimal value determining each parameter.In general, the input in training data set is the feature of the resource with association category classification, that is, be classified into the feature of the resource of the classification from predetermine class set.
As a part for training process, if the label of sorter to the specific resources prediction in training data set is different from the known expectation label of this specific training resource, namely, the category score that sorter generates is inconsistent with classification belonging to this resource, then sorter adjusts its parameter to reduce the anticipation error about this specific input by using traditional method based on gradient.In addition, as a part for back-propagation method, sorter sends rub-out signal to degree of depth network, and this allowable depth network adjusts the parameter of its intraware by continuous print back-propagation phase.
In some cases, such as, for large-scale training data set, can by any one in multitude of different ways by training process parallelization.Such as, can use " LargeScaleDistributedDeepNetworks ", JeffreyDean etc., NeuralInformationProcessingSystemsConference, one or more technology for the training parallelization by machine learning model described in 2012 are by training process parallelization.
The theme described in this instructions and the embodiment of feature operation can realize with Fundamental Digital Circuit, the computer software visibly embodied or firmware, computer hardware (comprising structure disclosed in this instructions and structural equivalents thereof) or the one or more combination in them.The embodiment of the theme described in this instructions can be implemented as one or more computer program, namely, one or more modules of the computer program instructions that tangible non-transient state program carrier is encoded, perform for data processing equipment, or for the operation of control data treating apparatus.Alternatively or extraly, programmed instruction can be coded on the artificial transmitting signal generated, electric signal, light signal or electromagnetic signal that such as machine generates, generates described signal with to information coding, to be sent to suitable acceptor device, perform for data processing equipment.Computer-readable storage medium can be machine readable storage device, machine readable storage substrate, random or serial-access storage equipment or the one or more combination in them.
The device of all kinds for the treatment of data, equipment and machine contained in term " data processing equipment ", for example comprises programmable processor, computing machine or multiple processor or computing machine.Described device can comprise dedicated logic circuit, such as, and FPGA (field programmable gate array) or ASIC (special IC).In addition to hardware, described device can also comprise the code of the running environment of the computer program that establishment is discussed, and such as, forms the code of processor firmware, protocol stack, data base management system (DBMS), operating system or the one or more combination in them.
Computer program (also can be called as or be described as program, software, software application, module, software module, script or code) (can comprise compiling or interpretative code by the programming language of arbitrary form, or declaratively or process programming language) write, and it can be disposed by arbitrary form, comprise and be deployed as independently program or be deployed as module, assembly, subroutine or be applicable to other unit of using in a computing environment.Computer program can but the file not necessarily corresponded in file system.Program can be stored in a part for the file preserving other program or data (such as, be stored in the one or more scripts in marking language document) in, be stored in the Single document being exclusively used in discussed program, or be stored in multiple coordinated files (such as, storing the file of one or more module, subroutine or code section).Computer program can be deployed as and performs on a computer or be positioned at the three unities or across the distribution of multiple places and by multiple computing machines of interconnection of telecommunication network perform.
The process described in this instructions and logic flow can be performed to carry out n-back test by operating input data and generate output by the one or more programmable calculators performing one or more computer program.Process and logic flow also can by dedicated logic circuits, and such as, FPGA (field programmable gate array) or ASIC (special IC) performs, and device also can be embodied as dedicated logic circuit, such as, FPGA or ASIC.
Be applicable to performing the computing machine of computer program to comprise, for example can be based on, universal or special microprocessor or they both, or the CPU (central processing unit) of other kinds arbitrarily.In general, CPU (central processing unit) will receive instruction and data from ROM (read-only memory) or random access memory or both.The necessary element of computing machine is for performing or the CPU (central processing unit) of operating instruction and the one or more memory devices for storing instruction and data.
In general, computing machine also will comprise the one or more mass memory units for storing data, or be operatively coupled to this one or more mass memory unit with from its receive data or to its transmit data or both, described mass memory unit is disk, magneto-optic disk or CD such as.But computing machine need not have such equipment.In addition, computing machine can be embedded in other equipment, the portable memory apparatus of described other equipment such as mobile phone, personal digital assistant (PDA), Mobile audio frequency or video player, game machine, GPS (GPS) receiver or such as USB (universal serial bus) (USB) flash drive, etc.
The computer-readable medium being suitable for storing computer program instructions and data comprises the nonvolatile memory of form of ownership, medium and memory devices, for example semiconductor memory devices is comprised, such as, EPROM, EEPROM and flash memory device; Disk, such as, internal hard drive or removable dish; Magneto-optic disk; And CDROM and DVD-ROM dish.Processor and storer by supplemented or can be incorporated in dedicated logic circuit.
Mutual for what support with user, the embodiment of the theme described in this instructions can realize on the computing machine with display device and keyboard and indicating equipment, described display device such as CRT (cathode-ray tube (CRT)) or LCD (liquid crystal display) monitor, for showing information to user, described indicating equipment such as mouse or tracking ball, can provide input to computing machine by described indicating equipment user.It is mutual that the equipment of other kind also can be used for supporting with user; Such as, the feedback being supplied to user can be the sensory feedback of arbitrary form, such as visual feedback, audio feedback or tactile feedback; And from user input can by comprise acoustics, voice or sense of touch input arbitrary form receive.In addition, computing machine sends document by the equipment used to user and receives document and user interactions from this equipment; Such as, by sending webpage and user interactions in response to the request received from the web browser on the client device of user to this web browser.
The embodiment of the theme described in this instructions can realize in computing systems, described computing system comprises such as the aft-end assembly of data server, or comprise the middleware component of such as application server, or comprise front end assemblies, such as, having can the client computer of the graphical user interface mutual with the implementation of theme that describes in this instructions or web browser by its user, or described computing system comprises the combination in any of one or more such rear end, middleware or front end assemblies.The assembly of system is by the digital data communication of arbitrary form or medium, and such as, communication network interconnects.The example of communication network comprises LAN (Local Area Network) (" LAN ") and wide area network (" WAN "), such as, and internet.
Computing system can comprise client and server.Client and server generally mutual away from and usually mutual by communication network.The relation of client and server is by means of running on the respective computers and mutually having the computer program of client-server relation and produce.
Although this instructions comprises many concrete implementation details; but these should not be interpreted as any scope of invention or may the restriction of scope of claimed content, but should be interpreted as may specific to the description of the feature of the specific embodiment of concrete invention.Some feature in the context of the embodiment of separating described in this manual also can combine realization in single embodiment.Otherwise the various feature described in the context of single embodiment also can be separated realization in many embodiment: or be realized with the sub-portfolio of any appropriate.In addition; although feature may be described to some combinative movement above; be required such protection even at first; but can leave out from this combination in some cases from one or more features of claimed combination, and claimed combination can point to the variant of sub-portfolio or sub-portfolio.
Similarly, although depict operation by particular order in the drawings, this should not be understood as that the result for realizing expecting requires that these operations perform by the particular order illustrated or by consecutive order, or requires that all illustrated operations are all performed.In some cases, multitask and parallel processing may be favourable.In addition, in above-described embodiment, various system module should not be understood to all require such separation in all embodiments with being separated of assembly, and should be understood that described program assembly and system generally jointly can be integrated in single software product or be encapsulated in multiple software product.
Describe the specific embodiment of theme.Other embodiments within the scope of the claims.Such as, the action of stating in claim can perform by different orders and still realize the result of expectation.As an example, for realizing the result expected, the particular order shown by the non-essential requirement of the process described in accompanying drawing or continuous print order.In some implementation, multitask and parallel processing may be favourable.
Claims (23)
1. a system, comprising:
Define the degree of depth network realized in one or more computing machine of multiple nonlinear operation layer, wherein, described degree of depth network comprises:
Imbedding function layer, is configured to:
Receive the input comprising multiple features of resource, wherein, each feature is the value of the respective attributes of described resource, and
Use each in feature described in corresponding imbedding function process to generate one or more numerical value, and
One or more neural net layer, is configured to:
Receive described numerical value, and
Process described numerical value to generate the replacing representation of the described feature of described resource, wherein,
Processing described numerical value comprises to the one or more nonlinear transformation of described numerical applications; And sorter, be configured to:
Process the described replacing representation of described input to generate the respective classes score of each classification in predetermine class set, wherein, each in described respective classes score measures the prediction possibility that described resource belongs to corresponding classification.
2. the system as claimed in claim 1, wherein, each in described imbedding function is the feature specific to individual features type, and wherein, the feature of each the reception respective type in described imbedding function, and convert to this feature application, this Feature Mapping is become numeric representation according to one group of imbedding function parameter by this conversion.
3. the system as claimed in claim 1, wherein, described predetermine class set comprises SPAM information category, and the category score of resource measures the prediction possibility that this resource is SPAM information resources.
4. the system as claimed in claim 1, wherein, described predetermine class set comprises for the respective classes of each in polytype SPAM information.
5. the system as claimed in claim 1, wherein, described predetermine class set comprises the respective classes for often kind of resource type in resource type group.
6. the system as claimed in claim 1, wherein, described sorter is also configured to provide described category score for the use when determining whether to carry out index to resource in search engine index to search system.
7. the system as claimed in claim 1, wherein, described sorter is also configured to provide described category score for generating Search Results in response to the search inquiry that receives and use during search results ranking to search system.
8. the system as claimed in claim 1, wherein, described numerical value is floating point values.
9. the system as claimed in claim 1, wherein, described numerical value is the round values quantized, and wherein, the integer-valued coded representation floating point values of described quantification.
10. the method performed by one or more computing machine, the method comprises:
Receive the input comprising multiple features of resource, wherein, each feature is the value of the respective attributes of described resource;
Use each in feature described in corresponding imbedding function process to generate one or more numerical value;
Use numerical value described in one or more neural net layer process to generate the replacing representation of the described feature of described resource, wherein, process described numerical value and comprise to the one or more nonlinear transformation of described numerical applications; And
Use the described replacing representation inputted described in sorter process to generate the respective classes score of each classification in predetermine class set, wherein, each in described respective classes score measures the prediction possibility that described resource belongs to corresponding classification.
11. methods as claimed in claim 10, wherein, each in described imbedding function is the feature specific to individual features type, and wherein, the feature of each the reception respective type in described imbedding function, and convert to this feature application, this Feature Mapping is become numeric representation according to one group of imbedding function parameter by this conversion.
12. methods as claimed in claim 10, wherein, described predetermine class set comprises SPAM information category, and the category score of resource measures the prediction possibility that this resource is SPAM information resources.
13. methods as claimed in claim 10, wherein, described predetermine class set comprises for the respective classes of each in polytype SPAM information.
14. methods as claimed in claim 10, wherein, described predetermine class set comprises the respective classes for often kind of resource type in resource type group.
15. methods as claimed in claim 10, also comprise:
There is provided described category score for the use when determining whether to carry out index to resource in search engine index to search system.
16. methods as claimed in claim 10, also comprise:
There is provided described category score for generating Search Results in response to the search inquiry that receives and use during search results ranking to search system.
17. 1 kinds with the computer-readable storage medium of computer program code, this program comprises instruction, and described instruction makes described one or more computing machine executable operations when being performed by one or more computing machine, and described operation comprises:
Receive the input comprising multiple features of resource, wherein, each feature is the value of the respective attributes of described resource;
Use each in feature described in corresponding imbedding function process to generate one or more numerical value;
Use numerical value described in one or more neural net layer process to generate the replacing representation of the described feature of described resource, wherein, process described numerical value and comprise to the one or more nonlinear transformation of described numerical applications; And
Use the described replacing representation inputted described in sorter process to generate the respective classes score of each classification in predetermine class set, wherein, each in described respective classes score measures the prediction possibility that described resource belongs to corresponding classification.
18. computer-readable storage mediums as claimed in claim 17, wherein, each in described imbedding function is the feature specific to individual features type, and wherein, the feature of each the reception respective type in described imbedding function, and convert to this feature application, this Feature Mapping is become numeric representation according to one group of imbedding function parameter by this conversion.
19. computer-readable storage mediums as claimed in claim 17, wherein, described predetermine class set comprises SPAM information category, and the category score of resource measures the prediction possibility that this resource is SPAM information resources.
20. computer-readable storage mediums as claimed in claim 17, wherein, described predetermine class set comprises for the respective classes of each in polytype SPAM information.
21. computer-readable storage mediums as claimed in claim 17, wherein, described predetermine class set comprises the respective classes for often kind of resource type in resource type group.
22. computer-readable storage mediums as claimed in claim 17, described operation also comprises:
There is provided described category score for the use when determining whether to carry out index to resource in search engine index to search system.
23. computer-readable storage mediums as claimed in claim 17, described operation also comprises:
There is provided described category score for generating Search Results in response to the search inquiry that receives and use during search results ranking to search system.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/802,462 US9147154B2 (en) | 2013-03-13 | 2013-03-13 | Classifying resources using a deep network |
US13/802,462 | 2013-03-13 | ||
PCT/US2014/026226 WO2014160282A1 (en) | 2013-03-13 | 2014-03-13 | Classifying resources using a deep network |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105210064A true CN105210064A (en) | 2015-12-30 |
CN105210064B CN105210064B (en) | 2020-08-04 |
Family
ID=50771570
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480026906.9A Active CN105210064B (en) | 2013-03-13 | 2014-03-13 | Classifying resources using deep networks |
Country Status (4)
Country | Link |
---|---|
US (2) | US9147154B2 (en) |
EP (1) | EP2973038A1 (en) |
CN (1) | CN105210064B (en) |
WO (1) | WO2014160282A1 (en) |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107346326A (en) * | 2016-05-05 | 2017-11-14 | 百度（美国）有限责任公司 | For generating the method and system of neural network model |
CN108604313A (en) * | 2016-02-12 | 2018-09-28 | 微软技术许可有限责任公司 | The predictive modeling of automation and frame |
CN109844773A (en) * | 2016-09-06 | 2019-06-04 | 渊慧科技有限公司 | Use convolutional neural networks processing sequence |
CN110309192A (en) * | 2018-03-27 | 2019-10-08 | Sap欧洲公司 | It is matched using the structured data of neural network encoder |
CN111886605A (en) * | 2018-03-22 | 2020-11-03 | 亚马逊技术股份有限公司 | Processing for multiple input data sets |
CN113887701A (en) * | 2016-12-20 | 2022-01-04 | 谷歌有限责任公司 | Generating outputs for neural network output layers |
TWI799330B (en) * | 2021-10-27 | 2023-04-11 | 美商萬國商業機器公司 | New features for black-box machine-learning models |
US11869530B2 (en) | 2016-09-06 | 2024-01-09 | Deepmind Technologies Limited | Generating audio using neural networks |
US11948066B2 (en) | 2016-09-06 | 2024-04-02 | Deepmind Technologies Limited | Processing sequences using convolutional neural networks |
Families Citing this family (63)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11048765B1 (en) | 2008-06-25 | 2021-06-29 | Richard Paiz | Search engine optimizer |
US9600919B1 (en) | 2009-10-20 | 2017-03-21 | Yahoo! Inc. | Systems and methods for assembling and/or displaying multimedia objects, modules or presentations |
US10387503B2 (en) * | 2011-12-15 | 2019-08-20 | Excalibur Ip, Llc | Systems and methods involving features of search and/or search integration |
US10296158B2 (en) | 2011-12-20 | 2019-05-21 | Oath Inc. | Systems and methods involving features of creation/viewing/utilization of information modules such as mixed-media modules |
US10504555B2 (en) | 2011-12-20 | 2019-12-10 | Oath Inc. | Systems and methods involving features of creation/viewing/utilization of information modules such as mixed-media modules |
US11099714B2 (en) | 2012-02-28 | 2021-08-24 | Verizon Media Inc. | Systems and methods involving creation/display/utilization of information modules, such as mixed-media and multimedia modules |
US9843823B2 (en) | 2012-05-23 | 2017-12-12 | Yahoo Holdings, Inc. | Systems and methods involving creation of information modules, including server, media searching, user interface and/or other features |
US10417289B2 (en) | 2012-06-12 | 2019-09-17 | Oath Inc. | Systems and methods involving integration/creation of search results media modules |
US10303723B2 (en) | 2012-06-12 | 2019-05-28 | Excalibur Ip, Llc | Systems and methods involving search enhancement features associated with media modules |
US9141916B1 (en) * | 2012-06-29 | 2015-09-22 | Google Inc. | Using embedding functions with a deep network |
US11741090B1 (en) | 2013-02-26 | 2023-08-29 | Richard Paiz | Site rank codex search patterns |
US11809506B1 (en) | 2013-02-26 | 2023-11-07 | Richard Paiz | Multivariant analyzing replicating intelligent ambience evolving system |
US9141906B2 (en) * | 2013-03-13 | 2015-09-22 | Google Inc. | Scoring concept terms using a deep network |
US9147154B2 (en) * | 2013-03-13 | 2015-09-29 | Google Inc. | Classifying resources using a deep network |
US9311386B1 (en) * | 2013-04-03 | 2016-04-12 | Narus, Inc. | Categorizing network resources and extracting user interests from network activity |
US10769191B2 (en) * | 2013-12-20 | 2020-09-08 | Google Llc | Classifying data objects |
US10467259B2 (en) | 2014-06-17 | 2019-11-05 | Maluuba Inc. | Method and system for classifying queries |
US9754188B2 (en) * | 2014-10-23 | 2017-09-05 | Microsoft Technology Licensing, Llc | Tagging personal photos with deep networks |
US11250369B2 (en) | 2015-01-23 | 2022-02-15 | Sponsorhouse, Inc. | Computerized system for detecting the exposure and impact of an entity within virtual networking platforms |
US10366324B2 (en) | 2015-09-01 | 2019-07-30 | Google Llc | Neural network for processing graph data |
US11423323B2 (en) * | 2015-09-02 | 2022-08-23 | Qualcomm Incorporated | Generating a sparse feature vector for classification |
JP6679266B2 (en) * | 2015-10-15 | 2020-04-15 | キヤノン株式会社 | Data analysis device, data analysis method and program |
WO2017083695A1 (en) | 2015-11-12 | 2017-05-18 | Google Inc. | Generating target sequences from input sequences using partial conditioning |
EP3188086B1 (en) * | 2015-12-30 | 2020-02-19 | Facebook, Inc. | Identifying entities using a deep-learning model |
US10402750B2 (en) | 2015-12-30 | 2019-09-03 | Facebook, Inc. | Identifying entities using a deep-learning model |
KR102648770B1 (en) * | 2016-07-14 | 2024-03-15 | 매직 립, 인코포레이티드 | Deep neural network for iris identification |
CN109661194B (en) | 2016-07-14 | 2022-02-25 | 奇跃公司 | Iris boundary estimation using corneal curvature |
KR102529137B1 (en) | 2016-08-22 | 2023-05-03 | 매직 립, 인코포레이티드 | Augmented reality display device with deep learning sensors |
RU2016138608A (en) | 2016-09-29 | 2018-03-30 | Мэджик Лип, Инк. | NEURAL NETWORK FOR SEGMENTING THE EYE IMAGE AND ASSESSING THE QUALITY OF THE IMAGE |
IL293688B2 (en) | 2016-10-04 | 2024-02-01 | Magic Leap Inc | Efficient data layouts for convolutional neural networks |
US11361242B2 (en) * | 2016-10-28 | 2022-06-14 | Meta Platforms, Inc. | Generating recommendations using a deep-learning model |
US10621747B2 (en) | 2016-11-15 | 2020-04-14 | Magic Leap, Inc. | Deep learning system for cuboid detection |
KR102531542B1 (en) | 2016-12-05 | 2023-05-10 | 매직 립, 인코포레이티드 | Virual user input controls in a mixed reality environment |
US10324993B2 (en) * | 2016-12-05 | 2019-06-18 | Google Llc | Predicting a search engine ranking signal value |
KR102499396B1 (en) * | 2017-03-03 | 2023-02-13 | 삼성전자 주식회사 | Neural network device and operating method of neural network device |
US10657376B2 (en) | 2017-03-17 | 2020-05-19 | Magic Leap, Inc. | Room layout estimation methods and techniques |
US10922583B2 (en) | 2017-07-26 | 2021-02-16 | Magic Leap, Inc. | Training a neural network with representations of user interface devices |
CN111602141B (en) * | 2017-08-17 | 2024-04-12 | 新加坡国立大学 | Image visual relationship detection method and system |
US10068557B1 (en) * | 2017-08-23 | 2018-09-04 | Google Llc | Generating music with deep neural networks |
CN111033524A (en) | 2017-09-20 | 2020-04-17 | 奇跃公司 | Personalized neural network for eye tracking |
US10410111B2 (en) * | 2017-10-25 | 2019-09-10 | SparkCognition, Inc. | Automated evaluation of neural networks using trained classifier |
IL273991B2 (en) | 2017-10-26 | 2023-11-01 | Magic Leap Inc | Gradient normalization systems and methods for adaptive loss balancing in deep multitask networks |
US20190236135A1 (en) * | 2018-01-30 | 2019-08-01 | Accenture Global Solutions Limited | Cross-lingual text classification |
US10721070B2 (en) | 2018-03-07 | 2020-07-21 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11394552B2 (en) | 2018-03-07 | 2022-07-19 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11489866B2 (en) | 2018-03-07 | 2022-11-01 | Private Identity Llc | Systems and methods for private authentication with helper networks |
US11170084B2 (en) | 2018-06-28 | 2021-11-09 | Private Identity Llc | Biometric authentication |
US11502841B2 (en) * | 2018-03-07 | 2022-11-15 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11392802B2 (en) | 2018-03-07 | 2022-07-19 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11138333B2 (en) | 2018-03-07 | 2021-10-05 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11210375B2 (en) | 2018-03-07 | 2021-12-28 | Private Identity Llc | Systems and methods for biometric processing with liveness |
US10938852B1 (en) | 2020-08-14 | 2021-03-02 | Private Identity Llc | Systems and methods for private authentication with helper networks |
US11789699B2 (en) | 2018-03-07 | 2023-10-17 | Private Identity Llc | Systems and methods for private authentication with helper networks |
US11265168B2 (en) | 2018-03-07 | 2022-03-01 | Private Identity Llc | Systems and methods for privacy-enabled biometric processing |
US11741364B2 (en) | 2018-04-10 | 2023-08-29 | Hookit, Llc | Deep neural networks modeling |
KR20200061164A (en) * | 2018-11-23 | 2020-06-02 | 삼성전자주식회사 | Neural network device for neural network operation, operating method of neural network device and application processor comprising neural network device |
US10733498B1 (en) * | 2018-12-10 | 2020-08-04 | Amazon Technologies, Inc. | Parametric mathematical function approximation in integrated circuits |
US11640522B2 (en) | 2018-12-13 | 2023-05-02 | Tybalt, Llc | Computational efficiency improvements for artificial neural networks |
CN110033091B (en) | 2018-12-13 | 2020-09-01 | 阿里巴巴集团控股有限公司 | Model-based prediction method and device |
WO2020142172A1 (en) | 2019-01-03 | 2020-07-09 | Sponsorhouse, Inc. DBA Hookit | Data prioritization through relationship analysis mapping |
US10685286B1 (en) | 2019-07-30 | 2020-06-16 | SparkCognition, Inc. | Automated neural network generation using fitness estimation |
US11222258B2 (en) * | 2020-03-27 | 2022-01-11 | Google Llc | Load balancing for memory channel controllers |
WO2023211439A1 (en) * | 2022-04-28 | 2023-11-02 | Rakuten Mobile, Inc. | User equipment artificial intelligence-machine-learning capability categorization system, method, device, and program |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1637744A (en) * | 2004-01-09 | 2005-07-13 | 微软公司 | Machine-learned approach to determining document relevance for search over large electronic collections of documents |
WO2009111212A2 (en) * | 2008-03-03 | 2009-09-11 | Microsoft Corporation | Locally computable spam detection features and robust pagerank |
Family Cites Families (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6044375A (en) | 1998-04-30 | 2000-03-28 | Hewlett-Packard Company | Automatic extraction of metadata using a neural network |
US20030225763A1 (en) | 2002-04-15 | 2003-12-04 | Microsoft Corporation | Self-improving system and method for classifying pages on the world wide web |
GB2435925A (en) * | 2006-03-09 | 2007-09-12 | Cytokinetics Inc | Cellular predictive models for toxicities |
TWI310150B (en) * | 2006-06-16 | 2009-05-21 | Darfon Electronics Corp | Laser mouse and control method thereof |
WO2008067565A1 (en) * | 2006-11-30 | 2008-06-05 | Google Inc. | Targeted content request |
US7895148B2 (en) | 2007-04-30 | 2011-02-22 | Microsoft Corporation | Classifying functions of web blocks based on linguistic features |
TWI446297B (en) * | 2007-12-28 | 2014-07-21 | 私立中原大學 | Drowsiness detection system |
US8180754B1 (en) | 2008-04-01 | 2012-05-15 | Dranias Development Llc | Semantic neural network for aggregating query searches |
US8832233B1 (en) * | 2011-07-20 | 2014-09-09 | Google Inc. | Experience sharing for conveying communication status |
US20130066814A1 (en) | 2011-09-12 | 2013-03-14 | Volker Bosch | System and Method for Automated Classification of Web pages and Domains |
US9213185B1 (en) * | 2012-01-06 | 2015-12-15 | Google Inc. | Display scaling based on movement of a head-mounted display |
US9146993B1 (en) * | 2012-03-16 | 2015-09-29 | Google, Inc. | Content keyword identification |
US9112870B1 (en) * | 2012-03-28 | 2015-08-18 | Emc Corporation | Processing device having session component with integrated support for message queuing protocol |
US9218573B1 (en) * | 2012-05-22 | 2015-12-22 | Google Inc. | Training a model using parameter server shards |
US9141916B1 (en) * | 2012-06-29 | 2015-09-22 | Google Inc. | Using embedding functions with a deep network |
US9210399B1 (en) * | 2012-08-03 | 2015-12-08 | Google Inc. | Wearable device with multiple position support |
US9037464B1 (en) * | 2013-01-15 | 2015-05-19 | Google Inc. | Computing numeric representations of words in a high-dimensional space |
US9141906B2 (en) * | 2013-03-13 | 2015-09-22 | Google Inc. | Scoring concept terms using a deep network |
US9147154B2 (en) * | 2013-03-13 | 2015-09-29 | Google Inc. | Classifying resources using a deep network |
US9269048B1 (en) * | 2013-03-14 | 2016-02-23 | Google Inc. | Distribution shared content based on a probability |
CN104469256B (en) * | 2013-09-22 | 2019-04-23 | 思科技术公司 | Immersion and interactive video conference room environment |
-
2013
- 2013-03-13 US US13/802,462 patent/US9147154B2/en active Active
-
2014
- 2014-03-13 WO PCT/US2014/026226 patent/WO2014160282A1/en active Application Filing
- 2014-03-13 EP EP14725807.3A patent/EP2973038A1/en not_active Ceased
- 2014-03-13 CN CN201480026906.9A patent/CN105210064B/en active Active
-
2015
- 2015-08-24 US US14/834,274 patent/US9449271B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1637744A (en) * | 2004-01-09 | 2005-07-13 | 微软公司 | Machine-learned approach to determining document relevance for search over large electronic collections of documents |
WO2009111212A2 (en) * | 2008-03-03 | 2009-09-11 | Microsoft Corporation | Locally computable spam detection features and robust pagerank |
Non-Patent Citations (1)
Title |
---|
RENQIANG MIN 等: "A Deep Non-Linear Feature Mapping for Large-Margin kNN Classification", 《2009 IEEE INTERNATIONAL CONFERENCE ON DATA MINING》 * |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108604313A (en) * | 2016-02-12 | 2018-09-28 | 微软技术许可有限责任公司 | The predictive modeling of automation and frame |
CN107346326A (en) * | 2016-05-05 | 2017-11-14 | 百度（美国）有限责任公司 | For generating the method and system of neural network model |
CN107346326B (en) * | 2016-05-05 | 2023-10-31 | 百度（美国）有限责任公司 | Method and system for information retrieval |
CN109844773A (en) * | 2016-09-06 | 2019-06-04 | 渊慧科技有限公司 | Use convolutional neural networks processing sequence |
US11948066B2 (en) | 2016-09-06 | 2024-04-02 | Deepmind Technologies Limited | Processing sequences using convolutional neural networks |
CN109844773B (en) * | 2016-09-06 | 2023-08-01 | 渊慧科技有限公司 | Processing sequences using convolutional neural networks |
US11869530B2 (en) | 2016-09-06 | 2024-01-09 | Deepmind Technologies Limited | Generating audio using neural networks |
CN113887701A (en) * | 2016-12-20 | 2022-01-04 | 谷歌有限责任公司 | Generating outputs for neural network output layers |
CN113887701B (en) * | 2016-12-20 | 2023-06-02 | 谷歌有限责任公司 | Method, system and storage medium for generating output for neural network output layer |
CN111886605B (en) * | 2018-03-22 | 2024-03-22 | 亚马逊技术股份有限公司 | Processing for multiple input data sets |
CN111886605A (en) * | 2018-03-22 | 2020-11-03 | 亚马逊技术股份有限公司 | Processing for multiple input data sets |
CN110309192A (en) * | 2018-03-27 | 2019-10-08 | Sap欧洲公司 | It is matched using the structured data of neural network encoder |
CN110309192B (en) * | 2018-03-27 | 2024-02-13 | Sap欧洲公司 | Structural data matching using neural network encoders |
TWI799330B (en) * | 2021-10-27 | 2023-04-11 | 美商萬國商業機器公司 | New features for black-box machine-learning models |
Also Published As
Publication number | Publication date |
---|---|
US20140279774A1 (en) | 2014-09-18 |
US9147154B2 (en) | 2015-09-29 |
EP2973038A1 (en) | 2016-01-20 |
US20160048754A1 (en) | 2016-02-18 |
WO2014160282A1 (en) | 2014-10-02 |
US9449271B2 (en) | 2016-09-20 |
CN105210064B (en) | 2020-08-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105210064A (en) | Classifying resources using a deep network | |
US20200302340A1 (en) | Systems and methods for learning user representations for open vocabulary data sets | |
Bhattacharjee et al. | Active learning based news veracity detection with feature weighting and deep-shallow fusion | |
CN105144164A (en) | Scoring concept terms using a deep network | |
CN102483745B (en) | Co-selected image classification | |
Tong et al. | A shilling attack detector based on convolutional neural network for collaborative recommender system in social aware network | |
US7406452B2 (en) | Machine learning | |
US20200110842A1 (en) | Techniques to process search queries and perform contextual searches | |
CN110019790B (en) | Text recognition, text monitoring, data object recognition and data processing method | |
CN102508859A (en) | Advertisement classification method and device based on webpage characteristic | |
CN110909182A (en) | Multimedia resource searching method and device, computer equipment and storage medium | |
Gupta et al. | Fake news detection using passive-aggressive classifier | |
US20210272013A1 (en) | Concept modeling system | |
CN103631787A (en) | Webpage type recognition method and webpage type recognition device | |
CN113590948A (en) | Information recommendation method, device, equipment and computer storage medium | |
CN113704620B (en) | User tag updating method, device, equipment and medium based on artificial intelligence | |
Murty et al. | Dark web text classification by learning through SVM optimization | |
CN112989182B (en) | Information processing method, information processing device, information processing apparatus, and storage medium | |
CN113468404A (en) | Push resource determination method based on big data mining and cloud computing AI (Artificial Intelligence) service system | |
CN116484105A (en) | Service processing method, device, computer equipment, storage medium and program product | |
CN114244611B (en) | Abnormal attack detection method, device, equipment and storage medium | |
Elnagar et al. | A cognitive framework for detecting phishing websites | |
Aljumah et al. | Android Apps Security Assessment using Sentiment Analysis Techniques: Comparative Study. | |
Nair et al. | A citation recommendation system using deep reinforcement learning | |
CN115982646B (en) | Management method and system for multisource test data based on cloud platform |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant |