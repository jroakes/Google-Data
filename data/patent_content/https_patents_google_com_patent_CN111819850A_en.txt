CN111819850A - Motion predictive coding by co-frame motion vectors - Google Patents
Motion predictive coding by co-frame motion vectors Download PDFInfo
- Publication number
- CN111819850A CN111819850A CN201980017993.4A CN201980017993A CN111819850A CN 111819850 A CN111819850 A CN 111819850A CN 201980017993 A CN201980017993 A CN 201980017993A CN 111819850 A CN111819850 A CN 111819850A
- Authority
- CN
- China
- Prior art keywords
- frame
- block
- motion vector
- current
- determining
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 239000013598 vector Substances 0.000 title claims description 445
- 238000000034 method Methods 0.000 claims description 46
- 230000004044 response Effects 0.000 claims description 24
- 238000006073 displacement reaction Methods 0.000 claims description 15
- 239000002131 composite material Substances 0.000 claims description 12
- 238000012935 Averaging Methods 0.000 claims description 4
- 238000004891 communication Methods 0.000 description 120
- 230000002123 temporal effect Effects 0.000 description 51
- 238000005192 partition Methods 0.000 description 36
- 238000010586 diagram Methods 0.000 description 28
- 238000000638 solvent extraction Methods 0.000 description 21
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 15
- 238000001914 filtration Methods 0.000 description 15
- 230000009467 reduction Effects 0.000 description 14
- 238000012545 processing Methods 0.000 description 11
- 239000007787 solid Substances 0.000 description 10
- 238000013500 data storage Methods 0.000 description 9
- 230000003287 optical effect Effects 0.000 description 9
- 239000011159 matrix material Substances 0.000 description 6
- 230000001131 transforming effect Effects 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 238000013139 quantization Methods 0.000 description 4
- 241000023320 Luma <angiosperm> Species 0.000 description 3
- 230000006835 compression Effects 0.000 description 3
- 238000007906 compression Methods 0.000 description 3
- 238000004590 computer program Methods 0.000 description 3
- OSWPMRLSEDHDFF-UHFFFAOYSA-N methyl salicylate Chemical compound COC(=O)C1=CC=CC=C1O OSWPMRLSEDHDFF-UHFFFAOYSA-N 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 230000000903 blocking effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 229910001416 lithium ion Inorganic materials 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- QELJHCBNGDEXLD-UHFFFAOYSA-N nickel zinc Chemical compound [Ni].[Zn] QELJHCBNGDEXLD-UHFFFAOYSA-N 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 230000003595 spectral effect Effects 0.000 description 2
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 1
- OJIJEKBXJYRIBZ-UHFFFAOYSA-N cadmium nickel Chemical compound [Ni].[Cd] OJIJEKBXJYRIBZ-UHFFFAOYSA-N 0.000 description 1
- 229910052804 chromium Inorganic materials 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 229910052987 metal hydride Inorganic materials 0.000 description 1
- 229910052759 nickel Inorganic materials 0.000 description 1
- PXHVJJICTQNCMI-UHFFFAOYSA-N nickel Substances [Ni] PXHVJJICTQNCMI-UHFFFAOYSA-N 0.000 description 1
- -1 nickel metal hydride Chemical class 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 229910052720 vanadium Inorganic materials 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/513—Processing of motion vectors
- H04N19/517—Processing of motion vectors by encoding
- H04N19/52—Processing of motion vectors by encoding by predictive encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/577—Motion compensation with bidirectional frame interpolation, i.e. using B-pictures
Abstract
Video coding may include: generating an encoded frame by encoding a current frame from an input bitstream; generating a reference common frame spatio-temporally corresponding to the current frame; and encoding the current frame using the reference common frame. The current frame is a frame from the input frame sequence, each frame from the input frame sequence having a respective ordinal position in the input frame sequence, and the current frame having a current ordinal position in the input frame sequence. Video coding may include including the encoded frames in an output bitstream and outputting the output bitstream.
Description
Background
Digital images and videos may be used, for example, for remote business conferencing via video conferencing, high definition video entertainment, video advertising, or sharing of user-generated content over the internet. High performance compression may be advantageous for transmission and storage due to the large amount of data involved in transmitting and processing image and video data.
Disclosure of Invention
It would be advantageous to provide high resolution images and video transmitted over a communication channel having limited bandwidth, such as image and video coding using motion prediction coding through co-frame motion vectors.
The present application relates to encoding and decoding image data, video stream data, or both, for transmission or storage. Aspects of systems, methods, and apparatus for encoding and decoding using motion predictive coding with co-frame motion vectors are disclosed herein.
One aspect is a method for video decoding, comprising: the decoded frame is generated by decoding a current frame from the encoded bitstream by a processor executing instructions stored on a non-transitory computer readable medium. The decoding includes identifying a current block from a current frame; identifying a previously decoded block based on coding information of a current block; determining whether the motion information of the previously decoded block includes a co-frame motion vector of the previously decoded block; and determining whether to identify a prediction block for decoding the current block based on the reference common frame. Decoding includes, in response to omitting the determination to identify the prediction block for decoding the current block based on the reference common frame and in response to a determination that the motion information of the previously decoded block includes a common frame motion vector of the previously decoded block, identifying an alignment block in the reference common frame based on a spatial position of the previously decoded block and the common frame motion vector of the previously decoded block; determining a motion vector of the aligned block based on the motion field information of the aligned block; determining a motion vector prediction for the current block based on the motion vector of the aligned block and a co-frame motion vector of a previously decoded block; determining a motion vector of the current block based on a motion vector prediction for the current block; and identifying a prediction block based on a reference frame indicated by the motion vector of the current block. Video coding includes, in response to a determination that a prediction block for decoding a current block is identified based on a reference co-frame and in response to a determination that motion information of a previously decoded block includes a co-frame motion vector of the previously decoded block, determining a co-frame motion vector prediction for the current block based on the co-frame motion vector of the previously decoded block. Video coding includes, in response to a determination to identify a predicted block for decoding a current block based on a reference co-frame and to omit a co-frame motion vector of a previously decoded block in response to motion information of the previously decoded block, identifying a forward motion vector from the motion information of the previously decoded block; identifying a backward motion vector according to motion information of a previously decoded block; determining a co-frame motion vector prediction for the current block based on the forward motion vector and the backward motion vector; determining a co-frame motion vector for the current block based on a co-frame motion vector prediction for the current block; and identifying a prediction block based on the co-frame motion vectors referencing the co-frame and the current block. Video coding includes generating a decoded block corresponding to a current block based on a prediction block, the decoded block being included in a decoded frame. The method includes outputting a reconstructed frame based on the decoded frame.
Another aspect is a method for video encoding, comprising: the encoded frame is generated by encoding a current frame from an input bitstream by a processor executing instructions stored on a non-transitory computer readable medium. The encoding includes identifying a current block from a current frame; identifying a previously coded block; determining whether the motion information of the previously coded block includes a co-frame motion vector of the previously coded block; and determining whether to identify a prediction block for encoding the current block based on the reference common frame. The encoding includes, in response to omitting the determination to identify the prediction block for encoding the current block based on the reference common frame and in response to a determination that the motion information of the previously coded block includes a common frame motion vector of the previously coded block, identifying an alignment block in the reference common frame based on a spatial position of the previously coded block and the common frame motion vector of the previously coded block; determining a motion vector of the aligned block based on the motion field information of the aligned block; determining a motion vector prediction for the current block based on the motion vector of the aligned block and a co-frame motion vector of a previously coded block; and including in the output bitstream an indication of motion vector prediction for the current block. The encoding includes, in response to a determination that a prediction block used to encode the current block is identified based on the reference co-frame and in response to a determination that the motion information of the previously coded block includes a co-frame motion vector of the previously coded block, determining a co-frame motion vector prediction for the current block based on the co-frame motion vector of the previously coded block. The encoding includes, in response to a determination to identify a prediction block for encoding the current block based on the reference co-frame and to omit a co-frame motion vector of a previously coded block in response to motion information of the previously coded block, identifying a forward motion vector from the motion information of the previously coded block; identifying a backward motion vector from motion information of a previously coded block; and determining a co-frame motion vector prediction for the current block based on the forward motion vector and the backward motion vector. The encoding includes including, in the output bitstream, an indication of a co-frame motion vector prediction for the current block. The method includes outputting an output bitstream.
Another aspect is a method for video encoding, comprising: an encoded frame is generated by a processor executing instructions stored on a non-transitory computer readable medium by encoding a current frame from an input bitstream. The encoding includes spatio-temporally generating a reference co-frame corresponding to a current frame, wherein the current frame is a frame from the input frame sequence, wherein each frame from the input frame sequence has a corresponding sequential position in the input frame sequence, and wherein the current frame has a current sequential position in the input frame sequence, and encoding the current frame using the reference co-frame. The method includes including the encoded frame in an output bitstream and outputting the output bitstream.
These and other variations will be described in more detail below.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views unless otherwise indicated or otherwise clear from the context.
FIG. 1 is a diagram of a computing device according to an embodiment of the present disclosure.
Fig. 2 is a diagram of a computing and communication system according to an embodiment of the present disclosure.
Fig. 3 is a diagram of a video stream for encoding and decoding according to an embodiment of the present invention.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present invention.
Fig. 6 is a block diagram of a representation of a portion of a frame according to an embodiment of the present invention.
Fig. 7 is a block diagram of an example of a sequence of frames according to an embodiment of the present invention.
Fig. 8 is a flowchart of an example of encoding using a reference co-frame 800 according to an embodiment of the present disclosure.
Fig. 9 is a flowchart of an example of decoding using a reference common frame 900 according to an embodiment of the present invention.
Fig. 10 is a block diagram of an example of a portion of coding a video sequence using a reference co-frame according to an embodiment of the present invention.
Fig. 11 is a diagram of an example of identifying a common frame motion vector according to an embodiment of the present invention.
Fig. 12 is a flowchart of an example of an inter-coded motion data reduction part according to an embodiment of the present invention.
Fig. 13 illustrates an example of determining a motion vector prediction of a current block based on a co-frame motion vector for a previously coded block according to an embodiment of the present disclosure, and a flowchart of an example of determining a co-frame motion vector prediction for a current block according to an embodiment of the present disclosure.
Fig. 14 is a diagram of an example of determining a motion vector prediction of a current block based on a co-frame motion vector of a previously coded block according to an embodiment of the present invention.
Fig. 15 is a flowchart of an example of inter prediction using a reference common frame according to an embodiment of the present invention.
Fig. 16 is a flowchart of an example of determining a prediction block based on a co-frame motion vector from a previously coded block according to an embodiment of the present invention.
Fig. 17 is a flowchart of an example of determining a prediction block of a current block based on a reference common frame according to an embodiment of the present disclosure.
Fig. 18 is a diagram of an example of determining a co-frame motion vector prediction of a current block based on a motion vector of a previously coded block according to an embodiment of the present invention.
Detailed Description
Image and video compression schemes may include: breaking an image or frame into smaller parts, such as blocks; and generating an output bitstream using a technique that limits information included in each block in the output. In some implementations, the information included in each block in the output can be limited by reducing spatial redundancy, reducing temporal redundancy, or a combination thereof. For example, temporal redundancy or spatial redundancy may be reduced by: a frame, or a portion thereof, is predicted based on information available to both the encoder and decoder, and information representing the difference or residue between the predicted frame and the original frame is included in the encoded bitstream. The residual information may be further compressed by transforming the residual information into transform coefficients, quantizing the transform coefficients, and entropy coding the quantized transform coefficients. Other coding information, such as motion information, may be included in the encoded bitstream, which may include sending differential information based on predictions of the encoded information, which may be entropy coded to further reduce the corresponding bandwidth utilization. The encoded bitstream can be decoded to recreate the block and source images from limited information.
For example, inter prediction may include encoding a current block from a current frame by encoding a difference between the current block and a prediction block. The prediction block may be generated based on image data from one or more reference frames available at the encoder and decoder, such as one or more frames previously reconstructed in coding order, which may correspond to display frames that are sequentially before or after the current frame in input or display order. Some reference frames or substitute reference frames may be constructed, which are reference frames having a different temporal position from that of the current frame that are used for coding and omitted from display. Some motion information may not be available for video coding using inter-prediction based on reference frames having a different temporal position than that of the current frame.
Video coding using motion prediction coding by co-frame motion vectors can improve the accuracy and efficiency of prediction coding by generating a reference co-frame and predicting a current frame with reference to the reference co-frame. The reference co-frame may have a spatiotemporal position corresponding to the temporal position of the current frame, which may include the input sequence or frame index position. The reference co-frame may be generated based on optical flow estimation, e.g., based on motion field information of the temporal position of the current frame, which may be referred to as a motion field-based reference frame or a motion field reference frame, and may include a linear projection of previously generated motion information. The prediction block from the reference common frame may be indicated using a common frame motion vector that indicates an offset or displacement from a spatial position in the reference common frame that corresponds to the spatial position of the current block in the current frame.
In order to improve coding efficiency for coding motion information of a block encoded with reference to a previously reconstructed frame at a different input sequence position from a current frame, the motion information may be indicated in the encoded bitstream using information indicating a motion vector prediction, which may be a prediction of a motion vector of a current block of the current frame, and may be generated based on a co-frame motion vector of a previously coded block adjacent to the current block in the current frame. The co-frame motion vector may relate to a reference co-frame that is temporally and spatially concurrent with the current frame.
In order to improve coding efficiency for coding motion information of blocks coded with reference to a common frame, the motion information may be indicated in the coded bitstream using information indicating a common frame motion vector prediction, which may be a prediction of a common frame motion vector of a current block of a current frame, and may be generated based on a common frame motion vector of a previously coded block adjacent to the current block in the current frame or based on a composite motion vector of the previously coded blocks.
Fig. 1 is a diagram of a computing device 100 according to an embodiment of the present disclosure. Computing device 100 is shown including memory 110, processor 120, User Interface (UI)130, electronic communication unit 140, sensors 150, power supply 160, and bus 170. As used herein, the term "computing device" includes any unit, or combination of units, disclosed herein that is capable of performing any method, or any one or more portions thereof.
Although shown as a single unit, memory 110 may comprise multiple physical units, e.g., one or more main memory units, such as a random access memory unit; one or more secondary data storage units, such as a magnetic disk; or a combination thereof. For example, the data 112 or a portion thereof, the instructions 114 or a portion thereof, or both, may be stored in a secondary storage unit and loaded or otherwise transferred to a primary storage unit in connection with processing the corresponding data 112, executing the corresponding instructions 114, or both. In some implementations, the memory 110, or a portion thereof, may be removable memory.
Although illustrated as being included in memory 110, in some embodiments, the instructions 114, or portions thereof, may be implemented as a special purpose processor or circuitry that may include dedicated hardware for performing any one of the methods, algorithms, aspects, or combinations thereof, as described herein. Portions of instructions 114 may be distributed across multiple processors on the same machine or on different machines, or across a network such as a local area network, a wide area network, the internet, or a combination thereof.
The user interface 130 may include any unit capable of interfacing with a user, such as a virtual or physical keypad, a touchpad, a display, a touch display, a speaker, a microphone, a camera, a sensor, or any combination thereof. For example, the user interface 130 may be an audiovisual display device, and the computing device 100 may render audio, such as decoded audio, using the user interface 130 audiovisual display device, such as in connection with displaying video, such as decoded video. Although shown as a single unit, the user interface 130 may comprise one or more physical units. For example, the user interface 130 may include an audio interface for performing audio communication with a user, and a touch display for performing visual and touch-based communication with the user.
Although electronic communication interface 142 is illustrated in fig. 1 as a wireless antenna, electronic communication interface 142 may be a wireless antenna as shown, a wired communication port such as an ethernet port, an infrared port, a serial port, or any other wired or wireless unit capable of interfacing with a wired or wireless electronic communication medium 180. Although fig. 1 shows a single electronic communication unit 140 and a single electronic communication interface 142, any number of electronic communication units and any number of electronic communication interfaces may be used.
The sensor 150 may include, for example, an audio sensing device, a visible light sensing device, a motion sensing device, or a combination thereof. For example, the sensor 150 may include a sound sensing device, such as a microphone, or any other sound sensing device now known or later developed, that may sense sounds, such as speech or other speech, emitted by a user operating the computing device 100 in the vicinity of the computing device 100. In another example, the sensor 150 may include a camera, or any other image sensing device now known or later developed, that may sense an image, such as an image of a user operating the computing device. Although a single sensor 150 is shown, the computing device 100 may include multiple sensors 150. For example, computing device 100 may include a first camera oriented with a field of view directed toward a user of computing device 100, and a second camera oriented with a field of view away from the user of computing device 100.
The power supply 160 may be any suitable device for powering the computing device 100. For example, the power supply 160 may include a wired external power interface; one or more dry cell batteries, for example, nickel cadmium (NiCd), nickel zinc (NiZn), nickel metal hydride (NiMH), lithium ion (Li-ion); a solar cell; a fuel cell; or any other device capable of powering computing device 100. Although a single power supply 160 is shown in fig. 1, the computing device 100 may include multiple power supplies 160, for example, a battery and a wired external power interface.
Although shown as separate units, electronic communication unit 140, electronic communication interface 142, user interface 130, power supply 160, or a portion thereof, may be configured as a combined unit. For example, electronic communication unit 140, electronic communication interface 142, user interface 130, and power supply 160 may be implemented to be able to interface with an external display device, provide communication, power, or both.
One or more of the memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, or power supply 160 may be operatively coupled by a bus 170. Although a single bus 170 is shown in FIG. 1, computing device 100 may include multiple buses. For example, memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, and bus 170 may receive power from power supply 160 through bus 170. In another example, the memory 110, processor 120, user interface 130, electronic communication unit 140, sensors 150, power supply 160, or a combination thereof may communicate data, for example, by sending and receiving electronic signals via bus 170.
Although not separately shown in fig. 1, one or more of the processor 120, the user interface 130, the electronic communication unit 140, the sensor 150, or the power supply 160 may include an internal memory, such as an internal buffer or register. For example, processor 120 may include an internal memory (not shown), and may read data 112 from memory 110 into the internal memory (not shown) for processing.
Although shown as separate elements, memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, power supply 160, and bus 170, or any combination thereof, may be integrated in one or more electronic units, circuits, or chips.
Fig. 2 is a diagram of a computing and communication system 200 according to an embodiment of the present disclosure. The illustrated computing and communication system 200 includes computing and communication devices 100A, 100B, 100C, access points 210A, 210B, and a network 220. For example, computing and communication system 200 may be a multiple-access system that provides communications, such as voice, audio, data, video, messaging, broadcast, or combinations thereof, to one or more wired or wireless communication devices, such as computing and communication devices 100A, 100B, 100C. Although fig. 2 shows three computing and communication devices 100A, 100B, 100C, two access points 210A, 210B, and one network 220 for simplicity, any number of computing and communication devices, access points, and networks may be used.
The computing and communication devices 100A, 100B, 100C may be, for example, computing devices such as computing device 100 shown in FIG. 1. For example, the computing and communication devices 100A, 100B may be user devices, such as mobile computing devices, laptop computers, thin clients, or smart phones, and the computing and communication device 100C may be a server, such as a mainframe or cluster. Although the computing and communication device 100A and the computing and communication device 100B are described as user devices and the computing and communication device 100C is described as a server, any computing and communication device may perform some or all of the functions of a server, some or all of the functions of a user device, or some or all of the functions of a server and a user device. For example, the server computing and communication device 100C may receive, encode, process, store, transmit audio data, or a combination thereof; and one or both of the computing and communication device 100A and the computing and communication device 100B may receive, decode, process, store, render audio data, or a combination thereof.
Each computing and communication device 100A, 100B, 100C may be configured to perform wired or wireless communications, for example, over the network 220, which may include User Equipment (UE), a mobile station, a fixed or mobile subscriber unit, a cellular telephone, a personal computer, a tablet computer, a server, consumer electronics, or any similar device. For example, the computing and communication devices 100A, 100B, 100C may be configured to transmit or receive wired or wireless communication signals. Although each computing and communication device 100A, 100B, 100C is shown as a single unit, the computing and communication devices may include any number of interconnected elements.
Each access point 210A, 210B may be any type of device configured to communicate with the computing and communication devices 100A, 100B, 100C, the network 220, or both, over a wired or wireless communication link 180A, 180B, 180C. For example, the access points 210A, 210B may include base stations, Base Transceiver Stations (BTSs), node-Bs, enhanced node-Bs (eNode-Bs), home node-Bs (HNode-Bs), wireless routers, wired routers, hubs, repeaters, switches, or any similar wired or wireless devices. Although each access point 210A, 210B is shown as a single unit, the access points may include any number of interconnected elements.
The network 220 may be any type of network configured to provide services such as voice, data, applications, voice over internet protocol (VoIP), or any other communication protocol or combination of communication protocols over wired or wireless communication links. For example, the network 220 may be a Local Area Network (LAN), a Wide Area Network (WAN), a Virtual Private Network (VPN), a mobile or cellular telephone network, the Internet, or any other means of electronic communication. The network may use communication protocols such as Transmission Control Protocol (TCP), User Datagram Protocol (UDP), Internet Protocol (IP), real-time transport protocol (RTP), hypertext transport protocol (HTTP), or a combination thereof.
The computing and communication devices 100A, 100B, 100C may communicate with each other via the network 220 using one or more wired or wireless communication links, or by a combination of wired and wireless communication links. For example, as shown, computing and communication devices 100A, 100B may communicate over wireless communication links 180A, 180B, and computing and communication device 100C may communicate over wired communication link 180C. Any of the computing and communication devices 100A, 100B, 100C may communicate using any one or more wired or wireless communication links. For example, a first computing and communication device 100A may communicate through a first access point 210A using a first type of communication link, a second computing and communication device 100B may communicate through a second access point 210B using a second type of communication link, and a third computing and communication device 100C may communicate through a third access point (not shown) using a third type of communication link. Similarly, the access points 210A, 210B may communicate with the network 220 over one or more types of wired or wireless communication links 230A, 230B. Although fig. 2 illustrates computing and communication devices 100A, 100B, 100C communicating over a network 220, the computing and communication devices 100A, 100B, 100C may communicate with each other over any number of communication links, such as direct wired or wireless communication links.
In some implementations, communication between one or more of the computing and communication devices 100A, 100B, 100C may omit communication over the network 220, and may include transferring data through another medium (not shown), such as a data storage device. For example, the server computing and communication device 100C may store audio data, e.g., encoded audio data, in a data storage device, e.g., a portable data storage unit, and one or both of the computing and communication device 100A or the computing and communication device 100B may access, read, or retrieve the stored audio data from the data storage unit, e.g., by physically disconnecting the data storage device from the server computing and communication device 100C and physically connecting the data storage device to the computing and communication device 100A or the computing and communication device 100B.
Other implementations of the computing and communication system 200 are possible. For example, in an embodiment, the network 220 may be an ad hoc network, and one or more of the access points 210A, 210B may be omitted. Computing and communication system 200 may include devices, units, or elements not shown in fig. 2. For example, the computing and communication system 200 may include many more communication devices, networks, and access points.
Fig. 3 is a diagram of a video stream 300 for encoding and decoding according to an embodiment of the present disclosure. A video stream 300, such as a video stream captured by a camera or a video stream generated by a computing device, may include a video sequence 310. Video sequence 310 may include a sequence of adjacent frames 320. Although three adjacent frames 320 are shown, the video sequence 310 may include any number of adjacent frames 320.
Each frame 330 from adjacent frames 320 may represent a single image from the video stream. Although not shown in fig. 3, frame 330 may include one or more segments, tiles, or planes that may be independently coded or otherwise processed, e.g., in parallel. Frame 330 may include blocks 340. Although not shown in fig. 3, a block may include pixels. For example, a block may include 16 × 16 pixel groups, 8 × 8 pixel groups, 8 × 16 pixel groups, or any other pixel groups. Unless otherwise indicated herein, the term "block" may include a super-block, a macroblock, a segment, a slice, or any other portion of a frame. A frame, block, pixel, or combination thereof may include display information, such as luminance information, chrominance information, or any other information that may be used to store, modify, transmit, or display a video stream or a portion thereof.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. In a device such as the computing device 100 shown in fig. 1 or the computing and communication devices 100A, 100B, 100C shown in fig. 2, the encoder 400 may be implemented as a computer software program, for example, stored in a data storage unit such as the memory 110 shown in fig. 1. The computer software program may include machine instructions that may be executed by a processor, such as processor 120 shown in fig. 1, and may cause a device to encode video data as described herein. The encoder 400 may be implemented as dedicated hardware included in the computing device 100, for example.
The encoder 400 may encode an input video stream 402 (e.g., the video stream 300 shown in fig. 3) to generate an encoded (compressed) bitstream 404. In some embodiments, the encoder 400 may include a forward path for generating the compressed bitstream 404. The forward path may include an intra/inter prediction unit 410, a transform unit 420, a quantization unit 430, an entropy encoding unit 440, or any combination thereof. In some implementations, the encoder 400 may include a reconstruction path (indicated by the broken connecting lines) for reconstructing the frame to encode other blocks. The reconstruction path may include a dequantization unit 450, an inverse transform unit 460, a reconstruction unit 470, a filtering unit 480, or any combination thereof. Other structural variations of the encoder 400 may be used to encode the video stream 402.
To encode the video stream 402, each frame within the video stream 402 may be processed in units of blocks. Thus, the current block may be identified from the blocks in the frame and may be encoded.
At the intra/inter prediction unit 410, the current block may be encoded using intra prediction, which may be within a single frame, or inter prediction, which may be from frame to frame. Intra-prediction may include generating a prediction block from samples in a current frame that have been previously encoded and reconstructed. Inter-prediction may include generating a prediction block from samples in one or more previously reconstructed reference frames. Generating the prediction block for the current block in the current frame may include performing motion estimation to generate a motion vector indicating an appropriate reference portion of the reference frame.
The intra/inter prediction unit 410 may subtract the prediction block from the current block (original block) to generate a residual block. Transform unit 420 may perform a block-based transform, which may include transforming the residual block into transform coefficients, e.g., in the frequency domain. Examples of block-based transforms include Karhunen-loeve transform (KLT), Discrete Cosine Transform (DCT), singular value decomposition transform (SVD), and Asymmetric Discrete Sine Transform (ADST). In an example, DCT may include transforming the blocks into the frequency domain. DCT may include using spatial frequency based transform coefficient values with the lowest frequency (i.e., DC) coefficient at the upper left of the matrix and the highest frequency coefficient at the lower right of the matrix.
The quantization unit 430 may convert the transform coefficients into discrete quantum values, which may be referred to as quantized transform coefficients or quantization levels. The quantized transform coefficients may be entropy encoded by entropy encoding unit 440 to produce entropy encoded coefficients. Entropy encoding may include using a probability distribution metric. Entropy coding coefficients and information for decoding the block, which may include the type of prediction used, the motion vector, and the quantizer value, may be output to the compressed bitstream 404. The compressed bitstream 404 may be formatted using various techniques such as Run Length Encoding (RLE) and zero run length coding.
The reconstruction path may be used to maintain reference frame synchronization between the encoder 400 and a corresponding decoder, such as the decoder 500 shown in fig. 5. The reconstruction path may be similar to the decoding process discussed below and may include decoding the encoded frame or a portion thereof, which may include decoding the encoded block, which may include dequantizing the quantized transform coefficients at dequantization unit 450 and inverse transforming the dequantized transform coefficients at inverse transformation unit 460 to produce a derivative residual block. The reconstruction unit 470 may add the prediction block generated by the intra/inter prediction unit 410 to the derivative residual block to create a decoded block. The filtering unit 480 may be applied to the decoded block to generate a reconstructed block, which may reduce distortion, such as blocking artifacts. Although one filtering unit 480 is shown in fig. 4, filtering the decoded block may include loop filtering, deblocking filtering, or other types of filtering, or combinations of filtering types. As indicated by the dashed line at 482, the reconstructed block may be stored or otherwise accessible as a reconstructed block for encoding another portion of the current frame, another frame, or both, which may be part of a reference frame. As indicated by the dashed line at 484, the coding information for the frame, e.g., the deblocking threshold index value, may be encoded, included in the compressed bitstream 404, or may be encoded and included in the compressed bitstream 404.
Other variations of the encoder 400 may be used to encode the compressed bitstream 404. For example, the non-transform based encoder 400 may directly quantize the residual block without the transform unit 420. In some embodiments, the quantization unit 430 and the dequantization unit 450 may be combined into a single unit.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. In a device such as the computing device 100 shown in fig. 1 or the computing and communication devices 100A, 100B, 100C shown in fig. 2, the decoder 500 may be implemented as a computer software program, for example, stored in a data storage unit such as the memory 110 shown in fig. 1. The computer software program may include machine instructions that may be executed by a processor, such as processor 120 shown in fig. 1, and may cause a device to decode video data as described herein. The decoder 500 may be implemented as dedicated hardware included in the computing device 100, for example.
The decoder 500 may receive a compressed bitstream 502, such as the compressed bitstream 404 shown in fig. 4, and may decode the compressed bitstream 502 to generate an output video stream 504. The decoder 500 may include an entropy decoding unit 510, a dequantization unit 520, an inverse transform unit 530, an intra/inter prediction unit 540, a reconstruction unit 550, a filtering unit 560, or any combination thereof. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 502.
The entropy decoding unit 510 may decode data elements within the compressed bitstream 502 using, for example, context-adaptive binary arithmetic decoding to produce a set of quantized transform coefficients. The dequantization unit 520 may dequantize the quantized transform coefficients, and the inverse transform unit 530 may inverse transform the dequantized transform coefficients to produce a block of derivative residues, which may correspond to the block of derivative residues generated by the inverse transform unit 460 shown in fig. 4. Using the header information decoded from the compressed bitstream 502, the intra/inter prediction unit 540 may generate a prediction block corresponding to the prediction block created in the encoder 400. At the reconstruction unit 550, the prediction block may be added to the derivative residual block to create a decoded block. The filtering unit 560 may be applied to the decoding block to reduce artifacts such as blocking artifacts, which may include loop filtering, deblocking filtering, or other types of filtering or combinations of filtering types, and may include generating a reconstruction block, which may be output as the output video stream 504.
Other variations of the decoder 500 may be used to decode the compressed bitstream 502. For example, the decoder 500 may generate the output video stream 504 without the filtering unit 560.
Fig. 6 is a block diagram of a representation of a portion 600 of a frame, such as frame 330 shown in fig. 3, according to an implementation of the present disclosure. As shown, the portion 600 of the frame includes four 64 x 64 blocks 610 in two rows and two columns in a matrix or cartesian plane. In some implementations, a 64 × 64 block may be the largest coding unit, with N being 64. Each 64 x 64 block may include four 32 x 32 blocks 620. Each 32 x 32 block may include four 16 x 16 blocks 630. Each 16 x 16 block may include four 8 x 8 blocks 640. Each 8 x 8 block 640 may include four 4 x 4 blocks 650. Each 4 x 4 block 650 may include 16 pixels, which 16 pixels may be represented in four rows and four columns in each respective block in a cartesian plane or matrix. The pixels may include information representing an image captured in a frame, such as luminance information, color information, and position information. In some implementations, a block, such as the illustrated 16 × 16 pixel block, may include: a luminance block 660, which may include luminance pixels 662; and two chroma blocks 670, 680, such as a U or Cb chroma block 670 and a V or Cr chroma block 680. The chroma blocks 670, 680 may include chroma pixels 690. For example, as shown, luma block 660 may include 16 × 16 luma pixels 662 and each chroma block 670, 680 may include 8 × 8 chroma pixels 690. Although one arrangement of blocks is shown, any arrangement may be used. Although fig. 6 illustrates an N × N block, in some implementations an N × M block may be used. For example, 32 × 64 blocks, 64 × 32 blocks, 16 × 32 blocks, 32 × 16 blocks, or any other size blocks may be used. In some implementations, an N × 2N block, a 2N × N block, or a combination thereof may be used.
In some implementations, video coding may include ordered block-level coding. Sequential block-level coding may include coding blocks of a frame in an order such as raster scan order, where blocks may be identified and processed starting with a block in the top left corner of the frame or a portion of the frame and proceeding from the top row to the bottom row and proceeding along the rows from left to right, each block being identified in turn for processing. For example, the 64 x 64 block in the top row and left column of the frame may be the first block being coded and the 64 x 64 block immediately to the right of the first block may be the second block being coded. The second row from the top may be the second row that is coded such that the 64 x 64 block in the left column of the second row may be coded after the 64 x 64 block in the rightmost column of the first row.
In some implementations, coding the block may include using quadtree coding, which may include coding smaller block units within the block in raster scan order. For example, the 64 x 64 block shown in the lower left corner of the portion of the frame shown in fig. 6 may be coded using quadtree coding, where the upper left 32 x 32 block may be coded, then the upper right 32 x 32 block may be coded, then the lower left 32 x 32 block may be coded, and then the lower right 32 x 32 block may be coded. Each 32 x 32 block may be coded using quadtree coding, where the top left 16 x 16 block may be coded, then the top right 16 x 16 block may be coded, then the bottom left 16 x 16 block may be coded, and then the bottom right 16 x 16 block may be coded. Each 16 x 16 block may be coded using quadtree coding, where the upper left 8 x 8 block may be coded, then the upper right 8 x 8 block may be coded, then the lower left 8 x 8 block may be coded, and then the lower right 8 x 8 block may be coded. Each 8 x 8 block may be coded using quadtree coding, where the upper left 4 x 4 block may be coded, then the upper right 4 x 4 block may be coded, then the lower left 4 x 4 block may be coded, and then the lower right 4 x 4 block may be coded. In some implementations, 8 x 8 blocks may be omitted for a 16 x 16 block and the 16 x 16 block may be coded using quadtree coding, where the top left 4 x 4 block may be coded and then the other 4 x 4 blocks in the 16 x 16 block may be coded in raster scan order.
In some implementations, video coding may include compressing information included in the original or input frames by, for example, omitting some information in the original frames from the corresponding encoded frames. For example, the coding may include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
In some implementations, reducing spectral redundancy may include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which may be referred to as a YUV or YCbCr color model or color space. Using the YUV color model may include representing a luminance component of a portion of a frame using a relatively large amount of information and representing each corresponding chrominance component of the portion of the frame using a relatively small amount of information. For example, a portion of a frame may be represented by a high-resolution luma component, which may include a 16 × 16 pixel block, and by two lower-resolution chroma components, each of which represents the portion of the frame as an 8 × 8 pixel block. A pixel may indicate a value, e.g., a value ranging from 0 to 255, and may be stored or transmitted using, e.g., eight bits. Although the present disclosure is described with reference to a YUV color model, any color model may be used.
In some implementations, reducing spatial redundancy can include transforming the blocks into the frequency domain using, for example, a Discrete Cosine Transform (DCT). For example, a unit of an encoder, such as transform unit 420 shown in fig. 4, may perform a DCT using spatial frequency based transform coefficient values.
In some implementations, reducing temporal redundancy can include using similarities between frames to encode frames using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of a video stream. For example, a block or pixel of the current frame may be similar to a spatially corresponding block or pixel of the reference frame. In some implementations, a block or pixel of the current frame may be similar to a block or pixel of the reference frame at a different spatial location, and reducing temporal redundancy may include generating motion information indicative of a spatial difference or translation between the location of the block or pixel in the current frame and a corresponding location of the block or pixel in the reference frame.
In some implementations, reducing temporal redundancy can include identifying a portion of a reference frame corresponding to a current block or pixel of a current frame. For example, a reference frame or a portion of a reference frame that may be stored in memory may be searched to identify a portion for generating a prediction for encoding a current block or pixel of a current frame with maximum efficiency. For example, the search may identify a portion of the reference frame for which a difference in pixel values between the current block and a prediction block generated based on the portion of the reference frame is minimized, and may be referred to as a motion search. In some implementations, the portion of the reference frame searched may be limited. For example, the portion of the searched reference frame that may be referred to as a search area may include a limited number of lines of the reference frame. In one example, identifying the portion of the reference frame used to generate the prediction may include computing a cost function, such as a Sum of Absolute Differences (SAD) between pixels of the portion of the search area and pixels of the current block.
In some implementations, this may be doneThe spatial difference between the position of the portion of the reference frame used to generate the prediction in the reference frame and the current block in the current frame is represented as a motion vector. The difference in pixel values between the prediction block and the current block may be referred to as differential data, residual data, prediction error, or residual block. In some implementations, generating the motion vector may be referred to as motion estimation, and may be based on using fx,yIndicating the pixels of the current block. Similarly, r may be based on usagex,yIndicating the pixels of the search area of the reference frame. The Motion Vector (MV) of the current block may be determined based on, for example, the SAD between pixels of the current frame and corresponding pixels of the reference frame.
Although described herein with reference to a matrix or cartesian representation of a frame for clarity, the frame may be stored, transmitted, processed, or any combination thereof in any data structure such that pixel values may be efficiently represented for a frame or image. For example, frames may be stored, transmitted, processed, or any combination thereof, in a two-dimensional data structure such as the matrix shown, or in a one-dimensional data structure such as a vector array. In one implementation, a representation of a frame, such as the two-dimensional representation shown, may correspond to a physical location in the rendering of the frame as an image. For example, a position in the upper left corner of a block in the upper left corner of a frame may correspond to a physical position in the upper left corner of the rendering of the frame as an image.
In some implementations, block-based coding efficiency may be improved by partitioning an input block into one or more prediction partitions, which may be rectangular partitions, including square partitions, used for prediction coding. In some implementations, video coding using predictive partitioning may include selecting a predictive partitioning scheme from among a plurality of candidate predictive partitioning schemes. For example, in some implementations, a candidate prediction partition scheme for a 64 × 64 coding unit may include prediction partitions of rectangular sizes ranging in size from 4 × 4 to 64 × 64, such as 4 × 4, 4 × 8, 8 × 4, 8 × 8, 8 × 16, 16 × 8, 16 × 16, 16 × 32, 32 × 16, 32 × 32, 32 × 64, 64 × 32, or 64 × 64. In some implementations, video coding using predictive partitions may include a full predictive partition search, which may include: selecting a prediction partitioning scheme by encoding the coding unit using each available candidate prediction partitioning scheme; and selecting an optimal scheme, such as the scheme that produces the smallest rate distortion error.
In some implementations, encoding a video frame may include identifying a predictive partitioning scheme for encoding a current block, e.g., block 610. In some implementations, identifying the prediction partitioning scheme may include determining whether to encode the block into a single prediction partition of maximum coding unit size (which may be 64 × 64 as shown), or to partition the block into multiple prediction partitions, which may correspond to sub-blocks such as 32 × 32 block 620, 16 × 16 block 630, or 8 × 8 block 640 as shown, and may include determining whether to partition into one or more smaller prediction partitions. For example, a 64 × 64 block may be partitioned into four 32 × 32 prediction partitions. Three of the four 32 x 32 prediction partitions may be encoded as 32 x 32 prediction partitions and the fourth 32 x 32 prediction partition may be further partitioned into four 16 x 16 prediction partitions. Three of the four 16 x 16 prediction partitions may be encoded as 16 x 16 prediction partitions and the fourth 16 x 16 prediction partition may be further partitioned into four 8 x 8 prediction partitions, each of which may be encoded as 8 x 8 prediction partitions. In some implementations, identifying the predictive partitioning scheme can include using a predictive partitioning decision tree.
In some implementations, video coding for a current block may include identifying an optimal predictive coding mode from among a plurality of candidate predictive coding modes, which may provide flexibility in processing video signals having various statistical properties and may improve compression efficiency. For example, the video coder may evaluate each candidate predictive coding mode to identify an optimal predictive coding mode, which may be, for example, a predictive coding mode that minimizes an error metric, such as a rate-distortion cost, for the current block. In some implementations, the complexity of searching for candidate predictive coding modes may be reduced by limiting the set of available candidate predictive coding modes based on the similarity between the current block and the corresponding prediction block. In some implementations, the complexity of searching each candidate predictive coding mode may be reduced by performing a directed refined mode search. For example, metrics may be generated for a limited set of candidate block sizes, such as 16 × 16, 8 × 8, and 4 × 4, the error metric associated with each block size may be in descending order, and additional candidate block sizes, such as 4 × 8 and 8 × 4 block sizes, may be evaluated.
In some implementations, block-based coding efficiency may be improved by partitioning a current residual block into one or more transform partitions, which may be rectangular partitions for transform coding, including square partitions. In some implementations, video coding using transform partitioning may include selecting a uniform transform partitioning scheme. For example, a current residual block, such as block 610, may be a 64 × 64 block and may be transformed without partitioning using a 64 × 64 transform.
Although not explicitly shown in fig. 6, the residual block may be transform partitioned using a uniform transform partitioning scheme. For example, a 64 × 64 residual block may be transform partitioned using a uniform transform partitioning scheme including four 32 × 32 transform blocks, using a uniform transform partitioning scheme including sixteen 16 × 16 transform blocks, using a uniform transform partitioning scheme including sixty-four 8 × 8 transform blocks, or using a uniform transform partitioning scheme including 256 4 × 4 transform blocks.
In some implementations, video coding using transform partitions may include identifying multiple transform block sizes of residual blocks using multi-form transform partition coding. In some implementations, the multi-form transform partition coding may include: it is recursively determined whether to transform the current block using a current block size transform or transform the current block by partitioning the current block and performing a multi-form transform partition coding on each partition. For example, the lower left block 610 shown in fig. 6 may be a 64 × 64 residual block, and the multi-form transform partition coding may include determining whether to code the current 64 × 64 residual block using a 64 × 64 transform or to code the 64 × 64 residual block by partitioning the 64 × 64 residual into partitions such as four 32 × 32 blocks 620 and multi-form transform partition coding each partition. In some implementations, determining whether to transform the current block may be based on comparing a cost for encoding the current block using the current block size transform to a sum of costs for encoding each partition using the partition size transform.
Fig. 7 is a block diagram of an example of a frame sequence 700 in accordance with an implementation of the present disclosure. The sequence 700 represents a scene where the square 702 moves from the upper left to the lower right of the field of view. The sequence 700 includes input frames 710-780 that are shown from left to right in a temporal order, which may be referred to as an input order or frame index order FI.1-FI.8. For example, the first input frame 710 may have a frame index of one (1), the second input frame 720 may have a frame index of two (2), the third input frame 730 may have a frame index of three (3), the fourth input frame 740 may have a frame index of four (4), the fifth input frame 750 may have a frame index of five (5), the sixth input frame 760 may have a frame index of six (6), the seventh input frame 770 may have a frame index of seven (7), and the eighth input frame 780 may have a frame index of eight (8).
The position of the square 702 in each respective frame 710-780 is shown as a solid square. For example, the position of the square 702 in the first frame 710 is shown at the upper left portion of the first frame 710, while the position of the square 702 in the eighth frame 780 is shown at the lower right portion of the eighth frame 780. The square 702 moves along a non-linear path 790 shown using solid lines. An estimated linear path 795 from the position of the square 702 at the upper left of the first frame 710 to the position of the square 702 at the lower right of the eighth frame 780 is shown using a dashed line. The estimated position of the square 702 along the estimated linear path 795 is illustrated using the dashed squares in the second frame 720, third frame 730, fourth frame 740, fifth frame 750, sixth frame 760 and seventh frame 770.
Fig. 8 is a flow diagram of an example of encoding using a reference co-frame 800 according to an implementation of the present disclosure. Encoding using reference co-frame 800 may be implemented in an encoder, such as encoder 400 shown in fig. 4. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may implement encoding using the reference common frame 800. As shown in fig. 8, encoding using a reference common frame 800 includes identifying a current input frame at 810, identifying a motion field reference frame at 820, determining an estimated motion field at 830, generating a reference common frame at 840, motion estimating at 850, motion data reduction at 860, and outputting at 870.
A current input frame is identified at 810. Identifying the current frame includes identifying an input frame, such as one of the input frames 710-780 shown in FIG. 7. Identifying the current input frame may include: determining that a current input frame is a frame from an input frame sequence; identifying a coding order for coding an input frame sequence; determining a current coding order; and identifying a current input frame according to the current coding order. For example, the frame sequence may include eight frames such as frames 710-780 shown in fig. 7 and a defined coding order such as the input order, or the first frame followed by an eighth frame followed by a fifth frame followed by a third frame followed by a second frame followed by a fourth frame followed by a sixth frame followed by a seventh frame.
The current input frame may be a first frame in a sequence of frames, such as the first frame 710 shown in fig. 7, or may otherwise be identified as a golden frame, a key frame, or the encoder may otherwise identify a coding mode of the current frame as intra-coding, may intra-code the current frame to generate a first encoded frame, may decode and reconstruct the first encoded frame to generate a first reconstructed frame, and may omit encoding the current frame using the reference common frame 800.
Motion field reference frames are identified at 820. The current frame identified at 810 may be a frame identified for inter-prediction coding, such as one of the second frame 720 through the eighth frame 280 shown in fig. 7, and the encoder may identify a reference frame available for inter-prediction coding as a motion field reference frame for coding the current frame. The motion field reference frames may be frames generated based on information currently available for decoding the encoded video sequence, such as previously reconstructed frames, which may include: a backward reference frame, which may be, for example, a previously reconstructed frame that is sequentially prior to the current frame in time or frame index order; or a forward reference frame, which may be a previously reconstructed frame that follows the current frame in time or frame index order, for example. For simplicity and clarity, the available reference frames identified at 820 may be referred to herein as motion field reference frames. The reference common frame may be generated based on available reference frames or a subset thereof, such as the two closest reference frames in the forward and backward directions.
For example, the current frame may be a second frame that is coded in coding order, such as the eighth frame 780 shown in fig. 7, which may be the last frame in the sequence of frames. A first reconstructed frame corresponding to the first coded frame may be identified as a motion field reference frame. The encoder may determine that the motion field reference frame omits inter motion information. The second encoded frame may be generated by inter-coding the current frame with reference to the first reconstructed frame as a reference frame. A second reconstructed frame may be generated based on the second encoded frame for use as a reference frame. In addition, the use of the reference co-frame 800 for encoding can be omitted for the current frame.
In another example, two or more available reference frames, which may include a forward reference frame, a backward reference frame, or both, may be identified as motion field reference frames.
An estimated motion field is determined at 830. The encoder may determine that the motion field reference frame includes inter-frame motion information representing motion that linearly intersects a current temporal position, which is a temporal position or a frame index position of the current frame. For example, motion information indicating motion between a first frame of the sequence and a last frame of the sequence, such as a motion vector of the last frame of the sequence that references a portion of the first reconstructed frame, intersects each other frame in the sequence.
An estimated motion field may be determined for a current temporal location corresponding to the current frame. Determining the estimated motion field may include identifying motion information, such as one or more motion vectors, that intersects the current temporal location, e.g., on a per-pixel basis or a per-block basis. For example, the motion field reference frame may have a frame index greater than a current frame index of the current frame, the motion vector of the motion field reference frame may relate to a reconstructed reference frame having a frame index prior to the current frame index, and the motion vector may be identified as intersecting the current temporal position. In another example, the motion field reference frame may have a frame index greater than the current frame index, the motion vector of the motion field reference frame may relate to a reconstructed reference frame having a frame index greater than the current frame index, and the motion vector may be identified as disjoint to the current temporal location. In another example, the motion field reference frame may have a frame index less than the current frame index, the motion vectors of the motion field reference frame may relate to a reconstructed reference frame having a frame index less than the current frame index, and the motion vectors may be identified as disjoint from the current temporal location. Each block or each pixel of each available reference frame may be evaluated to identify intersecting motion information.
Linear projection may be used to project the intersecting motion information to the current temporal location. For motion vectors from intersecting motion information, a linear projection at the intersection of the motion vector and the current temporal location can be used to identify the current spatial location in the estimated motion field. Available forward reference frames, such as the closest available forward reference frame in temporal order, may be identified and a forward component of the estimated motion field, which may be an estimated forward motion vector, may be generated by: for example, linear projection is used to project from the spatial locations in the estimated motion field to the identified forward reference frame based on the corresponding motion vectors that intersect the current temporal location at the current spatial location. Available backward reference frames, such as the nearest available backward reference frame in temporal order, may be identified and a backward component of the estimated motion field, which may be an estimated backward motion vector, may be generated by: a linear projection is used, for example, to project from a spatial location in the estimated motion field to the identified backward reference frame based on a corresponding motion vector that intersects the current temporal location at the current spatial location. An estimated motion field, which may include a forward component and a backward component, may be determined for each spatial location, such as each block location, at the estimated motion field location, which may have dimensions equivalent to the current input frame. For simplicity and clarity, the available forward reference frames and the available backward reference frames may be referred to as motion field projection frames.
A reference co-frame is generated at 840. The reference common frame may be generated by: pixel values from a reference frame indicated by the estimated motion field, such as a motion field projection frame, are interpolated. Although shown separately in fig. 8, motion field determination at 830 and reference co-frame generation at 840 may be performed in combination and may include multiple times or processing stages, such as multi-scale processing, which can include occlusion detection. Generating the reference common frame may include using a per-pixel motion field or a per-block motion field. In some implementations, generating the reference common frame may include using an affine isomorphic model. Other interpolation models may be used.
Motion estimation is performed on the current frame using the reference common frame at 850. Motion estimation may be similar to that described with respect to fig. 4, except as described herein or otherwise clear from context. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may perform motion estimation. Motion estimation may be included in the block-based coding for the current frame, which may include motion estimation at 850, motion data reduction at 860, and output at 870, such as in a defined order, such as a raster scan order or another defined order, for each block of the current frame. In some implementations, the encoder can determine respective portions of the estimated motion field at 830 and generate respective portions of the reference common frame in a block-by-block manner for blocks coded with respect to the reference common frame at 840.
The motion estimation may include: identifying a current block of a current input frame; and generating a prediction block for encoding the current block. A prediction block may be generated based on a single reference frame, such as a backward reference frame, and may be referred to as a single reference coded block. A prediction block may be generated based on multiple reference frames, such as a combination of backward and forward reference frames, and may be referred to as a composite reference coded block. A prediction block may be generated based on a reference co-frame and may be referred to as a co-frame reference coded block.
For example, motion estimation may include identifying a current block and generating a prediction block for the current block based on a reference common frame, such as by performing a motion search in the reference common frame. The prediction block may spatially correspond to a portion of the reference common frame, and the motion estimation may include identifying a common frame motion vector that indicates a displacement of the current block between a spatial position in the current frame and a spatial position in the reference common frame corresponding to the prediction block. The co-frame motion vector may represent the spatial divergence between the motion-based reference co-frame of the motion field linear projection and the non-linear motion captured by the input frame. An example of a portion of coding a video sequence, such as video sequence 700 shown in fig. 7, using reference co-frames is shown in fig. 10. An example of generating the co-frame motion vector is shown in fig. 11. Other encoding elements may be used, such as those described with respect to fig. 4.
Motion data reduction is performed at 860. Motion data reduction may reduce bandwidth utilization for storing or transmitting motion information identified at 850 by: available previously generated contextual motion information that may be available for decoding motion information is identified and the motion information is coded based on the contextual motion information. For example, a motion vector of a previously coded block adjacent to the current block may be identical to a motion vector identified for the current block, and the motion vector identified for the current block may be coded as a reference to the previously coded adjacent block. In another example, the motion vector of the previously coded neighboring block may be similar to the motion vector of the current block, differential motion information based on the difference between the motion information identified at 850 and the motion vector of the previously coded neighboring block may be determined, and the motion vector identified for the current block may be coded as a reference to the previously coded neighboring block and the differential motion information. Identifying contextual motion information may include evaluating a plurality of neighboring blocks and identifying contextual motion information based on defined criteria, which may include ordering criteria. An example of motion data reduction is shown in fig. 12.
Data generated by encoding using the reference co-frame 800 or a portion thereof is output at 870. For example, information, such as a frame identifier, indicating the reference frames, such as forward reference frames and backward reference frames, used to generate motion fields, co-frame motion vectors, and motion field motion vectors may be stored, such as on a per-block basis. The co-frame motion vectors or corresponding differential motion information and the residue between the current block and the prediction block may be encoded and included in an output bitstream, which may be stored and transmitted or otherwise provided to a decoder for decoding the encoded video.
Fig. 9 is a flow diagram of an example of decoding using a reference common frame 900 according to an implementation of the present disclosure. Decoding using the reference common frame 900 may be implemented in a decoder, such as the decoder 500 shown in fig. 5. For example, the intra/inter prediction unit 540 of the decoder 500 shown in fig. 5 may implement decoding using the reference common frame 900. As shown in fig. 9, decoding using the reference common frame 900 includes identifying a current encoded frame at 910, identifying a motion field reference frame at 920, determining an estimated motion field at 930, generating a reference common frame at 940, reconstructing the current frame at 950, and outputting at 960.
A current encoded frame is identified at 910. Identifying the current encoded frame may include decoding or partially decoding encoded frame data from the encoded bitstream. For example, a decoder may receive a compressed bitstream, such as compressed bitstream 404 shown in fig. 4, including encoded data, and may decode or partially decode the compressed bitstream to identify encoded frame data, such as by entropy decoding and dequantizing frame data. Identifying the current encoded frame may include identifying residual data output by the encoder, such as shown at 860 in fig. 8 or 410 in fig. 4.
Motion field reference frames are identified at 920. The current encoded frame identified at 910 may be a frame identified as inter-coded, such as one of the second through eighth frames 720 through 780 shown in fig. 7, and the decoder may identify a reference frame available for inter-prediction coding as a motion field reference frame for coding the current frame. The motion field reference frames may be frames generated based on information currently available for decoding the decoded video sequence, such as previously reconstructed frames, which may include: a backward reference frame, which may be, for example, a previously reconstructed frame that is sequentially prior to the current frame in time or frame index order; or a forward reference frame, which may be a previously reconstructed frame that follows the current frame in time or frame index order, for example. For simplicity and clarity, the available reference frames identified at 920 may be referred to herein as motion field reference frames.
For example, the current frame may be the second frame coded in the coding order, such as the eighth frame 780 shown in fig. 7, which may be the last frame in the sequence of frames. A first reconstructed frame corresponding to the first coded frame may be identified as a motion field reference frame. The decoder may determine that the motion field reference frame omits inter-frame motion information. The second decoded frame may be generated by decoding the currently encoded frame using inter-coding to reference the first reconstructed frame as a reference frame. A second reconstructed frame may be generated based on the second decoded frame for use as a reference frame. In addition, decoding using the reference common frame 900 may be omitted for the current frame.
In another example, two or more motion field reference frames, which may include a forward reference frame, a backward reference frame, or both, may be identified as motion field reference frames. Inter-frame motion information may be available for one or more of the identified motion field reference frames.
An estimated motion field is determined at 930. The decoder may determine that the motion field reference frame includes inter-frame motion information representing motion that linearly intersects a current temporal position, which is a temporal position or a frame index position of the current frame. For example, motion information indicative of motion between a first frame of the sequence and a last frame of the sequence, such as a motion vector of the last frame of the sequence referencing a portion of the first reconstructed frame, intersects each other frame in the sequence.
An estimated motion field may be determined for a current temporal location corresponding to the current frame. Determining the estimated motion field may include identifying motion information, such as one or more motion vectors, that intersects the current temporal location, e.g., on a per-pixel basis or a per-block basis. For example, the motion field reference frame may have a frame index greater than a current frame index of the current frame, the motion vector of the motion field reference frame may relate to a reconstructed reference frame having a frame index prior to the current frame index, and the motion vector may be identified as intersecting the current temporal position. In another example, the motion field reference frame may have a frame index greater than the current frame index, the motion vector of the motion field reference frame may relate to a reconstructed reference frame having a frame index greater than the current frame index, and the motion vector may be identified as disjoint to the current temporal location. In another example, the motion field reference frame may have a frame index less than the current frame index, the motion vectors of the motion field reference frame may relate to a reconstructed reference frame having a frame index less than the current frame index, and the motion vectors may be identified as disjoint from the current temporal location. Each block or each pixel of each motion field reference frame may be evaluated to identify intersecting motion information.
Linear projection may be used to project the intersecting motion information to the current temporal location. For motion vectors from intersecting motion information, a linear projection at the intersection of the motion vector and the current temporal location can be used to identify the current spatial location in the estimated motion field. Available forward reference frames, such as the closest available forward reference frame in temporal order, may be identified and a forward component of the estimated motion field, which may be an estimated forward motion vector, may be generated by: the spatial locations in the estimated motion field are projected to the identified forward reference frame based on the corresponding motion vectors that intersect the current temporal location at the current spatial location, e.g., using a linear projection. Available backward reference frames, such as the nearest available backward reference frame in temporal order, may be identified and the backward component of the estimated motion field, which may be a backward motion vector, may be generated by: the spatial locations in the estimated motion field are projected to the identified backward reference frame based on the corresponding motion vector that intersects the current temporal location at the current spatial location, e.g., using a linear projection. An estimated motion field, which may include a forward component and a backward component, may be determined for each spatial location, such as each block location, at a current temporal location, which may have dimensions equivalent to a current encoded frame. For simplicity and clarity, the available forward reference frames and the available backward reference frames may be referred to as motion field projection frames.
A reference co-frame is generated at 940. The reference common frame may be generated by interpolating pixel values from a reference frame indicated by the estimated motion field, such as a motion field projection frame. Although shown separately in fig. 9, motion field determination at 930 and reference co-frame generation at 940 may be performed in a combined manner and may include multiple times or processing stages, such as multi-scale processing, which can include occlusion detection. Generating the reference common frame may include using a per-pixel motion field or a per-block motion field. In some implementations, generating the reference common frame may include using an affine isomorphic model. Other interpolation models may be used.
A reconstructed current frame may be generated at 950. Decoding using the reference co-frame 900 may be included in block-based decoding for the current frame, which may be similar to block-based decoding with respect to fig. 5, except as described herein or otherwise clear from context. For example, the current frame may be decoded on a block basis, which may include, for example, decoding each block of the current frame in a defined order, such as a raster scan order or another defined order. In some implementations, the decoder can determine respective portions of the estimated motion field at 930 and generate respective portions of the reference common frame in a block-by-block manner for blocks coded with respect to the reference common frame at 940. Generating the current block of the reconstructed current frame may include using inter prediction of the reference common frame. An example of inter prediction using a reference common frame is shown in fig. 15. Other decoding elements may be used, such as those described with respect to fig. 15.
Data generated by decoding using the reference co-frame 900 or a portion thereof is output at 960. For example, a reconstructed frame may be stored, such as to be used as a reference frame for decoding another encoded frame. In another example, the reconstructed frame may be output to a presentation device for presentation to a user.
Fig. 10 is a block diagram of an example of a portion of a video sequence being coded using a reference co-frame 1000 according to an implementation of the present disclosure. As used herein, the term coding or variants thereof may refer to encoding, decoding, or corresponding variants thereof. The portion of coding a video sequence using reference co-frame 1000 may be implemented in an encoder, such as encoder 400 shown in fig. 4. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may implement a portion that uses the reference co-frame 1000 to code a video sequence. The portion of coding a video sequence using a reference co-frame 1000 may be implemented in a decoder, such as decoder 500 shown in fig. 5. For example, the intra/inter prediction unit 540 of the decoder 500 shown in fig. 5 may implement a portion that uses the reference common frame 1000 to code a video sequence.
As shown at the top of fig. 10, the portion of the video sequence that is coded using the reference co-frame 1000 is described with reference to the input sequence 700 shown in fig. 7. Coding the input sequence 700 may include coding the input frames 710-780 in a coding order different from the input order. For example, the first input frame 710 may be first coded in coding order, the eighth input frame 780 may be second coded in coding order, the fifth input frame 750 may be third coded in coding order, the third input frame 730 may be fourth coded in coding order, the second input frame 720 may be fifth coded in coding order, the fourth input frame 740 may be sixth coded in coding order, the sixth input frame 760 may be seventh coded in coding order, and the seventh input frame 770 may be eighth coded in coding order.
A first coding section 1010 is shown below the input sequence 700 in coding order. The first encoding portion 1010 includes encoding at least a portion of the first input frame 710 prior to encoding the other input frames 720-780. A first coded frame 1012 may be generated in the first coding section 1010. For example, the first input frame 710 may be intra-coded to generate a first coded frame 1012, as indicated by the real directional line between the first coded frame 1012 and the first input frame 710.
A second coded portion 1020 is shown below the first coded portion 1010 in coding order. The second coding portion 1020 includes coding at least a portion of the eighth input frame 780 before coding the second 720 through seventh input frames 770. As indicated by the stippled background at 1014, a first reconstructed frame 1014 may be generated based on the first encoded frame 1012, which may be a reconstruction of the first input frame 710. A second coded frame 1022 may be generated in the second coded portion 1020. As indicated by the real direction line 1024 between the second encoded frame 1022 and the first reconstructed frame 1014, the eighth input frame 780 may be inter-coded to generate the second encoded frame 1022 using the first reconstructed frame 1014. A portion of the second coded frame 1022 is shown with a back-slashed background to indicate inter-prediction from a reconstructed frame with a lower frame index that precedes the eighth input frame 780 in input sequence order. Although not shown in fig. 10, one or more portions of the second coded frame 1022 may be generated using intra-coding based on the eighth input frame 780.
A third coded portion 1030 is shown below the second coded portion 1020 in the order of coding. The third encoding portion 1030 includes encoding at least a portion of the fifth input frame 750 before encoding the second input frame 720, the third input frame 730, the fourth input frame 740, the sixth input frame 760, and the seventh input frame 770. As indicated by the stippled background at 1026, a second reconstructed frame 1026, which may be a reconstruction of the eighth input frame 780, may be generated based on the second encoded frame 1022.
The third coded portion 1030 shown in FIG. 10 includes five rows R1-R5 that include respective instances of generating the third coded frame, or portions thereof. For example, inter-prediction may be used to generate a third coded frame with reference to the first reconstructed frame 1014 as shown in the first or top row R1. A third coded frame may be generated using inter-prediction with reference to the second reconstructed frame 1026 as shown in the second row R2. Inter prediction, such as bi-prediction or composite prediction, may be used to generate a third coded frame with reference to the first reconstructed frame 1014 and the second reconstructed frame 1026 as shown in the third row R3. Inter prediction may be used to generate a third coded frame using reference to a reference co-frame as shown in the fourth row R4 and the fifth row R5.
A first example of generating a third coded frame is shown at 1032 in the first row R1 of the third coded portion 1030 using the first reconstructed frame 1014 as a reference frame. A portion of the third coded frame is shown with a reverse diagonal background at 1032 in the first example to indicate inter prediction from a reconstructed frame with a lower frame index that precedes the fifth input frame 750 in input sequence order.
A second example of generating a third coded frame is shown at 1034 in the second row R2 of the third coded portion 1030 that uses the second reconstructed frame 1026 as a reference frame. A portion of the third coded frame is shown at 1034 in the second example using a forward slope background to indicate inter prediction from a reconstructed frame with a larger frame index following the fifth input frame 750 in input sequence order.
A third example of generating a third coded frame is shown at 1036 in the third row R3 of the third coded portion 1030 using the first reconstructed frame 1014 as a first reference frame and the second reconstructed frame 1026 as a second reference frame. A portion of the third coded frame 1036 is shown at 1036 using a cross-hatched background to indicate bi-directional or composite inter-frame prediction from a forward reconstructed frame with a larger frame index following the fifth input frame 750 in input sequence order and a backward reconstructed frame with a lower frame index preceding the fifth input frame 750 in input sequence order.
The fourth row R4 of the third coded portion 1030 includes a representation of the motion field 1038 at the video sequence location corresponding to the fifth frame index as indicated by the dashed rectangle. The fifth row R5 of the third coded portion 1030 includes a reference co-frame 1040.
The reference common frame 1040 is generated to be used as a reference frame for inter prediction for coding a frame at a corresponding frame index. The reference common frame 1040 is spatially and temporally concurrent with the input frame with the corresponding frame index. The reference common frame 1040 is generated based on information other than the corresponding input frame. The reference common frame 1040 may be generated by an encoder based on information other than the corresponding input frame that may be used to code the corresponding input frame. The reference common frame 1040 may be independently generated by a decoder for decoding an encoded frame having a corresponding frame index.
The reference co-frame 1040 may be generated based on optical flow estimation and may be referred to as an optical flow-based reference frame. Optical flow estimation may include determining motion fields 1038 corresponding to respective frame indices, and reference common frame 1040 may be referred to as a motion field-based reference frame. Motion field 1038 can be a per pixel motion field or a per block motion field.
Determining motion field 1038 can include identifying reference frames, which can be previously reconstructed frames that can be used to code the frame at the respective frame index for which the corresponding motion information intersects the video sequence location corresponding to the respective frame index, such as the motion field reference frames described in fig. 8 and 9. For example, as indicated in the first, second, and third rows R1-R3 of the third coding section 1030, the first reconstructed frame 1014 and the second reconstructed frame 1026 are reference frames used to code the fifth input frame 750 at a fifth frame index. As represented by the real directional lines 1024 in the second coded portion 1020, the motion information corresponding to the second reconstructed frame 1026 includes motion vectors that reference the first reconstructed frame 1014. For simplicity and clarity, the reference frame (e.g., first reconstructed frame 1014 in this example) referenced by motion information corresponding to the identified previously reconstructed motion field reference frame (e.g., second reconstructed frame 1026 in this example) may be referred to herein as a previous reference frame, and the corresponding motion vector (e.g., the motion vector represented by real direction line 1024 in second coded portion 1020) may be referred to herein as a track motion vector.
The trajectory motion vector 1042 corresponding to the motion vector represented by the real direction line 1024 in the second coded part 1020 is indicated by a dotted direction line in the fourth row R4. The projected spatial position 1044 of the intersection of the trajectory motion vector 1042 with the video sequence position corresponding to the fifth frame index is indicated by the dashed square in the fourth row R4. The projected spatial position 1044 of the intersection of the trajectory motion vector 1042 and the video sequence position corresponding to the fifth frame index corresponds to the probability: motion of a scene or portion thereof captured by a sequence of frames has a trajectory between respective frames along trajectory motion vectors 1042.
The trajectory motion vector 1042 can be projected linearly to estimate the motion field 1038 at the corresponding video sequence location. The fifth row R5 includes forward projection motion vectors 1046 and backward projection motion vectors 1048 that are projected from respective video sequence locations, such as the video sequence location corresponding to the fifth frame index, to motion field projection frames, which may be reference frames that may be used to code the frames at the respective frame indices, such as the first reconstructed frame 1014 and the second reconstructed frame 1026 in this example, using linear projection based on trajectory motion vectors 1042.
The reference common frame 1040 may be interpolated based on values, such as pixel values, from the motion field projection frames, which may be reconstructed frames 1014, 1026 indicated by projected motion vectors 1046, 1048. A portion 1050 of the reference common frame 1040 interpolated based on values, e.g. pixel values, from the motion field projection frames, which may be reconstructed frames 1014, 1026 indicated by projected motion vectors 1046, 1048, may be indicated by a dashed box in the fifth row R5.
The fifth input frame 750, or a portion thereof, such as a block, may be encoded for inter prediction using the reference common frame 1040. For example, as indicated by the solid square with a shallow stippled background, the fifth row R5 of the third coded portion 1030 includes a representation of the prediction block 1052 that is used to inter-code the fifth input frame 750 based on the reference common frame 1040. Inter-prediction of the prediction block 1052 based on the reference common frame 1040 may include identifying a common frame motion vector 1054 as indicated by a real directional line from the prediction block 1052 to a corresponding portion 1050 of the reference common frame 1040. The common frame motion vector 1054 may indicate a motion difference, such as a difference corresponding to non-linear motion, between the corresponding input frame (e.g., the fifth input frame 750) and the motion field interpolated reference common frame 1040.
Although one trajectory motion vector 1042 is shown in fig. 10, a reference common frame may be generated based on multiple trajectory motion vectors associated with various reconstructed reference frames. For example, a reference co-frame at a fourth frame index corresponding to the fourth input frame 740 may be generated based on motion information between the eighth frame and the first frame, the fifth frame and the first frame, the third frame and the fifth frame, the third frame and the eighth frame, the second frame and the fifth frame, the second frame and the eighth frame, or a combination thereof, that intersects a video sequence position corresponding to the fourth frame index.
Fig. 11 is a diagram of an example of identifying a common frame motion vector 1100 according to an implementation of the present disclosure. Except as described herein or otherwise clear from context, identifying a common frame motion vector 1100 may be similar to identifying a common frame motion vector as described with respect to motion estimation at 850 as shown in fig. 8. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may implement identifying the co-frame motion vector 1100. Although fig. 11 shows spatial locations using one vertical spatial dimension for simplicity and clarity, other spatial dimensions, such as a horizontal dimension, may be used.
Fig. 11 shows a representation of the first reconstructed frame 1110 as a solid vertical line on the left. The vertical component of a first spatial position 1112 of a portion, e.g., a pixel or block, of the first reconstructed frame 1110 is shown as a solid circle near the top of the representation of the first reconstructed frame 1110.
The representation of the second reconstructed frame 1120 is displayed as a solid vertical line to the right. The vertical component of a second spatial position 1122 of a portion, e.g., a pixel or block, of the second reconstructed frame 1120 is shown as a solid circle near the bottom of the representation of the second reconstructed frame 1120.
The representation of the current frame 1130 is shown as a solid vertical line in the middle. The vertical component of a current spatial position 1132 of a portion, such as a pixel or block, of current frame 1130 is shown as a solid circle near the top of the representation of current frame 1130. With respect to current frame 1130, first reconstructed frame 1110 is a backward reference frame and second reconstructed frame 1120 is a forward reference frame. Although three frames are shown in fig. 11, other frames may be used, such as a frame temporally preceding the first reconstructed (reference) frame 1110, a frame between the first reconstructed (reference) frame 1110 and the current frame 1130, a frame between the current frame 1130 and the second reconstructed (reference) frame 1120, a frame after the second reconstructed (reference) frame 1120, or any combination thereof.
The temporal order position or frame index of the current frame 1130 may be expressed as n. The first reconstructed frame 1110 may precede the current frame 1130 in coding order, may precede the current frame 1130 in temporal order, and may have a temporal order position or frame index that is lower than the temporal order position or frame index n of the current frame 1130. The time or frame index distance between the time order position or frame index of the first reconstructed frame 1110 and the time order position or frame index n of the current frame 1130 may be expressed as d1, and the time order position or frame index of the first reconstructed frame 1110 may be expressed as n-d 1.
Second reconstructed frame 1120 may precede current frame 1130 in coding order and may follow current frame 1130 in temporal order. The second reconstructed frame 1120 may have a temporal sequential position or frame index that is greater than the temporal sequential position or frame index n of the current frame 1130. The time or frame index distance between the time order position or frame index of the second reconstruction frame 1120 and the time order position or frame index n of the current frame 1130 may be expressed as d2, and the time order position or frame index of the second reconstruction frame 1120 may be expressed as n + d 2.
The non-linear motion 1140 of content captured by a first input frame corresponding to a first reconstructed frame 1110 at a first spatial position 1112, a current frame 1130 at a current spatial position 1132, and a second input frame corresponding to a second reconstructed frame 1120 at a second spatial position 1122 is shown as a solid curve. Estimated linear motion 1150 between first spatial position 1112 and second spatial position 1122 is illustrated using a dashed line. The vertical component of a portion, e.g., a spatial location 1152 of a pixel or block, of a reference co-frame (not separately shown) at frame index n of current frame 1130 is shown as a dashed circle at the intersection of estimated linear motion 1150 and the temporal location corresponding to current frame 1130.
The current portion at the current spatial location 1132 of the current frame 1130 may be predicted based on the referenced co-frame, e.g., using motion estimation, to identify the spatial location 1152 from the referenced co-frame, which may include identifying a co-frame motion vector that indicates a spatial difference or displacement from the current spatial location 1132 to the spatial location 1152 in the referenced co-frame. The co-frame motion vectors 1160 may be similar to motion vectors, except as described herein or otherwise clear from context.
Fig. 12 is a flow diagram of an example of an inter-coded motion data reduction 1200 portion according to an implementation of the present disclosure. Motion data reduction 1200 may be implemented in an encoder, such as encoder 400 shown in fig. 4. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may implement the motion data reduction 1200. As shown in fig. 12, motion data reduction 1200 includes identifying a current block at 1210, identifying a previously coded block at 1220, and determining whether inter prediction for the current block is based on a reference co-frame at 1230.
A current block is identified at 1210. For example, the current frame may be encoded on a block basis, which may include, for example, encoding each block of the current frame in a defined order, such as a raster scan order or another defined order. The current block may be identified based on motion estimation at 850 and motion data reduction at 860 as shown in fig. 8.
Previously coded blocks are identified at 1220. For example, the encoder may search neighboring blocks using a defined search procedure to generate an ordered set of candidate previously coded blocks, which may include spatially neighboring previously coded blocks, temporally co-located previously coded blocks, or a combination thereof, and may identify the candidate previously coded blocks as previously coded blocks from the set of candidate previously coded blocks.
Identifying previously coded blocks at 1220 can include identifying coding information, such as motion information, of previously coded blocks. For example, the motion information of a previously coded block may include a motion vector (single reference) related to a reference frame. In another example, the motion information of a previously coded block may include motion vectors (composite reference) related to the corresponding reference frame. In another example, the motion information of previously coded blocks may include a co-frame motion vector (co-frame reference) that refers to a co-frame.
The encoder determines whether inter prediction for the current block is based on a reference co-frame at 1230.
Inter prediction for the current block may be based on a reference frame or a combination of reference frames, rather than a reference co-frame, and motion data reduction 1200 may include determining whether motion information of a previously coded block includes a co-frame motion vector of the previously coded block at 1240. The motion information of the previously coded block may include a co-frame motion vector of the previously coded block, and the motion data reduction 1200 may include: a motion vector prediction for the current block is determined based on the co-frame motion vector of the previously coded block at 1250 and an indication of the motion vector prediction may be output at 1260. An example of determining a motion vector prediction for a current block based on a co-frame motion vector of a previously coded block is shown at 1300 in fig. 13.
Inter prediction for the current block may be based on a reference co-frame, and motion data reduction 1200 may include identifying a co-frame motion vector prediction for the current block at 1270 and may include outputting an indication of the co-frame motion vector prediction at 1280. An example of determining a co-frame motion vector prediction for a current block is shown at 1310 in FIG. 13.
Fig. 13 illustrates a flow diagram of an example of determining a motion vector prediction 1300 for a current block based on a co-frame motion vector of a previously coded block according to an implementation of the present disclosure and an example of determining a co-frame motion vector prediction for a current frame 1310 according to an implementation of the present disclosure. Determining the motion vector prediction 1300 for the current block may be implemented in an encoder, such as encoder 400 shown in fig. 4. For example, the intra/inter prediction unit 410 of the encoder 400 shown in fig. 4 may implement determining the motion vector prediction 1300 for the current block. As shown in fig. 13, determining a motion vector prediction 1300 for a current block includes identifying an aligned block at 1302, determining motion information of the aligned block at 1304, and determining a motion vector prediction at 1306.
An alignment block is identified at 1302. The aligned block may be identified based on a co-frame motion vector from a previously coded block adjacent to the current block, such as a co-frame motion vector identified from motion information of a previously decoded block as shown in fig. 12. The aligned block may be a block from a reference co-frame, such as a 4 x 4 block. The spatial position of the alignment block may be centered at a position in the reference common frame that is: spatially shifted or offset by a position corresponding to the spatial position of the co-frame motion vector from the previously coded block in the current frame.
Motion information for the aligned block is determined at 1304. Determining motion information for the aligned block may include determining motion vectors for the projected reference frame from the reference common frame to the motion of the reference common frame. Determining motion information of the aligned block may include: a first motion vector (opfl _ mv1) is determined, such as a backward motion vector indicating a portion of a backward reference frame. Determining motion information of the aligned block may include: a second motion vector (opf1_ mv2) is determined, such as a forward motion vector indicating a portion of a forward reference frame. The motion vector of the alignment block may be determined by averaging the motion fields corresponding to the alignment block.
A motion vector prediction for the current block is determined at 1306. The motion vector prediction may indicate a displacement between a previously coded block adjacent to a current block of the current frame and the corresponding reference frame identified at 1304. A first motion vector prediction (MV _ pred1), which may be expressed as MV _ pred1 ═ cf _ MV + opfl _ MV1, may be determined based on the first motion vector (opfl _ MV1) of the aligned block and the co-frame motion vector (cf _ MV). A second motion vector prediction (MV _ pred2), which may be expressed as MV _ pred2 ═ cf _ MV + opfl _ MV2, may be determined based on the second motion vector (opfl _ MV2) of the aligned block and the co-frame motion vector (cf _ MV). Motion vector predictions for previously coded blocks that are adjacent to the current block may be identified as motion vector predictions for the current block. The current block may be coded with reference to a single reference frame, and motion vector predictions for previously coded blocks that are adjacent to the current block and associated with the single reference frame may be identified as motion vector predictions for the current block.
Determining the co-frame motion vector prediction for the current block 1310 may be implemented in an encoder, such as encoder 400 shown in FIG. 4. For example, the intra/inter prediction unit 410 of the encoder 400 shown in FIG. 4 may implement determining a co-frame motion vector prediction for the current block 1310. As shown in fig. 13, determining the co-frame motion vector prediction for the current block 1310 includes determining, at 1312, whether the motion information of the previously coded block includes a co-frame motion vector of the previously coded block.
The motion information of the previously coded block may omit the co-frame motion vector of the previously coded block, and determining the co-frame motion vector prediction for the current block 1310 may include identifying the motion vector of the previously coded block at 1314 and determining the co-frame motion vector prediction based on the motion vector of the previously coded block at 1316.
Although not separately shown in fig. 13, determining the co-frame motion vector prediction for the current block 1310 may include determining whether motion information of a previously coded block includes a composite motion vector, such as a backward motion vector and a forward motion vector. In some implementations, the motion information of the previously coded block may include a single motion vector, and determining a co-frame motion vector prediction for the current block 1310 based on the previously coded block may additionally be omitted.
In some implementations, the motion information of the previously coded block may include a backward motion vector and a forward motion vector, and a co-frame motion vector prediction may be determined based on the backward motion vector and the forward motion vector at 1316. The backward motion vector of the previously coded block may relate to a backward reference frame corresponding to the first ordinal position (d1) at a first temporal distance from the current frame. The forward motion vector of the previously coded block may relate to a forward reference frame corresponding to the second ordinal position (d2) at a second temporal distance from the current frame. Determining the co-frame motion vector prediction (cf _ MV _ pred) based on the backward motion vector (MV _ ref1) and the forward motion vector (MV _ ref2) may include: determining a sum of a product of a quotient of the second ordinal position (d2) and a sum of the first ordinal position (d1) and the second ordinal position (d2) and a backward motion vector (MV _ ref1) and a sum of a quotient of the first ordinal position (d1) and a sum of the first ordinal position (d1) and the second ordinal position (d2) and a forward motion vector (MV _ ref2), which may be expressed as follows:
cf _ MV _ pred ═ d2/(d1+ d2) × MV _ ref1+ d1/(d1+ d2) × MV _ ref2 [ equation 1]
The motion information of the previously coded block may include a co-frame motion vector of the previously coded block, and determining a co-frame motion vector prediction for the current block 1310 may include using the co-frame motion vector of the previously coded block as the co-frame motion vector prediction for the current block at 1318.
Fig. 14 is a diagram of an example of determining a motion vector prediction 1400 for a current block based on a co-frame motion vector of a previously coded block according to an implementation of the present disclosure. Determining the motion vector prediction 1400 for the current block may be similar to determining the motion vector prediction for the current block as shown at 1300 in fig. 13, except as described herein or otherwise clear from the context.
FIG. 14 shows a representation of a current block 1410 from a current frame. Neighboring blocks 1420, which are previously coded blocks, are shown adjacent to the current block 1410. The bold directional line indicates the co-frame motion vector 1430 of the neighboring block 1420. The co-frame motion vector 1430 indicates the spatial displacement between the spatial location in the reference co-frame that spatially corresponds to the neighboring block 1420 in the current frame and the spatial location of the aligned block 1440 in the reference co-frame. Alignment block 1440 may be a block from a reference co-frame, such as a 4 x 4 block.
Backward motion vector prediction 1470 for neighboring block 1420 indicates the spatio-temporal displacement between the position in the backward reference frame indicated by backward motion vector 1460 of alignment block 1440 and the position in the current frame of neighboring block 1420. Forward motion vector prediction 1472 for neighboring block 1420 indicates the spatio-temporal displacement between the position in the forward reference frame indicated by forward motion vector 1462 of aligned block 1440 and the position in the current frame of neighboring block 1420.
The backward motion vector prediction 1480 for the current block 1410 indicates a spatial-temporal displacement between a location in a backward reference frame indicated by the backward motion vector prediction 1480 for the current block 1410 and a location in a current frame of the current block 1410 and is indicated using a dashed direction line. The forward motion vector prediction 1482 for the current block 1410 indicates a spatial-temporal displacement between a location in a forward reference frame indicated by the forward motion vector prediction 1482 for the current block 1410 and a location in a current frame of the current block 1410 and is indicated using a dashed direction line.
Fig. 15 is a flow diagram of an example of inter prediction 1500 using a reference common frame according to an implementation of the present disclosure. The inter prediction 1500 using a reference common frame may be implemented in a decoder, such as the decoder 500 shown in fig. 5. For example, the intra/inter prediction unit 540 of the decoder 500 shown in fig. 5 may implement inter prediction 1500 using a reference common frame.
As shown in fig. 15, inter prediction 1500 using a reference common frame includes: the method includes identifying a current block at 1510, decoding current block motion information at 1520, identifying a previously decoded block at 1530, and determining whether to identify a prediction block based on a reference co-frame at 1540.
A current block from a current frame is identified at 1510. For example, the current frame may be decoded on a block basis, which may include, for example, decoding each block of the current frame in a defined order, such as a raster scan order or another defined order.
The coded information for the current block is decoded at 1520. Decoding the coding information of the current block may include decoding the coding information of the current block from an encoded bitstream.
Previously decoded blocks are identified at 1530. A previously decoded block may be identified based on coding information of the current block. For example, the coding information of the current block decoded at 1520 may identify a coding mode of the current block, such as an inter prediction mode, and may identify a previously decoded block based on the coding mode of the current block. In some implementations, the decoder may search neighboring blocks using a defined search procedure to generate an ordered set of candidate previously decoded blocks, which may include spatially neighboring previously decoded blocks, temporally co-located previously decoded blocks, or a combination thereof, and coding information of the current block, such as a coding mode, may indicate a candidate previously decoded block from the set of candidate previously decoded blocks as a previously decoded block.
Identifying the previously decoded block at 1530 may include identifying coding information, such as motion information, for the previously decoded block. For example, the motion information of a previously coded block may include a motion vector (single reference) related to a reference frame. In another example, the motion information of a previously coded block may include motion vectors (composite reference) related to the corresponding reference frame. In another example, the motion information of previously coded blocks may include a co-frame motion vector (co-frame reference) that refers to a co-frame.
A determination is made at 1540 whether to identify a prediction block based on the reference co-frame. Whether to identify the prediction block based on the reference co-frame may be determined based on coding information of the current block.
The coding information of the current block may indicate that the current block is coded with reference to a reference frame, such as a temporal reference frame, which may be a previously reconstructed frame and may be different from the reference common frame. The decoder may determine that the coding information for the current block indicates that the current block is coded with reference to a reference frame, the decoder may determine that a prediction block identified for decoding the current block based on a reference co-frame may be omitted, and the decoder may determine whether the motion information of a previously decoded block includes a co-frame motion vector of the previously decoded block at 1550. The decoder may determine at 1550 that the motion information of the previously decoded block includes a co-frame motion vector of the previously decoded block, and the decoder may determine 1560 a prediction block for the current block based on the co-frame motion vector of the previously decoded block. An example of determining a prediction block for a current block based on a co-frame motion vector of a previously decoded block is shown in fig. 15.
Although the inter-prediction 1500 using a reference co-frame is described with reference to one previously decoded block, the inter-prediction 1500 using a reference co-frame may include identifying a plurality of previously decoded blocks at 1530 and determining a corresponding candidate prediction block at 1560 for each previously decoded block determined to include a co-frame motion vector at 1550.
The coding information of the current block may indicate that the current block is coded with reference to the reference common frame. The decoder may determine that the coding information for the current block indicates that the current block is coded with a reference common frame, and the decoder may identify a prediction block for the current block based on the reference common frame at 1570. An example of determining a prediction block for a current block based on a reference common frame is shown in fig. 16. Although the inter-prediction 1500 using a reference co-frame is described with reference to one previously decoded block, the inter-prediction 1500 using a reference co-frame may include identifying a plurality of previously decoded blocks at 1530 and, for each previously decoded block, determining a corresponding candidate prediction block at 1570.
Fig. 16 is a flow diagram of an example of determining a prediction block 1600 based on a co-frame motion vector from a previously coded block according to an implementation of the present disclosure. Determining the prediction block 1600 may be implemented in a decoder, such as the decoder 500 shown in fig. 5. For example, the intra/inter prediction unit 540 of the decoder 500 shown in fig. 5 may implement determining the prediction block 1600.
As shown in fig. 16, determining the prediction block 1600 includes identifying an aligned block at 1610, determining motion information of the aligned block at 1620, determining a motion vector prediction at 1630, determining a motion vector at 1640, and identifying the prediction block at 1650.
An alignment block is identified at 1610. The aligned block may be identified based on a co-frame motion vector from a previously coded block adjacent to the current block, such as a co-frame motion vector identified from motion information of a previously decoded block as shown in fig. 14. The aligned block may be a block from a reference co-frame, such as a 4 x 4 block. The spatial position of the alignment block may be centered at a position in the reference common frame that is: a position spatially corresponding to the spatial position of the previously coded block in the current frame shifted or offset by the common frame motion vector (cf _ MV).
Motion information for the aligned block is determined 1620. Determining motion information of the aligned block may include: motion vectors indicating the displacement of the motion projected reference frame from the reference common frame to the reference common frame are determined. Determining motion information of the aligned block may include: a first motion vector (opfl _ mv1) is determined, such as a backward motion vector indicating a portion of a backward reference frame. Determining motion information of the aligned block may include: a second motion vector (opf1_ mv2) is determined, such as a forward motion vector indicating a portion of a forward reference frame. The motion vector of the alignment block may be determined by averaging the motion fields corresponding to the alignment block.
A motion vector prediction for the current block is determined at 1630. The motion vector prediction may indicate a displacement between a previously coded block adjacent to the current block in the current frame and the corresponding reference frame identified at 1620. A first motion vector prediction (MV _ pred1), which may be expressed as MV _ pred1 ═ cf _ MV + opfl _ MV1, may be determined based on the first motion vector (opfl _ MV1) of the aligned block and the co-frame motion vector (cf _ MV). A second motion vector prediction (MV _ pred2), which may be expressed as MV _ pred2 ═ cf _ MV + opfl _ MV2, may be determined based on the second motion vector (opfl _ MV2) of the aligned block and the co-frame motion vector (cf _ MV). Motion vector predictions for previously coded blocks that are adjacent to the current block may be identified as motion vector predictions for the current block.
The motion vector for the current block is determined 1640. Although not separately shown in fig. 16, determining the motion vector of the current block may include determining whether the motion information of the current block includes a differential motion vector. The coding information of the current block may omit the differential motion vector, and the motion vector prediction for the current block may be identified as the motion vector of the current block. The coding information of the current block may include a differential motion vector, and the sum of the backward differential motion vector and the backward motion vector prediction may be identified as a backward motion vector of the current block, and the sum of the forward differential motion vector and the forward motion vector prediction may be identified as a forward motion vector of the current block.
A prediction block for decoding the current block is identified at 1650. For example, a composite reference prediction block may be identified based on the motion vector of the current block determined at 1640.
Fig. 17 is a flowchart of an example of determining a prediction block 1700 for a current block based on a reference common frame according to an implementation of the present disclosure. The determination of the prediction block 1700 for the current block based on the reference co-frame may be implemented in a decoder, such as the decoder 500 shown in fig. 5. For example, the intra/inter prediction unit 540 of the decoder 500 shown in fig. 5 may implement the prediction block 1700 determined for the current block based on the reference common frame.
As shown in fig. 17, determining a prediction block 1700 for the current block based on the reference common frame includes: at 1710, it is determined whether the motion information of the previously coded block includes a co-frame motion vector of the previously coded block.
The motion information of the previously coded block may omit a co-frame motion vector of the previously coded block, and determining the prediction block 1700 for the current block based on the reference co-frame may include: the motion vector of the previously coded block is identified at 1720, the co-frame motion vector prediction for the current block is determined at 1730, the co-frame motion vector for the current block is determined at 1740, and the prediction block is identified at 1750.
One or more motion vectors for a previously coded block may be identified at 1720 from motion information for the previously coded block. Although not separately shown in fig. 17, determining the prediction block 1700 for the current block based on the reference common frame may include determining whether motion information of a previously coded block includes a composite motion vector, such as a backward motion vector and a forward motion vector. In some implementations, the motion information of the previously coded block may include a single motion vector, and determining a prediction block 1700 for the current block based on the reference common frame based on the previously coded block may additionally be omitted.
In some implementations, the motion information of the previously coded block may include a backward motion vector and a forward motion vector, and a co-frame motion vector prediction for the current block may be determined at 1730 based on the backward motion vector and the forward motion vector. The backward motion vector of the previously coded block may relate to a backward reference frame at a first temporal distance (d1) from the current frame. The forward motion vector of the previously coded block may relate to a forward reference frame at a second temporal distance (d2) from the current frame. The determination of the co-frame motion vector prediction (cf _ MV _ pred) based on the backward motion vector (MV _ ref1) and the forward motion vector (MV _ ref2) can be expressed as shown in equation 1.
The motion information of the previously coded block may include a co-frame motion vector of the previously coded block, and determining the prediction block for the current block based on the reference co-frame 1700 may include: the co-frame motion vector of the previously coded block is used as the co-frame motion vector prediction for the current block at 1740.
The co-frame motion vector for the current block is determined at 1750. Although not separately shown in fig. 17, determining the co-frame motion vector of the current block may include determining whether the motion information of the current block includes a differential co-frame motion vector. The coding information of the current block may omit the differential co-frame motion vector and may identify the co-frame motion vector prediction for the current block as the co-frame motion vector of the current block. The coding information of the current block may include a differential common frame motion vector, and the sum of the differential common frame motion vector and the common frame motion vector prediction may be identified as the common frame motion vector of the current block.
At 1760, a prediction block for decoding the current block is identified. For example, a prediction block may be identified based on a portion of a reference co-frame indicated by the co-frame motion vector identified at 1750 relative to the position of the current block in the current frame.
Fig. 18 is a diagram of an example of determining a co-frame motion vector prediction for a current block 1800 based on motion vectors of previously coded blocks according to an implementation of the present disclosure. Determining a co-frame motion vector prediction for current block 1800 based on motion vectors of previously coded blocks may be similar to determining a co-frame motion vector prediction as shown at 1730 in fig. 17, except as described herein or otherwise clear from the context.
FIG. 18 shows a representation of a current block 1810 from a current frame. An adjacent block 1820, which is a previously coded block, is shown adjacent to the current block 1810. The backward motion vector 1830 of the neighboring block 1820 and the forward motion vector 1832 of the neighboring block 1820 may be identified based on the motion information of the previously coded neighboring block 1820. The backward motion vector 1830 of the neighboring block 1820 may refer to a backward reference frame (not shown) used to code the neighboring block 1820. The time or frame index distance between the backward reference frame and the current frame is indicated by d 1. The forward motion vectors 1832 of the neighboring block 1820 may relate to a forward reference frame (not shown) used to code the neighboring block 1820. The time or frame index distance between the forward reference frame and the current frame is indicated by d 2.
The bold direction line indicates the common frame motion vector prediction 1840 for the neighboring block 1820. The co-frame motion vector prediction 1840 may be generated based on the backward motion vector 1830 of the neighboring block 1820 and the forward motion vector 1832 of the neighboring block 1820, with the time or frame index distance between the backward reference frame and the current frame indicated at d1 and the time or frame index distance between the forward reference frame and the current frame indicated at d 2. The bold dashed direction line indicates a common frame motion vector prediction 1850 for the current block 1810, which may be identified based on the common frame motion vector prediction 1840 for the neighboring block 1820.
The word "example" or "exemplary" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "exemplary" is intended to present concepts in a concrete fashion. The term "or" as used in this application is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, or clear from context, "X includes a or B" is intended to mean any of the natural inclusive permutations. That is, if X comprises A; x comprises B; or X includes both A and B, then "X includes A or B" satisfies any of the foregoing. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Moreover, unless described as such, the use of the term "an embodiment" or "one embodiment" or "an implementation" or "one implementation" throughout is not intended to refer to the same embodiment or implementation. As used herein, the terms "determine" and "identify," or any variation thereof, include selecting, determining, calculating, looking up, receiving, determining, establishing, obtaining, or otherwise identifying or determining in any way using one or more of the devices shown in fig. 1.
Moreover, for simplicity of explanation, while the figures and descriptions herein may include a sequence or series of steps or stages, elements of the methods disclosed herein may occur in various orders and/or concurrently. Additionally, elements of the methods disclosed herein may appear with other elements not explicitly shown and described herein. Furthermore, one or more elements of the methods described herein may be omitted from embodiments of the methods in accordance with the disclosed subject matter.
Implementations of the transmitting computing and communication device 100A and/or the receiving computing and communication device 100B (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuitry. In the claims, the term "processor" should be understood to include any of the foregoing hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. Furthermore, the parts of the transmitting computing and communication device 100A and the receiving computing and communication device 100B do not have to be implemented in the same way.
Further, in one embodiment, for example, the transmitting computing and communication device 100A or the receiving computing and communication device 100B may be implemented using a computer program that, when executed, implements any of the respective methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be utilized which may contain specialized hardware for carrying out any of the methods, algorithms, or instructions described herein.
The transmitting computing and communication device 100A and the receiving computing and communication device 100B may be implemented on a computer in a real-time video system, for example. Alternatively, the transmitting computing and communication device 100A may be implemented on a server and the receiving computing and communication device 100B may be implemented on a device separate from the server, such as a handheld communication device. In this case, the transmission computing and communication device 100A may encode the content into an encoded video signal using the encoder 400 and transmit the encoded video signal to the communication device. The communication device may then in turn decode the encoded video signal using the decoder 500. Alternatively, the communication device may decode content stored locally on the communication device, e.g., content not transmitted by the transmitting computing and communication device 100A. Other suitable transmit computing and communication device 100A and receive computing and communication device 100B embodiments are available. For example, the receiving computing and communication device 100B may be a generally stationary personal computer rather than a portable communication device, and/or a device including the encoder 400 may also include the decoder 500.
Furthermore, all or portions of the embodiments may take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be any apparatus that can, for example, tangibly embody, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device.
Other suitable media may also be used. The above embodiments have been described to allow easy understanding of the present application without limitation. On the contrary, this application is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (20)
1. A method, comprising:
generating, by a processor, a decoded frame by decoding a current frame from an encoded bitstream, wherein decoding comprises:
identifying a current block from the current frame;
identifying a previously decoded block based on coding information of the current block;
determining whether the motion information of the previously decoded block includes a co-frame motion vector of the previously decoded block;
determining whether to identify a prediction block for decoding the current block based on a reference common frame;
in response to determining to omit identifying the prediction block for decoding the current block based on the reference common frame:
in response to determining that the motion information of the previously decoded block includes the co-frame motion vector of the previously decoded block:
identifying an aligned block in the reference common frame based on a spatial position of the previously decoded block and the common frame motion vector of the previously decoded block;
determining a motion vector of the aligned block based on the motion field information of the aligned block;
determining a motion vector prediction for the current block based on the motion vector of the aligned block and the co-frame motion vector of the previously decoded block;
determining a motion vector for the current block based on the motion vector prediction for the current block; and is
Identifying the prediction block based on a reference frame indicated by the motion vector of the current block;
in response to determining to identify the prediction block for decoding the current block based on the reference common frame:
in response to determining that the motion information of the previously decoded block includes the co-frame motion vector of the previously decoded block:
determining a co-frame motion vector prediction for the current block based on the co-frame motion vector of the previously decoded block;
in response to determining that the motion information of the previously decoded block omits the co-frame motion vector of the previously decoded block:
identifying a forward motion vector from the motion information of the previously decoded block;
identifying a backward motion vector from the motion information of the previously decoded block;
determining a co-frame motion vector prediction for the current block based on the forward motion vector and the backward motion vector;
determining a co-frame motion vector for the current block based on the co-frame motion vector prediction for the current block; and is
Identifying the prediction block based on the reference common frame and the common frame motion vector of the current block;
generating a decoded block corresponding to the current block based on the prediction block; and is
Including the decoded block in the decoded frame; and
outputting a reconstructed frame based on the decoded frame.
2. The method of claim 1, wherein decoding comprises:
generating the reference common frame for the current frame based on a motion field at a spatio-temporal location corresponding to a spatio-temporal location of the current frame such that the reference common frame indicates linear motion at the spatio-temporal location among a sequence of frames, wherein the sequence of frames includes the current frame.
3. The method of claim 1 or 2, wherein:
the co-frame motion vector of the previously decoded block indicates a spatial displacement between a position of the previously decoded block in the current frame and a position in the reference co-frame; and is
The common frame motion vector for the current block indicates a spatial displacement between a position of the current block in the current frame and a position in the reference common frame.
4. The method of any of claims 1-3, wherein determining whether to identify the prediction block for decoding the current block based on the reference co-frame comprises:
decoding the coding information from the current block of the encoded bitstream; and
determining, based on the coding information for the current block, whether to identify the prediction block for decoding the current block based on the reference common frame.
5. The method of any of claims 1 to 4, wherein determining the motion vector of the aligned block comprises:
obtaining the motion vector of the aligned block by averaging the motion field information corresponding to the aligned block.
6. The method of any of claims 1-5, wherein determining the motion vector prediction for the current block based on the motion vector of the aligned block and the co-frame motion vector of the previously decoded block comprises:
determining a motion vector prediction for the previously decoded block based on the motion vector of the aligned block and the co-frame motion vector of the previously decoded block; and
identifying the motion vector prediction of the previously decoded block as the motion vector prediction of the current block.
7. The method of any of claims 1 to 6, wherein:
determining the motion vector of the aligned block comprises:
determining a first motion vector of the aligned block; and
determining a second motion vector for the aligned block; and is
Determining the motion vector prediction for the current block comprises:
determining a first motion vector prediction for the current block as a sum of the first motion vector of the aligned block and the co-frame motion vector; and
determining a second motion vector prediction for the current block as a sum of the second motion vector of the aligned block and the co-frame motion vector.
8. The method of any of claims 1 to 7, wherein:
the current frame is a frame from a sequence of frames, wherein each frame from the sequence of frames has a respective ordinal position in the sequence of frames, wherein the current frame has a current ordinal position in the sequence of frames;
the backward motion vector indicates a backward reference frame at a first ordinal position before the current ordinal position;
the forward motion vector indicates a forward reference frame at a second ordinal position after the current ordinal position; and is
Determining the co-frame motion vector prediction for the current block comprises:
determining a sum of a result of multiplying the backward motion vector by a quotient of the second ordinal position and the sum of the first ordinal position and the second ordinal position and a result of multiplying the forward motion vector by a quotient of the first ordinal position and the sum of the first ordinal position and the second ordinal position.
9. A method, comprising:
generating, by a processor, an encoded frame by encoding a current frame from an input bitstream, wherein the encoding comprises:
identifying a current block from the current frame;
identifying a previously coded block;
determining whether the motion information of the previously coded block includes a co-frame motion vector of the previously coded block;
determining whether to identify a prediction block for encoding the current block based on a reference common frame;
in response to determining to omit identifying the prediction block for encoding the current block based on the reference common frame:
in response to determining that the motion information of the previously coded block includes the co-frame motion vector of the previously coded block:
identifying an aligned block in the reference common frame based on a spatial location of the previously coded block and the common frame motion vector of the previously coded block;
determining a motion vector of the aligned block based on the motion field information of the aligned block;
determining a motion vector prediction for the current block based on the motion vector of the aligned block and the co-frame motion vector of the previously coded block; and
including an indication of the motion vector prediction for the current block in an output bitstream; and is
In response to determining that the prediction block for encoding the current block is identified based on the reference co-frame, determining a co-frame motion vector prediction for the current block, wherein determining the co-frame motion vector prediction for the current block comprises:
in response to determining that the motion information of the previously coded block includes the co-frame motion vector of the previously coded block:
determining the co-frame motion vector prediction for the current block based on the co-frame motion vector of the previously coded block;
in response to determining that the motion information of the previously coded block omits the co-frame motion vector of the previously coded block:
identifying a forward motion vector from the motion information of the previously coded block;
identifying a backward motion vector from the motion information of the previously coded block; and
determining the co-frame motion vector prediction for the current block based on the forward motion vector and the backward motion vector; and is
Including the indication of the co-frame motion vector prediction for the current block in the output bitstream; and
outputting the output bitstream.
10. The method of claim 9, wherein encoding comprises:
generating the reference common frame for the current frame based on a motion field at a spatio-temporal location corresponding to a spatio-temporal location of the current frame such that the reference common frame indicates linear motion at the spatio-temporal location among a sequence of frames, wherein the sequence of frames includes the current frame.
11. The method of claim 9 or 10, wherein:
the co-frame motion vector of the previously coded block indicates a spatial displacement between a position of the previously coded block in the current frame and a position in the reference co-frame; and is
The common frame motion vector for the current block indicates a spatial displacement between a position of the current block in the current frame and a position in the reference common frame.
12. The method of any of claims 9 to 11, wherein determining the motion vector of the aligned block comprises:
obtaining the motion vector of the aligned block by averaging the motion field information corresponding to the aligned block.
13. The method of any of claims 9 to 12, wherein determining the motion vector prediction for the current block based on the motion vector of the aligned block and the co-frame motion vector of the previously coded block comprises:
determining a motion vector prediction for the previously coded block based on the motion vector of the aligned block and the co-frame motion vector of the previously coded block; and
identifying the motion vector prediction of the previously coded block as the motion vector prediction of the current block.
14. The method of any of claims 9 to 13, wherein:
determining the motion vector of the aligned block comprises:
determining a first motion vector of the aligned block; and
determining a second motion vector for the aligned block; and is
Determining the motion vector prediction for the current block comprises:
determining a first motion vector prediction for the current block as a sum of the first motion vector of the aligned block and the co-frame motion vector; and
determining a second motion vector prediction for the current block as a sum of the second motion vector of the aligned block and the co-frame motion vector.
15. The method of any of claims 9 to 14, wherein:
the current frame is a frame from an input frame sequence, wherein each frame from the input frame sequence has a respective sequential position in the input frame sequence, wherein the current frame has a current sequential position in the input frame sequence;
the backward motion vector indicates a backward reference frame at a first ordinal position before the current ordinal position;
the forward motion vector indicates a forward reference frame at a second ordinal position after the current ordinal position; and is
Determining the co-frame motion vector prediction for the current block comprises:
determining a sum of a result of multiplying the backward motion vector by a quotient of the second ordinal position and the sum of the first ordinal position and the second ordinal position and a result of multiplying the forward motion vector by a quotient of the first ordinal position and the sum of the first ordinal position and the second ordinal position.
16. A method, comprising:
generating, by a processor, an encoded frame by encoding a current frame from an input bitstream, wherein the encoding comprises:
generating a reference common frame that spatio-temporally corresponds to the current frame, wherein the current frame is a frame from an input frame sequence, wherein each frame from the input frame sequence has a respective sequential position in the input frame sequence, and wherein the current frame has a current sequential position in the input frame sequence; and
encoding the current frame using the reference common frame;
including the encoded frame in an output bitstream; and
outputting the output bitstream.
17. The method of claim 16, wherein encoding the current frame using the reference common frame comprises:
generating a prediction block for encoding a current block in the current frame based on a reconstructed reference frame, wherein the reconstructed reference frame has a different sequential position in the input frame sequence than the current sequential position;
determining a motion vector prediction for the current block based on a co-frame motion vector of a previously coded block adjacent to the current block in the current frame; and
including an indication of the motion vector prediction for the current block in the output bitstream.
18. The method of claim 17, wherein determining the motion vector prediction for the current block comprises:
identifying an aligned block of the reference common frame based on a spatial location of the previously coded block and the common frame motion vector of the previously coded block;
determining a motion vector of the aligned block based on the motion field information of the aligned block;
determining the motion vector prediction for the current block based on the motion vector of the aligned block and the co-frame motion vector of the previously coded block comprises:
determining a motion vector prediction for the previously coded block based on the motion vector of the aligned block and the co-frame motion vector of the previously coded block; and
identifying the motion vector prediction of the previously coded block as the motion vector prediction of the current block.
19. The method of any of claims 16 to 18, wherein encoding the current frame using the reference common frame comprises:
generating a prediction block for encoding a current block in the current frame based on the reference common frame;
determining a co-frame motion vector prediction for the current block, wherein determining the co-frame motion vector prediction for the current block comprises:
in response to determining that motion information of previously coded blocks neighboring the current block in the current frame includes a co-frame motion vector of the previously coded blocks, determining the co-frame motion vector prediction for the current block based on the co-frame motion vector of the previously coded blocks; and
in response to determining that the motion information of the previously coded block includes a composite motion vector of the previously coded block, determining the co-frame motion vector prediction for the current block based on the composite motion vector of the previously coded block; and
including an indication of the co-frame motion vector prediction for the current block in the output bitstream.
20. The method of claim 19, wherein determining the co-frame motion vector prediction for the current block based on the composite motion vector for the previously coded block comprises:
identifying a forward motion vector from the motion information of the previously coded block;
identifying a backward motion vector from the motion information of the previously coded block; and
determining a co-frame motion vector prediction for the current block based on the forward motion vector and the backward motion vector.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/131,133 US11665365B2 (en) | 2018-09-14 | 2018-09-14 | Motion prediction coding with coframe motion vectors |
US16/131,133 | 2018-09-14 | ||
PCT/US2019/034107 WO2020055464A1 (en) | 2018-09-14 | 2019-05-28 | Motion prediction coding with coframe motion vectors |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111819850A true CN111819850A (en) | 2020-10-23 |
CN111819850B CN111819850B (en) | 2024-03-29 |
Family
ID=66858037
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980017993.4A Active CN111819850B (en) | 2018-09-14 | 2019-05-28 | Method and apparatus for motion predictive coding by co-frame motion vectors |
Country Status (4)
Country | Link |
---|---|
US (2) | US11665365B2 (en) |
EP (1) | EP3744097A1 (en) |
CN (1) | CN111819850B (en) |
WO (1) | WO2020055464A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11166015B2 (en) * | 2019-03-06 | 2021-11-02 | Tencent America LLC | Method and apparatus for video coding |
US20220385748A1 (en) * | 2021-05-27 | 2022-12-01 | Qualcomm Incorporated | Conveying motion data via media packets |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120314027A1 (en) * | 2004-12-17 | 2012-12-13 | Dong Tian | Method and System for Processing Multiview Videos for View Synthesis Using Motion Vector Predictor List |
US20130004093A1 (en) * | 2011-06-29 | 2013-01-03 | Toshiyasu Sugio | Image coding method, image decoding method, image coding apparatus, image decoding apparatus, and image coding and decoding apparatus |
CN107205156A (en) * | 2016-03-18 | 2017-09-26 | 谷歌公司 | Pass through the motion-vector prediction of scaling |
Family Cites Families (51)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5574663A (en) * | 1995-07-24 | 1996-11-12 | Motorola, Inc. | Method and apparatus for regenerating a dense motion vector field |
US6055274A (en) * | 1997-12-30 | 2000-04-25 | Intel Corporation | Method and apparatus for compressing multi-view video |
GB9928022D0 (en) * | 1999-11-26 | 2000-01-26 | British Telecomm | Video coding and decording |
FR2813485B1 (en) * | 2000-08-24 | 2003-12-26 | France Telecom | METHOD FOR CONSTRUCTING AT LEAST ONE IMAGE INTERPOLED BETWEEN TWO IMAGES OF AN ANIMATED SEQUENCE, CORRESPONDING CODING AND DECODING METHODS, SIGNAL AND DATA MEDIUM |
US6825885B2 (en) * | 2001-02-13 | 2004-11-30 | Koninklijke Philips Electronics N.V. | Motion information coding and decoding method |
FI20012115A (en) * | 2001-11-01 | 2003-05-02 | Oplayo Oy | Image Interpolation |
CA2574127A1 (en) * | 2002-01-18 | 2003-07-31 | Kabushiki Kaisha Toshiba | Video encoding method and apparatus and video decoding method and apparatus |
US20040001546A1 (en) * | 2002-06-03 | 2004-01-01 | Alexandros Tourapis | Spatiotemporal prediction for bidirectionally predictive (B) pictures and motion vector prediction for multi-picture reference motion compensation |
CN1312927C (en) * | 2002-07-15 | 2007-04-25 | 株式会社日立制作所 | Moving picture encoding method and decoding method |
US8824553B2 (en) * | 2003-05-12 | 2014-09-02 | Google Inc. | Video compression method |
TWI220366B (en) * | 2003-08-11 | 2004-08-11 | Mediatek Inc | Scalable video format conversion system |
US7567617B2 (en) | 2003-09-07 | 2009-07-28 | Microsoft Corporation | Predicting motion vectors for fields of forward-predicted interlaced video frames |
ZA200805337B (en) * | 2006-01-09 | 2009-11-25 | Thomson Licensing | Method and apparatus for providing reduced resolution update mode for multiview video coding |
US8953684B2 (en) * | 2007-05-16 | 2015-02-10 | Microsoft Corporation | Multiview coding with geometry-based disparity prediction |
KR101396365B1 (en) * | 2007-08-28 | 2014-05-30 | 삼성전자주식회사 | Method and apparatus for spatiotemporal motion estimation and motion compensation of video |
CN101562745B (en) * | 2008-04-18 | 2012-07-04 | 华为技术有限公司 | Method and device for encoding and decoding multi-viewpoint video image |
US20100201870A1 (en) * | 2009-02-11 | 2010-08-12 | Martin Luessi | System and method for frame interpolation for a compressed video bitstream |
EP2494780B1 (en) * | 2009-10-29 | 2020-09-02 | Vestel Elektronik Sanayi ve Ticaret A.S. | Method and device for processing a video sequence |
JP2012034225A (en) * | 2010-07-30 | 2012-02-16 | Canon Inc | Motion vector detection device, motion vector detection method and computer program |
EP2604036B1 (en) * | 2010-08-11 | 2018-03-07 | GE Video Compression, LLC | Multi-view signal codec |
US8736767B2 (en) * | 2010-09-29 | 2014-05-27 | Sharp Laboratories Of America, Inc. | Efficient motion vector field estimation |
GB2486733A (en) * | 2010-12-24 | 2012-06-27 | Canon Kk | Video encoding using multiple inverse quantizations of the same reference image with different quantization offsets |
WO2012096164A1 (en) * | 2011-01-12 | 2012-07-19 | パナソニック株式会社 | Image encoding method, image decoding method, image encoding device, and image decoding device |
US9008176B2 (en) * | 2011-01-22 | 2015-04-14 | Qualcomm Incorporated | Combined reference picture list construction for video coding |
US9866859B2 (en) * | 2011-06-14 | 2018-01-09 | Texas Instruments Incorporated | Inter-prediction candidate index coding independent of inter-prediction candidate list construction in video coding |
MX341889B (en) * | 2011-06-30 | 2016-09-07 | Sony Corp | Image processing device and method. |
EP2769549A1 (en) * | 2011-10-21 | 2014-08-27 | Dolby Laboratories Licensing Corporation | Hierarchical motion estimation for video compression and motion analysis |
JP6260534B2 (en) * | 2012-09-03 | 2018-01-17 | ソニー株式会社 | Image processing apparatus and method |
US20140086328A1 (en) * | 2012-09-25 | 2014-03-27 | Qualcomm Incorporated | Scalable video coding in hevc |
US9485515B2 (en) | 2013-08-23 | 2016-11-01 | Google Inc. | Video coding using reference motion vectors |
WO2014120368A1 (en) * | 2013-01-30 | 2014-08-07 | Intel Corporation | Content adaptive entropy coding for next generation video |
US9521389B2 (en) * | 2013-03-06 | 2016-12-13 | Qualcomm Incorporated | Derived disparity vector in 3D video coding |
WO2015004606A1 (en) * | 2013-07-09 | 2015-01-15 | Nokia Corporation | Method and apparatus for video coding involving syntax for signalling motion information |
US9762927B2 (en) * | 2013-09-26 | 2017-09-12 | Qualcomm Incorporated | Sub-prediction unit (PU) based temporal motion vector prediction in HEVC and sub-PU design in 3D-HEVC |
US9667996B2 (en) * | 2013-09-26 | 2017-05-30 | Qualcomm Incorporated | Sub-prediction unit (PU) based temporal motion vector prediction in HEVC and sub-PU design in 3D-HEVC |
FR3011429A1 (en) * | 2013-09-27 | 2015-04-03 | Orange | VIDEO CODING AND DECODING BY HERITAGE OF A FIELD OF MOTION VECTORS |
KR20150106381A (en) * | 2014-03-11 | 2015-09-21 | 삼성전자주식회사 | Method and apparatus for deriving disparity vector for inter layer video encoding, method and apparatus for deriving disparity vector for inter layer video decoding |
US20180176599A1 (en) * | 2014-03-14 | 2018-06-21 | Samsung Electronics Co., Ltd. | Multi-layer video encoding method and multi-layer video decoding method using depth block |
WO2015137780A1 (en) * | 2014-03-14 | 2015-09-17 | 삼성전자 주식회사 | Multi-layer video encoding method and multi-layer video decoding method using pattern information |
JP2017518706A (en) * | 2014-06-18 | 2017-07-06 | サムスン エレクトロニクス カンパニー リミテッド | Multi-layer video encoding method and multi-layer video decoding method using depth block |
WO2015194877A1 (en) * | 2014-06-18 | 2015-12-23 | 삼성전자 주식회사 | Multi-layer video encoding method and multi-layer video decoding method using depth blocks |
US9992512B2 (en) * | 2014-10-06 | 2018-06-05 | Mediatek Inc. | Method and apparatus for motion vector predictor derivation |
US9769499B2 (en) * | 2015-08-11 | 2017-09-19 | Google Inc. | Super-transform video coding |
AU2015406855A1 (en) * | 2015-08-24 | 2018-03-15 | Huawei Technologies Co., Ltd. | Motion vector field coding and decoding method, coding apparatus, and decoding apparatus |
WO2017036399A1 (en) | 2015-09-02 | 2017-03-09 | Mediatek Inc. | Method and apparatus of motion compensation for video coding based on bi prediction optical flow techniques |
US10375413B2 (en) | 2015-09-28 | 2019-08-06 | Qualcomm Incorporated | Bi-directional optical flow for video coding |
JP6768145B2 (en) * | 2016-08-15 | 2020-10-14 | ノキア テクノロジーズ オーユー | Video coding and decoding |
US10931969B2 (en) | 2017-01-04 | 2021-02-23 | Qualcomm Incorporated | Motion vector reconstructions for bi-directional optical flow (BIO) |
US10701391B2 (en) * | 2017-03-23 | 2020-06-30 | Qualcomm Incorporated | Motion vector difference (MVD) prediction |
US10602180B2 (en) * | 2017-06-13 | 2020-03-24 | Qualcomm Incorporated | Motion vector prediction |
US11025950B2 (en) * | 2017-11-20 | 2021-06-01 | Google Llc | Motion field-based reference frame rendering for motion compensated prediction in video coding |
-
2018
- 2018-09-14 US US16/131,133 patent/US11665365B2/en active Active
-
2019
- 2019-05-28 WO PCT/US2019/034107 patent/WO2020055464A1/en unknown
- 2019-05-28 CN CN201980017993.4A patent/CN111819850B/en active Active
- 2019-05-28 EP EP19730647.5A patent/EP3744097A1/en not_active Withdrawn
-
2023
- 2023-05-25 US US18/323,613 patent/US20230308679A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120314027A1 (en) * | 2004-12-17 | 2012-12-13 | Dong Tian | Method and System for Processing Multiview Videos for View Synthesis Using Motion Vector Predictor List |
US20130004093A1 (en) * | 2011-06-29 | 2013-01-03 | Toshiyasu Sugio | Image coding method, image decoding method, image coding apparatus, image decoding apparatus, and image coding and decoding apparatus |
CN107205156A (en) * | 2016-03-18 | 2017-09-26 | 谷歌公司 | Pass through the motion-vector prediction of scaling |
Non-Patent Citations (1)
Title |
---|
DA LIU ET AL: "\"direct mode coding for b pictures using virtual reference picture\"", 《IEEE》, pages 1 - 4 * |
Also Published As
Publication number | Publication date |
---|---|
US11665365B2 (en) | 2023-05-30 |
WO2020055464A1 (en) | 2020-03-19 |
US20230308679A1 (en) | 2023-09-28 |
EP3744097A1 (en) | 2020-12-02 |
CN111819850B (en) | 2024-03-29 |
WO2020055464A9 (en) | 2020-10-01 |
US20200092576A1 (en) | 2020-03-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10009625B2 (en) | Low-latency two-pass video coding | |
GB2546888B (en) | Tile copying for video compression | |
US10694180B2 (en) | Entropy coding transform partitioning information | |
CN110741645B (en) | Blockiness reduction | |
US10701398B2 (en) | Context adaptive scan order for entropy coding | |
US20210274183A1 (en) | Efficient context handling in arithmetic coding | |
US20230308679A1 (en) | Motion prediction coding with coframe motion vectors | |
EP3335424A1 (en) | Super-transform video coding | |
US20170237939A1 (en) | Loop filtering for multiform transform partitioning | |
US11528498B2 (en) | Alpha channel prediction | |
CN107302701B (en) | Decoding interpolation filter types | |
WO2019070322A1 (en) | Warped reference motion vectors for video compression | |
EP3219103B1 (en) | Alternating block constrained decision mode coding | |
KR20200002035A (en) | Dual Deblocking Filter Thresholds | |
US20230291925A1 (en) | Inter-Intra Prediction With Implicit Models | |
WO2024005777A1 (en) | Circular-shift transformation for image and video coding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |