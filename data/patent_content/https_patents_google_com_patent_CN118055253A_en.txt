CN118055253A - Optical flow estimation for motion compensated prediction in video coding - Google Patents
Optical flow estimation for motion compensated prediction in video coding Download PDFInfo
- Publication number
- CN118055253A CN118055253A CN202410264512.2A CN202410264512A CN118055253A CN 118055253 A CN118055253 A CN 118055253A CN 202410264512 A CN202410264512 A CN 202410264512A CN 118055253 A CN118055253 A CN 118055253A
- Authority
- CN
- China
- Prior art keywords
- reference frame
- frame
- block
- current
- optical flow
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000033001 locomotion Effects 0.000 title claims abstract description 228
- 230000003287 optical effect Effects 0.000 title abstract description 137
- 238000000034 method Methods 0.000 claims description 166
- 239000013598 vector Substances 0.000 claims description 47
- 230000004044 response Effects 0.000 claims description 10
- 230000008569 process Effects 0.000 description 106
- 238000012545 processing Methods 0.000 description 34
- 230000006870 function Effects 0.000 description 31
- 230000015654 memory Effects 0.000 description 29
- 238000010586 diagram Methods 0.000 description 24
- 238000002156 mixing Methods 0.000 description 18
- 238000004891 communication Methods 0.000 description 11
- 230000006835 compression Effects 0.000 description 9
- 238000007906 compression Methods 0.000 description 9
- 238000001914 filtration Methods 0.000 description 9
- 238000012935 Averaging Methods 0.000 description 4
- 238000004364 calculation method Methods 0.000 description 4
- 238000013139 quantization Methods 0.000 description 4
- 239000002131 composite material Substances 0.000 description 3
- 238000001514 detection method Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 108091026890 Coding region Proteins 0.000 description 2
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 2
- 230000000903 blocking effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 239000003550 marker Substances 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000007670 refining Methods 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 230000003321 amplification Effects 0.000 description 1
- 238000000137 annealing Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000002474 experimental method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000003199 nucleic acid amplification method Methods 0.000 description 1
- 238000011084 recovery Methods 0.000 description 1
- 238000000611 regression analysis Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Abstract
Optical flow estimation for motion compensated prediction in video coding is disclosed. An optical flow reference frame portion (e.g., a block or an entire frame) is generated that may be used for inter-prediction of a block of a current frame in a video sequence. The forward and backward reference frames are used for optical flow estimation that produces a motion field for the pixels of the current frame. The motion field is used to warp some or all of the pixels of the reference frame into pixels of the current frame. The warped reference frame pixels are blended to form an optical flow reference frame portion. Inter prediction may be performed as part of encoding or decoding a portion of a current frame.
Description
Description of the division
The application belongs to a divisional application of Chinese application patent application 201880036783.5 with the application date of 2018, 5 month and 10 days.
Technical Field
The present disclosure relates to optical flow estimation for motion compensated prediction in video coding.
Background
The digital video stream may represent video using a series of frames or still images. Digital video may be used for a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or sharing of user-generated video. Digital video streams may contain large amounts of data and consume large amounts of computing or communication resources of computing devices to process, transmit, or store video data. Various methods have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.
A compression technique uses a reference frame to generate a prediction block corresponding to a current block to be encoded. In addition to the value of the current block itself, the difference between the predicted block and the current block may be encoded to reduce the amount of encoded data.
Disclosure of Invention
The present disclosure relates generally to encoding and decoding video data, and more particularly to block-based optical flow estimation using motion compensated prediction for use in video compression. Frame-level based optical flow estimation is also described, which can interpolate co-located (co-located) reference frames for motion compensated prediction in video compression.
The present disclosure describes encoding and decoding methods and apparatus. A method according to an embodiment of the present disclosure determines a first frame portion of a first frame to be predicted, the first frame being in a video sequence; determining a first reference frame from the video sequence for forward inter-prediction of the first frame; determining a second reference frame from the video sequence for backward inter-prediction of the first frame; generating an optical flow reference frame portion for inter-prediction of the first frame portion by performing optical flow estimation using the first reference frame and the second reference frame; and performing a prediction process of the first frame portion using the optical stream reference frame. For example, the first frame portion and the optical flow reference frame portion may be blocks or entire frames.
An apparatus according to an embodiment of the present disclosure includes a non-transitory storage medium or memory and a processor. The medium includes instructions executable by a processor to perform a method comprising: determining a first frame in the video sequence to be predicted; and determining availability of a first reference frame for forward inter prediction of the first frame and a second reference frame for backward inter prediction of the first frame. The method further includes, in response to determining availability of both the first reference frame and the second reference frame: generating respective motion fields for pixels of the first frame portion using the first reference frame and the second reference frame as inputs to an optical flow estimation process; warping the first reference frame portion into a first frame portion using the motion field to form a first warped reference frame portion, the first reference frame portion comprising pixels of the first reference frame co-located with pixels of the first frame portion; warping the second reference frame portion into the first frame portion using the motion field to form a second warped reference frame portion, the second reference frame portion comprising pixels of the second reference frame co-located with pixels of the first frame portion; and blending the first warped reference frame portion and the second warped reference frame portion to form an optical flow reference frame portion for inter-prediction of at least one block of the first frame.
Another apparatus according to an embodiment of the disclosure also includes a non-transitory storage medium or memory and a processor. The medium includes instructions executable by a processor to perform a method comprising: generating an optical flow reference frame portion for inter prediction of a block of a first frame of a video sequence using a first reference frame of the video sequence and a second reference frame of the video sequence by initializing a motion field of pixels of the first frame portion for optical flow estimation in a first processing level, the first processing level representing motion of a downscaling (downscale) within the first frame portion and comprising one of a plurality of levels; and for each of the plurality of levels: warping the first reference frame portion into a first frame portion using the motion field to form a first warped reference frame portion; warping the second reference frame portion into the first frame portion using the motion field to form a second warped reference frame portion; estimating a motion field between the first warped reference frame part and the second warped reference frame part using the optical flow estimate; and updating the motion field of the pixels of the first frame portion using the motion field between the first and second warped reference frame portions. The method further comprises the steps of: for the final grade of the plurality of grades: warping the first reference frame portion into a first frame portion using the updated motion field to form a final first warped reference frame portion; warping the second reference frame portion into a first frame portion using the updated motion field to form a final second warped reference frame portion; and blending the final first warped reference frame part and the final second warped reference frame part to form the optical flow reference frame part.
Another apparatus according to an embodiment of the present disclosure includes a non-transitory storage medium or memory and a processor. The medium includes instructions executable by a processor to perform a method comprising: determining a first frame portion of a first frame to be predicted, the first frame being in a video sequence; determining a first reference frame from the video sequence for forward inter-prediction of the first frame; determining a second reference frame from the video sequence for backward inter-prediction of the first frame; generating an optical flow reference frame portion by performing optical flow estimation using the first reference frame and the second reference frame for inter-prediction of the first frame portion; and performing a prediction process of the first frame portion using the optical stream reference frame.
These and other aspects of the disclosure are disclosed in the following detailed description of the embodiments, appended claims and accompanying drawings.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views unless otherwise specified.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device that may implement a sending station or a receiving station.
Fig. 3 is a diagram of a typical video stream to be encoded and subsequently decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a block diagram of an example of a reference frame buffer.
Fig. 7 is a diagram of a group of frames in the display order of a video sequence.
Fig. 8 is a diagram of an example of a coding sequence of the frame group of fig. 7.
Fig. 9 is a diagram for explaining a linear projection of a motion field according to the teachings herein.
Fig. 10 is a flow chart of a process for motion compensated prediction of a video frame using at least a portion of a reference frame generated using optical flow estimation.
FIG. 11 is a flowchart of a process for generating an optical flow reference frame portion.
FIG. 12 is a flowchart of another process for generating an optical flow reference frame portion.
Fig. 13 is a diagram illustrating the process of fig. 11 and 12.
Fig. 14 is a diagram illustrating occlusion of a target.
Fig. 15 is a diagram illustrating a technique of optimizing a decoder.
Detailed Description
Video streams may be compressed by various techniques to reduce the bandwidth required to transmit or store the video stream. The video stream may be encoded into a bit stream, which involves compression, which is then transferred to a decoder that may decode or decompress the video stream to prepare it for viewing or further processing. Compression of video streams typically exploits the spatial and temporal correlation of video signals through spatial and/or motion compensated prediction. For example, inter prediction uses one or more motion vectors to generate a block (also referred to as a prediction block), which is similar to a current block to be encoded using previously encoded and decoded pixels. By encoding the motion vector and the difference between the two blocks, a decoder receiving the encoded signal can recreate the current block. Inter prediction may also be referred to as motion compensated prediction.
Each motion vector used to generate the prediction block in the inter prediction process relates to a frame other than the current frame, i.e., a reference frame. The reference frame may be located before or after the current frame in the video stream sequence and may be a frame reconstructed before being used as a reference frame. In some cases, there may be three reference frames used to encode or decode a block of a current frame of a video sequence. One is a frame that may be referred to as a golden frame. The other is the most recently encoded or decoded frame. The last is an alternate reference frame that is encoded or decoded before one or more frames in the sequence but is displayed after those frames in output display order. In this way, the alternative reference frame is a reference frame that can be used for backward prediction. One or more forward and/or backward reference frames may be used to encode or decode a block. The effectiveness of a reference frame when used to encode or decode a block within a current frame may be measured based on a resulting signal-to-noise ratio or other rate-distortion metric.
In this technique, the pixels forming the prediction block are obtained directly from one or more of the available reference frames. The reference pixel block or a linear combination thereof is used to predict a given coded block in the current frame. Such direct, block-based predictions do not capture the real motion activity available from the reference frame. For this reason, motion compensated prediction accuracy may be affected.
To more fully utilize motion information from available bi-directional reference frames (e.g., one or more forward reference frames and one or more backward reference frames), embodiments of the teachings herein describe a reference frame portion collocated with a current coded frame portion that uses motion fields of each pixel calculated by optical flow to estimate true motion activity in a video signal. Interpolation of the reference frame portion that allows tracking complex non-translational motion activities exceeds the performance of conventional block-based motion compensated prediction determined directly from the reference frame. The use of such reference frame portions may improve the prediction quality. As used herein, a frame portion refers to some of all frames, such as blocks, slices, or entire frames. The frame portions in one frame are juxtaposed with the frame portions in another frame if they are of the same size and are located at the same pixel location within the size of each frame.
Further details of interpolating reference frame portions for video compression and reconstruction using optical flow estimation are initially described herein with reference to a system in which the teachings herein can be implemented.
Fig. 1 is a schematic diagram of a video encoding and decoding system 100. For example, the transmitting station 102 may be a computer having an internal hardware configuration such as that described in fig. 2. However, other suitable implementations of the sending station 102 are also possible. For example, the processing of the sending station 102 may be distributed among multiple devices.
The network 104 may connect the transmitting station 102 and the receiving station 106 to encode and decode video streams. In particular, the video stream may be encoded in the transmitting station 102 and the encoded video stream may be decoded in the receiving station 106. For example, the network 104 may be the Internet. Network 104 may also be a Local Area Network (LAN), wide Area Network (WAN), virtual Private Network (VPN), cellular telephone network, or any other manner of transmitting video streams from sending station 102 to receiving station 106 in this example.
In one example, the receiving station 106 may be a computer having an internal hardware configuration such as that described in fig. 2. However, other suitable implementations of the receiving station 106 are also possible. For example, the processing of the receiving station 106 may be distributed among multiple devices.
Other implementations of the video encoding and decoding system 100 are also possible. For example, embodiments may omit network 104. In another embodiment, the video stream may be encoded and then stored for later transmission to the receiving station 106 or any other device having a non-transitory storage medium or memory. In one embodiment, the receiving station 106 receives (e.g., via the network 104, a computer bus, and/or some communication path) the encoded video stream and stores the video stream for later decoding. In an example embodiment, real-time transport protocol (RTP) is used to transmit encoded video over network 104. In another embodiment, a transport protocol other than RTP may be used, such as a hypertext transfer protocol (HTTP) based video streaming protocol.
When used in a video conferencing system, for example, the sending station 102 and/or the receiving station 106 may include the following capabilities to encode and decode a video stream. For example, the receiving station 106 may be a video conference participant that receives the encoded video bitstream from the video conference server (e.g., the sending station 102) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 that may implement a sending station or a receiving station. For example, computing device 200 may implement one or both of transmitting station 102 and receiving station 106 of fig. 1. Computing device 200 may be in the form of a computing system including multiple computing devices or one computing device, such as a mobile phone, tablet computer, laptop computer, notebook computer, desktop computer, or the like.
The CPU 202 in the computing device 200 may be a central processing unit. Alternatively, CPU 202 may be any other type of device or devices capable of manipulating or processing information now present or later developed. Although the disclosed embodiments may be practiced with one processor as shown, such as CPU 202, advantages in speed and efficiency may be realized through the use of more than one processor.
In an implementation, the memory 204 in the computing device 200 may be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device. Any other suitable type of storage device or non-transitory storage medium may be used as memory 204. Memory 204 may include code and data 206 accessed by CPU 202 using bus 212. The memory 204 may further include an operating system 208 and application programs 210, the application programs 210 including at least one program that allows the CPU 202 to perform the methods described herein. For example, application 210 may include applications 1 through N, which applications 1 through N further include video coding applications that perform the methods described herein. Computing device 200 may also include secondary storage 214, which may be, for example, a memory card for use with a mobile computing device. Since video communication sessions may contain a large amount of information, they may be stored in whole or in part in secondary storage 214, which may be loaded into memory 204 for processing as needed.
Computing device 200 may also include one or more output devices, such as a display 218. In one example, the display 218 may be a touch sensitive display that combines the display with touch sensitive elements operable to sense touch inputs. A display 218 may be coupled to CPU 202 via bus 212. Other output devices may be provided in addition to the display 218 or in lieu of the display 218, allowing a user to program or otherwise use the computing device 200. When the output device is or includes a display, the display may be implemented in a variety of ways, including by a Liquid Crystal Display (LCD), a Cathode Ray Tube (CRT) display, or a Light Emitting Diode (LED) display, such as an Organic LED (OLED) display.
Computing device 200 may also include or be in communication with an image sensing device 220, such as a camera or any other image sensing device 220 now present or later developed capable of sensing images, such as images of a user operating computing device 200. The image sensing device 220 may be positioned such that it is oriented towards a user operating the computing device 200. In an example, the position and optical axis of the image sensing device 220 may be configured such that the field of view includes an area directly adjacent to the display 218 and visible from the display 218 therein.
Computing device 200 may also include or be in communication with sound sensing device 222, such as a microphone or any other sound sensing device now present or later developed capable of sensing sound in the vicinity of computing device 200. The sound sensing device 222 can be positioned toward a user operating the computing device 200 and can be configured to receive sound, such as speech or other utterances uttered by the user while the user is operating the computing device 200.
Although fig. 2 depicts the CPU 202 and memory 204 of the computing device 200 as being integrated into one unit, other configurations may be utilized. The operation of the CPU 202 may be distributed across multiple machines (where a single machine may have one or more processors) capable of being coupled directly or across a local area network or other network. The memory 204 may be distributed across multiple machines, such as network-based memory or memory in multiple machines performing the operations of the computing device 200. Although depicted as a single bus, the bus 212 of the computing device 200 may be comprised of multiple buses. Further, secondary storage 214 may be directly coupled to other components of computing device 200 or may be accessible via a network and may include an integrated unit such as a memory card or multiple units such as multiple memory cards. Computing device 200 may thus be implemented in a variety of configurations.
Fig. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded. Video stream 300 includes video sequence 302. At the next level, the video sequence 302 includes a plurality of adjacent frames 304. Although three frames are depicted as adjacent frames 304, the video sequence 302 may include any number of adjacent frames 304. Adjacent frames 304 may then be further subdivided into individual frames, e.g., frames 306. At the next level, the frame 306 may be divided into a series of planes or segments 308. For example, segment 308 may be a subset of frames that allow parallel processing. Segment 308 may also be a subset of frames that are capable of separating video data into separate colors. For example, a frame 306 of color video data may include a luminance plane and two chrominance planes. Segment 308 may be sampled at different resolutions.
Whether or not frame 306 is divided into segments 308, frame 306 may be further subdivided into blocks 310, which blocks 310 may contain data corresponding to, for example, 16 x 16 pixels in frame 306. The block 310 may also be arranged to include data from one or more segments 308 of pixel data. The block 310 may also have any other suitable size, such as 4 x 4 pixels, 8 x 8 pixels, 16 x 8 pixels, 8 x 16 pixels, 16 x 16 pixels, or larger. The terms block and macroblock are used interchangeably herein unless otherwise indicated.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, the encoder 400 may be implemented in the transmitting station 102, such as by providing a computer software program stored in a memory, such as the memory 204. The computer software program may include machine instructions that, when executed by a processor such as CPU 202, cause sending station 102 to encode video data in the manner described in fig. 4. Encoder 400 may also be implemented as dedicated hardware included in, for example, transmitting station 102. In one particularly desirable embodiment, encoder 400 is a hardware encoder.
The encoder 400 has the following stages for performing various functions in the forward path (represented by the real connection line) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy encoding stage 408. The encoder 400 may also include a reconstruction path (indicated by dashed connecting lines) for reconstructing the frame for encoding of future blocks. In fig. 4, the encoder 400 has the following stages for performing various functions in the reconstruction path: a dequantization stage 410, an inverse transformation stage 412, a reconstruction stage 414, and a loop filtering stage 416. Other structural variations of encoder 400 may be used to encode video stream 300.
When video stream 300 is presented for encoding, a corresponding frame 304, such as frame 306, may be processed in units of blocks. In the intra/inter prediction stage 402, the corresponding block may be encoded using intra prediction (also referred to as intra prediction) or inter prediction (also referred to as inter prediction). In any case, a prediction block may be formed. In the case of intra prediction, a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter prediction, a prediction block may be formed from samples in one or more previously constructed reference frames. The specification of the reference frame for a chunk will be discussed in more detail below.
Next, still referring to fig. 4, the prediction block may be subtracted from the current block in an intra/inter prediction stage 402 to produce a residual block (also referred to as a residual). The transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using a block-based transform. The quantization stage 406 uses the quantizer values or quantization levels to convert the transform coefficients into discrete magnitudes, referred to as quantized transform coefficients. For example, the transform coefficients may be divided by the quantizer values and truncated. The quantized transform coefficients are then entropy encoded by an entropy encoding stage 408. The entropy encoded coefficients are then output to the compressed bitstream 420 along with other information for decoding the block that may include, for example, the type of prediction used, the type of transform, motion vectors, quantizer values, etc. The compressed bitstream 420 may be formatted using various techniques, such as Variable Length Coding (VLC) or arithmetic coding. The compressed bitstream 420 may also be referred to as an encoded video stream or an encoded video bitstream, and the terms will be used interchangeably herein.
The reconstruction path (shown with dashed lines) in fig. 4 may be used to ensure that the encoder 400 and a decoder 500 (described below) use the same reference frame to decode the compressed bitstream 420. The reconstruction path performs functions similar to those that occur during the decoding process described in more detail below, including dequantizing quantized transform coefficients in dequantization stage 410 and inverse transforming the dequantized transform coefficients in inverse transformation stage 412 to produce a derivative residual block (also referred to as a derivative residual). In reconstruction stage 414, the predicted block predicted in intra/inter prediction stage 402 may be added to the derivative residual to create a reconstructed block. Loop filtering stage 416 may be applied to the reconstructed block to reduce distortion such as blocking artifacts.
Other variations of encoder 400 may be used to encode compressed bit stream 420. For example, a non-transform based encoder may directly quantize the residual signal without the transform stage 404 for some blocks or frames. In another embodiment, the encoder may have a quantization stage 406 and a dequantization stage 410 combined into a common stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 may be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program may include machine instructions that, when executed by a processor such as CPU 202, cause receiving station 106 to decode video data in the manner described in fig. 5. Decoder 500 may also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106.
Similar to the reconstruction path of encoder 400 described above, in one example, decoder 500 includes the following stages for performing various functions to produce output video stream 516 from compressed bitstream 420: entropy decoding stage 502, dequantization stage 504, inverse transform stage 506, intra/inter prediction stage 508, reconstruction stage 510, loop filtering stage 512, and deblocking filtering stage 514. Other structural variations of decoder 500 may also be used to decode compressed bit stream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 may be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients. Dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by quantizer values), and inverse transform stage 506 inverse transforms the dequantized transform coefficients to produce derivative residuals that may be the same as the derivative residuals created by inverse transform stage 412 in encoder 400. Using header information decoded from compressed bitstream 420, decoder 500 may use intra/inter prediction stage 508 to create the same prediction block as created in encoder 400, e.g., in intra/inter prediction stage 402. In the reconstruction stage 510, a prediction block may be added to the derivative residual to create a reconstructed block. Loop filtering stage 512 may be applied to the reconstructed block to reduce blocking artifacts.
Other filtering may be applied to the reconstructed block. In this example, deblocking filtering stage 514 is applied to reconstructed blocks to reduce block distortion, and the results are output as output video stream 516. The output video stream 516 may also be referred to as a decoded video stream, and the terms will be used interchangeably herein. Other variations of decoder 500 may be used to decode compressed bit stream 420. For example, decoder 500 may generate output video stream 516 without deblocking filtering stage 514.
Fig. 6 is a block diagram of an example of a reference frame buffer 600. The reference frame buffer 600 stores reference frames for encoding or decoding blocks of frames of a video sequence. In this example, the reference FRAME buffer 600 includes reference FRAMEs identified as LAST FRAME 602, GOLDEN FRAME goldence FRAME 604, and alternate reference FRAME ALTREF FRAME 606. The frame header of the reference frame may include a virtual index to the location within the reference frame buffer where the reference frame is stored. The reference frame map may map a virtual index of the reference frame to a physical index of a memory where the reference frame is stored. In the case where two reference frames are the same frame, these reference frames will have the same physical index, even though they have different virtual indexes, the number, type, and name used of the reference locations within the reference frame buffer 600 are merely examples.
The reference frames stored in the reference frame buffer 600 may be used to identify motion vectors for predicting blocks of a frame to be encoded or decoded. Depending on the type of prediction used to predict the current block of the current frame, different reference frames may be used. For example, in bi-prediction, the block of the current FRAME may be forward predicted using any FRAME stored as either LAST_FRAME 602 or GOLDEN_FRAME 604, and backward predicted using the FRAME stored as ALTREF _FRAME 606.
There may be a limited number of reference frames that can be stored in the reference frame buffer 600. As shown in fig. 6, reference frame buffer 600 may store up to eight reference frames, where each stored reference frame may be associated with a different virtual index of the reference frame buffer. Although three of the eight spaces in the reference FRAME buffer 600 are used by FRAMEs designated as LAST_FRAME 602, GOLDEN_FRAME 604, and ALTREF _FRAME 606, five spaces may still be used to store other reference FRAMEs. For example, one or more of the available space in the reference frame buffer 600 may be used to store further reference frames, particularly some or all of the interpolated reference frames described herein. Although reference frame buffer 600 is shown as being capable of storing up to eight reference frames, other embodiments of reference frame buffer 600 are also capable of storing additional or fewer reference frames.
In some embodiments, the alternate reference FRAME designated ALTREF _frame 606 may be a FRAME of the video sequence that is far from the current FRAME in display order but that was encoded or decoded before it was displayed. For example, the alternate reference frame may be ten, twelve or more (or less) frames after the current frame in display order. A further alternative reference frame may be a frame that is closer to the current frame in display order.
The alternate reference frame may not directly correspond to a frame in the sequence. Instead, the alternative reference frames may be generated using one or more frames that have been applied with filtering, combined together, or combined together and filtered. The alternate reference frame may not be displayed. Instead, it may be a frame or portion of a frame that is generated and transmitted for use only in the prediction process (i.e., it is omitted when the decoded sequence is displayed).
Fig. 7 is a diagram of a group of frames in the display order of a video sequence. In this example, a group of frames is preceded by a frame 700 and includes eight frames 702-716, the frame 700 may be referred to as a key frame or overlay frame in some cases. No block in frame 700 is inter predicted using the reference frames of the frame set. In this example, frame 700 is a key (also referred to as an intra-prediction frame), which refers to the state in which a predicted block within a frame is predicted using intra-prediction only. However, frame 700 may be an overlay frame, which is an inter-predicted frame that may be a reconstructed frame of a previous frame group. In inter-prediction frames, at least some of the prediction blocks are predicted using inter-prediction. For example, the number of frames forming each frame group may vary depending on the configuration of video space/time features and other coding such as key frame intervals selected for random access or error recovery.
The coding order of each frame group may be different from the display order. This allows frames located after a current frame in the video sequence to be used as reference frames for encoding the current frame. A decoder, such as decoder 500, may share a common set of coding structures with an encoder, such as encoder 400. The group coding structure assigns the different roles that the corresponding frames within the group may play in the reference buffer (e.g., last frame, alternate reference frame, etc.) and defines or indicates the coding order for the frames within the group.
Fig. 8 is a diagram of an example of a coding sequence of the frame group of fig. 7. The coding order of fig. 8 is associated with a first set of coding structures, so a single backward reference frame is available for each frame in the set. Because the encoding and decoding order is the same, the order shown in FIG. 8 is generally referred to herein as a coding order. The key or overlay FRAME 700 is designated as the GOLDEN FRAME in a reference FRAME buffer, such as GOLDEN FRAME 604 in reference FRAME buffer 600. Frame 700 is intra-predicted in this example so it does not require a reference frame, but neither does an overlay frame, like frame 700, which is a reconstructed frame from a previous group, use the reference frame of the current frame group. The final FRAME 716 in the group is designated as an alternate reference FRAME in the reference FRAME buffer, such as ALTREF _frame 606 in the reference FRAME buffer 600. In this coding order, frame 716 is coded out of display order after frame 700 to provide a backward reference frame for each of the remaining frames 702-714. In the coded blocks of frame 716, frame 700 serves as an available reference frame for the blocks of frame 716. Fig. 8 is merely one example of a coding order of a group of frames. Other sets of coding structures may specify one or more unused or additional frames for forward and/or backward prediction.
As briefly mentioned above, the available reference frame portions may be reference frame portions that are interpolated using optical flow estimation. For example, the reference frame portion may be a block, a slice, or an entire frame. When frame-level optical flow estimation is performed as described herein, the resulting reference frame is referred to herein as a co-located reference frame because the size is the same as the current frame. The interpolated reference frame may also be referred to herein as an optical flow reference frame.
Fig. 9 is a diagram for explaining a linear projection of a motion field according to the teachings herein. Within the hierarchical coding framework, the optical flow (also called motion field) of the current frame can be estimated using the most recently available reconstructed (e.g., reference) frames before and after the current frame. In fig. 9, reference frame 1 is a reference frame that can be used for forward prediction of the current frame 900, and reference frame 2 is a reference frame that can be used for backward prediction of the current frame 900. Using the examples of fig. 6-8 to illustrate, if the current FRAME 900 is FRAME 706, then the immediately preceding or following FRAME 704 (e.g., the reconstructed FRAME stored in reference FRAME buffer 600 as LAST FRAME 602) may be used as reference FRAME 1, and FRAME 716 (e.g., the reconstructed FRAME stored in reference FRAME buffer 600 as ALTREF FRAME 606) may be used as reference FRAME 2.
Knowing the display indices of the current frame and the reference frame, a motion vector can be projected to a pixel in the current frame 900 between pixels in reference frames 1 and 2, assuming that the motion field is linear in time. In the simple example described with respect to fig. 6-8, the index of the current frame 900 is 3, the index of reference frame 1 is 0, and the index of reference frame 2 is 716. In fig. 9, a projected motion vector 904 for a pixel 902 of a current frame 900 is shown. By using the previous example to explain, the display index of the group of frames of FIG. 7 will indicate that frame 704 is closer in time to frame 706 than frame 716. Thus, a single motion vector 904 shown in fig. 9 represents different amounts of motion between reference frame 1 and current frame 900 and between reference frame 2 and current frame 900. However, between reference frame 1, current frame 900, and reference frame 2, projection motion field 906 is linear.
Selecting the nearest available reconstructed forward and backward reference frames and assuming that the motion fields of the corresponding pixels of the current frame are linear in time allows for generating an interpolated reference frame using optical flow estimates to be performed at both the encoder and decoder (e.g., at intra/inter prediction stage 402 and intra/inter prediction stage 508) without conveying additional information. In addition to the most recent available reconstructed reference frames, different frames may be used as a specified prior between encoder and decoder. In some implementations, identification of frames for optical flow estimation may be transmitted. The generation of interpolated frames is discussed in more detail below.
FIG. 10 is a flow chart of a method or process 1000 of motion compensated prediction of frames of a video sequence using at least a portion of a reference frame generated using optical flow estimation. For example, the reference frame portion may be a block, a slice, or an entire frame. The optical flow reference frame portion may also be referred to herein as a co-located reference frame portion. For example, process 1000 may be implemented as a software program that may be executed by a computing device, such as transmitting station 102 or receiving station 106. For example, a software program may include machine-readable instructions that may be stored in a memory, such as memory 204 or secondary storage 214, and that when executed by a processor, such as CPU 202, may cause the computing device to perform process 1000. Process 1000 may be implemented using dedicated hardware or firmware. Some computing devices may have multiple memories or processors, and the operations described in process 1000 may be distributed using multiple processors, memories, or both.
In 1002, a current frame to be predicted is determined. The frames may be encoded, and thus predicted, in any order, such as in the coding order shown in fig. 8. The frame to be predicted may also be referred to as a first frame, a second frame, a third frame, etc. The labels of the first, second, etc. do not necessarily indicate the order of the frames. Rather, unless otherwise indicated, a label is used herein to distinguish a current frame from another. At the encoder, frames may be processed in units of blocks in a block coding order such as a raster scan order. At the decoder, frames may also be processed in units of blocks according to the receipt of their coded residuals within the coded bit stream.
In 1004, forward and backward reference frames are determined. In the examples described herein, the forward and backward reference frames are the most recent reconstructed frames before and after (e.g., in display order) a current frame, such as current frame 900. Although not explicitly shown in fig. 10, if either the forward or backward reference frames are not present, the process 100 ends. The current frame is then processed without regard to optical flow.
Assuming that forward and backward reference frames are present in 1004, the reference frames may be used to generate an optical flow reference frame portion in 1006. The generation of the optical flow reference frame portion is described in more detail with reference to fig. 11-14. In some implementations, the optical flow reference frame portions may be stored in defined locations within the reference frame buffer 600. First, optical flow estimation according to the teachings herein is described.
Optical flow estimation may be performed for the corresponding pixels of the current frame portion by minimizing the following Lagrangian function (1):
J＝Jdata+λJspatial (1)
In function (1), J data is a data penalty based on the assumption that the brightness is constant (i.e., the assumption that the intensity value of a small portion of the image remains unchanged over time despite the change in position). J spatial is a spatial penalty based on the smoothness of the motion field (i.e., the characteristic that neighboring pixels may belong to the same object item in the image, resulting in substantially the same image motion). The lagrangian parameter lambda controls the importance of the smoothness of the motion field. A larger value λ of the parameter λ results in a smoother motion field and may better explain motion at larger scales. Conversely, smaller values of the parameter λ may more effectively accommodate movement of object edges and small objects.
According to an embodiment of the teachings herein, the data penalty may be represented by a data penalty function:
Jdata＝(Exu+Eyv+Et)2 (2)
The horizontal component of the motion field of the current pixel is denoted by u and the vertical component of the motion field is denoted by v. Colloquially, E x、Ey and E t are derivatives of pixel values of the reference frame portion with respect to the horizontal axis x, the vertical axis y, and time t (e.g., represented by a frame index). The horizontal and vertical axes are defined with respect to the pixel arrays forming the current frame, such as current frame 900, and the reference frames, such as reference frames 1 and 2.
In the data penalty function, the derivatives E x、Ey and E t can be calculated according to the following functions (3), (4) and (5):
Et＝E(r2)-E(r1) (5)
The variable E (r1) is the pixel value at the projection position in reference frame 1 of the motion field based on the current pixel position of the current frame being encoded. Similarly, the variable E (r2) is a pixel value at a projection position in the reference frame 2 based on the motion field of the current pixel position of the current frame being encoded.
The variable index r1 is a display index of reference frame 1, where the display index of a frame is its index in the display order of the video sequence. Similarly, the variable index r2 is the display index of reference frame 2, and the variable index cur is the display index of the current frame 900.
Variable(s)Is the horizontal derivative calculated at reference frame 1 using a linear filter. Variable/>Is the horizontal derivative calculated at reference frame 2 using a linear filter. Variable/>Is the vertical derivative calculated at reference frame 1 using a linear filter. Variable/>Is the vertical derivative calculated at reference frame 2 using a linear filter.
In embodiments taught herein, the linear filter used to calculate the horizontal derivative is a 7-tap filter with filter coefficients [ -1/60,9/60, -45/60,0, 45/60, -9/60,1/60 ]. The filters may have different frequency profiles, different numbers of taps, or both. The linear filter used to calculate the vertical derivative may be the same as or different from the linear filter used to calculate the horizontal derivative.
The spatial penalty may be represented by a spatial penalty function:
Jspatial＝(Δu)2+(Δv)2 (6)
In the spatial penalty function (6), deltau is the Laplacian of the horizontal component u of the motion field and Deltav is the Laplacian of the vertical component v of the motion field.
FIG. 11 is a flow chart of a method or process 1100 for generating an optical flow reference frame portion. In this example, the optical flow reference frame portion is the entire reference frame. Process 1100 may implement step 1006 of process 1000. For example, process 1100 may be implemented as a software program that may be executed by a computing device, such as transmitting station 102 or receiving station 106. For example, a software program may include machine-readable instructions that may be stored in a memory, such as memory 204 or secondary storage 214, and that when executed by a processor, such as CPU 202, may cause the computing device to perform process 1100. Process 1100 may be implemented using dedicated hardware or firmware. As noted above, multiple processors, memories, or both may be used.
Because the forward and backward reference frames may be far from each other, there may be severe motion between them, reducing the accuracy of the constant brightness assumption. To reduce potential errors in the motion of pixels resulting from such problems, the estimated motion vector from the current frame to the reference frame may be used to initialize the optical flow estimation of the current frame. In 1102, all pixels in the current frame may be assigned an initialized motion vector. They define an initial motion field that can be used to warp the reference frames into the current frame of the first processing level to shorten the length of motion between the reference frames.
The motion field mv cur for the current pixel may be initialized with a motion vector representing the difference between the estimated motion vector mv r2 pointing from the current pixel to the backward reference frame, which in this example is reference frame 2, and the estimated motion vector mv r2 pointing from the current pixel to the forward reference frame, which in this example is reference frame 1, according to the following equation:
mvcur＝-mvr1+mvr2
if one of the motion vectors is not available, the initial motion can be inferred using the available motion vector according to one of the following functions:
mv cur＝-mvr1·(indexr2-indexr1)/(indexcur-indexr1), or
mvcur＝mvr2·(indexr2-indexr1)/(indexr2-indexcur)。
In the case that no motion vector reference is available for the current pixel, one or more spatial neighbors with initialized motion vectors may be used. For example, the average of the available neighbor initialization motion vectors may be used.
In the example of initializing the motion field of the first processing level in 1102, reference frame 2 may be used to predict pixels of reference frame 1, where reference frame 1 is the last frame before the current frame is coded. Motion vectors projected onto the current frame using linear projection in a similar manner as shown in fig. 9 produce motion fields mv cur at intersecting pixel locations, such as motion field 906 at pixel location 902.
Fig. 11 relates to initializing a motion field of a first processing level, because there are desirably multiple processing levels for process 110. This can be seen with reference to fig. 13, which is a diagram illustrating the process 1100 of fig. 11 (and the process 1200 of fig. 12 discussed below). The following description uses the phrase sports field. This phrase is intended to collectively refer to the motion field of each pixel unless the context clearly dictates otherwise. Thus, the phrase "multiple motion fields" or "motion field" is used interchangeably when referring to more than one motion field. Further, the phrase optical flow may be used interchangeably with the phrase motion field when referring to the movement of a pixel.
To estimate the motion field/optical flow of the pixels of the frame, a pyramid or a multi-layer structure may be used. In a pyramid structure, for example, the reference frame is scaled down to one or more different scales. Then, the optical flow is estimated first to obtain the motion field at the highest level of the pyramid (first processing level), i.e. using the reference frame that is most scaled down. Thereafter, the motion field is enlarged (up-scale) and used to initialize the optical flow estimation at the next level. This process of magnifying the motion field, using it to initialize the optical flow estimation of the next level, and obtaining the motion field continues until the lowest level of the pyramid is reached (i.e., until the optical flow estimation is completed for the reference frame at full scale).
The basis for this process is that large movements are more easily captured when the image is zoomed out. However, scaling the reference frame itself using a simple rescaling filter may reduce the reference frame quality. To avoid losing detailed information due to rescaling, the pyramid structure scales the derivatives instead of the pixels of the reference frame to estimate the optical flow. This pyramid scheme represents a regression analysis of the light flow estimates. The scheme is shown in fig. 13 and implemented by process 1100 of fig. 11 and process 1200 of fig. 12.
After initialization, the Lagrangian parameter λ is set for solving the Lagrangian function (1) in 1104. Desirably, the process 1100 uses multiple values of the Lagrangian parameter λ. The first value to which the lagrangian parameter λ is set in 1104 may be a relatively large value, such as 100. While it is desirable for process 1100 to use multiple values of the Lagrangian parameter λ in Lagrangian function (1), only one value may be used, as described in process 1200 below.
In 1106, the reference frame is warped to the current frame according to the motion field of the current processing level. Warping the reference frame to the current frame may be performed using sub-pixel position rounding (rounding). Notably, prior to performing the warping, the motion field mv cur used at the first processing level is scaled down from its full resolution value to the resolution of that level. The motion field is scaled down as discussed in more detail below.
Knowing the optical flow mv cur, the motion field that distorts the reference frame 1 is inferred from the following linear projection assumption (e.g., the motion is projected linearly over time):
mvr1＝(indexcur-indexr1)/(indexr2-indexr1)·mvcur
to perform the warping, the horizontal component U r1 and the vertical component U r1 of the motion field mv r1 can be rounded to 1/8 pixel precision for the Y component and 1/16 pixel precision for the U and V components. Other values of sub-pixel position rounding may be used. After rounding, a warped image is computed As a reference pixel given by a motion vector mv r1. The subpixel interpolation may be performed using a conventional subpixel interpolation filter.
The same warping method is performed on reference frame 2 to obtain a warped imageWherein the motion field is calculated by the formula:
mvr2＝(indexr2-indexcur)/(indexr2-indexr1)·mvcur
At the end of the calculation at 1106, there are two warped reference frames. In 1108, two warped reference frames are used to estimate the motion field between them. Estimating the motion field in 1108 may include a number of steps.
First, the derivatives E x、Ey and E t are calculated using the functions (3), (4), and (5). When calculating the derivative, the frame boundary of the warped reference frame may be expanded by copying the nearest available pixels. In this way, pixel values (i.e., E (r1) and/or E (r2)) may be obtained when the projection position is outside of the warped reference frame. Then, if there are multiple layers, the derivative is scaled down to the current level. As shown in fig. 13, the reference frames are used to calculate derivatives at the original scale to capture details. The scaled down derivative at each level 1 may be calculated by averaging over a block of 2 1×21. Notably, because the calculation of the derivatives and by averaging them are linear operations, both operations can be combined in a single linear filter to calculate the derivatives in each level 1. This may reduce the complexity of the computation.
Once the derivative is scaled down to the current processing level, optical flow estimation may be performed according to the lagrangian function (1), if applicable. More specifically, by setting the derivative of the lagrangian function (1) with respect to the horizontal component u of the motion field and the vertical component v of the motion field to zero (i.e.,And/>) The components u and v for all N pixels of a frame can be solved for using 2*N linear equations. This is due to the fact that the laplace operator approximates a two-dimensional (2D) filter. Instead of directly solving the linear equation, which is accurate but highly complex, an iterative method with faster but less accurate results can be used to minimize the lagrangian function (1).
In 1108, the motion field of the pixels of the current frame is updated or refined using the estimated motion field between the warped reference frames. For example, the current motion field of a pixel may be updated by adding the estimated motion field of the pixel on a pixel-by-pixel basis.
Once the motion field is estimated at 1108, a query is made at 1110 to determine if there is an additional value of the lagrangian parameter λ available. A smaller value of the lagrangian parameter lambda may account for smaller scale movements. If additional value exists, process 1100 may return to 1104 to set the next value for the Lagrangian parameter λ. For example, process 1100 may be repeated while reducing the lagrangian parameter λ by half in each iteration. The motion field updated in 1108 is the current motion field used to warp the reference frame in 1106 in such next iteration. Then, the motion field is again estimated at 1108. 1104. Processing in 1106 and 1108 continues until all possible lagrangian parameters in 1110 are processed. In the example, there are three levels for the pyramid as shown in fig. 13, so in the example, the minimum value of the lagrangian parameter λ is 25. This repeated process of modifying the Lagrangian parameters may be referred to as annealing the Lagrangian parameters.
Once there are no remaining values for the lagrangian parameter λ at 1110, the process 1100 proceeds to 1112 to determine if there are more processing levels to process. If there are additional processing levels in 1112, process 1100 proceeds to 1114, where the motion field is enlarged before processing the next layer using each of the available values of the Lagrangian parameter λ starting at 1104. The enlarging of the motion field may be performed using any known technique, including but not limited to the inverse of the previously described downscaling calculation.
Typically, the optical flow is estimated first to obtain the motion field at the highest level of the pyramid. The motion field is then enlarged and used at the next level to initialize the optical flow estimation. This process of amplifying the motion field, using it to initialize the optical flow estimation of the next level, and obtaining the motion field continues until the lowest level of the pyramid is reached in 1112 (i.e., until the optical flow estimation is completed for the derivative calculated at full scale).
Once the levels are at a level where the reference frames are not scaled down (i.e., they are at their original resolution), process 1100 proceeds to 1116. For example, the number of levels may be three, such as in the example of fig. 13. In 1116, the warped reference frames are mixed to form optical flow reference frame E (cur). Note that the warped reference frame mixed in 1116 may be a full-scale reference frame, which is warped again according to the procedure described in 1106 using the motion field estimated in 1108. In other words, the full-scale reference frame may be warped twice-once using the initial enlarged motion field from the previous layer of processing and again after refining the motion field at the full-scale level. The blending may be performed using the following temporal linearity assumption (e.g., frames are separated by equal segment intervals):
In some embodiments, values that preferentially warp pixels in only one of the reference frames, rather than blending, are contemplated. For example, if the reference pixels in reference frame 1 (represented by mv r1) are outside the boundary (e.g., outside the size of the frame), and the reference pixels in reference frame 2 are not, then the pixels in the warped image caused by reference frame 2 are used only according to the following equation:
an optional occlusion detection may be performed as part of the blending. Occlusion of objects and background typically occurs in video sequences, where portions of objects appear in one reference frame but are hidden in another. In general, the above optical flow estimation method cannot estimate the movement of the object in this case because the luminance constant assumption is violated. If the size of the occlusion is relatively small, the smoothness penalty function can estimate motion very accurately. That is, if an ambiguous motion field at a hidden portion is smoothed by an adjacent motion vector, the motion of the entire object is accurate.
However, even in this case, the above simple mixing method does not necessarily give satisfactory interpolation results. This can be demonstrated by referring to fig. 14, which is a diagram illustrating object occlusion. In this example, the occluded part of object a is displayed in reference frame 1 and hidden by object B in reference frame 2. Because the hidden portion of object a is not displayed in reference frame 2, the reference pixels from reference frame 2 are from object B. In this case, it is desirable to use only warped pixels from the reference frame 1. Thus, using techniques to detect occlusion instead of or in addition to the above blending may provide better blending results and thus better reference frames.
For detection of occlusion it is observed from fig. 14 that when occlusion occurs and the motion field is quite accurate, the motion vector of the occluded part of object a points to object a in reference frame 2. This may lead to the following. The first case is to warp pixel valuesAnd/>Are very different because they come from two different objects. The second case is that the pixels in object B are referenced by multiple motion vectors for the occlusion parts of object B in the current frame and object a in the current frame.
With these observations, the following conditions can be established to determine occlusion and use E cur aloneWherein similar conditions apply equally to/>, using E cur alone
Greater than a threshold T pixel; and/>
Greater than a threshold T ref.
Is the total number of times a reference pixel in reference frame 1 is referenced by any pixel in the current co-located frame. Assuming that the subpixel interpolation described above exists, when the reference subpixel position is within one pixel length of the pixel position of interest,/>Is counted. Furthermore, if mv r2 points to a subpixel location, then the/>, of four neighboring pixelsAs a total reference number for the current sub-pixel location. /(I)May be defined in a similar manner.
Accordingly, occlusion may be detected in the first reference frame using the first warped reference frame and the second warped reference frame. The blending of the warped reference frame may then include filling pixel locations of the optical flow reference frame corresponding to the occlusion with pixel values from the second warped reference frame. Similarly, occlusion may be detected in the second reference frame using the first warped reference frame and the second warped reference frame. The blending of the warped reference frames may then include filling pixel locations of the optical flow reference frames corresponding to the occlusion with pixel values from the first warped reference frame.
Experiments have shown that process 1100 provides a significant compression performance gain. These performance gains include: for a low resolution set of frames, 2.5% gain in PSNR and 3.3% gain in SSIM, and for a medium resolution set of frames, 3.1% in PSNR and 4.0% in SSIM. However, and as described above, the optical flow estimation performed according to the lagrangian function (1) uses 2*N linear equations to solve the horizontal component u and the vertical component v of the motion field for all N pixels of the frame. In other words, the computational complexity of the optical flow estimation is a polynomial function of the frame size, which places a burden on the complexity of the decoder. Accordingly, sub-frame-based (e.g., block-based) optical flow estimation is described next, which may reduce decoder complexity over the frame-based optical flow estimation described with respect to fig. 11.
FIG. 12 is a flow chart of a method or process 1200 for generating an optical flow reference frame portion. In this example, the optical flow reference frame portion is less than the entire reference frame. The co-located frame portions in this example are described with reference to blocks, but other frame portions may be processed according to fig. 12. Process 1200 may implement step 1006 of process 1000. For example, process 1200 may be implemented as a software program that may be executed by a computing device, such as transmitting station 102 or receiving station 106. For example, a software program may include machine-readable instructions that may be stored in a memory, such as memory 204 or secondary storage 214, and that when executed by a processor, such as CPU 202, may cause the computing device to perform process 1200. Process 1200 may be implemented using dedicated hardware or firmware. As noted above, multiple processors, memories, or both may be used.
In 1202, all pixels in the current frame may be assigned an initialization motion vector. They define an initial motion field that can be used to warp the reference frames into the current frame of the first processing level to shorten the length of motion between the reference frames. The initialization in 1202 may be performed using the same process as described with respect to the initialization in 1102, and thus, description will not be repeated here.
In 1204, reference frames, such as reference frames 1 and 2, are warped into the current frame according to the motion field initialized in 1202. The warping in 1204 may be performed using the same processing as described with respect to the warping in 1106, unless the motion field mw cur initialized in 1202 is desirably not scaled down from its full resolution value before warping the reference frame.
At the end of the computation in 1204, there are two warped reference frames at full resolution. As with process 1100, process 1200 may use a multi-level process similar to that described with respect to fig. 13 to estimate a motion field between two reference frames. Colloquially, process 1200 calculates derivatives of the levels, performs optical flow estimation using the derivatives, and enlarges the resulting motion field for the next level until all levels are considered.
More specifically, a motion field mv cur for a block at the current (or first) processing level is initialized at 1206. The block may be a block of the current frame selected in a scanning order (e.g., raster scanning order) of the current frame. The motion field mv cur of a block comprises the motion field of the corresponding pixel of the block. In other words, in 1206, all pixels with the current block are assigned an initialization motion vector. The initialization motion vector is used to warp the reference blocks into the current block to shorten the length between the reference blocks in the reference frame.
In 1206, the motion field mv cur is scaled down from its full resolution value to that level of resolution. In other words, the initialization in 1206 may include: the motion field of the corresponding pixel of the block is scaled down from the full resolution value initialized in 1202. The scaling down may be performed using any technique such as the scaling down described above.
In 1208, the co-located reference block corresponding to the motion field in each of the warped reference frames is warped to the current block. The warping of the reference block is performed similarly to process 1100 in 1106. That is, knowing the optical flow mv cur of the pixels of the reference block in reference frame 1, the motion field for the warp is deduced by the following linear projection assumption (e.g., the motion is projected linearly over time):
mvr1＝(indexcur-indexr1)/(indexr2-indexr1)·mucur
To perform the warping, the horizontal component U r1ur1 and the vertical component U r1 of the motion field mv r1 can be rounded to 1/8 pixel precision for the Y component and 1/16 pixel precision for the U and V components. Other values may be used. After rounding, a warped block is calculated As a reference pixel given by a motion vector mv r1, the warped block is e.g./>The subpixel interpolation may be performed using a conventional subpixel interpolation filter.
The same warping method is performed on the reference blocks of reference frame 2 to obtain warped blocks, e.g.,Wherein the motion field is calculated by:
mvr2＝(indexr2-indexcur)/(indexr2-indexr1)·mvcur
At the end of the computation in 1208, there are two warped reference blocks. In 1210, two warped reference blocks are used to estimate the motion field between them. The process in 1210 may be similar to the process described with respect to the process in 1108 of fig. 11.
More specifically, the two warped reference blocks may be at full resolution. According to the pyramid structure in fig. 13, the derivatives E x、Ey and E t are calculated using functions (3), (4), and (5). When calculating the derivative of the frame level estimate, the frame boundary may be expanded by copying the nearest available pixels to obtain pixel values outside the boundary, as described with respect to process 110. However, for other frame portions, neighboring pixels are typically available in the reference frame warped in 1204. For example, for block-based estimation, pixels of neighboring blocks are available in a warped reference frame unless the block itself is at a frame boundary. Thus, for pixels outside the boundary associated with the warped reference frame portion, pixels in the adjacent portion of the warped reference frame may be used as pixel values E (r1) and E (r2), if applicable. If the projected pixel is outside the frame boundary, then the most recently available (i.e., within the boundary) pixel may still be used. After the derivatives are calculated, they can be scaled down to the current level. The scaled derivative of each level l may be calculated by averaging over a block of 2 l×2l, as previously discussed. The complexity of the computation may be reduced by combining two linear operations of computing the derivative and averaging the derivative in a single linear filter, but this is not required
Continuing with the process in 1210, optical flow estimation may be performed to estimate the motion field between the warped reference parts using the scaled down derivative as input to the Lagrangian function (1). The horizontal component u and the vertical component v of the motion field of all N pixels of a part-here a block-may be obtained by setting the derivative of the lagrangian function (1) with respect to the horizontal component u and the vertical component v to zero (i.e.,And/>) And 2*N linear equations are solved for determination. For this purpose, there are two alternative ways to solve the motion vectors outside the boundary. One way is to assume zero correlation with neighboring blocks and assume that the motion vector outside the boundary is the same as the motion vector at the nearest boundary position to the pixel position outside the boundary. Another way is to use the initialized motion vector for the current pixel (i.e., the motion field initialized in 1206) as a motion vector for a pixel location outside the boundary corresponding to the current pixel.
After estimating the motion field, the current motion field at the level is updated or refined using the estimated motion field between the warped reference blocks to complete the process in 1210. For example, the current motion field for a pixel may be updated by adding the estimated motion field for the pixel on a pixel-by-pixel basis.
In process 100, an additional loop is included to set the decreasing value of the Lagrangian parameter λ so that at each level, smaller and smaller values of the Lagrangian parameter λ are used to estimate and refine the motion field. In process 1200, such loops are omitted. That is, in the process 1200 as shown, only one value of the Lagrangian parameter λ is used to estimate the motion field at the current processing level. This may be a relatively small value, such as 25. Other values of the lagrangian parameter lambda are possible, for example, depending on smoothness of motion, image resolution or other variables.
In other implementations, the process 1200 may include additional loops for changing the lagrangian parameter λ. In embodiments that include such loops, the lagrangian parameter λ may be set prior to estimating the motion field in 1210 such that the reference block is warped in 1208 and the motion field estimated and updated in 1210 is repeated until all values of the lagrangian parameter λ have been used, as described with respect to the processing in 1104 and 1110 in process 1100.
After estimating and updating the motion field in 1210, process 1200 proceeds to query at 1212. This is done after the motion field estimation and update at the level at 1210, which is first and only once when using a single value of the lagrangian parameter λ. When modifying the plurality of values of the lagrangian parameter λ at the processing level, the process 1200 proceeds to the query at 1212 after estimating and updating the motion field using the final value of the lagrangian parameter λ at 1210.
If additional processing levels exist in response to the query in 1212, process 1200 proceeds to 1214, where the motion field is enlarged before processing the next layer begins in 1206. The amplification may be performed according to any known technique.
Typically, the optical flow is estimated first to obtain the motion field at the highest level of the pyramid. The motion field is then enlarged and used at the next level to initialize the optical flow estimation. This process of amplifying the motion field, using it to initialize the optical flow estimation of the next level, and obtaining the motion field continues until the lowest level of the pyramid is reached at 1212 (i.e., until the optical flow estimation is completed for the derivative calculated at full scale).
Once the levels are at a level where the reference frames were not scaled down (i.e., they were at the original resolution), process 1200 proceeds to 1216. For example, the number of levels may be three, such as in the example of fig. 13. In 1216, the warped reference blocks are mixed to form optical flow reference blocks (e.g., E (cur), previously described). Note that the warped reference block mixed in 1216 may be a full-scale reference block, which is warped again according to the procedure described in 1208 using the motion field estimated in 1210. In other words, the full-scale reference block may be warped twice—once using the initial enlarged motion field from the previously processed layer and again after refining the motion field at the full-scale level. Blending may be performed similarly to the process described in 1116 using time-linear assumptions. An optional occlusion detection as described in 1116 and illustrated by the example in fig. 14 is incorporated into 1216 as part of the mixing.
After generating the co-located reference block in 1216, process 1200 proceeds to 1218 to determine whether there is a further frame portion (here a block) for prediction. If so, process 1200 repeats beginning at 1206 for the next block. The blocks may be processed in scan order. Once there are no further block considerations in responding to the query in 1218, process 1200 ends.
Referring again to fig. 10, process 1200 may implement 1006 in process 1000. At the end of the process at 1006, whether performed according to process 1100, process 1200, or variations of any of those described herein, there are one or more warped reference frame parts.
At 1008, a prediction process is performed using the optical flow reference frame portion generated at 1006. Performing the prediction process at the encoder may include: a prediction block is generated from an optical flow reference frame for a current block of the frame. The optical flow reference frame may be an optical flow reference frame output by process 1100 and stored in a reference frame buffer, such as reference frame buffer 600. The optical flow reference frame may be an optical flow reference frame generated by combining optical flow reference portions output by process 1200. Combining the optical flow reference portions may include: the optical flow reference portions (e.g., co-located reference blocks) are arranged according to pixel locations of respective current frame portions used to generate each of the optical flow reference portions. The resulting optical flow reference frame may be stored for use in a reference frame buffer of an encoder, such as reference frame buffer 600 of encoder 400.
Generating the prediction block at the encoder may include: a co-located block in the optical flow reference frame is selected as the prediction block. Generating the prediction block at the encoder may also include: a motion search is performed in the optical flow reference frame to select the best matching prediction block for the current block. However, the prediction block is generated at the encoder, and the resulting residual may be further processed, such as using the lossy encoding process described with respect to encoder 400 of fig. 4.
At the encoder, process 1000 may form part of a rate-distortion loop for the current block using various prediction modes, including both one or more intra-prediction modes as well as single and composite inter-prediction modes using available prediction frames for the current frame. A single inter prediction mode uses only a single forward or backward reference frame for inter prediction. The composite inter prediction mode uses both forward and backward reference frames for inter prediction. In the rate-distortion loop, the rate (e.g., number of bits) used to encode the current block using the corresponding prediction mode is compared to the distortion resulting from the encoding. The distortion may be calculated as the difference between the pixel values of the block before encoding and after decoding. The difference may be the absolute difference or some other measure of the accumulated error of the block capturing the frame.
In some implementations, it may be desirable to limit the use of optical flow reference frames to a single inter prediction mode. That is, in any composite reference mode, the optical flow reference frame is excluded as a reference frame. This may simplify the rate-distortion loop and is expected to have little impact on the encoding of the block, since the optical flow reference frame already takes into account both the forward and backward references. According to embodiments described herein, a marker may be encoded into a bitstream to indicate whether an optical flow reference frame is available to encode a current frame. In an example, when any single block within the current frame is encoded using the optical stream reference frame block, the marker may be encoded. In the case that an optical flow reference frame is available for the current frame, an additional flag or other indicator (e.g., at the block level) may be included that indicates whether the current block is encoded by inter prediction using the optical flow reference frame.
The prediction process in 1008 may be repeated for all blocks of the current frame until the current frame is encoded.
In the decoder, performing a prediction process using the optical flow reference frame at 1008 may result from determining that the optical flow reference frame is available for decoding the current frame. In some embodiments, the determination is made by examining a flag indicating that at least one block of the current frame is encoded using the optical stream reference frame portion. The prediction process in the decoder performed 1008 may include: a prediction block is generated. Generating the prediction block may include: inter prediction decoded from an encoded bitstream, such as in a block header, is used. The flag or indicator may be decoded to determine the inter prediction mode. When the inter prediction mode is an optical flow reference frame mode (i.e., inter prediction of a block using an optical flow reference frame portion), a predicted block of the current block to be decoded is generated using pixels of the optical flow reference frame portion and a motion vector mode and/or a motion vector.
The same process of generating optical flow reference frames for use in a prediction process as part of decoding may be performed in a decoder, such as decoder 500, as in an encoder. For example, when the flag indicates that at least one block of the current frame is encoded using the optical flow reference frame portion, the entire optical flow reference frame may be generated and stored for use in the prediction process. However, by modifying the process 1200 to limit execution of the process 1200 in which the coded blocks are identified as using co-located/optical flow reference frames as inter-prediction frames, a saving in computational power at the decoder is achieved. This can be described with reference to fig. 15, fig. 15 being a diagram illustrating a technique of optimizing a decoder.
In fig. 15, pixels are displayed along grid 1500, where w represents pixel locations along a first axis of grid 1500 and where y represents pixel locations along a second axis of grid 1500. Grid 1500 represents pixel locations of portions of the current frame. To perform the prediction process at the decoder at 1008, the processes in 1006 and 1008 may be combined. For example, the prediction process in 1008 may include, prior to performing the process in 1006: a reference block (e.g., header information, such as a motion vector) for encoding the current block is found. In fig. 15, the motion vector of the current coding block 1502 points to the reference block represented by the inner dashed line 1504. The current coding block 1502 includes 4×4 pixels. The reference block location is indicated by dashed line 1504 because the reference block is located in the reference frame, not in the current frame.
Once the reference blocks are located, all reference blocks spanned (i.e., overlapped) by the reference blocks are identified. This may include: the reference block size is extended by half the filter length at each boundary to account for the sub-pixel interpolation filter. In fig. 15, a subpixel interpolation filter length L is used to extend the reference block to the boundary represented by the outer dashed line 1506. It is relatively common that motion vectors result in reference blocks that are not perfectly aligned with the full pixel position. The hatched area in fig. 15 indicates the full pixel position. All reference blocks overlapping the full pixel location are identified. Assuming that the block size is the same as the current coding block 1502, a first reference block co-located with the current block, a second reference block above the first reference block, two reference blocks extending from the left side of the first reference block, and two reference blocks extending from the left side of the second reference block are identified.
Once a reference block is identified, process 1200 is performed at 1006 on only blocks within the current frame that are co-located with the identified reference block to produce a co-located/optical flow estimation reference block. In the example of fig. 15, this would result in six optical flow reference frame portions.
According to this modified procedure, it is ensured that both the encoder and the decoder have the same predictor, while the decoder does not need to calculate all of the co-located reference frames. Notably, reference blocks of subsequent blocks, including any extension edges, may overlap with one or more reference blocks identified during decoding of the current block. In this case, it is necessary to perform optical flow estimation only once on any one of the identified blocks to further reduce the computational requirements of the decoder. In other words, the reference block generated in 1216 may be stored for decoding other blocks of the current frame.
Regardless of how the prediction block is generated in the decoder, the decoded residual from the current block of the encoded bitstream may be combined with the prediction block to form a reconstructed block, as described with respect to decoder 500 of fig. 5.
The prediction process at 1008, whether performed after process 1200 or in conjunction with process 1200, may be repeated for all blocks of the current frame that are encoded using the optical stream reference frame portion until the current frame is decoded. When processing blocks in decoding order, blocks that are not partially encoded using the optical stream reference frame may be conventionally decoded according to a prediction mode decoded for the block from the encoded bitstream.
The complexity of solving the optical flow equations for N pixels in a frame or block may be represented by O (N x M), where M is the number of iterations of solving the linear equation. M is not related to the number of levels or the number of values of the lagrangian parameter λ. Instead, M relates to the accuracy of the calculation to solve the linear equation. The larger the value of M results in higher accuracy. In view of this complexity, moving from a frame level to a sub-frame level (e.g., block-based) estimation provides several options for reducing decoder complexity. First, and because the constraint on motion field smoothness is relaxed at the block boundaries, it is easier to converge to a solution when solving the linear equation for deblocking, resulting in a smaller M of similar accuracy. Second, due to the smoothness penalty factor, the solution of the motion vector involves its neighboring motion vectors. The motion vectors at the block boundaries have fewer neighboring motion vectors, yielding faster computations. Third, and as described above, optical flow need only be calculated for portions of blocks of the co-located reference frame that are identified by those coded blocks of the co-located reference frame, rather than the entire frame for inter prediction.
For simplicity of explanation, each of the processes 1000, 1100, and 1200 are depicted and described as a series of steps or operations. However, steps or operations according to the present disclosure may occur in various orders and/or concurrently. In addition, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a method in accordance with the disclosed subject matter.
The above-described encoding and decoding aspects illustrate some examples of encoding and decoding techniques. However, it is to be understood that the encoding and decoding of these terms as used in the claims may mean compression, decompression, transformation, or any other processing or change to the data.
The word "example" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" is intended to present concepts in a concrete fashion. As used in this disclosure, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, as apparent from the context, "X includes A or B" is intended to mean any natural inclusive permutation. That is, if X includes A; x comprises B; or X includes A and B, then "X includes A or B" is satisfied in any of the above cases. In addition, the articles "a" and "an" as used in this disclosure and the appended claims may generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, the terms "embodiment" or "one embodiment" are not intended to refer to the same example or embodiment throughout, unless so described.
Embodiments of the sending station 102 and/or the receiving station 106 (and algorithms, methods, instructions, etc. stored thereon and/or executed by the encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuit. In the claims, the term "processor" should be understood to include any of the foregoing hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. Furthermore, portions of the sending station 102 and the receiving station 106 need not necessarily be implemented in the same manner.
Further, in one aspect, for example, the sending station 102 or the receiving station 106 may be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, performs any of the corresponding methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be utilized which may contain other hardware for carrying out any of the methods, algorithms, or instructions described herein.
For example, the sending station 102 and the receiver 106 may be implemented on computers in a video conferencing system. Alternatively, the sending station 102 may be implemented on a server and the receiving station 106 may be implemented on a device separate from the server, such as a handheld communication device. In this case, the transmitting station 102 may encode the content into an encoded video signal using the encoder 400 and transmit the encoded video signal to the communication device. The communication device may then decode the encoded video signal using decoder 500. Alternatively, the communication device may decode content stored locally on the communication device, such as content that is not transmitted by the sending station 102. Other suitable transmitting station 102 and receiving station 106 implementations are also available. For example, the receiving station 106 may be a generally stationary personal computer rather than a portable communication device, and/or a device including the encoder 400 may also include the decoder 500.
Further, all or part of the embodiments of the present disclosure may take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be, for example, any apparatus that may tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
Further embodiments are summarized in the following examples:
Example 1: a method, the method comprising: determining a first frame in the video sequence to be predicted; determining a first reference frame from a video sequence for forward inter-prediction of the first frame; determining a second reference frame from the video sequence for backward inter-prediction of the first frame; generating an optical flow reference frame for inter-prediction of the first frame by performing optical flow estimation using the first reference frame and the second reference frame; and performing a prediction process of the first frame using the optical flow reference frame.
Example 2: the method of example 1, wherein generating the optical flow reference frame comprises: the optical flow estimation is performed for a respective pixel of the first frame by minimizing a lagrangian function.
The method of example 1 or 2, wherein the optical flow estimation generates a respective motion field of pixels of the first frame, and generating the optical flow reference frame comprises: warping the first reference frame into the first frame using the motion field to form a first warped reference frame; warping the second reference frame into the first frame using the motion field to form a second warped reference frame; and blending the first warped reference frame and the second warped reference frame to form an optical flow reference frame.
The method of example 3, wherein mixing the first warped reference frame and the second warped reference frame comprises: the co-located pixel values of the first and second warped reference frames are combined by scaling the co-located pixel values using a distance between the first and second reference frames and between the current frame and each of the first and second reference frames.
The method of example 3 or 4, wherein mixing the first warped reference frame and the second warped reference frame comprises: filling the pixel locations of the optical flow reference frame by one of: co-located pixel values of the first warped reference frame and the second warped reference frame are combined or a single pixel value of one of the first warped reference frame or the second warped reference frame is used.
Example 6: the method of any one of examples 3 to 5, further comprising: detecting occlusion in the first reference frame using the first warped reference frame and the second warped reference frame, wherein mixing the first warped reference frame and the second warped reference frame comprises: pixel locations of the optical flow reference frame corresponding to the occlusion are filled with pixel values from the second warped reference frame.
Example 7: the method of any of examples 1-6, wherein performing the prediction process comprises: only optical flow reference frames are used for single reference inter prediction of blocks of the first frame.
Example 8: the method of any of examples 1-7, wherein the first reference frame is a reconstructed frame that is closest to the first frame in display order of the video sequence that is usable for forward inter-prediction of the first frame, and the second frame is a reconstructed frame that is closest to the first frame in the display order that is usable for backward inter-prediction of the first frame.
Example 9: the method of any of examples 1-8, wherein performing the prediction process comprises: determining a reference block within the optical flow reference frame that is co-located with a first block of the first frame; and encoding a residual of the reference block and the first block.
Example 10: an apparatus, the apparatus comprising: a processor; and a non-transitory storage medium including instructions executable by the processor to perform a method comprising: determining a first frame in the video sequence to be predicted; determining availability of a first reference frame for forward inter-prediction of the first frame and a second reference frame for backward inter-prediction of the first frame; in response to determining the availability of both the first reference frame and the second reference frame: generating respective motion fields of the first frame using the first reference frame and the second reference frame using the optical flow estimate; warping the first reference frame into the first frame using the motion field to form a first warped reference frame; warping the second reference frame into the first frame using the motion field to form a second warped reference frame; and blending the first warped reference frame and the second warped reference frame to form an optical flow reference frame for inter-prediction of a block of the first frame.
Example 11: the apparatus of example 10, wherein the method further comprises: a prediction process is performed on the first frame using the optical flow reference frame.
Example 12: the apparatus of example 10 or 11, wherein the method further comprises: the optical flow reference frame is used for single reference inter prediction of a block of the first frame only.
Example 13: the apparatus of any of examples 11 to 12, wherein generating the respective motion fields comprises: an output of a lagrangian function is calculated for respective pixels of the first frame using the first reference frame and the second reference frame.
Example 14: the apparatus of example 13, wherein calculating the output of the lagrangian function comprises: calculating a first set of motion fields for the pixels of the current frame using a first value of a lagrangian parameter; and using the first set of motion fields as input to the lagrangian function, using a second value of the lagrangian parameter to compute a refined set of motion fields for the pixels of the current frame, wherein the second value of the lagrangian parameter is less than the first value of the lagrangian parameter, and the first and second warped reference frames are warped using the refined set of motion fields.
Example 15: an apparatus, comprising: a processor; and a non-transitory storage medium including instructions executable by the processor to perform a method comprising: generating an optical flow reference frame for inter-prediction of a first frame of a video sequence by using a first reference frame from the video sequence and a second reference frame of the video sequence by: initializing a motion field of pixels of the first frame for optical flow estimation in a first processing level, the first processing level representing reduced motion within the first frame and comprising one of a plurality of levels; for each of the plurality of levels: warping the first reference frame into the first frame using the motion field to form a first warped reference frame; warping the second reference frame into the first frame using the motion field to form a second warped reference frame; estimating a motion field between the first warped reference frame and the second warped reference frame using the optical flow estimate; and updating the motion field of pixels of the first frame using the motion field between the first and second warped reference frames; for a final grade of the plurality of grades: warping the first reference frame into the first frame using the updated motion field to form a final first warped reference frame; warping the second reference frame into the first frame using the updated motion field to form a final second warped reference frame; and blending the final first warped reference frame and the final second warped reference frame to form an optical flow reference frame.
Example 16: the apparatus of example 15, wherein the optical flow estimates use a lagrangian function for respective pixels of a frame.
Example 17: the apparatus of example 16, wherein the method further comprises: for each of the plurality of levels: initializing the lagrangian parameters in the lagrangian function to a maximum for a first iteration of: distorting the first reference frame, distorting the second reference frame, estimating the motion field and updating the motion field; the following additional iterations are performed: the first reference frame is warped, the second reference frame is warped, the motion field is estimated, and the motion field is estimated using smaller and smaller values of the set of possible values of the lagrangian parameter.
Example 18: the apparatus of example 16 or 17, wherein estimating the motion field comprises: calculating derivatives of pixel values of the first and second warped reference frames with respect to a horizontal axis, a vertical axis, and time; narrowing the derivative in response to the grade being different from the final grade; the derivative is used to solve a linear equation representing the lagrangian function.
Example 19: the apparatus of any one of examples 15 to 18, wherein the method further comprises: inter-predicting the first frame using the optical flow reference frame.
Example 20: the apparatus of any of examples 15 to 19, wherein the processor and the non-transitory storage medium form a decoder.
The foregoing examples, implementations and aspects have been described in order to allow the disclosure to be readily understood and not to be limiting. On the contrary, the invention is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (20)
1. A method, comprising:
decoding a motion vector of a current block of a current frame from a compressed bitstream;
identifying a location of a reference block within the non-generated reference frame;
The reference block is generated using a forward reference frame and a backward reference frame without generating the non-generated reference frame by:
identifying an extended reference block by extending the reference block by a number of pixels related to a filter length of a filter for sub-pixel interpolation at each boundary of the reference block; and
Generating pixel values of only the extended reference block by performing projection using the forward reference frame and the backward reference frame, without generating an entire non-generated reference frame; and
The current block is decoded based on the reference block and the motion vector.
2. The method of claim 1, further comprising:
a flag indicating that the current block is encoded using the non-generated reference frame is decoded from the compressed bitstream.
3. The method of claim 2, further comprising:
In response to the flag indicating that the current block is encoded using the non-generated reference frame, decoding an index of a reference frame used to decode the current block from the compressed bitstream is omitted.
4. A method according to any one of claims 1 to 3, wherein the non-generated reference frame is co-located in time with the current frame.
5. A method according to any one of claims 1 to 3, further comprising:
a motion prediction using the non-generated reference frame is determined to be available for the current frame.
6. The method of claim 5, wherein the motion prediction using the non-generated reference frame is determined to be available in response to a determination that a reference frame buffer includes the forward reference frame and the backward reference frame.
7. A method according to any one of claims 1 to 3, wherein the number of pixels is equal to half the filter length.
8. An apparatus, comprising:
a processor configured to:
decoding a motion vector of a current block of a current frame from a compressed bitstream;
identifying a location of a reference block within the non-generated reference frame;
generating the reference block using a forward reference frame and a backward reference frame without generating the non-generated reference frame, wherein generating the reference block comprises:
identifying an extended reference block by extending the reference block by a number of pixels related to a filter length of a filter for sub-pixel interpolation at each boundary of the reference block; and
Generating pixel values of only the extended reference block by performing projection using the forward reference frame and the backward reference frame, without generating an entire non-generated reference frame; and
The current block is decoded based on the reference block and the motion vector.
9. The device of claim 8, wherein the processor is further configured to:
a flag indicating that the current block is encoded using the non-generated reference frame is decoded from the compressed bitstream.
10. The device of claim 9, wherein the processor is further configured to:
In response to the flag indicating that the current block is encoded using the non-generated reference frame, decoding an index of a reference frame used to decode the current block from the compressed bitstream is omitted.
11. The device of claim 8, wherein the non-generated reference frame is co-located in time with the current frame.
12. The device of claim 8, wherein the processor is further configured to:
the motion prediction using the non-generated reference frame is determined to be available for the current frame.
13. The apparatus of claim 12, wherein the motion prediction using the non-generated reference frame is determined to be available in response to a determination that a reference frame buffer includes the forward reference frame and the backward reference frame.
14. The apparatus of any of claims 8 to 13, wherein the number of pixels is equal to half the filter length.
15. A non-transitory computer readable storage medium having stored thereon a compressed bitstream, wherein the compressed bitstream is configured for decoding by operations comprising:
decoding a motion vector of a current block of a current frame from a compressed bitstream;
identifying a location of a reference block within the non-generated reference frame;
The reference block is generated using a forward reference frame and a backward reference frame without generating the non-generated reference frame by:
identifying an extended reference block by extending the reference block by a number of pixels related to a filter length of a filter for sub-pixel interpolation at each boundary of the reference block; and
Generating pixel values of only the extended reference block by performing projection using the forward reference frame and the backward reference frame, without generating an entire non-generated reference frame; and
The current block is decoded based on the reference block and the motion vector.
16. The non-transitory computer-readable storage medium of claim 15, wherein the operations further comprise:
a flag indicating that the current block is encoded using the non-generated reference frame is decoded from the compressed bitstream.
17. The non-transitory computer-readable storage medium of claim 16, wherein the operations further comprise:
In response to the flag indicating that the current block is encoded using the non-generated reference frame, decoding an index of a reference frame used to decode the current block from the compressed bitstream is omitted.
18. The non-transitory computer-readable storage medium of claim 15, wherein the non-generated reference frame is co-located in time with the current frame.
19. The non-transitory computer-readable storage medium of any one of claims 15 to 18, wherein the operations further comprise:
a motion prediction using the non-generated reference frame is determined to be available for the current frame.
20. The non-transitory computer-readable storage medium of claim 19, wherein the motion prediction using the non-generated reference frame is determined to be available in response to determining that a reference frame buffer includes the forward reference frame and the backward reference frame.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/683,684 | 2017-08-22 | ||
US15/817,369 | 2017-11-20 |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880036783.5A Division CN110741640B (en) | 2017-08-22 | 2018-05-10 | Optical flow estimation for motion compensated prediction in video coding |
Publications (1)
Publication Number | Publication Date |
---|---|
CN118055253A true CN118055253A (en) | 2024-05-17 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110741640B (en) | Optical flow estimation for motion compensated prediction in video coding | |
US11284107B2 (en) | Co-located reference frame interpolation using optical flow estimation | |
CN111757106B (en) | Method and apparatus for coding a current block in a video stream using multi-level compound prediction | |
US11876974B2 (en) | Block-based optical flow estimation for motion compensated prediction in video coding | |
JP7279154B2 (en) | Motion vector prediction method and apparatus based on affine motion model | |
US10506249B2 (en) | Segmentation-based parameterized motion models | |
CN110741641B (en) | Method and apparatus for video compression | |
CN110692246B (en) | Method and apparatus for motion compensated prediction | |
CN110312130B (en) | Inter-frame prediction and video coding method and device based on triangular mode | |
US11115678B2 (en) | Diversified motion using multiple global motion models | |
US8170110B2 (en) | Method and apparatus for zoom motion estimation | |
CN111886868B (en) | Method and apparatus for adaptive temporal filtering for substitute reference frame rendering | |
US11917128B2 (en) | Motion field estimation based on motion trajectory derivation | |
CN118055253A (en) | Optical flow estimation for motion compensated prediction in video coding | |
WO2023205371A1 (en) | Motion refinement for a co-located reference frame |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication |