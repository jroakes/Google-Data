JP6851474B2 - Structured Orthogonal Random Features for Kernel-Based Machine Learning - Google Patents
Structured Orthogonal Random Features for Kernel-Based Machine Learning Download PDFInfo
- Publication number
- JP6851474B2 JP6851474B2 JP2019522814A JP2019522814A JP6851474B2 JP 6851474 B2 JP6851474 B2 JP 6851474B2 JP 2019522814 A JP2019522814 A JP 2019522814A JP 2019522814 A JP2019522814 A JP 2019522814A JP 6851474 B2 JP6851474 B2 JP 6851474B2
- Authority
- JP
- Japan
- Prior art keywords
- matrices
- diagonal
- matrix
- kernel
- orthogonal
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000010801 machine learning Methods 0.000 title claims description 49
- 239000011159 matrix material Substances 0.000 claims description 98
- 230000015654 memory Effects 0.000 claims description 60
- 238000000034 method Methods 0.000 claims description 44
- 230000009466 transformation Effects 0.000 claims description 23
- 238000012545 processing Methods 0.000 claims description 21
- 238000004590 computer program Methods 0.000 claims description 13
- 238000005315 distribution function Methods 0.000 claims description 7
- 239000013598 vector Substances 0.000 description 38
- 230000008569 process Effects 0.000 description 17
- 238000004891 communication Methods 0.000 description 12
- 230000006870 function Effects 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 7
- 238000013459 approach Methods 0.000 description 4
- 230000008901 benefit Effects 0.000 description 4
- 230000006835 compression Effects 0.000 description 3
- 238000007906 compression Methods 0.000 description 3
- 239000007787 solid Substances 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 230000006399 behavior Effects 0.000 description 2
- 238000007796 conventional method Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 229940050561 matrix product Drugs 0.000 description 2
- 238000012706 support-vector machine Methods 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000002860 competitive effect Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 235000013410 fast food Nutrition 0.000 description 1
- 238000007667 floating Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 238000000611 regression analysis Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/10—Machine learning using kernel methods, e.g. support vector machines [SVM]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/14—Fourier, Walsh or analogous domain transformations, e.g. Laplace, Hilbert, Karhunen-Loeve, transforms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/17—Function evaluation by approximation methods, e.g. inter- or extrapolation, smoothing, least mean square method
- G06F17/175—Function evaluation by approximation methods, e.g. inter- or extrapolation, smoothing, least mean square method of multidimensional data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Description
関連出願の相互参照
本出願は、その開示全体が参照により本明細書に組み込まれている、2016年10月26日に出願した米国仮出願第62/413,011号の利益を主張する、2017年10月25日に出願した米国出願第15/793,455号の継続出願であり優先権を主張するものである。
Cross-reference to related applications This application claims the benefit of US Provisional Application No. 62 / 413,011 filed October 26, 2016, the entire disclosure of which is incorporated herein by reference. It is a continuation of US Application No. 15 / 793,455 filed on 25th March and claims priority.
多くのシステムは、大規模な機械学習を使用して、音声認識、コンピュータビジョン、画像および音声ファイル検索およびカテゴリ化などの困難な問題を達成する。多層ニューラルネットワークのディープラーニングは、有効な大規模アプローチである。カーネル法、たとえば、ガウスおよび多項式カーネルも、より小規模な問題に対して使用されてきているが、カーネル法をスケーリングすることは困難であることが分かっている。 Many systems use large-scale machine learning to accomplish difficult problems such as speech recognition, computer vision, image and audio file retrieval and categorization. Deep learning of multi-layer neural networks is an effective large-scale approach. Kernel methods, such as the Gauss and polynomial kernels, have also been used for smaller problems, but scaling the kernel method has proven difficult.
実装形態では、ガウスカーネルに対して、コンパクト、高速、正確であるカーネル近似法を提供する。これらの実装形態は、構造化直交ランダム特徴(Structured Orthogonal Random Features)(SORF)と呼ばれる新しいフレームワークに従ってガウスカーネルに対する不偏推定量を生成する。カーネルへの不偏推定量KSORFは、行列の対の集まりの積を使用して計算される線形変換行列WSORFを伴い、各対は直交行列と指定された確率分布に従う実数である要素を有するそれぞれの対角行列とを含む。典型的には、直交行列は、ウォルシュアダマール行列であり、指定された確率分布は、ラーデマッヘル分布であり、線形変換行列WSORFを形成するために掛け合わされる行列の対が少なくとも2つ、通常は3つある。 The implementation provides a compact, fast, and accurate kernel approximation for the Gauss kernel. These implementations generate an unbiased estimator for the Gaussian kernel according to a new framework called Structured Orthogonal Random Features (SORF). An unbiased estimator to the kernel K SORF involves a linear transformation matrix W SORF calculated using the product of a collection of pairs of matrices, each pair having an element that is an orthogonal matrix and a real number that follows the specified probability distribution. Includes each diagonal matrix. Typically, the orthogonal matrix is the Walsh-Hadamard matrix, the specified probability distribution is the Rademacher distribution, and there are at least two pairs of matrices that are multiplied to form the linear transformation matrix W SORF, usually There are three.
一態様により、カーネルを使用してデータにクラス分類演算を実行するカーネルベースの機械学習システムに対する入力を生成する方法は、カーネルベースの機械学習システムの処理回路によって、対角行列の集まりを生成するステップであって、対角行列の集まりの各々は、ゼロである非対角要素と、指定された確率分布関数に従って分布する値を有する対角要素とを有する、ステップを含むことができる。この方法は、処理回路によって、直交行列の集まりを生成するステップであって、直交行列の集まりの各々は、相互に直交する行を有する、ステップも含むことができる。この方法は、対角行列の集まりの各々について、処理回路によって、行列の対の集まりを形成するステップであって、行列の対の集まりの各々は、(i)その対角行列と、(ii)直交行列の集まりのそれぞれの直交行列とを含む、ステップとをさらに含むことができる。この方法は、線形変換行列を生成するために、処理回路によって、行列の対の集まりの各々の積を生成するステップであって、線形変換行列は、カーネルベースの機械学習システムによって使用されるカーネルの不偏推定量である、ステップをさらに含むことができる。 In one aspect, the method of generating input to a kernel-based machine learning system that uses the kernel to perform classification operations on data produces a collection of diagonal matrices by the processing circuit of the kernel-based machine learning system. A step, each set of diagonal matrices, can include a step that has an off-diagonal element that is zero and a diagonal element that has values that are distributed according to a specified probability distribution function. This method is a step of generating a collection of orthogonal matrices by a processing circuit, and can also include a step in which each of the collections of orthogonal matrices has rows that are orthogonal to each other. This method is a step of forming a set of pairs of matrices by a processing circuit for each set of diagonal matrices, and each set of pairs of matrices is (i) its diagonal matrix and (ii). ) Can further include steps, including with each orthogonal matrix of a collection of orthogonal matrices. This method is the step of generating each product of a collection of pairs of matrices by a processing circuit to generate a linear transformation matrix, which is a kernel used by a kernel-based machine learning system. Can further include steps, which are unbiased estimates of.
1つまたは複数の実装形態の詳細は、添付図面と以下の説明とで述べられる。他の特徴は、説明と図面、さらには特許請求の範囲から明らかになるであろう。 Details of one or more implementations are given in the accompanying drawings and the following description. Other features will become apparent from the description and drawings, as well as the claims.
様々な図面内の類似の参照記号は、類似の要素を示す。 Similar reference symbols in various drawings indicate similar elements.
カーネル法は、非線形学習において使用される。カーネルは、2つのd次元データベクトルの間の類似関数を表す。カーネル法は、第1のデータベクトルxおよび第2のデータベクトルyにおけるカーネルk(x,y)を第1のデータベクトルの特徴ベクトル関数φ(x)と第2のデータベクトルの特徴ベクトル関数φ(y)との内積として近似的に表現することを伴う。特徴ベクトル関数は、典型的には2D>dである2D次元ベクトルである。カーネルを近似する乱択化フーリエ特徴(RFF)アプローチにより、カーネルが正定値およびシフト不変であるときに、特徴ベクトル関数は、 The kernel method is used in nonlinear learning. The kernel represents a similar function between two d-dimensional data vectors. In the kernel method, the kernel k (x, y) in the first data vector x and the second data vector y is the feature vector function φ (x) of the first data vector and the feature vector function φ of the second data vector. Accompanied by approximately expressing it as an inner product with (y). The feature vector function is typically a 2D dimensional vector with 2D> d. Due to the randomized Fourier feature (RFF) approach that approximates the kernel, the feature vector function is a feature vector function when the kernel is definite and shift invariant.
の形で表され、
ベクトルの各々wj, j∈{1,2, ...,D}は独立同分布に従い、カーネルk(x-y)のフーリエ変換、すなわち
Represented in the form of
Each of the vectors w j , j ∈ {1,2, ..., D} follows an independent identical distribution and is the Fourier transform of the kernel k (xy), i.e.
として定義される確率分布関数p(w)からサンプリングされる。
W=[w1,w2,...,wD]Tを定義する。近似カーネルの計算に必要なリソースは、線形変換Wxによって支配される。したがって、ガウスカーネル
It is sampled from the probability distribution function p (w) defined as.
Define W = [w 1 , w 2 , ..., w D ] T. The resources required to calculate the approximate kernel are dominated by the linear transformation Wx. Therefore, the Gauss kernel
は、線形変換行列 Is a linear transformation matrix
を使用して近似されるものとしてよく、Gは正規分布からサンプリングされる要素を有するd×d行列である。(D<dのときに、結果の最初のD次元が使用され、D>dのときに、複数の独立に生成されるランダム特徴が使用され、結果は連結される。)線形変換行列WRFFは、ガウスカーネルに対する不偏推定量KRFFをもたらす。 May be approximated using, where G is a d × d matrix with elements sampled from a normal distribution. (When D <d, the first D dimension of the result is used, and when D> d, multiple independently generated random features are used and the results are concatenated.) Linear transformation matrix W RFF Provides an unbiased estimator K RFF for the Gaussian kernel.
しかし残念なことに、カーネルを近似するRFFアプローチにはいくつかの不利点がある。たとえば、空間と時間における線形変換行列Gの計算コストはO(Dd)であり、これは高次元データの場合に高く付く。さらに、カーネルのこの近似における誤差は、不偏推定量KRFFの分散として表現されたときに、比較的大きなものとなる。 Unfortunately, however, the RFF approach that approximates the kernel has some disadvantages. For example, the computational cost of the linear transformation matrix G in space and time is O (Dd), which is high for high-dimensional data. Moreover, the error in this approximation of the kernel is relatively large when expressed as the variance of the unbiased estimator K RFF.
上で説明されているRFFアプローチとは対照的に、改善された技術は、構造化直交ランダム特徴(SORF)と呼ばれる新しいフレームワークに従ってガウスカーネルに対する不偏推定量を生成することを伴う。カーネルへの不偏推定量KSORFは、行列の対の集まりの積を使用して計算される線形変換行列WSORFを伴い、各対は直交行列と指定された確率分布に従う実数である要素を有するそれぞれの対角行列とを含む。典型的には、直交行列は、ウォルシュアダマール行列であり、指定された確率分布は、ラーデマッヘル分布であり、線形変換行列WSORFを形成するために掛け合わされる行列の対が少なくとも2つ、通常は3つある。 In contrast to the RFF approach described above, the improved technique involves generating an unbiased estimator for the Gaussian kernel according to a new framework called Structured Orthogonal Random Features (SORF). An unbiased estimator to the kernel K SORF involves a linear transformation matrix W SORF calculated using the product of a collection of pairs of matrices, each pair having an element that is an orthogonal matrix and a real number that follows the specified probability distribution. Includes each diagonal matrix. Typically, the orthogonal matrix is the Walsh-Hadamard matrix, the specified probability distribution is the Rademacher distribution, and there are at least two pairs of matrices that are multiplied to form the linear transformation matrix W SORF, usually There are three.
有利には、線形変換行列WSORFは、空間ではO(D)であり、時間ではOD(D log D)である計算コストを有する。さらに、KSORFの分散は、KRFFの分散よりも著しく小さい。 Advantageously, the linear transformation matrix W SORF has a computational cost of O (D) in space and OD (D log D) in time. Moreover, the variance of K SORF is significantly smaller than that of K RFF.
図1は、例示的な実装形態による大規模学習システムのブロック図である。システム100は、より正確であり、サポートベクターマシン(SVM)または他のタイプのカーネルベースの機械学習システムの計算効率の高い訓練およびテストを可能にする入力ベクトルの非線形マップを生成するために使用される。ベクトルは、クラス分類システム、クラスタ化システム、回帰システムなどの、様々な機械学習問題への入力として使用されることもあり得る、ガウスカーネルの近似である。たとえば、クラス分類システムはこれらの近似を使用し、線形クラス分類器を使用してデータ項目をクラスに分類することができる。図1におけるシステム100の描写は、サーバベースのクラス分類器システムとして説明されている。しかしながら、他の構成およびアプリケーションも使用され得る。たとえば、システム100は、クラスタ化システム、回帰システム、異常検出システムなどであってもよい。
FIG. 1 is a block diagram of a large-scale learning system according to an exemplary implementation.
大規模学習システム100は、多数の異なるデバイスの形態をとる1つまたは複数のコンピューティングデバイス、たとえば、標準的なサーバ、そのようなサーバのグループ、またはカーネルベースの機械学習サーバ120などのラックサーバシステムであってもよい。それに加えて、システム100は、パーソナルコンピュータ、たとえば、ラップトップコンピュータで実装されてもよい。カーネルベースの機械学習サーバ120は、図6に示されているようなコンピュータデバイス600または図7に示されているようなコンピュータデバイス700の一例であってよい。
The large-
カーネルベースの機械学習サーバ120は、ネットワークインターフェース122と、1つまたは複数の処理ユニット124と、メモリ126とを備える。ネットワークインターフェース122は、ネットワーク170から受信された電子的および/または光学的信号を、カーネルベースの機械学習サーバ120による使用のため電子的形態に変換するための、たとえば、イーサネット（登録商標）アダプタ、トークンリングアダプタ、および同様のものを備える。一組の処理ユニット124は、1つもしくは複数の処理チップおよび/またはアセンブリを備える。メモリ126は、1つまたは複数のROM、ディスクドライブ、ソリッドステートドライブ、および同様のものなどの、揮発性メモリ(たとえば、RAM)および不揮発性メモリの両方を含む。一組の処理ユニット124およびメモリ126は合わさって、本明細書で説明されているような様々な方法および機能を実行するように構成され配置される、制御回路を形成する。 The kernel-based machine learning server 120 includes a network interface 122, one or more processing units 124, and memory 126. Network interface 122, for example, an Ethernet® adapter, for converting electronic and / or optical signals received from network 170 into electronic form for use by a kernel-based machine learning server 120. Includes a token ring adapter, and similar. A set of processing units 124 comprises one or more processing chips and / or assemblies. Memory 126 includes both volatile and non-volatile memory, such as one or more ROMs, disk drives, solid state drives, and the like. The set of processing units 124 and memory 126 together form a control circuit that is configured and arranged to perform various methods and functions as described herein.
いくつかの実施形態において、カーネルベースの機械学習サーバ120のコンポーネントのうちの1つまたは複数は、メモリ126に記憶されている命令を処理するように構成されたプロセッサ(たとえば、処理ユニット124)であり得るか、または含むことができる。図1に示されているような命令の例は、直交行列マネージャ130、対角行列マネージャ140、および機械学習マネージャ150を含む。さらに、図1に例示されているように、メモリ126は、そのようなデータを使用するそれぞれのマネージャに関して説明されている、様々なデータを記憶するように構成される。
In some embodiments, one or more of the components of the kernel-based machine learning server 120 is a processor configured to process instructions stored in memory 126 (eg, processing unit 124). It is possible or can include. Examples of instructions as shown in FIG. 1 include an
カーネルベースの機械学習サーバ120は、データ項目から抽出された特徴ベクトルを使用し、たとえば、ガウスカーネルを介して、特徴の近似を生成するランダム化特徴マップを生成し得る。特徴ベクトルは、dの次元を有する浮動小数点数の配列、または言い換えると、d個の位置を有する配列と考えられ得る。データ項目は、たとえばファイルまたは検索項目のデータベースであってよい。たとえば、データ項目は、文書、画像、音声ファイル、動画ファイル、などの任意の種類のファイルであってよく、特徴ベクトルは、ファイルから抽出され得る。データ項目は、データベースレコードであってもよく、特徴は、データベース内の項目に関係するデータから抽出され得る。 The kernel-based machine learning server 120 may use feature vectors extracted from data items to generate randomized feature maps that generate feature approximations, for example via a Gauss kernel. The feature vector can be thought of as an array of floating point numbers with dimensions d, or in other words, an array with d positions. The data item may be, for example, a database of files or search items. For example, the data item can be any kind of file, such as a document, image, audio file, video file, etc., and the feature vector can be extracted from the file. The data item may be a database record and features can be extracted from the data related to the item in the database.
直交行列マネージャ130は、直交行列データ132を生成するように構成される。直交行列データ132は、直交基底を形成する行を有する1つまたは複数の行列を定義する数を含む。直交行列マネージャ132によって生成される直交行列のサイズは、次元dに基づく。たとえば、いくつかの実装形態において、直交行列マネージャ130は、ウォルシュアダマール行列を生成するように構成される。そのような行列は、以下の規則、
The
に従って生成され、
ただし、
Generated according to
However,
は、クロネッカー積を表す。したがって、ウォルシュアダマール行列は、2の冪乗の次元を有する正方行列である。受け取ったことに応答して、直交行列マネージャ130は、次いで、dより大きい2の最小の冪乗である次元を有するウォルシュアダマール行列を生成する。
Represents the Kronecker product. Therefore, the Walshuadamard matrix is a square matrix with dimensions of powers of two. In response to the receipt, the
対角行列マネージャ140は、対角行列データ144を生成するように構成される。対角行列データ144は、非対角要素としてゼロを有する行列を定義する数を含む。対角要素の値は、指定された確率分布関数142を介して定義される。対角行列の次元は、直交行列データ132の直交行列の次元と同じである。いくつかの実装形態において、対角要素の値は-1または1のいずれかであり、確率分布142はラーデマッヘル分布(すなわち、コイン投げ分布)である。
The
カーネルベースの機械学習サーバ120は、直交行列データ132および対角行列データ144から線形変換行列データ146を形成するように構成される。このようにして、対角行列データ132がN個の対角行列D1,D,...,DNを定義する数を含むときに、線形変換行列データ146を定義する線形変換行列WSORFは The kernel-based machine learning server 120 is configured to form linear transformation matrix data 146 from orthogonal matrix data 132 and diagonal matrix data 144. In this way, when the diagonal matrix data 132 contains the numbers that define N diagonal matrices D 1 , D , ..., D N, the linear transformation matrix W SORF that defines the linear transformation matrix data 146. Is
に等しく、
nはdより大きい2の最小の冪乗の指数であり、σはガウスカーネルの幅である。いくつかの実装形態において、Nは少なくとも2であり、典型的な実装形態では、Nは3に等しい。
Equal to
n is the smallest exponent of 2 greater than d, and σ is the width of the Gaussian kernel. In some implementations, N is at least 2, and in typical implementations, N is equal to 3.
システム100は、機械学習マネージャ150を使用して、ベクトルデータ152に対して画像検索、音声認識などを実行し得る。システム100は、ベクトルデータ152からベクトルを抽出するために従来の方法を使用するか、または抽出された特徴ベクトルデータ154に対して提供されてよい。いくつかの例として、抽出された特徴ベクトル154は、データ項目または音声波形における画像ファイルからのピクセルであってもよい」。
The
いくつかの実装形態において、メモリ126は、ランダムアクセスメモリ、ディスクドライブメモリ、フラッシュメモリ、および/または同様のものなどの任意のタイプのメモリであってよい。いくつかの実装形態において、メモリ126は、カーネルベースの機械学習サーバ120のコンポーネントに関連付けられている複数のメモリコンポーネント(たとえば、複数のRAMコンポーネントまたはディスクドライブメモリ)として実装され得る。いくつかの実装形態において、メモリ126は、データベースメモリであってよい。いくつかの実装形態において、メモリ126は、非ローカルメモリであるか、含むことができる。たとえば、メモリ126は、複数のデバイス(図示せず)によって共有されるメモリであり得るか、または含むことができる。いくつかの実装形態において、メモリ126は、ネットワーク内のサーバデバイス(図示せず)に関連付けられ、カーネルベースの機械学習サーバ120のコンポーネントにサービスを提供するように構成され得る。 In some implementations, the memory 126 may be any type of memory, such as random access memory, disk drive memory, flash memory, and / or the like. In some implementations, the memory 126 may be implemented as multiple memory components (eg, multiple RAM components or disk drive memory) associated with the components of the kernel-based machine learning server 120. In some implementations, the memory 126 may be a database memory. In some implementations, memory 126 can be or include non-local memory. For example, memory 126 can be or can include memory shared by multiple devices (not shown). In some implementations, the memory 126 may be associated with a server device (not shown) in the network and configured to serve the components of the kernel-based machine learning server 120.
圧縮コンピュータ120のコンポーネント(たとえば、モジュール、処理ユニット124)は、1つまたは複数のタイプのハードウェア、ソフトウェア、ファームウェア、オペレーティングシステム、ランタイムライブラリ、および/または同様のものを含むことができる1つまたは複数のプラットホーム(たとえば、1つまたは複数の類似のもしくは異なるプラットホーム)に基づき動作するように構成され得る。いくつかの実装形態において、カーネルベースの機械学習サーバ120のコンポーネントは、デバイスのクラスタ(たとえば、サーバファーム)内で動作するように構成され得る。そのような実装形態において、カーネルベースの機械学習サーバ120のコンポーネントの機能および処理は、デバイスのクラスタの複数のデバイスに分散させることができる。 One or more components of the compression computer 120 (eg, modules, processing units 124) can include one or more types of hardware, software, firmware, operating systems, runtime libraries, and / or the like. It may be configured to operate on multiple platforms (eg, one or more similar or different platforms). In some implementations, the components of the kernel-based machine learning server 120 may be configured to operate within a cluster of devices (eg, a server farm). In such an implementation, the functionality and processing of the components of the kernel-based machine learning server 120 can be distributed across multiple devices in a cluster of devices.
カーネルベースの機械学習サーバ120のコンポーネントは、属性を処理するように構成されている任意のタイプのハードウェアおよび/またはソフトウェアであり得るか、または含むことができる。いくつかの実装形態において、図1のカーネルベースの機械学習サーバ120のコンポーネント内に示されているコンポーネントの1つまたは複数の部分は、ハードウェアベースのモジュール(たとえば、デジタルシグナルプロセッサ(DSP)、フィールドプログラマブルゲートアレイ(FPGA)、メモリ)、ファームウェアモジュール、および/またはソフトウェアベースのモジュール(たとえば、コンピュータコードのモジュール、コンピュータで実行できるコンピュータ可読命令のセット)であり得るか、または含むことができる。たとえば、いくつかの実装形態において、カーネルベースの機械学習サーバ120のコンポーネントの1つまたは複数の部分は、少なくとも1つのプロセッサ(図示せず)により実行されるように構成されているソフトウェアモジュールであり得るか、または含むことができる。いくつかの実装形態において、コンポーネントの機能は、図1に示されているものと異なるモジュールおよび/または異なるコンポーネントに含まれ得る。 The components of the kernel-based machine learning server 120 can be or include any type of hardware and / or software that is configured to handle attributes. In some implementations, one or more of the components shown within the components of the kernel-based machine learning server 120 in Figure 1 is a hardware-based module (eg, a digital signal processor (DSP)). It can or can be a field programmable gate array (FPGA), memory), a firmware module, and / or a software-based module (eg, a module of computer code, a set of computer-readable instructions that can be executed by a computer). For example, in some implementations, one or more parts of the kernel-based machine learning server 120 components are software modules that are configured to run on at least one processor (not shown). Can be obtained or included. In some implementations, component functionality may be contained in different modules and / or different components than those shown in Figure 1.
図示されていないけれども、いくつかの実装形態において、カーネルベースの機械学習サーバ120(またはその一部)のコンポーネントは、たとえば、データセンター(たとえば、クラウドコンピューティング環境)、コンピュータシステム、1つまたは複数のサーバ/ホストデバイス、および/または同様のものの中で動作するように構成され得る。いくつかの実装形態において、カーネルベースの機械学習サーバ120(またはその一部)のコンポーネントは、ネットワーク内で動作するように構成され得る。したがって、カーネルベースの機械学習サーバ120(またはその一部)のコンポーネントは、1つまたは複数のデバイスおよび/または1つまたは複数のサーバデバイスを備えることができる様々なタイプのネットワーク環境内で機能するように構成され得る。たとえば、ネットワークは、ローカルエリアネットワーク(LAN)、ワイドエリアネットワーク(WAN)、および/または同様のものであり得るか、または含むことができる。ネットワークは、ワイヤレスネットワークおよび/またはたとえば、ゲートウェイデバイス、ブリッジ、スイッチ、および/または同様のものを使用して実装されるワイヤレスネットワークであり得るか、または含むことができる。ネットワークは、インターネットプロトコル(IP)および/または専用プロトコルなどの様々なプロトコルに基づく1つまたは複数のセグメントを含むことができ、および/または部分を有することができる。ネットワークは、インターネットの少なくとも一部を含むことができる。 Although not shown, in some implementations, the components of the kernel-based machine learning server 120 (or part thereof) are, for example, a data center (eg, a cloud computing environment), a computer system, one or more. Can be configured to work within a server / host device, and / or something similar. In some implementations, the components of the kernel-based machine learning server 120 (or part thereof) may be configured to operate within the network. Therefore, the components of the kernel-based machine learning server 120 (or part thereof) work within various types of network environments that can include one or more devices and / or one or more server devices. Can be configured as For example, the network can be or include a local area network (LAN), a wide area network (WAN), and / or the like. The network can be or can be a wireless network and / or, for example, a wireless network implemented using gateway devices, bridges, switches, and / or the like. The network can include one or more segments based on various protocols such as Internet Protocol (IP) and / or dedicated protocols, and / or can have parts. The network can include at least a portion of the Internet.
いくつかの実施形態において、圧縮コンピュータ120のコンポーネントのうちの1つまたは複数は、メモリに記憶されている命令を処理するように構成されたプロセッサであり得るか、または含むことができる。たとえば、直交行列マネージャ130(および/またはその一部)、対角行列マネージャ140(および/またはその一部)、ならびに機械学習マネージャ150(および/またはその一部)は、1つまたは複数の機能を実装するプロセスに関係する命令を実行するように構成されているプロセッサとメモリとの組合せであり得る。 In some embodiments, one or more of the components of the compression computer 120 may be or may be a processor configured to process instructions stored in memory. For example, the Orthogonal Matrix Manager 130 (and / or part of it), the Diagonal Matrix Manager 140 (and / or part of it), and the Machine Learning Manager 150 (and / or part of it) have one or more functions. It can be a combination of processor and memory that is configured to execute instructions related to the process that implements.
図2は、カーネルを使用して図1に示されているデータにクラス分類演算を実行するカーネルベースの機械学習システムに対する入力を生成する例示的な方法200を示すフローチャートである。方法200は、カーネルベースの機械学習サーバ120のメモリ126に常駐し、一組の処理ユニット124によって実行される、図1に関連して説明されているソフトウェア構成要素によって実行され得る。
FIG. 2 is a flow chart illustrating an
202において、カーネルベースの機械学習サーバ120は、ガウスカーネルを使用してクラス分類されるべきベクトルデータの次元dを受け取る。 At 202, the kernel-based machine learning server 120 receives dimension d of vector data to be categorized using the Gauss kernel.
204において、カーネルベースの機械学習サーバ120は、dより大きい2の最小の冪乗である、すなわち In 204, the kernel-based machine learning server 120 is the smallest power of 2, greater than d, ie.
である、次元2nを有するウォルシュアダマール行列Hnを生成する。
Generates a Walshe-Hadamard matrix H n with
206において、カーネルベースの機械学習サーバ120は、対角行列D1,D2,...,DNの集まりを生成し、各対角行列は-1または1のいずれかに等しい対角要素を有し、ラーデマッヘル分布に従ってランダムにサンプリングされる。 At 206, the kernel-based machine learning server 120 produces a collection of diagonal matrices D 1 , D 2 , ..., D N , where each diagonal matrix is a diagonal element equal to either -1 or 1. And are randomly sampled according to the Rademacher distribution.
208において、カーネルベースの機械学習サーバ120は、行列積HnD1HnD2...HnDNを形成する。 At 208, the kernel-based machine learning server 120 forms a matrix product H n D 1 H n D 2 ... H n D N.
210において、カーネルベースの機械学習サーバ120は、行列積に量 At 210, the kernel-based machine learning server 120 is a quantity in a matrix product.
を掛けて線形変換行列WSORFを形成する。 Multiply to form the linear transformation matrix W SORF.
図3Aは、ガウスカーネルおよび他の近似の正確な計算に関して、平均平方誤差(MSE)に関する上で説明されている構造化直交ランダム特徴(SORF)近似の結果を示している。結果内にプロットされている他の近似は以下を含む。 Figure 3A shows the results of the Structured Orthogonal Random Features (SORF) approximation described above for Mean Squared Error (MSE) with respect to the accurate calculation of the Gaussian kernel and other approximations. Other approximations plotted in the results include:
上で説明されている乱択化フーリエ特徴(RFF)近似: Randomized Fourier Feature (RFF) Approximation as described above:
Gは、ガウスカーネルのフーリエ変換である正規分布に従ってサンプリングされた要素を有する行列である。 G is a matrix with elements sampled according to a normal distribution, which is the Fourier transform of the Gauss kernel.
直交ランダム特徴(ORF)近似。これは Orthogonal random feature (ORF) approximation. is this
の形式の近似であり、
Qはグラム・シュミットの正規直交化法によってランダムサンプリングされたガウス行列Gから導出された直交行列であり、Sは、カイ分布に従ってランダムに生成される要素を有する対角行列である。
Is an approximation of the form of
Q is an orthogonal matrix derived from the Gaussian matrix G randomly sampled by the Gram-Schmidt normal orthogonalization method, and S is a diagonal matrix with elements randomly generated according to the chi distribution.
準モンテカルロ(QMC)近似。これはサンプルが決定論的低変位ランクシーケンス(deterministic, low-displacement rank sequence)に従って選択されることを除きORFに類似している。 Quasi-Monte Carlo (QMC) approximation. This is similar to the ORF, except that the samples are selected according to a deterministic, low-displacement rank sequence.
ファストフードは、 Fast food
の形式のカーネルへの構造化行列近似であり、
Hはアダマール行列であり、Gは正規分布に従って分布する値を有するランダムガウス行列であり、Πは置換行列であり、Bはランダム2値行列であり、Sは線形変換行列の行が規定されている挙動に従うノルムを有することを確実にするスケーリング行列である。
Is a structured matrix approximation to the kernel in the form of
H is the Hadamard matrix, G is the random Gaussian matrix with values distributed according to the normal distribution, Π is the permutation matrix, B is the random binary matrix, and S is the row of the linear transformation matrix. It is a scaling matrix that ensures that it has a norm that follows the behavior.
循環は、コンパクト非線形マップに基づくカーネルへの近似である。 Circulation is an approximation to the kernel based on a compact nonlinear map.
プロット302、304、306、308、310、および312において、次元dはプロットとプロットとの間で2倍に増える。各プロットは、D>dについてMSE対特徴ベクトル長さDの挙動を示している。SORF近似は、上で説明されている近似のうちの他のものよりも著しく小さいMSEを有し、他の近似に関するSORFに対するMSEにおける減少は増大するdとともにより明らかになることに留意されたい。
In
図3Bは、様々な近似に対するMSEの追加のプロットを示している。プロット314および316において、SORF、ORF、およびRFFに対するMSEは、d=1024において2つの異なるガウス幅について比較される。これらの場合において、SORFおよびORFは、RFFに関する幅のより大きい値に対してより良好なMSE値を有する。プロット318では、ウォルシュアダマール行列と対角行列との積の少なくとも2つの対が望ましいMSE値に必要であることは明らかである。
Figure 3B shows additional plots of MSE for various approximations. In
図4は、変化するdおよびDに対するSORF、ORF、およびRFFカーネル近似に基づく線形SVMクラス分類器によるクラス分類精度の表を示している。この表から、SORF近似が競合的であるか、またはRFFよりよいが、時間および空間/メモリのリソース要求条件は大幅に下がる。ここでもまた、線形変換行列WSORFは、空間ではO(D)であり、時間ではO(D log D)である計算コストを有することが推論され得る。 Figure 4 shows a table of classification accuracy with a linear SVM classifier based on SORF, ORF, and RFF kernel approximations for varying d and D. From this table, the SORF approximation is competitive or better than RFF, but the time and space / memory resource requirements are significantly reduced. Again, it can be inferred that the linear transformation matrix W SORF has a computational cost of O (D) in space and O (D log D) in time.
図5は、開示されている主題による、クラス分類エンジンにより球面ランダム特徴を使用するための例示的なプロセス500のフローチャートである。プロセス500は、図1のシステム100などの、大規模学習システムによって実行され得る。プロセス500は、図1のカーネルベースの機械学習サーバ120を使用して、入力ベクトルを、入力ベクトルの非線形近似である、第2のベクトルに変換する一例である。プロセス400は、大規模学習システムが入力データ項目を受け取ること(502)から始まるものとしてよい。入力データ項目は、特徴ベクトルの抽出元となる任意の項目であってよい。例は、画像、文書、動画ファイル、音声ファイル、メタデータを含む実体、ユーザプロファイルなどを含む。システムは、従来の技術を使用して入力データ項目から特徴を抽出し得る(504)。システムは、特徴ベクトルを単位l2ノルムに正規化する(506)ものとしてよい。システムは、ガウスランダム化特徴マップ(たとえば、図2のプロセス200の一部として生成される)を使用して、入力データ項目に対する近似された特徴ベクトルを生成する(508)ものとしてよい。近似された特徴ベクトルは、入力特徴ベクトルと異なる次元を有する非線形近似であってもよい。
FIG. 5 is a flow chart of an
システムは、クラス分類器への入力として近似された特徴ベクトルを提供し得る(510)。クラス分類器は、データ項目の大きなストアへのアクセス権を有し得る。データ項目は、対応する近似された特徴ベクトルをすでに有し得るか、またはデータ項目に対する近似された特徴ベクトルの生成を開始し得る。いくつかの実装形態において、クラス分類器は、入力データ項目に対する近似された特徴ベクトルとデータ項目のストアとの間のドット積を計算するものとしてよい。いくつかの実装形態において、クラス分類器は、このドット積を使用して入力データ項目に対してラベル、クラス分類などを決定し得る。たとえば、クラス分類器は、画像を動物、人、建物などのクラスに分類するものとしてよい。いくつかの実装形態において、クラス分類器は、その入力データ項目に最も類似するデータストア内の項目を決定し得る。したがって、システムは、クラス分類器から入力データ項目に対するクラス分類を取得し得る(512)。次いで、プロセス500は終了する。
The system may provide an approximated feature vector as an input to the classifier (510). A classifier may have access to a large store of data items. The data item may already have a corresponding approximated feature vector or may start generating an approximated feature vector for the data item. In some implementations, the classifier may compute the dot product between the approximate feature vector for the input data item and the store of the data item. In some implementations, the classifier can use this dot product to determine labels, classifications, etc. for input data items. For example, a classifier may classify images into classes such as animals, people, and buildings. In some implementations, the classifier may determine the items in the data store that most closely resemble its input data items. Therefore, the system can obtain the classification for the input data item from the classifier (512).
図5のプロセスは、構造化直交ランダム特徴を使用する一例である。特徴マップ(たとえば、カーネルベースの機械学習サーバ120からの出力)は、限定はしないが、クラスタ化、回帰、異常分析などを含む、任意の機械学習アプリケーションにおいて使用され得る。したがって、たとえば、交互ステップ512は、入力データ項目に対するクラスタ割り当てを取得すること、入力データ項目に対する回帰分析を取得することなどを含み得る。さらに、球面ランダム特徴は、機械学習クラス分類器の訓練例として、たとえば、プロセス500が実行される前に行われる訓練モードで使用され得る。
The process in Figure 5 is an example of using structured orthogonal random features. Feature maps (eg, output from kernel-based machine learning server 120) can be used in any machine learning application, including, but not limited to, clustering, regression, anomaly analysis, and so on. Thus, for example, alternating
図6は、一般的なコンピュータデバイス600の例を示しており、これは本明細書において説明されている技術とともに使用され得る、カーネルベースの機械学習サーバ120、および/または図1のクライアント170であってよい。コンピューティングデバイス600は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、携帯電話、スマートフォン、タブレット、サーバ、およびウェアラブルデバイスを含む、他のコンピューティングデバイスなどの様々な例示的な形態のコンピューティングデバイスを表すことが意図されている。ここに示されているコンポーネント、それらの接続および関係、ならびにそれらの機能は、例示であることのみを意図されており、本明細書において説明され、および/または請求されている発明の実装を制限することを意図されていない。
FIG. 6 shows an example of a
コンピューティングデバイス600は、プロセッサ602と、メモリ604と、記憶装置デバイス606と、インターフェース608を介して接続されている拡張ポート610とを備える。いくつかの実装形態において、コンピューティングデバイス600は、インターフェース608を介して接続される、コンポーネントは他にもあるがとりわけ、トランシーバー646、通信インターフェース644、およびGPS(全地球測位システム)受信機モジュール648を備え得る。デバイス600は、必要ならば、デジタル信号処理回路を備えることができる、通信インターフェース644を通じてワイヤレス方式で通信し得る。コンポーネント602、604、606、608、610、640、644、646、および648の各々は、共通マザーボード上に取り付けられるか、または適宜他の仕方で取り付けられ得る。
The
プロセッサ602は、ディスプレイ616などの、外部入力/出力デバイス上にGUIに対するグラフィック情報を表示するためメモリ604内に、または記憶装置デバイス606上に、記憶されている命令を含む、コンピューティングデバイス600内で実行する命令を処理することができる。ディスプレイ616は、モニターまたはフラットタッチスクリーンディスプレイであってよい。いくつかの実装形態において、複数のプロセッサおよび/または複数のバスが、適宜、複数のメモリおよび複数の種類のメモリとともに使用され得る。また、複数のコンピューティングデバイス600が、必要な操作の一部を提供する各のデバイスと接続され得る(たとえば、サーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして)。
メモリ604は、コンピューティングデバイス600内の情報を記憶する。一実装形態において、メモリ604は、1つまたは複数の揮発性メモリユニットである。別の実装形態では、メモリ604は、1つまたは複数の不揮発性メモリユニットである。メモリ604は、磁気ディスクまたは光ディスクなどのコンピュータ可読媒体の他の形態のものであってもよい。いくつかの実装形態において、メモリ604は、拡張インターフェースを通じて用意される拡張メモリを備え得る。
The
記憶装置デバイス606は、コンピューティングデバイス600用の大容量記憶装置を構成することができる。一実装形態において、記憶装置デバイス606は、ストレージエリアネットワークまたは他の構成のデバイスを含む、フロッピィディスクデバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリもしくは他の類似のソリッドステートメモリデバイス、またはデバイスアレイなどのコンピュータ可読媒体であり得るか、または含むことができる。コンピュータプログラム製品は、そのようなコンピュータ可読媒体で有形に具現化され得る。コンピュータプログラム製品は、実行されたときに、上で説明されているような1つまたは複数の方法を実行する命令も含み得る。コンピュータまたは機械可読媒体は、メモリ604、記憶装置デバイス606、またはプロセッサ602上のメモリなどの記憶装置デバイスである。
The storage device 606 can be configured as a mass storage device for the
インターフェース608は、コンピューティングデバイス600に対して帯域幅集約的オペレーションを管理する高速コントローラ、または低帯域幅集約オペレーションを管理する低速コントローラ、またはそのようなコントローラの組合せであってよい。デバイス600と他のデバイスとの近距離通信を行うことを可能にするために外部インターフェース640が用意されてもよい。いくつかの実装形態において、コントローラ608は、記憶装置デバイス606および拡張ポート614に結合され得る。様々な通信ポート(たとえば、USB、Bluetooth（登録商標）、イーサネット（登録商標）、ワイヤレスイーサネット（登録商標）)を含み得る、拡張ポートは、キーボード、ポインティングデバイス、スキャナ、またはたとえば、ネットワークアダプタを通じて、スイッチまたはルータなどネットワーキングデバイスなどの1つまたは複数の入力/出力デバイスに結合され得る。
コンピューティングデバイス600は、図に示されているように、数多くの異なる形態で実装され得る。たとえば、標準的なサーバ630として、またはそのようなサーバのグループにおいて何倍もの数で実装され得る。これは、また、ラックサーバシステムの一部としても実装され得る。それに加えて、ラップトップコンピュータ622などのパーソナルコンピュータ、またはスマートフォン636で実装されてもよい。システム全体は、互いに通信する複数のコンピューティングデバイス600から構成されてもよい。他の構成も可能である。
The
図7は、一般的なコンピュータデバイス700の例を示しており、これは本明細書において説明されている技術とともに使用され得る、図1のサーバ110であってよい。コンピューティングデバイス700は、サーバ、ブレードサーバ、データセンター、メインフレーム、および他の大規模コンピューティングデバイスなどの様々な例示的な形態の大規模データ処理デバイスを表すことが意図されている。コンピューティングデバイス700は、1つまたは複数の通信ネットワークによって相互接続される、ネットワーク接続ストレージノードを場合によっては含む、複数のプロセッサを有する分散システムであってもよい。ここに示されているコンポーネント、それらの接続および関係、ならびにそれらの機能は、例示であることのみを意図されており、本明細書において説明され、および/または請求されている発明の実装を制限することを意図されていない。
FIG. 7 shows an example of a
分散コンピューティングシステム700は、任意の数のコンピューティングデバイス780を備え得る。コンピューティングデバイス780は、ローカルもしくはワイドエリアネットワーク、専用光リンク、モデム、ブリッジ、ルータ、有線もしくはワイヤレスネットワークなどの上で通信するサーバまたはラックサーバ、メインフレームなどを含み得る。
The distributed
いくつかの実装形態において、各コンピューティングデバイスは複数のラックを備え得る。たとえば、コンピューティングデバイス780aは、複数のラック758a〜758nを備える。各ラックは、プロセッサ752a〜752nおよび762a〜762nなどの、1つまたは複数のプロセッサを備え得る。プロセッサは、データプロセッサ、ネットワーク接続ストレージデバイス、および他のコンピュータ制御デバイスを含み得る。いくつかの実装形態において、1つのプロセッサは、マスタプロセッサとして動作し、スケジューリングおよびデータ分配タスクを制御し得る。プロセッサは1つまたは複数のラックスイッチ758を通じて相互接続されるものとしてよく、1つまたは複数のラックはスイッチ778を通じて接続されるものとしてよい。スイッチ778は、複数の接続されているコンピューティングデバイス700の間の通信を処理し得る。
In some implementations, each computing device may have multiple racks. For example, the
各ラックは、メモリ754およびメモリ764などのメモリ、ならびに756および766などの記憶装置を備え得る。記憶装置756および766は、大容量記憶装置を実現するものとしてよく、ストレージエリアネットワークまたは他の構成のデバイスを含む、ネットワーク接続ディスク、フロッピィディスク、ハードディスク、光ディスク、テープ、フラッシュメモリもしくは他の類似のソリッドステートメモリデバイス、またはデバイスアレイなどの揮発性または不揮発性記憶装置を含み得る。記憶装置756または766は、複数のプロセッサ、複数のラック、または複数のコンピューティングデバイスの間で共有されてよく、プロセッサのうちの1つまたは複数によって実行可能な命令を記憶するコンピュータ可読媒体を含むものとしてよい。メモリ754および764は、たとえば、1つもしくは複数の揮発性メモリユニット、1つもしくは複数の不揮発性メモリユニット、および/または磁気もしくは光ディスク、フラッシュメモリ、キャッシュ、ランダムアクセスメモリ(RAM)、リードオンリーメモリ(ROM)、およびこれらの組合せなどの他の形態のコンピュータ可読媒体を含み得る。メモリ754などのメモリは、プロセッサ752a〜752nの間で共有されてもよい。インデックスなどのデータ構造体は、たとえば、記憶装置756とメモリ754とにまたがって記憶され得る。コンピューティングデバイス700は、コントローラ、バス、入出力デバイス、通信モジュールなどの、図示されていない他のコンポーネントを含み得る。
Each rack may include memory such as
システム100などの、システム全体は、互いに通信する複数のコンピューティングデバイス700から構成されてもよい。たとえば、デバイス780aは、デバイス780b、780c、および780dと通信するものとしてよく、これらはシステム100と総称されてよい。別の例として、図1のシステム100は、1つまたは複数のコンピューティングデバイス700を備え得る。コンピューティングデバイスのうちのいくつかは、互いに地理的に近い場所に配置されてよく、他のデバイスは地理的に離れた場所に配置されてよい。システム700のレイアウトは、一例にすぎず、他のレイアウトまたは構成を取ってもよい。
The entire system, such as the
様々な実装は、記憶装置システム、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスからデータおよび命令を受信し、記憶装置システム、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスにデータおよび命令を送信するように結合された、専用または汎用のものとしてよい、少なくとも1つのプログラム可能なプロセッサを備えるプログラム可能なシステム上で実行可能であり、および/または解釈可能である1つまたは複数のコンピュータプログラムによる実装を含むことができる。 Various implementations receive data and instructions from the storage system, at least one input device, and at least one output device, and send data and instructions to the storage system, at least one input device, and at least one output device. One or more computer programs that are executable and / or interpretable on a programmable system with at least one programmable processor, which may be combined to transmit, dedicated or general purpose. Can include implementation by.
これらのコンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとも呼ばれる)は、プログラム可能なプロセッサ用の機械語命令を含み、高水準手続き型および/またはオブジェクト指向プログラミング言語で、および/またはアセンブリ/機械語で実装され得る。本明細書で使用されているように、「機械可読媒体」、「コンピュータ可読媒体」という用語は、機械語命令および/またはデータをプログラム可能なプロセッサに供給するために使用される任意の非一時的コンピュータプログラム製品、装置、および/またはデバイス(たとえば、磁気ディスク、光ディスク、メモリ(リードアクセスメモリを含む)、プログラム可能論理デバイス(PLD))を指す。 These computer programs (also called programs, software, software applications, or code) include machine language instructions for programmable processors, in high-level procedural and / or object-oriented programming languages, and / or assembly / machines. Can be implemented in words. As used herein, the terms "machine readable medium", "computer readable medium" are any non-temporary used to supply machine language instructions and / or data to a programmable processor. Computer program Refers to products, devices, and / or devices (eg, magnetic disks, optical disks, memory (including read access memory), programmable logical devices (PLD)).
本明細書で説明されているシステムおよび技術は、バックエンドコンポーネントを含む(たとえば、データサーバとして)、またはミドルウェアコンポーネントを含む(たとえば、アプリケーションサーバとして)、またはフロントエンドコンポーネントを含む(たとえば、ユーザと本明細書で説明されているシステムおよび技術の実装とのインタラクティブな操作に使用されるグラフィカルユーザインターフェースまたはウェブブラウザを有するクライアントコンピュータ)コンピューティングシステム、またはそのようなバックエンド、ミドルウェア、またはフロントエンドコンポーネントの任意の組合せで実装され得る。システムのコンポーネントは、デジタルデータ通信の任意の形態または媒体(たとえば、通信ネットワーク)によって相互接続され得る。通信ネットワークの例は、ローカルエリアネットワーク(「LAN」)、ワイドエリアネットワーク(「WAN」)、およびインターネットを含む。 The systems and technologies described herein include back-end components (eg, as a data server), middleware components (eg, as an application server), or front-end components (eg, with a user). Client computers with graphical user interfaces or web browsers used for interactive operation with the implementations of the systems and technologies described herein) Computing systems, or such back-end, middleware, or front-end components. Can be implemented in any combination of. The components of the system can be interconnected by any form or medium of digital data communication (eg, a communication network). Examples of communication networks include local area networks (“LAN”), wide area networks (“WAN”), and the Internet.
コンピューティングシステムは、クライアントおよびサーバを含むことができる。クライアントおよびサーバは、一般に、互いに隔てられており、典型的には、通信ネットワークを通じてインタラクティブな操作を行う。クライアントとサーバとの関係は、コンピュータプログラムが各コンピュータ上で実行され、互いとの間にクライアント-サーバ関係を有することによって発生する。 The computing system can include clients and servers. Clients and servers are generally isolated from each other and typically operate interactively through a communication network. A client-server relationship arises when a computer program runs on each computer and has a client-server relationship with each other.
これで多数の実装形態が説明された。それにもかかわらず、本発明の趣旨と範囲とから逸脱することなく、様々な修正を加えられ得る。それに加えて、図に示されている論理の流れは、望ましい結果を達成するために、図示されている特定の順序、または順番を必要としない。それに加えて、他のステップが提示されるか、または説明されているフローから、ステップが取り除かれてもよく、他のコンポーネントが説明されているシステムに追加されるか、または取り除かれてもよい。したがって、他の実装形態は、以下の請求項の範囲内に収まる。 This describes a number of implementations. Nevertheless, various modifications can be made without departing from the gist and scope of the present invention. In addition, the logical flow shown in the figure does not require the specific order, or order shown, to achieve the desired result. In addition, steps may be removed from the flow in which other steps are presented or described, and other components may be added to or removed from the system in which they are described. .. Therefore, other implementations fall within the scope of the following claims.
100 システム
110 サーバ
120 カーネルベースの機械学習サーバ、圧縮コンピュータ
122 ネットワークインターフェース
124 処理ユニット
126 メモリ
130 直交行列マネージャ
132 直交行列データ
140 対角行列マネージャ
142 確率分布関数
144 対角行列データ
146 線形変換行列データ
150 機械学習マネージャ
152 ベクトルデータ
154 特徴ベクトルデータ
170 ネットワーク、クライアント
200 方法、プロセス
302、304、306、308、310、312、314、316、318 プロット
400 プロセス
500 プロセス
512 交互ステップ
600 コンピュータデバイス、一般的なコンピュータデバイス、コンピューティングデバイス、デバイス
602 プロセッサ、コンポーネント
604 メモリ、コンポーネント
606 記憶装置デバイス、コンポーネント
608 インターフェース、コントローラ、コンポーネント
610 拡張ポート、コンポーネント
614 拡張ポート
616 ディスプレイ
622 ラップトップコンピュータ
630 標準的なサーバ
636 スマートフォン
640 外部インターフェース、コンポーネント
644 通信インターフェース、コンポーネント
646 トランシーバー、コンポーネント
648 GPS(全地球測位システム)受信機モジュール、コンポーネント
700 コンピュータデバイス、一般的なコンピュータデバイス、コンピューティングデバイス、分散コンピューティングシステム、システム
752a〜752n、762a〜762n プロセッサ
754 メモリ
756、766 記憶装置
758 ラックスイッチ
758a〜758n ラック
764 メモリ
778 スイッチ
780 コンピューティングデバイス
780a コンピューティングデバイス、デバイス
780b、780c、780d デバイス
100 systems
110 server
120 Kernel-based machine learning server, compression computer
122 Network interface
124 Processing unit
126 memory
130 Orthogonal Matrix Manager
132 Orthogonal matrix data
140 Diagonal matrix manager
142 Probability distribution function
144 Diagonal matrix data
146 Linear transformation matrix data
150 Machine Learning Manager
152 vector data
154 Feature vector data
170 network, client
200 methods, processes
302, 304, 306, 308, 310, 312, 314, 316, 318 plots
400 processes
500 processes
512 alternating steps
600 computer devices, general computer devices, computing devices, devices
602 processor, component
604 Memory, components
606 Storage devices, components
608 Interfaces, controllers, components
610 expansion port, component
614 expansion port
616 display
622 laptop computer
630 standard server
636 smartphone
640 External interface, components
644 Communication interface, components
646 transceiver, component
648 GPS (Global Positioning System) receiver module, component
700 computer devices, general computer devices, computing devices, distributed computing systems, systems
752a to 752n, 762a to 762n processors
754 memory
756, 766 Storage device
758 rack switch
758a ~ 758n rack
764 memory
778 switch
780 computing device
780a computing device, device
780b, 780c, 780d devices
Claims (17)
前記カーネルベースの機械学習システムの処理回路によって、対角行列の集まりを生成するステップであって、対角行列の前記集まりの各々は、ゼロである非対角要素と、指定された確率分布関数に従って分布する値を有する対角要素とを有する、ステップと、
前記処理回路によって、直交行列の集まりを生成するステップであって、直交行列の前記集まりの各々は、相互に直交する行を有する、ステップと、
対角行列の前記集まりの各々について、前記処理回路によって、行列の対の集まりを形成するステップであって、行列の対の前記集まりの各々は、(i)その対角行列と、(ii)直交行列の前記集まりのそれぞれの直交行列とを含む、ステップと、
線形変換行列を生成するために、前記処理回路によって、行列の対の前記集まりの各々の積を生成するステップであって、前記線形変換行列は、前記カーネルベースの機械学習システムによって使用される前記カーネルの不偏推定量である、ステップと
を含む、方法。 A method of generating input to a kernel-based machine learning system that uses the kernel to perform classification operations on data.
A step of generating a collection of diagonal matrices by the processing circuit of the kernel-based machine learning system, where each of the collections of diagonal matrices is an off-diagonal element that is zero and a specified probability distribution function. With steps and with diagonal elements having values distributed according to
A step of generating a set of orthogonal matrices by the processing circuit, wherein each of the sets of orthogonal matrices has rows that are orthogonal to each other.
For each of the groups of diagonal matrices, the processing circuit forms a set of pairs of matrices, each of which is (i) its diagonal matrix and (ii). A step that includes each orthogonal matrix of the collection of orthogonal matrices.
To generate a linear transformation matrix, the processing circuit generates the product of each of the pairs of matrices, wherein the linear transformation matrix is used by the kernel-based machine learning system. it is unbiased estimator of the kernel, and the step method.
対角行列の集まりを生成するステップであって、対角行列の前記集まりの各々は、ゼロである非対角要素と、指定された確率分布関数に従って分布する値を有する対角要素とを有する、ステップと、
直交行列の集まりを生成するステップであって、直交行列の前記集まりの各々は、相互に直交する行を有する、ステップと、
対角行列の前記集まりの各々について、行列の対の集まりを形成するステップであって、行列の対の前記集まりの各々は、(i)その対角行列と、(ii)直交行列の前記集まりのそれぞれの直交行列とを含む、ステップと、
線形変換行列を生成するために、行列の対の前記集まりの各々の積を生成するステップであって、前記線形変換行列は、前記カーネルベースの機械学習システムによって使用される前記カーネルの不偏推定量である、ステップと
を含む方法を実行させるコードを含む、コンピュータプログラム。 A computer program, when executed by the processing circuits of the computer that is configured to generate an input to a kernel-based machine learning system to perform the classification operation on data using kernel, In the processing circuit
A step of generating a set of diagonal matrices, each of which has an off-diagonal element that is zero and a diagonal element that has values that are distributed according to a specified probability distribution function. , Steps and
A step in which a set of orthogonal matrices is generated, wherein each of the sets of orthogonal matrices has rows that are orthogonal to each other.
For each of the collections of diagonal matrices, a step of forming a collection of pairs of matrices, each of which collection of pairs of matrices is (i) its diagonal matrix and (ii) said collection of orthogonal matrices. Steps and, including their respective orthogonal matrices of
To generate a linear transformation matrix, the step of generating the product of each of the collections of a pair of matrices, the linear transformation matrix is an unbiased estimate of the kernel used by the kernel-based machine learning system. it includes code for executing a method comprising the steps, the computer program.
メモリと、
前記メモリに結合されている制御回路であって、
対角行列の集まりを生成することであって、対角行列の前記集まりの各々は、ゼロである非対角要素と、指定された確率分布関数に従って分布する値を有する対角要素とを有する、生成することと、
直交行列の集まりを生成することであって、直交行列の前記集まりの各々は、相互に直交する行を有する、生成することと、
対角行列の前記集まりの各々について、行列の対の集まりを形成することであって、行列の対の前記集まりの各々は、(i)その対角行列と、(ii)直交行列の前記集まりのそれぞれの直交行列とを含む、形成することと、
線形変換行列を生成するために、行列の対の前記集まりの各々の積を生成することであって、前記線形変換行列は、前記カーネルベースの機械学習システムによって使用される前記カーネルの不偏推定量である、生成することと
を行うように構成される制御回路と
を備える、電子装置。 An electronic device that is configured to generate inputs to a kernel-based machine learning system that uses the kernel to perform classification operations on data.
Memory and
A control circuit coupled to the memory.
To generate a collection of diagonal matrices, each of which collections of diagonal matrices has an off-diagonal element that is zero and a diagonal element that has values that are distributed according to a specified probability distribution function. , To generate and
To generate a collection of orthogonal matrices, each of which has rows that are orthogonal to each other.
For each of the collections of diagonal matrices is to form a collection of pairs of matrices, each of which collection of pairs of matrices is (i) its diagonal matrix and (ii) said collection of orthogonal matrices. Including each orthogonal matrix of
To generate a linear transformation matrix is to generate the product of each of the collections of pairs of matrices, the linear transformation matrix being the unbiased estimate of the kernel used by the kernel-based machine learning system. in it, and a configured control circuit to perform and that the produced electronic device.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662413011P | 2016-10-26 | 2016-10-26 | |
US62/413,011 | 2016-10-26 | ||
US15/793,455 | 2017-10-25 | ||
US15/793,455 US20180114145A1 (en) | 2016-10-26 | 2017-10-25 | Structured orthogonal random features for kernel-based machine learning |
PCT/US2017/058415 WO2018081351A2 (en) | 2016-10-26 | 2017-10-26 | Structured orthogonal random features for kernel-based machine learning |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2019533867A JP2019533867A (en) | 2019-11-21 |
JP6851474B2 true JP6851474B2 (en) | 2021-03-31 |
Family
ID=61969717
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019522814A Active JP6851474B2 (en) | 2016-10-26 | 2017-10-26 | Structured Orthogonal Random Features for Kernel-Based Machine Learning |
Country Status (4)
Country | Link |
---|---|
US (1) | US20180114145A1 (en) |
JP (1) | JP6851474B2 (en) |
CN (1) | CN109997131B (en) |
WO (1) | WO2018081351A2 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11295223B2 (en) | 2018-06-12 | 2022-04-05 | International Business Machines Corporation | Quantum feature kernel estimation using an alternating two layer quantum circuit |
DE102018211595B3 (en) | 2018-07-12 | 2019-09-19 | BSH Hausgeräte GmbH | Data compression and data decompression for user devices based on a Walsh-Hadamard transformation |
US11562046B2 (en) | 2018-11-26 | 2023-01-24 | Samsung Electronics Co., Ltd. | Neural network processor using dyadic weight matrix and operation method thereof |
CN111313865A (en) * | 2018-12-12 | 2020-06-19 | 哈尔滨工业大学 | Step length regularization method for random Fourier feature kernel LMS algorithm |
US20210019654A1 (en) * | 2019-07-19 | 2021-01-21 | Google Llc | Sampled Softmax with Random Fourier Features |
CN112949658B (en) * | 2021-03-10 | 2022-07-08 | 清华大学 | Deep learning method with stable performance |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2002057426A2 (en) * | 2001-01-19 | 2002-07-25 | U.S. Army Medical Research And Materiel Command | A method and apparatus for generating two-dimensional images of cervical tissue from three-dimensional hyperspectral cubes |
US6993193B2 (en) * | 2002-03-26 | 2006-01-31 | Agilent Technologies, Inc. | Method and system of object classification employing dimension reduction |
US7174040B2 (en) * | 2002-07-19 | 2007-02-06 | Intel Corporation | Fast method for training and evaluating support vector machines with a large set of linear features |
US8380647B2 (en) * | 2009-08-14 | 2013-02-19 | Xerox Corporation | Training a classifier by dimension-wise embedding of training data |
WO2011159255A2 (en) * | 2010-06-14 | 2011-12-22 | Blue Prism Technologies Pte Ltd | High-dimensional data analysis |
US8429102B2 (en) * | 2011-03-31 | 2013-04-23 | Mitsubishi Electric Research Laboratories, Inc. | Data driven frequency mapping for kernels used in support vector machines |
CN105022740A (en) * | 2014-04-23 | 2015-11-04 | 苏州易维迅信息科技有限公司 | Processing method and device of unstructured data |
US9606934B2 (en) * | 2015-02-02 | 2017-03-28 | International Business Machines Corporation | Matrix ordering for cache efficiency in performing large sparse matrix operations |
CN105117708A (en) * | 2015-09-08 | 2015-12-02 | 北京天诚盛业科技有限公司 | Facial expression recognition method and apparatus |
-
2017
- 2017-10-25 US US15/793,455 patent/US20180114145A1/en active Pending
- 2017-10-26 WO PCT/US2017/058415 patent/WO2018081351A2/en active Application Filing
- 2017-10-26 JP JP2019522814A patent/JP6851474B2/en active Active
- 2017-10-26 CN CN201780072443.3A patent/CN109997131B/en active Active
Also Published As
Publication number | Publication date |
---|---|
CN109997131A (en) | 2019-07-09 |
JP2019533867A (en) | 2019-11-21 |
CN109997131B (en) | 2024-01-12 |
WO2018081351A3 (en) | 2018-06-07 |
WO2018081351A2 (en) | 2018-05-03 |
US20180114145A1 (en) | 2018-04-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6851474B2 (en) | Structured Orthogonal Random Features for Kernel-Based Machine Learning | |
US20210089612A1 (en) | Pattern change discovery between high dimensional data sets | |
US11106790B2 (en) | Dimensionality reduction of computer programs | |
US10474430B2 (en) | Mixed-precision processing elements, systems, and methods for computational models | |
US9852492B2 (en) | Face detection | |
US20170371856A1 (en) | Personalized summary generation of data visualizations | |
US20210264195A1 (en) | Technologies for enabling analytics of computing events based on augmented canonicalization of classified images | |
US9152703B1 (en) | Systems and methods for clustering data samples | |
US9684705B1 (en) | Systems and methods for clustering data | |
US20210397942A1 (en) | Learning to search user experience designs based on structural similarity | |
CN108108769B (en) | Data classification method and device and storage medium | |
WO2023282847A1 (en) | Detecting objects in a video using attention models | |
US10430454B2 (en) | Systems and methods for culling search results in electronic discovery | |
US10366344B1 (en) | Systems and methods for selecting features for classification | |
US11223543B1 (en) | Reconstructing time series datasets with missing values utilizing machine learning | |
US10382462B2 (en) | Network security classification | |
US11531830B2 (en) | Synthetic rare class generation by preserving morphological identity | |
CN117813602A (en) | Principal component analysis | |
Wei et al. | Sketch and Scale Geo-distributed tSNE and UMAP | |
WO2016053343A1 (en) | Intent based clustering | |
Katariya et al. | Agglomerative clustering in web usage mining: a survey | |
EP4189609A1 (en) | Quantum computing device for determining a network parameter | |
Sharma et al. | Image segmentation in constrained IoT servers | |
US11538248B2 (en) | Summarizing videos via side information | |
US20230205599A1 (en) | Method, apparatus, and computer readable medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20190626 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20190626 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20200831 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20200831 |
|
A601 | Written request for extension of time |
Free format text: JAPANESE INTERMEDIATE CODE: A601Effective date: 20201130 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20201216 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20210208 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20210309 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6851474Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |