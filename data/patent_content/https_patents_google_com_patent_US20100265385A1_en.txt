US20100265385A1 - Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same - Google Patents
Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same Download PDFInfo
- Publication number
- US20100265385A1 US20100265385A1 US12/703,367 US70336710A US2010265385A1 US 20100265385 A1 US20100265385 A1 US 20100265385A1 US 70336710 A US70336710 A US 70336710A US 2010265385 A1 US2010265385 A1 US 2010265385A1
- Authority
- US
- United States
- Prior art keywords
- data
- light field
- image
- file
- electronic
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/79—Processing of colour television signals in connection with recording
- H04N9/80—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback
- H04N9/82—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only
- H04N9/8205—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only involving the multiplexing of an additional signal and the colour video signal
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0075—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for altering, e.g. increasing, the depth of field or depth of focus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
- H04N5/77—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera
- H04N5/772—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera the recording apparatus and the television camera being placed in the same enclosure
Definitions
- the present inventions are directed to, among other things, Light Field Data Acquisition Devices (as defined in the Detailed Description, for example, light field cameras) that acquire Light Field Data (as also defined in the Detailed Description) or information, post-processing systems relating to such devices, and methods of using such cameras and systems.
- the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device used in post-processing of the image data captured or acquired thereby.
- the present inventions are directed to providing or communicating (i) such characteristics, parameters and/or configurations and/or (ii) information which is representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the image data acquisition device (for example, an optical and/or a geometric model of the image data acquisition device that is associated with certain acquired Light Field Data).
- an optical and/or a geometric model of the image data acquisition device for example, an optical and/or a geometric model of the image data acquisition device that is associated with certain acquired Light Field Data.
- such characteristics, parameters and/or configurations of the light field camera facilitate such cameras and/or systems to generate, manipulate and/or edit Light Field Data (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition or recording of the Light Field Data and/or information) of, for example, a scene.
- the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may provide information which is representative of an optical and/or a geometric model of the image data acquisition device (which may include, for example, the camera optics (for example, one or more lenses of any kind or type), imaging sensors to obtain and/or acquire the Light Field Data or information, and relative distances between the elements of the image data acquisition device).
- the image data acquisition device which may include, for example, the camera optics (for example, one or more lenses of any kind or type), imaging sensors to obtain and/or acquire the Light Field Data or information, and relative distances between the elements of the image data acquisition device.
- post-processing circuitry for example, circuitry which is disposed in or integrated into an image data acquisition device (see FIG. 1B ) or post-processing circuitry which is external to the image data acquisition device (see FIG.
- the Light Field Data Acquisition Device may obtain, receive, acquire and/or determine such characteristics, parameters and/or configurations of the Light Field Data Acquisition Device and may determine, analyze and/or interpret the ray-geometry corresponding to one, some or all of imaging sensor pixel values associated with the imaging sensor in order to generate, manipulate and/or edit image data and/or information of, for example, a scene (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition and/or recording of the image data or information).
- configuration data The data which is representative of the characteristics, parameters and/or configurations (hereinafter collectively “configuration data”) of the Light Field Data Acquisition Device may be obtained, determined and/or recorded before, during and/or after collection or acquisition of Light Field Data by the imaging sensor of the acquisition device (for example, light field camera).
- configuration data may be stored in the same data file and/or file format as the associated Light Field Data or in a different data file and/or different file format as the associated Light Field Data.
- the configuration data file is associated with a plurality of files each containing Light Field Data.
- Such configuration data may be transmitted, provided and/or communicated to a external post-processing system together with or separate from the image data. (See, for example, FIG. 1C ). Indeed, the data may be transmitted serially or in parallel with the electronic data files containing the Light Field Data.
- a characteristic of a Light Field Data Acquisition Device provides the user the ability to compute images that are focused over a range of depths, corresponding to a range of virtual image planes about the physical plane where the light field sensor (which may include a microlens array and a photo sensor array) was positioned during data acquisition.
- this range of focusing corresponds to the range of (virtual) image plane depths a distance of c about the physical light field sensor plane.
- FIG. 2A the “world” or everything outside of the Light Field Data Acquisition device is to the left of the lens plane, and the device internals are illustrated to the right of the lens plane.
- FIG. 2A is not drawn to scale; indeed, ⁇ 1 and ⁇ 2 are often smaller than v (for example, ⁇ 1 ⁇ 0.01*v and ⁇ 2 ⁇ 0.01*v).
- refocusable image data are image data or information, no matter how acquired or obtained, that may be focused and/or re-focused after acquisition or recording of the data or information.
- refocusable image data or information is/are Light Field Data or information acquired or obtained, for example, via a Light Field Data Acquisition Device.
- the techniques of generating, manipulating and/or editing Light Field Data or information may be implemented via circuitry and techniques on/in the Light Field Data Acquisition Device and/or external post-processing system.
- the present inventions are neither limited to any single aspect nor embodiment, nor to any combinations and/or permutations of such aspects and/or embodiments.
- each of the aspects of the present inventions, and/or embodiments thereof may be employed alone or in combination with one or more of the other aspects and/or embodiments thereof. For the sake of brevity, many of those permutations and combinations will not be discussed and/or illustrated separately herein.
- certain of the present inventions are directed to a method of generating and outputting image data corresponding to a scene, comprising: (a) acquiring Light Field Data which is representative of a light field from the scene, wherein the Light Field Data is acquired using a data acquisition device; (b) acquiring configuration data which is representative of how light rays optically propagate through the data acquisition device; (c) generating first image data using the Light Field Data and the configuration data, wherein the first image data includes a focus or focus depth that is different from a focus or focus depth of the Light Field Data; (e) generating a first electronic data file including (1) the first image data, (2) the Light Field Data, and (3) the configuration data; and (f) outputting the first electronic data file (for example, to memory, processing circuitry, a Standard Display Mechanism (such as a printer or display)).
- generating the first electronic data file further includes arranging the first image data of the first electronic data file in a Standard Image Format (for example, JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats).
- generating a first electronic data file further includes interleaving, threading, watermarking, encoding, multiplexing and/or meshing the first image data and the Light Field Data.
- generating the first electronic data file may further include generating a header of the first electronic data file, wherein the header includes the configuration data.
- the method of this aspect of the inventions may further include: (g) reading the first electronic data file; (h) displaying the first image data; (i) receiving a user input; (j) generating second image data, in response to the user input, using (1) the Light Field Data of the electronic data file and (2) the configuration data, wherein the second image data is different from the first image data; (k) generating a second electronic data file including (1) the second image data, (2) the Light Field Data, and (3) the configuration data; and (l) outputting the second electronic data file (for example, to memory, processing circuitry, a Standard Display Mechanism (such as a printer or display)).
- the second image data may include a focus or focus depth that is different from the focus or focus depth of the first image data.
- the second image data may be arranged in a Standard Image Format.
- generating a second electronic data file may further include interleaving, threading, watermarking, encoding, multiplexing and/or meshing the second image data and the Light Field Data.
- the method of this aspect of the inventions may further include compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the first electronic data file is the compressed Light Field Data.
- the method may further include: (g) reading the first electronic data file; (h) displaying the first image data; (i) receiving a user input; (j) generating second image data, in response to the user input, using (1) the Light Field Data of the electronic data file and (2) the configuration data, wherein the second image data is different from the first image data; (k) generating a second electronic data file including the second image data; and (l) outputting the second electronic data file.
- acquiring configuration data includes acquiring an N-bit key; and the method further includes determining optical model data by correlating the N-bit key to predetermined optical model data and wherein generating first image data includes generating first image data using the Light Field Data and the optical model data.
- the configuration data may include data which is representative of an Aperture Function or an Exit Pupil which is associated with the acquisition of the Light Field Data.
- the configuration data may include data which is representative of a mapping from a two-dimensional position on a captured 2D array of pixel values of the data acquisition device to a four-dimensional parameterization of the light field from the scene.
- certain of the present inventions are directed to a system to generate an image of a scene, comprising read circuitry to read a first electronic data file which is stored in a memory, wherein the first electronic data file includes (i) first image data, (ii) Light Field Data which is representative of a light field from the scene, and (iii) configuration data which is representative of how light rays optically propagate through a Light Field Data acquisition device.
- the system further includes a display to visually output an image using the first image data, a user interface to receive a user input, and processing circuitry, coupled to the read circuitry, the display and the user interface, to: (i) determine an optical model data using the configuration data, wherein the optical model data is representative of an optical model of the Light Field Data acquisition device, (ii) generate second image data, in response to the user input, using the Light Field Data and the optical model data, wherein the second image data includes a focus or focus depth that is different from a focus or focus depth of the first image data, and (iii) generate a second electronic data file including the second image data.
- the system of this aspect further includes write circuitry, coupled to the processing circuitry, to write the second electronic data file to the memory.
- the second electronic data file further includes (i) the Light Field Data which is representative of a light field from the scene, and (ii) the configuration data and/or the optical model data.
- the configuration data may include data which is representative of an Aperture Function or an Exit Pupil which is associated with the Light Field Data acquisition device that acquired the Light Field Data.
- the processing circuitry may generate the second electronic data file by interleaving, threading, watermarking, encoding, multiplexing and/or meshing the second image data and the Light Field Data.
- the second electronic data file includes a header or the processing circuitry may generate a header of the second electronic data file, wherein the header includes the configuration data and/or the optical model data.
- the processing circuitry may generate the first electronic data file by compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the second electronic data file is the compressed Light Field Data.
- the processing circuitry arranges the first image data and/or the second image data of the second electronic data file in a Standard Image Format (for example, JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats).
- a Standard Image Format for example, JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats.
- the configuration data of the first electronic data file includes an N-bit key
- the processing circuitry determines the optical model data by correlating the N-bit key to a plurality of different, predetermined optical model data.
- certain of the present inventions are directed to a light field acquisition device for acquiring light field image data of a scene, comprising: optics, a light field sensor, located in the optical path of the optics, to acquire light field image data, a user interface to receive a user input, wherein, in response to the user input, the light field sensor acquires the light field image data of the scene, and processing circuitry, coupled the light field sensor and the user interface, to generate and output an electronic data file, the processing circuitry to: (a) determine configuration data which is representative of how light rays optically propagate through the optics and light field sensor, and (b) generate and output the electronic data file, wherein the electronic data file includes (i) image data (which may be arranged in a Standard Image Format), (ii) Light Field Data which is representative of a light field from the scene, and (iii) configuration data (for example, (1) data which is representative of an Aperture Function or Exit Pupil of the light field acquisition device and/or (2) data which is representative of
- the processing circuitry generates the electronic data file by interleaving, threading, watermarking, encoding, multiplexing and/or meshing the image data and the Light Field Data.
- the processing circuitry generates the electronic data file by forming a header, wherein the header includes the configuration data.
- the processing circuitry generates the electronic data file by compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the electronic data file is the compressed Light Field Data.
- the configuration data of the electronic data file may include an N-bit key which is representative of predetermined optical model data.
- the processing circuitry may generate a header of the electronic data file, wherein the header includes the configuration data and/or the optical model data.
- FIG. 1A is a block diagram representation of an exemplary Light Field Data Acquisition Device
- FIG. 1B is a block diagram representation of an exemplary Light Field Data Acquisition Device including, among other things, post-processing circuitry integrated therein;
- FIGS. 1C and 1F are block diagram representations of exemplary Light Field Data acquisition systems including a Light Field Data Acquisition Device and post-processing circuitry;
- FIG. 1D is a block diagram representation of an exemplary Light Field Data Acquisition Device including memory (integrated therein) to store Light Field Data;
- FIG. 1E is a block diagram representation of an exemplary Light Field Data Acquisition Device including, among other things, post-processing circuitry and memory integrated therein;
- FIG. 1G is a block diagram of an exemplary Light Field Data Acquisition Device including optics, a coded aperture, and sensor to record, acquire, sample and/or capture Light Field Data, including memory integrated therein;
- FIG. 1H is a block diagram representation of an exemplary Light Field Data Acquisition Device including a plurality of optics and sensors to record, acquire, sample and/or capture Light Field Data, including memory integrated therein;
- FIG. 2A is an illustrative diagram representation of certain optical characteristics of an exemplary Light Field Data Acquisition Device including certain focus planes such as a far-focus plane, a physical light field sensor plane, and the close-focus plane;
- FIG. 2B is an illustrative diagram representation of an exemplary light field sensor including, among other things, a microlens array and imaging sensor, which may be separated by (or substantially separated by) the focal length of the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions;
- FIG. 2C is an illustrative diagram representation of the light field sensor plane, which may be disposed at the principal plane of the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions;
- FIG. 2D is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, a main lens (representing the optics), a microlens array and an imaging sensor, illustrating two exit pupil locations which provide or result in different locations of the centers of projected lenslets in the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions; notably, the positioning the exit pupil at the location corresponding to the Exit Pupil 2 results in larger disk images projected onto the surface of the imaging sensor relative to the location corresponding to the Exit Pupil 1;
- FIG. 3A is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil is recorded and/or stored as a single number that is the distance of the center of the exit pupil from the microlens array (or imaging sensor surface in an alternative embodiment);
- FIG. 3B is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil may be a location (for example, the center of the exit pupil) in three-dimensional space;
- FIG. 3C is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil may be a location and shape (in the illustrative embodiment, the location of the center of the exit pupil and a disk of a specified radius) in three-dimensional space;
- FIG. 4 is an illustrative diagram representation of the propagation of an exemplary light ray from the world, though a lens into a light field acquisition device and impinging on the plane of the light field sensor; wherein for a given light ray (represented by a 3D position and 3D direction vector) that enters the acquisition device, the post-processing circuitry/system may calculate or determine how the rays propagate within the acquisition device between the last lens element of the optics and the microlens array of the light field sensor array by “tracing” the light ray through the lens elements of the optics according to the way the ray would physically refract and propagate through each element of the optics based on physical laws given the glass type, curvature and thickness of each lens element of the optics;
- FIG. 5 illustrates a magnified view of a set of projected lenslet disks of the microlens array onto the surface of an imaging sensor (or portion thereof); notably, the locations, size and shape of the projected disks are overlayed onto the captured image; determining the centers and sizes of microlens disks may be performed based on the key optical parameters detailed herein;
- FIG. 6 illustrates a magnified view of the surface of an exemplary imaging sensor (or portion thereof) highlighting/outlining the radius of projected disks lenslet disks of the microlens array, the spacing between neighboring centers (pitch) of the lenslet disks of the microlens array, X and Y translation offsets and rotation; the X and Y offset values in this exemplary illustration are the spatial distance between the center pixel on the sensor and the center of a central projected microlens disk; and the spacing between neighboring disk centers is the pitch of the projected microlens array.
- the diameter of the projected disks appears approximately the same size in the illustration as the pitch, the numbers are different and may be used for different purposes;
- FIGS. 7A and 7B are block diagram representations of exemplary grid architectures of the microlens array, including a hexagonal grid ( FIG. 7A ) and a square grid ( FIG. 7B ) wherein the pitch of the lenslets of the array of such architectures is highlighted/outlined in conjunction therewith;
- FIGS. 8A-8C are block diagram representations of exemplary grid architectures of the microlens array, including a hexagonal grid ( FIG. 8A ), a square grid ( FIG. 8B ) and square and octagonal grid ( FIG. 8C ); notably, the pattern of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device;
- FIG. 9 is a block diagram representation of sensor pixels of a sensor array (of, for example, a light field sensor) wherein the pitch of the pixels of the sensor array of such architecture is highlighted/outlined in conjunction therewith; the pitch of the pixels/sensors of the sensor may be characterized as the distance between the centers of neighboring sensor pixels and such pitch may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device;
- FIG. 10 is an illustrative diagram representation of a collimated light source, microlens array, and image sensor to create an image of points of light or small disks of light; in this illustrative embodiment the sensor, at any time in the manufacturing process after the microlens array has been fastened to the sensor, samples the light rays of a collimated light source wherein all the light rays are perpendicular to the surface of the light field sensor;
- FIG. 11 is an exemplary illustration of the resulting image provides a grid of points of light or small images of disks, one per lenslet in the microlens array for microlens array to imaging sensor registration;
- the registration may employ an image of point-lights or small disks (for example, as produced via the architecture of FIGS. 10 and/or 12 );
- the X and Y offsets are these distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels;
- FIG. 12 is an illustrative diagram representation of an aperture, microlens array and image sensor for registration of the microlens array to the image sensor wherein the small aperture provides a near-uniform light source; an image may be captured from the fully or near fully assembled light field acquisition device of uniform or near uniform field of light (for example, a white wall) when the acquisition device is “stopped down” (i.e.
- the resulting image will be a grid of points of light or small images of disks, one per lenslet in the microlens array; the X and Y offsets are this distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See, FIG. 11 );
- FIGS. 13A and 13B are block diagram representations of an exemplary Light Field Data Acquisition Devices including, among other things, sensor (for example, linear or rotary potentiometers, encoders and/or piezo-electric or MEMS transducers, and/or image sensors such as CCDs or CMOS—notably, any sensor whether now known or later developed is intended to fall within the scope of the present inventions) to sense, detect and/or determine (i) the configuration of the lens system of the acquisition device, and/or (ii) determine the Exit Pupil or Aperture Function of the lens system of the Light Field Data Acquisition Device relative to the microlens array (for example, one or more of the size and/or shape and/or other characteristics of the Exit Pupil (relative to the microlens array); notably, the sensors may be employed in any of the acquisition devices described and/or illustrated herein, including those of FIGS. 1 A- 1 H—for the sake of conciseness, such sensors will not be illustrated therewith;
- sensor for example,
- FIGS. 14A-14C are illustrative diagram representations of a microlens array and image sensor highlighting disks of light projected by a lenslet onto the microlens array; notably, the spacing between neighboring disk centers is the pitch of the projected microlens array; the radius of each projected lenslet disk may be considered extent of the disk of light projected by a lenslet in the microlens array, wherein (i) the size of projected disks may be smaller than spacing between disks ( FIG. 14A ), (ii) the size of projected disks may be nearly the same as the spacing between disks ( FIG. 14B ), and (iii) the size of projected disks may be larger than spacing between disks ( FIG. 14C );
- FIGS. 15A and 15B are block diagram representations of exemplary electronic light field data files having one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the file format or structure of the Light Field Data file may include a start code and/or end code to indicate the beginning and/or end, respectively, of a set of a Light Field Data; notably, the electronic data file format or structure may have a header section containing metadata which may include and/or consist of Light Field Configuration Data (see, FIG. 15B );
- FIG. 15C is a bock diagram of exemplary electronic file having Light Field Configuration Data which is associated with one or more electronic light field data files having one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions;
- FIGS. 16A and 16B are block diagram representations of memory (which may store, among other things, the electronic data files having one or more sets of Light Field Data) in communication with post-processing circuitry, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the memory may be separate from or integrated with the post-processing circuitry ( FIGS. 16A and 16B , respectively);
- FIGS. 16C and 16D are block diagram representations of exemplary Light Field Data Acquisition Devices, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the exemplary Light Field Data Acquisition Devices include a display (Standard Display Mechanism) to allow the user to view an image or video generated using one or more sets of Light Field Data in a Light Field Data File;
- a display Standard Display Mechanism
- FIGS. 16E and 16F are block diagram representations of exemplary Light Field Data Acquisition Devices, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein the Light Field Data Acquisition Device couples to external systems/devices (for example, external storage, video display, printer, recording device and/or processor circuitry) including an external display to allow the user to view an image or video generated using one or more sets of Light Field Data in a Light Field Data File; such external devices or circuitry may facilitate, for example, storage of electronic data files that include light field image data, electronic files that include Light Field Configuration Data and/or electronic files that include a combination thereof;
- external systems/devices for example, external storage, video display, printer, recording device and/or processor circuitry
- FIG. 16G is a block diagram representation of memory (which may store the electronic data files having one or more sets of Light Field Data and/or Light Field Configuration Data) in communication with post-processing circuitry, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the post-processing circuitry includes write circuitry and read circuitry to communicate with the memory, and the processing circuitry to implement, for example, Light Field Processing that includes generating, manipulating and/or editing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field) the image data corresponding to the Light Field Data—after acquisition or recording thereof;
- the post-processing circuitry includes write circuitry and read circuitry to communicate with the memory, and the processing circuitry to implement, for example, Light Field Processing that includes generating, manipulating and/or editing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field) the image data corresponding to the Light
- FIG. 17A is a block diagram representation of exemplary electronic data files having a image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format, as defined in the Detailed Description, and one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions;
- FIGS. 17B-17D are block diagram representations of exemplary electronic data files having image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files may include one or more headers having metadata which includes, for example, one or more sets of Light Field Configuration Data;
- FIGS. 17E and 17F are block diagram representations of exemplary electronic data files having image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of “raw” image data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files may include one or more headers having metadata;
- FIGS. 18A-18E are exemplary processing flows for post-processing the exemplary electronic data files having data (for example, one or more sets of Light Field Data), according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the exemplary post-processing flows may be employed in conjunction with the electronic data files of FIGS. 17A-17F ;
- data for example, one or more sets of Light Field Data
- FIG. 19 is a block diagram representation of exemplary electronic data files in conjunction with exemplary processing flows for post-processing data contained therein, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files include image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of Light Field Data, and the processing may utilize any Standard Display Mechanism to view the Standard Image portion of the electronic data file; notably, such electronic files may include one or more headers (not illustrated) having metadata (which includes, for example, one or more sets of Light Field Configuration Data); moreover, in the processing flows; and
- FIG. 20 is a block diagram representation of an exemplary user interface of, for example, the Light Field Data Acquisition Device and/or post-processing system, according to certain aspects of the present invention; notably, in one embodiment, the user interface may include an output device/mechanism (for example, display and/or speaker) and/or user input device/mechanism (for example, buttons, switches, touch screens, pointing device (for example, mouse or trackball) and/or microphone) to allow a user/operator to monitor, control and/or program the operation of the Light Field Data Acquisition Devices and/or post-processing circuitry/system.
- an output device/mechanism for example, display and/or speaker
- user input device/mechanism for example, buttons, switches, touch screens, pointing device (for example, mouse or trackball) and/or microphone
- the present inventions are directed to, among other things, Light Field Data Acquisition Devices (for example, light field cameras), post-processing systems relating thereto, and methods of using such devices and systems.
- the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device used to implement post-processing of the image data captured or acquired thereby (or example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition and/or recording of the image data).
- the present inventions are directed to transmitting, providing or communicating such characteristics, parameters and/or configurations to post-processing circuitry—whether such post-processing circuitry is disposed in/on the Light Field Data Acquisition Device (see FIGS. 1B and 1E ) or external thereto (see FIGS. 1C and 1F ).
- the data which is representative of the characteristics, parameters and/or configurations (collectively “configuration data”) of the Light Field Data Acquisition Device may be obtained, determined and/or recorded before, during and/or after collection or acquisition of Light Field Data by the imaging sensor of the acquisition device (for example, light field camera).
- such configuration data may be employed by post-processing circuitry to generate, derive, calculate, estimate and/or determine an optical and/or geometric model of the Light Field Data Acquisition Device (for example, an optical and/or geometric model of the particular device which is associated with specific acquired Light Field Data).
- the post-processing circuitry may employ the optical and/or geometric model of the Light Field Data Acquisition Device to generate, manipulate and/or edit (for example, define and/or redefine the focus of the light field image data) the light field image data which is associated with or corresponds to the optical and/or geometric model of the Light Field Data Acquisition Device employed to acquire or collect such Light Field Data.
- Light Field Data means, among other things, a set of values, where each value represents the light traveling along each geometric light ray (or bundle of rays approximating a geometric light ray) within a corresponding set of light rays.
- Light Field Data represents the 2D image data sampled by and read from the image sensor pixel array in a light field acquisition device (for example, a light field camera comprising a main lens, microlens array and a photo sensor, such as the one shown in United States Patent Application Publication 2007/0252074, and/or the provisional application to which it claims priority, and/or Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006, all of which are incorporated here in their entirety by reference; and/or the block diagram illustration of a light field camera in FIGS.
- a light field acquisition device for example, a light field camera comprising a main lens, microlens array and a photo sensor, such as the one shown in United States Patent Application Publication 2007/0252074, and/or the provision
- the Light Field Data may be represented as a function L(x,y,u,v) where L is the amount of light (for example, radiance) traveling along a ray (x,y,u,v) that passes through the optical aperture of the camera lens at 2D position (u,v) and the sensor at 2D position (x,y)—see, for example, the Patent Application Publication 2007/0252074 and PhD thesis mentioned above.
- Light Field Data may mean the image data collected with a coded aperture system. (See FIG. 1G ) and/or data encoded and/or recorded in the frequency spectrum of the light field.
- Light Field Data may be a collection of images focused at different depths and/or a collection of images from different viewpoints. (See FIG.
- Light Field Data may mean any collection of images or lighting data that may be used to generate, derive, calculate, estimate and/or determine a full or partial representation or approximation of a light field function L(x,y,u,v) as described above.
- Light Field Configuration Data means data that may be used to interpret (in whole or in part) Light Field Data.
- Light Field Configuration Data are data that may be used to interpret how the values in the Light Field Data relate to or map the characteristics of light flowing on particular light rays or sets of light rays in the scene pertaining to the light field. Such characteristics may include or depend upon, for example, the intensity, color, wavelength, polarization, etc. of the light in the scene.
- the Light Field Configuration Data may be representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the image data acquisition device (for example, an optical and/or a geometric model of the image data acquisition device that is associated with certain acquired Light Field Data).
- Light Field Configuration Data may include one, some or all of the following, and/or data representative of and/or used in generating, deriving, calculating, estimating and/or determining one, some or all of the following:
- Light Field Configuration Data is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, in some exemplary embodiments, Light Field Configuration Data may encompass any information now known or later developed which is representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the Light Field Data Acquisition Device.
- Aperture Function is a term for data that relates to and/or represents the transfer of light through an optical system.
- Aperture Function is a function that specifies for geometric light rays striking a sensor, how much light passes from the outside of the acquisition device, through the lens (or plurality of lenses) and strikes the sensor along that ray trajectory.
- the Aperture Function may be represented by a 4D function, A(x,y,u,v), where A represents the fraction of light transferred through the lens (or plurality of lenses) along a ray (x,y,u,v) that strikes the sensor at 2D position (x,y) and from a direction (u,v) on the hemisphere of incoming directions.
- the Aperture Function corresponding to such a function may be represented or approximated by an Exit Pupil, as described below.
- data representing the Exit Pupil is recorded and/or stored as a single number that is the distance of the center of the exit pupil from the microlens array or imaging sensor surface (see FIG. 3A ).
- data representing the Exit Pupil may be a location (for example, the center of the exit pupil) in 3 dimensional space (see FIG. 3B ).
- data representing the Exit Pupil may be a location and shape (for example, the location of the center of the exit pupil and a disk of a specified radius) in 3 dimensional space (see FIG. 3C ).
- data representing the Exit Pupil may be recorded and/or stored in many different forms, and data representing the Exit Pupil is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, data representing the Exit Pupil may be in any form now known or later developed.
- Light Field Processing means processing Light Field Data to, for example, compute an output result, for example, an image.
- Light Field Processing encompasses generating, manipulating and/or editing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field relative to the relative to focus and/or depth of field provided by the optics of the acquisition device during acquisition, sampling and/or capture of the Light Field Data) the image data corresponding to the Light Field Data—after acquisition or recording thereof.
- Light Field Processing may use Light Field Configuration Data in interpreting Light Field Data in order to implement a particular processing to produce a particular output result. Different types of Light Field Processing may be used in different embodiments of the present invention.
- Light Field Processing may include refocusing—that is, processing the Light Field Data to compute an image in which at least part of the image is refocused (relative to the optical focus of the acquisition system) at a desired or virtual focus plane in the scene.
- Light Field Processing may include aberration correction, in which the light rays in the light field are processed in order to reduce the effects of optical aberration in the optical system used to record the Light Field Data.
- aberration correction is implemented according to the methods of processing L(x,y,u,v) light field functions with the geometric and/or optical model of the Light Field Data Acquisition Device as shown in U.S.
- Light Field Processing may include changing (increasing or decreasing) the depth field, in which the light rays are processed in order to compute an image in which the depth of field is changed (increased or decreased) to, for example, provide a different range of depths in the world into focus or a predetermined focus.
- Light Field Processing may include processing to correct for inherent lens aberrations in the recorded Light Field Data—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene.
- Light Field Processing includes simulating novel lens systems—after initial acquisition or recording of the Light Field Data and/or information) of, for example, a scene.
- Light Field Processing includes changing the viewing perspective—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene.
- Light Field Processing includes creating holographic images from Light Field Data—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene.
- Light Field Processing is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, Light Field Processing encompasses any act of act of generating, manipulating, editing and/or processing Light Field Data now known or later developed.
- Standard Image Format is a term used to denote images or image data arranged, organized and/or stored (hereinafter, in this context, “stored”) in a standard encoding for storage, display or transmission.
- Exemplary embodiments include JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats.
- the Light Field Data Acquisition Device means any device or system for acquiring, recording, measuring, estimating, determining and/or computing Light Field Data.
- the Light Field Data Acquisition Device may include optics 12 (including, for example, a main lens), light field sensor 14 including microlens array 15 and sensor 16 (for example, a photo sensor).
- the microlens array 15 is incorporated into the optical path to facilitate acquisition, capture, sampling of, recording and/or obtaining Light Field Data via sensor 16 .
- Such Light Field Data may be stored in memory 18 .
- the light field data acquisition device 10 may also include control circuitry to manage or control (automatically or in response to user inputs) the acquisition, sampling, capture, recording and/or obtaining of Light Field Data.
- the light field data acquisition device 10 may store the Light Field Data (for example, output by sensor 16 ) in external data storage and/or in on-system data storage. All permutations and combinations of data storage formats of the Light Field Data and/or a representation thereof are intended to fall within the scope of the present inventions.
- light field data acquisition device 10 of the present inventions may be a stand-alone acquisition system/device (see, FIGS. 1A , 1 C, 1 D and 1 F) or may be integrated with post-processing circuitry 20 (see, FIGS. 1B and 1E ).
- light field data acquisition device 10 may be integrated (or substantially integrated) with post-processing circuitry 20 which may perform Light Field Processing (for example, be employed to generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition or recording of the Light Field Data) Light Field Image Data and/or information of, for example, a scene); and, in other exemplary embodiments, light field data acquisition device 10 is separate from post-processing circuitry 20 .
- the post-processing circuitry 20 includes processing circuitry (for example, one or more processors, one or more state machines, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays) to implement or perform Light Field Processing.
- a Light Field Data Acquisition Device during capture and/or acquisition, may have a light field sensor located such that the “optical depth of field” with respect to the light field sensor does not include the location of a subject. (See, FIG. 2A ).
- the “optical depth of field” may be characterized as depth of field the device would have if used as a conventional imaging device containing a conventional imaging sensor.
- the location of light field sensor plane 22 may be considered the same as the principal plane of the elements in the microlens array 15 .
- the location of light field sensor plane 22 may be referred to as the location and/or placement of the light field sensor 14 (for example, when describing the location and/or placement relative to other components and/or modules in light field data acquisition device 10 (for example, optics 12 )).
- Light Field Data Acquisition Devices above are described to illustrate the underlying principles. Indeed, any device now known or later developed for acquiring, recording, measuring, estimating, determining, and/or computing Light Field Data is intended to fall within the scope of the term Light Field Data Acquisition Device and to fall within the scope of the present inventions.
- the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording Light Field Configuration Data (for example, one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device, for example, the Light Field Data Acquisition Device illustrated in FIGS. 1A-2C ).
- the Light Field Configuration Data may provide information which is representative of an optical and/or a geometric model of the image data acquisition device (which may include, for example, the camera optics (for example, one or more lenses of any kind or type), imaging sensors to obtain and/or acquire the Light Field Data or information, and relative distances between the elements of the image data acquisition device).
- the Light Field Configuration Data may include data which enables or facilitates computation, estimation, determination, representation of how light rays optically propagate (for example, refract, reflect, attenuate, scatter and/or disperse) through the Light Field Data Acquisition Device and to acquisition, capture, sampling of and/or recording by the sensor (for example, sensor 16 of FIGS. 2A-2C ).
- post-processing circuitry (which may be integrated into the image data acquisition system (see, for example, FIGS. 1A , 1 B and 1 E) or separate therefrom (see, for example, FIGS. 1C and 1F ) may obtain, receive and/or acquire (i) Light Field Data and/or (ii) the Light Field Configuration Data (which may be stored in memory 18 ).
- the post-processing circuitry may determine, analyze and/or interpret the ray-geometry corresponding to one, some or all of imaging sensor pixel values associated with the imaging sensor of the image data acquisition system and thereby perform Light Field Processing (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof.)
- Light Field Processing for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof.
- Light Field Configuration Data may be obtained, determined and/or recorded before, during and/or after collection, acquisition and/or sampling of image data by the imaging sensor of the acquisition device (for example, light field acquisition device 10 of FIGS. 1A-2C ).
- Such Light Field Configuration Data may be stored in the same data file and/or file format as the associated image data or in a different data file and/or different file format.
- such Light Field Configuration Data may be provided and/or communicated to a separate post-processing system together with (for example, concurrently, serially or in parallel) or separate from the associated image data (for example, before during and/or after collection, acquisition and/or sampling of image data). (See, for example, FIGS. 1C and 1F ).
- the Light Field Data Acquisition Device and/or post-processing circuitry/system stores, records and/or determines the data or information to construct an optical and/or geometric model of the Light Field Data Acquisition Device.
- the Light Field Data Acquisition Device and/or post-processing circuitry/system may, in conjunction with the Light Field Data, store, record and/or determine predetermined and/or selected characteristics, parameters and/or configurations of the Light Field Data Acquisition Device.
- the Light Field Data Acquisition Device stores or records the predetermined or selected Light Field Configuration Data during, concurrently and/or immediately after collection, acquisition and/or sampling of Light Field Data.
- the Light Field Data Acquisition Device may store Light Field Configuration Data in, for example, a header of an electronic data file that contains the associated image data.
- the Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a separate electronic data file which is different from the electronic data file that contains the associated Light Field Data and/or image data.
- the Light Field Configuration Data may be associated with one or more electronic data files which include the associated Light Field Data and/or image data (i.e., the data which was acquired by the device that was configured in accordance with the data of the associated Light Field Configuration Data).
- the Light Field Data Acquisition Device determines, records and/or stores Light Field Configuration Data, immediately prior to, concurrently, and/or immediately after certain light field acquisition parameters may change or vary between successive or multiple acquisitions (for example, the zoom and focus position of the optics are determined, acquired, recorded, and/or stored prior to, at the time of acquisition, after acquisition, for example, before one or more of certain parameters of the Light Field Configuration Data change or vary).
- the Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a header of an electronic data file that includes the associated Light Field and/or image data.
- the Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a separate electronic data file which is different from the data file that contains the associated Light Field and/or image data.
- the post-processing system may perform Light Field Processing on (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field)) the image data—after acquisition or recording thereof—to generate or display a predetermined, selected and/or desired image.
- the post-processing system employs the Light Field Configuration Data to construct or re-construct an optical and/or geometric model of the Light Field Data Acquisition Device used to acquire or capture the image data.
- the post-processing system may obtain the Light Field Configuration Data with or separately from the Light Field Data and/or image data.
- Light Field Configuration Data may include a representation of an optical and/or geometric model for a Light Field Data capture system that includes or provides information to convert or correlate data from an image sensor pixel to a representation of incoming light rays.
- the optical and/or geometric model takes as input a location on the imaging sensor (for example, the X and Y offsets of a pixel), and provides a 4 dimensional representation of the set of light rays captured by that pixel location (for example, a set of rays in (x,y,u,v) ray-space as described above. See, for example, United States Patent Application Publication 2007/0252074, and the provisional application to which it claims priority (namely, Ser. Nos. 60/615,179 and 60/647,492, and Ren Ng's PhD dissertation, “Digital Light Field Photography”).
- the optical and/or geometric model may include (i) the number, shape (for example curvature and thickness), absolute and/or relative position of the optical elements within the device (including but not necessarily limited to lens elements, mirror elements, microlens array elements, and image sensor elements); (ii) characteristics, parameters, configurations and/or properties of one, some or all of the elements (for example, glass type, index of refraction, Abbe number); (iii) manufacturing tolerances; (iv) measured manufacturing deviations; (v) tilts and/or decenters of optical elements in a lens stack; (vi) coating information, etc.
- the geometric and/or optical model is/are sufficient to enable computation, estimation, determination, representation and/or derivation of the trajectory of at least one light ray that enters the device (for example, enters the first lens element of the device), providing the location and direction of the light ray within the device (for example, within the body of the camera between a lens and a microlens array) and/or the termination position of the ray on the device's image sensor and/or the amount, color and/or dispersion of light propagated through the optics of the system.
- the geometric and optical model comprises a representation of one or more (or all) of the following: (i) the curvature and thickness of each lens element, (ii) the spacing and orientation between lens elements, (iii) the type of glass for each element, (iv) the spacing and orientation between the last lens element and the microlens array, (v) the geometric shape of the microlens array, including a hexagonal pattern with given pitch and curvature of lenslets, (vi) the relative spacing and orientation between the microlens array and image sensor array, and (vii) the pitch, pattern and relative orientation of the image sensor array.
- This geometric and optical model may be used to determine the Ray Transfer Function of the acquisition device according to a computational simulation of the optical effect of the acquisition device on the rays that enter the device.
- the post-processing circuitry/system may calculate the ray that propagates within the body of the acquisition device between the last lens element of the optics and the microlens array of the light field sensor by tracing the ray through the lens elements of the optics according to the way the ray would physically refract and propagate through each element based on physical laws given the glass type, curvature and thickness of each lens element (see, for example, FIG. 4 ).
- Light Field Configuration Data may be determined, stored and/or recorded—and associated with Light Field Data acquired or collected using such exposure characteristics, parameters and/or configurations. While in certain embodiments, all of the exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are employed to perform Light Field Processing (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof), less than all may be determined, stored and/or recorded with the associated or corresponding Light Field Data. Accordingly, in certain embodiments, one or more (and, as such, less than all) may be determined, stored and/or recorded for the associated or corresponding Light Field Data.
- Light Field Processing for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof)
- less than all may be determined, stored and/or recorded with the associated or corresponding Light Field Data. Accordingly, in certain embodiments, one or
- Light Field Configuration Data for example, including exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device
- Light Field Configuration Data may be determined, stored and/or recorded with the associated or corresponding Light Field Data.
- those permutations and combinations will not be discussed separately herein.
- the present invention is not limited to any single aspect or embodiment thereof nor to any combinations and/or permutations of such aspects and/or embodiments of determining, storing and/or recording such Light Field Configuration Data with the associated or corresponding Light Field Data.
- the Aperture Function or Exit Pupil may be characterized in some embodiments as a three dimensional image of the aperture of the main lens.
- the relative location of the exit pupil on the imaging sensor location may, at least in part, be determined by the optics of the Light Field Data Acquisition Device. As such, the relative location of the exit pupil on the imaging sensor location may depend on, for example, the lens, the zoom, and the focus. (See, for example, FIG.
- Example Pupil 1 projects a first set of rays on the lenslets of the microlens array and “Exit Pupil 2” projects a second set of rays on the lenslets of the microlens array—which impacts the projection of the disks of light from each lenslet onto the surface of the imaging sensor).
- the size of the projected microlens pitch (for example, the distance between projected lenslet centers on the surface of the imaging sensor, or in other exemplary embodiments the distance across a lenslet along a line between the centers of neighboring lenslets on opposing sides when projected onto) may be characterized or determined using the following relationship:
- the X-offset, Y-offset, pattern, sensor pixel pitch and rotation of the microlens array relative to the sensor may determine how the disks align with the sensor pixels.
- the sensor pitch allows the model to map geometric coordinates (generally measured in millimeters or microns) to pixel locations.
- the sensor pitch and microlens array grid pattern may be known values based on the manufacturing specifications.
- the x-offset, y-offset, and rotation of the microlens array relative to the imaging sensor can be determined through a registration procedure.
- an image may be acquired using the combination of the microlens array with the imaging sensor, at any time in the manufacturing process after the microlens array has been fastened to the imaging sensor, of a collimated light source with all light rays perpendicular to the surface of the light field sensor.
- the resulting image will be a grid of points of light or small images of disks, one per lenslet in the microlens array.
- the X and Y offsets are this distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See FIGS. 10 and 11 ).
- an image may be captured from the fully or near fully assembled light field acquisition device of uniform or near uniform field of light (for example, a white wall) when the acquisition device is “stopped down” (i.e. has its optical lens aperture reduced in size) to the minimum available aperture.
- the resulting image may be a grid of small disks or points of light, one per lenslet in the microlens array.
- the X and Y offsets are this distances from the center of the recorded image to the center of a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference is angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See FIGS. 11 and 12 ).
- one of the measured or known values may be left unspecified and other measured or known values may be stored in units relative to the unspecified parameter.
- the sensor pixel pitch may not be specified and some or all of the distance parameters (for example, the separation of the microlens array from the sensor, the x-offset and y-offset of the microlens array relative to the imaging sensor, and/or the pitch of the microlens array) may then have units that are relative to pitch of the sensor pixels.
- all of the characteristics, parameters and/or configurations may be employed to model the projection of the microlens disk images onto the sensor surface.
- certain of the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be fixed or nearly fixed, constant or nearly constant, predetermined and/or implicit for a given or predetermined model, version or series of Light Field Data Acquisition Device.
- a lens may be designed such that the Exit Pupil does not vary from picture to picture. As a result, the Exit Pupil may be implicit.
- microlens array for a particular device model, version or series may be considered fixed (particularly in those situations where the manufacturing is within certain tolerances) and, as such, these characteristics, parameters and/or configurations may be predetermined or implied.
- microlens pitch, focal length, sensor pitch, and microlens pattern may also be constant across the focal plane for a particular device model, version or series of a particular model and, as such, these characteristics, parameters and/or configurations may be predetermined or implied.
- the Light Field Configuration Data may for some exemplary embodiments be categorized into three categories.
- the first of these categories may be referred to as “model static light field configuration data”, and is Light Field Configuration Data that is identical or nearly identical for all light field acquisition devices of a particular model, series or version of a particular model (for example, the pitch of a sensor pixel may be model static).
- the second of these categories may be referred to as “device static light field configuration data”, and may be Light Field Configuration Data that is fixed or nearly fixed for all light fields acquired by that device (for example, the x-offset, y-offset and rotation of the microlens array relative to the sensor surface may in some instances be device static), excluding model static light field configuration data.
- the third category may be referred to as “dynamic light field configuration data” and is Light Field Configuration Data that may vary between successive or a plurality of acquisitions from a given or particular Light Field Data Acquisition Device (for example, the zoom and/or optical focus position when acquisition via a given or particular device is performed).
- those characteristics, parameters and/or configurations of the Light Field Data Acquisition Device which are fixed, constant, predetermined and/or implicit may be determined (i) on an individual basis during and/or after manufacture, (ii) using empirical data of one or more Light Field Data Acquisition Devices, (iii) using statistical approximations, for example, based on one or more empirical data of one or more Light Field Data Acquisition Devices, and/or using computer-based modeling.
- such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be determined using any technique or device whether now known or later developed.
- data which is representative of such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be stored in memory in or on the Light Field Data Acquisition Device.
- model, version or series of a particular model of the light field acquisition device may be stored in memory and such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) may be determined therefrom.
- the one, some or all of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device is/are stored or recorded (in the same and/or a different data file) in memory in or on the Light Field Data Acquisition Device. (See, for example, FIGS. 1D and 1E ).
- the one, some or all of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be stored in resident memory, in a data file with the associated or corresponding Light Field Data and/or in a data file which is different from the file of the associated or corresponding Light Field Data.
- model and/or static light field configuration data may be stored or recorded in memory in or on the Light Field Data Acquisition Device before, during, concurrently with or after exposure (i.e., acquisition or sampling of the Light Field Data).
- the model and/or device static light field configuration data may be appended to the associated or corresponding Light Field Data prior to (for example, immediately prior to) communicating the associated or corresponding Light Field Data to a post-processing circuitry/system.
- the post-processing system may acquire the data which is representative of such constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) and the associated or corresponding Light Field Data (in the same or different data file)—and generate, manipulate and/or edit one or more images using the Light Field Data (for example, adjust the depth of focus) after acquisition or recording of such Light Field Data.
- the Light Field Data Acquisition Device the model and/or device static light field configuration data
- the associated or corresponding Light Field Data in the same or different data file
- one, some or all of the model and/or device static light field configuration data is/are stored in memory in or on the post-processing system.
- the post-processing system may determine one or more of the constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device based on, for example, data which is representative of the model, version or series of a particular model of the light field acquisition device.
- Such data which is representative of the model, version or series of a particular model of the light field acquisition device may be stored in a data file that is communicated to the post-processing system via the user (for example, via the user interface) and/or via the Light Field Data Acquisition Device (for example, in a data file containing (i) Light Field Data and (ii) the model static light field configuration data, or in a data file which is different from the Light Field Data).
- the memory in or on the post-processing system may include a look-up table or the like providing such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device.
- the user may input data, via the user interface, to indicate the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device which is associated with the Light Field Data.
- the post-processing system may correlate the Light Field Data with the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device via data stored in a data file and/or data provided by the Light Field Data Acquisition Device to the post-processing system (for example, in those instances where the Light Field Data Acquisition Device is connected to post-processing system).
- the data which is representative of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device data may be data of the model, version or series of a particular model of the light field acquisition device.
- one, some or all of the model and/or static light field configuration data is/are made available to the post-processing system through a predetermined retrieval system.
- the post-processing system may query a database from a local, networked and/or internet source or sources to recover one, some or all of the model and/or static light field configuration data.
- the post-processing system may check for and/or install software updates from a local, networked and/or external (for example, Internet) source or sources.
- Such data which is representative of the model, version or series of a particular model of the light field acquisition device may be stored in a data file that is communicated to the post-processing system via the user (for example, via the user interface) and/or via the Light Field Data Acquisition Device (for example, in a data file containing (i) Light Field Data and (ii) the model static light field configuration data, or in a data file which is different from the Light Field Data).
- characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are not fixed, constant, predetermined and/or implicit, such characteristics, parameters and/or configurations may be determined using any technique or device whether now known or later developed.
- one or more sensors are employed to determine the Exit Pupil or Aperture Function of the lens system of the Light Field Data Acquisition Device relative to the microlens array.
- one or more sensors may sense, detect and/or determine one or more of the size and/or shape and/or other characteristics of the Exit Pupil (relative to the microlens array) by sensing, detecting and/or determining the configuration of the lens system of the Light Field Data Acquisition Device. (See, for example, FIGS. 13A and 13B ).
- an image sensor array with known microlens array between it and the optical system is used to sense the exit pupil of the optical system. Based on the image signal that appears on the image sensor array, the shape and/or location of the exit pupil is deduced from the separation between the microlens disk images that appear under each microlens.
- the shape of the exit pupil may be determined by the shape of the individual microlens images (which may overlap)—for example, a circular image indicates a circular exit pupil, a hexagonal image indicates a hexagonal exit pupil and a square image indicates a square exit pupil.
- the shape may vary across the image sensor, indicating a change in the shape of the exit pupil from that apparent viewpoint on the sensor.
- the distance of the exit pupil from the microlens array and sensor may be determined by the pitch (distance between the relative centers) of the microlens images. As shown in FIG. 2D , a smaller pitch indicates a further distance, according to the linear geometric relationship shown in the Figure.
- the distance L between the optical exit pupil and the microlens array may be characterized the following equation:
- a sensor or other mechanism is used to detect, determine, measure, or keep track of the configuration of a zoom lens.
- a sensor may detect, determine and/or measure the position of a stepper motor used to drive the zoom lens, and this position may be used as an indicator of the zoom lens configuration.
- the configuration of the zoom lens may be combined with a database or table that maps the configuration to a pre-determined exit pupil configuration.
- the number of stepper motor positions may be discrete and finite, and an N-bit key may be used to uniquely denote each position, with each N-bit key corresponding to an entry in the database or table that corresponds to the pre-determined exit pupil configuration that relates to the corresponding stepper motor position.
- the Light Field Data Acquisition Device connects to a post-processing system
- such connection may be via wired and/or wireless architectures using any signaling technique now known or later developed.
- the configuration data may be provided and/or communicated to a post-processing system together with or separate from the associated Light Field Data using any format now known or later developed.
- the model and/or device static light field configuration data may be provided and/or communicated to a post-processing system together with or separate from dynamic light field configuration data (i.e., characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are not fixed, constant, predetermined and/or implicit).
- model and/or device static light field configuration data may be provided and/or communicated to a post-processing system upon initial connection and thereafter dynamic light field configuration data may be communicated to a post-processing system together with or separate from associated Light Field Data. All communication strategies, formats, techniques and/or architectures relating thereto are intended to fall within the scope of the present inventions.
- the Light Field Data Acquisition Device acquires, determines, stores and/or records data which is representative of the Exit Pupil and sensor pitch.
- the light field acquisition device acquires, determines, stores and/or records data or information pertaining to the lens configuration (for example, zoom position and range) when the Light Field Data is acquired, collected, sampled and/or obtained (i.e., at the time of “exposure” or when the “shot” is taken).
- Circuitry in the light field acquisition device may calculate, determine and/or estimate the location of the exit pupil using, for example, the lens configuration.
- an optical and/or geometric model for a light field capture system may include or provide information to convert or correlate data from an image sensor pixel to a representation of incoming light rays.
- the post-processing circuitry having a model to convert or correlate pixel values to the incoming light rays from the light field acquisition device, may perform Light Field Processing (for example, compute images including, for example, images having different focal planes, as well as computing images which correct for, capture or address artifacts).
- the present inventions in certain aspects, record, store and/or determine the optical parameters of the main lens system and the light field capture sensor which facilitates determining an optical and/or geometric model of certain aspects of the Light Field Data Acquisition Device.
- post-processing circuitry may generate an optical and/or geometric model that “maps” or correlates sensor pixels to geometric rays of light or sets of geometric rays.
- the Exit Pupil or Aperture Function may be considered a compact parameter of the Light Field Data Acquisition Device that describes or characterizes the lens system (which may include one or more lenses of any kind or type) of the acquisition device.
- post-processing circuitry may employ data which is representative of the Exit Pupil or Aperture Function (for example, size and/or shape of the exit pupil in some embodiments) to facilitate and/or allow Light Field Processing, including, for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device.
- the lens system may be characterized or represented by a set of lens formulas that describe the shape, refraction, and/or spacing between each of the lens elements. Such formulas or relationships may describe how light rays are determined to traverse or pass through the optical system before acquisition by the image sensor. Indeed, a characterization or representation of the how light rays will traverse or pass through the optical system facilitates ray-tracing computation of the ray distortion function which may be employed for correction of optical aberrations. (See, for example, Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006, page 135).
- the lens system may be described by formulas and discrete approximation of the Ray Correction Function (or ray distortion function) itself.
- vignetting of the lens affects light captured on the sensor surface
- An example of such vignetting is darkening of photographs towards the corner, due to eclipsing and/or reduction of the area and/or occlusion (for example, due to internal blockages by boundaries of lens elements or by apertures or by other opaque elements within the barrel of the lens) of the exit pupil from oblique views.
- Light fields captured with some lens systems may encounter artifacts around the edge of the image if the vignetting of the lens system is not characterized and modeled.
- the lens system is characterized, described and/or represented by the exit pupil parameter and a formula that characterizes the eclipsing of the exit pupil based on the pixel location. In this way, vignetting may be corrected, reduced, minimized, and/or eliminated by normalizing by the area of the eclipsed exit pupil at each pixel location.
- a lookup table or the like may be used to test if rays are subject to vignetting.
- a binary lookup table accessed using the “discretized” X, Y, U, and V components of a geometric ray or set of rays may be checked when a system is performing Light Field Processing. If the binary lookup table stores a false or zero value for the geometric ray parameters, the information (for example, a pixel value) associated with that geometric ray may be discarded.
- a numeric lookup table with values ranging from 0.0 to 1.0 may be checked when a system is performing Light Field Processing, accessed using the X, Y, U and V components of a geometric ray or set of rays.
- the information (for example, a pixel value) associated with the geometric ray parameters may be modified and/or adjusted (for example, by adjusting the pixel value to account for the occlusion or by using the lookup up value to normalize the pixel value) when a system is performing Light Field Processing.
- the lookup table may be obtained empirically, for example during a calibration step during the manufacture of the Light Field Data Acquisition Device, by using the device to acquire Light Field Data of a pre-determined Light Field (for example, a scene with constant and even (or nearly constant and nearly even) illumination, or otherwise predetermined and known scene or light field), and storing a lookup table of values normalized by dividing for each value in the Light Field Data, the empirically recorded value by the pre-determined scene or light field.
- the lookup table is used during Light Field Processing by normalizing each value in the Light Field Data by scaling it by the inverse of the matching value in the lookup table.
- the lookup table may be stored as a normalized sensor image in the Light Field Configuration Data that is supplied to Light Field Processing.
- each value is weighted proportional to the inverse of the corresponding image sensor value in the normalized sensor image (lookup table).
- the lookup table is represented by an analytic function that approximates the normalized sensor image (for example, for compactness, efficiency and/or optimization).
- the analytic function and/or approximation used may be a stored subset of the sensor image (for example, the values under one microlens) combined with a process or procedure to map or correlate sensor image pixels in other parts of the image to a corresponding location in the stored subset.
- the mapping or correlation process or procedure is to determine the 2D offset from a predetermined location, for example, the center of the closest microlens, and use the value in the stored subset at the same or nearly the same 2D offset from the center of the microlens in the stored subset. Indeed, methods for determining the 2D offset depend on the pattern of the microlens array, and mathematics are discussed below for exemplary embodiments that utilize data in the Light Field Configuration Data regarding the location of centers and radii of the microlens images in the sensor image.
- the optical model for converting from recorded pixel data to geometric light ray information may be constructed based on or using one or more of the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device that describe the light field sensor and the main lens system or optical system of the of the Light Field Data Acquisition Device.
- the model and/or device static light field configuration data may be stored in memory on the Light Field Data Acquisition Device, for example, non-volatile memory (such as, a ROM-like memory—for example, electrically programmable read-only memory (“EPROM”), electrically erasable programmable read-only memory (“EEPROM”) and/or Flash memory (for example, NOR or NAND)).
- non-volatile memory such as, a ROM-like memory—for example, electrically programmable read-only memory (“EPROM”), electrically erasable programmable read-only memory (“EEPROM”) and/or Flash memory (for example, NOR or NAND)
- EPROM electrically programmable read-only memory
- EEPROM electrically erasable programmable read-only memory
- Flash memory for example, NOR or NAND
- model and/or device static light field configuration data include the microlens pitch, microlens pattern, and/or sensor pixel pitch.
- the model and/or device static light field configuration data may
- the Size and shape of the exit pupil may be determined based on, for example, zoom and/or focus position of the optical system of the Light Field Data Acquisition Device.
- the Exit Pupil or Aperture Function presentation as related to one or more of the zoom and focus positions, may be predetermined and/or stored in memory (in the form of a look-up table, formula, or the like) on the Light Field Data Acquisition Device and/or in memory on an external post-processing system.
- Certain device static light field configuration data may vary for each individual Light Field Data Acquisition Device.
- certain characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be stored and/or updated during, for example, a registration procedure, after construction of the Light Field Data Acquisition Device.
- This data or information may be stored in non-volatile memory (for example, SRAM, NOR or NAND Flash or EEPROM) on or in the Light Field Data Acquisition Device, and, indeed, may be set as part of the device calibration process after construction of the acquisition device.
- the Light Field Data Acquisition Device may include one or more interchangeable lenses.
- the Light Field Data Acquisition Device may be provided with (for example, by the user via the user interface) and/or detect (for example, via data acquired from the interchangeable lens) details and/or changes to the optical system thereof.
- the Light Field Data Acquisition Device retrieves information from the interchangeable lens to determine the characteristics, parameters and/or configurations and/or changes thereto of the optical system.
- the user may input, via the user interface, the characteristics, parameters and/or configurations and/or changes thereto of the optical system. Such information may be passed using any communication techniques, circuitry, (electrical or mechanical) interfaces and/or architectures whether now known or later developed.
- the interchangeable lens i.e., the lens which is incorporated into or on the optical system of the Light Field Data Acquisition Device
- the Light Field Data Acquisition Device contains a lookup table in memory that correlates or “maps” (i) the zoom and focus of the optical system to (ii) representation of an Exit Pupil (or Aperture Function).
- This Exit Pupil may be provided to post-processing circuitry (disposed on the Light Field Data Acquisition Device and/or a stand-alone post-processing system) to facilitate and/or enable Light Field Processing.
- the Exit Pupil may be determined by a mathematical relationship based on the zoom and focus of the optical system.
- the determination of the size and shape of the exit pupil may depend on different parameters (for example, in an embodiment with a fixed zoom position, the exit pupil may vary only with changes in the focus position).
- a firmware update is applied to the Light Field Data Acquisition Device in the event that an interchangeable lens is incorporated into optical system.
- This update may be implemented as a “patch” and may be installed by the user or may be installed automatically when the lens is first coupled to the Light Field Data Acquisition Device.
- the firmware update may provide a mechanism for determining certain optical parameters of the optical or lens system based on information the lens may provide at exposure or Light Field Data collection, acquisition and/or capture.
- the firmware update may allow the Light Field Data Acquisition Device to look up data representing the Exit Pupil or Aperture Function of the lens based on the configuration of the lens (for example, one or more predetermined zoom and focus positions of the lens).
- memory in the Light Field Data Acquisition Device includes data of one or more interchangeable lenses that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device.
- the memory (for example, non-volatile memory) includes data which is representative of the characteristics, parameters and/or configurations of a plurality of interchangeable lenses that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device.
- memory resident in the Light Field Data Acquisition Device may contain a lookup table that “maps” (i) the zoom and focus to (ii) data representing the Exit Pupil.
- the resident memory includes a plurality of mathematical relationships wherein a selected one of the predetermined mathematical relationships, based on a particular interchangeable lens implemented or incorporated into the optical system of the Light Field Data Acquisition Device, is employed to determine the exit pupil size and/or shape based on, for example, a particular zoom and focus of the optical system.
- the memory of the Light Field Data Acquisition Device contains a database of all available interchangeable lenses that will fit the body. Each entry may be keyed by information made available to the camera by the lens system.
- the key may be unique in the form of a lens model number, or unique by a combination of available parameters such as min zoom, max zoom, and f-number. Indeed, this key may be used to lookup a particular or predetermined mathematical relationship for determining the exit pupil size and/or shape—for example, by converting from the current zoom and focus position of the lens to the exit pupil location.
- an interchangeable lens when an interchangeable lens is attached to a Light Field Data Acquisition Device, that device may query an external source or sources (for example, an Internet-capable camera may query a networked database) for updates using a wired (for example, a USB cable connection to a local computer) or wireless (for example, a Wi-Fi enabled device) connection.
- the Light Field Data Acquisition Device may query the external source or sources the first time, each time or any time an interchangeable lens is attached to the device to check for and/or update data which is representative of the characteristics, parameters and/or configurations of a plurality of interchangeable lens that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device.
- zoom and focus are often used as exemplary characteristics of a lens' optical configuration. Wherever “zoom and focus” are used herein in this context, it should be understood that any subset of the characteristics of an optical systems' configuration, and indeed any of the representations of the optical and/or geometric models of the acquisition device, may be substituted in place of zoom and focus characteristics, or in addition thereto, and such substitutions and generalizations are intended to fall within the scope of the present inventions.
- data which is representative of the microlens array may be recorded or stored to allow for images to be processed via post-processing circuitry.
- data may be stored in a separate configuration data file or together with the Light Field Data file.
- the configuration data file may be associated with one or more files each containing Light Field Data.
- the configuration data file may be stored in a header of the electronic file that stores the Light Field Data.
- the configuration data file may be stored in a separate electronic file relative to the electronic file including the associated Light Field Data.
- the electronic file including the configuration data may be associated with and separate from a plurality of electronic files each containing different Light Field Data.
- the relevant/associated Light Field Configuration Data may be stored in a Standard Image Format, in a header in the Light Field Data file.
- the header includes, among other things, Light Field Configuration Data (for example, including data which is representative of the characteristics, parameters and/or configurations of the optical system).
- Post-processing circuitry in, for example, a stand-alone post-processing system, may read or interpret the header of the data file to facilitate construction of a model to use for processing the associated or corresponding Light Field Data.
- the post-processing circuitry may convert or interpret data from the image sensor to perform Light Field Processing, for example, generate, manipulate and/or edit one or more images using the Light Field Data (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device).
- the post-processing circuitry may convert or interpret Light Field Data from image sensor pixel locations to a representation of incoming light rays.
- the system acquires or determines data which is representative of the center locations and sizes of the projected microlens disks on the surface of the imaging sensor.
- the locations, size and shape of the projected disks are overlayed onto the captured image.
- Information of the locations, size and shapes of the projected image of the lenslets may be employed by post-processing circuitry for Light Field Processing. Indeed, determining the centers and sizes of microlens disks may be determined based on the key optical parameters listed previously, for example using the calculation procedures described in an exemplary embodiment below.
- the system may store or record data which is representative of the (i) X and Y offset of center lenslet projection to the center of the image sensor (or any other offsets to represent translation of microlens array relative to the image sensor), (ii) rotation of microlens array relative to imaging sensor, (iii) microlens grid pattern (for example, hexagonal or square), (iv) radius of the projected lenslets, and/or (v) spacing between neighboring centers of projected lenslets.
- Such data may be employed by the post-processing circuitry to determine an optical and/or geometric model that may be used for Light Field Processing (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device).
- the X and Y offset values are the spatial distance between the center pixel on the sensor and the center of a central projected microlens disk (in those situations where the microlens include a disk shape).
- the spacing between neighboring disk centers is the pitch of the projected microlens array.
- the radius of each projected lenslet disk may be considered the extent of the disk of light projected by a lenslet in the microlens array (See FIGS. 14A-14C ). Note that although the diameter of the projected disks appears approximately the same size in the illustration as the pitch, the numbers are different and are used for differing purposes.
- circuitry may construct a geometric or optical model which converts sensor locations (for example, the X and Y location of pixel coordinates) into information representing a set of incoming light rays in the following manner:
- the X and Y offsets specify the location of the image formed by a central microlens on the surface of the sensor, referred to as MLXOnSensor and MLYOnSensor, respectively.
- the size of the image formed by the microlens is specified by the radius of the projected lenslet, referred to as MLROnSensor.
- the pixel may be considered as in the projected image of specified microlens.
- the locations of the centers of all projected microlens images onto the sensor surface may be determined by adding the spacing between neighboring lenslets, accounting for rotation.
- the 4 neighboring microlenses are centered at the following locations:
- the location of and spacing of the projection of the microlens disk on the sensor surface may determine the location and extents in X and Y components/coordinates of the 4 dimensional set of geometric rays.
- the X and Y components of all pixels contained in a projected microlens image may be considered centered at the center of the microlens projection and have the same size as the entire area of the microlens.
- the U and V components may be considered angular information and may in some embodiments be determined in the following manner for a pixel centered at PXOnSensor, PYOnSensor under a microlens centered at MLXOnSensor, MLYOnSensor:
- V (PYOnSensor ⁇ MLYOnSensor)/MLSpacing
- U and V are in a normalized coordinate space ranging between ⁇ 0.5 and 0.5.
- any sensor pixels not illuminated may not be used for Light Field Processing.
- the locations and sizes of the projected lenslet disks onto the captured image are regular or near regular in appearance.
- the system may store or record data which is representative of the (i) location and orientation of microlens array relative to the imaging sensor in 3-space, (ii) microlens grid pattern (for example, hexagonal or square), (iii) lens formulas or approximations for microlens array, (iv) lens formulas and spacings, or approximations for the main lens system, and/or (v) location and orientation of the light field sensor relative to the main lens system. (See, for example, FIGS. 5 and 6 ). Such data may be employed by the post-processing circuitry to determine a model of the optical path of the Light Field Data Acquisition Device.
- the model may be employed by post-processing circuitry to perform Light Field Processing (for example, generate, manipulate and/or edit one or more images using the light field image data (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the light field image data by the Light Field Data Acquisition Device)).
- Light Field Processing for example, generate, manipulate and/or edit one or more images using the light field image data (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the light field image data by the Light Field Data Acquisition Device)).
- the post-processing system acquires and/or determines (i) the x-offset of a central lenslet in the microlens array relative to the center of the imaging sensor, (ii) the y-offset of a central lenslet in the microlens array relative to the center of the imaging sensor, (iii) the rotation of the microlens array (iv) the separation of the microlens array from the imaging sensor, (v) the pitch of the microlens array, (vi) the pattern of the microlens array, and (vii) the location of the center of the exit pupil relative to the microlens array.
- the system provides system, electronic, scene-dependent and/or optical characteristics, parameters, properties, models, and/or configurations via a lookup system.
- the Light Field Data Acquisition Device stores or saves the Light Field Data with one or more keys identifying the optical system or components of the optical system of the acquisition device.
- a Light Field Data Acquisition Device may store a plurality of keys (for example, two keys), wherein one key may uniquely identify the Light Field Data Acquisition Device, and another key may uniquely identify the characteristics, parameters and/or configurations of the acquisition device when the Light Field Data was taken, acquired, and/or captured.
- the Light Field Data Acquisition Device may have a fixed number of zoom configurations and a unique identifier for each of the zoom configurations—wherein each correspond to a predetermined key.
- the Light Field Data Acquisition Device may (in addition thereto or in lieu thereof) store or save an N-bit key to identify the characteristics, parameters and/or configurations of the acquisition device associated with or corresponding to the acquired or captured Light Field Data.
- the Light Field Data Acquisition Device includes a key uniquely identifying one, some or all of the (dynamic and/or static) exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device, including:
- the relative offsets and rotation of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device or may vary between Light Field Data Acquisition Devices (even between models, versions or pieces thereof). (See, FIG. 6 ); and/or
- the N-bit key may be provided to post-processing circuitry which, using a look-up table or the like, may construct or reconstruct optical properties of the Light Field Data Acquisition Device.
- the post-processing circuitry may access a resident memory, local database or query an external source (for example, an Internet source) for the optical information associated with the N-bit key.
- one some or all of the keys may be encodings or representations of values for specific characteristics, parameters, models and or configurations within the Light Field Configuration Data.
- the focal length of the zoom configuration may be represented or stored as an N-bit key, where the value of the N-bits encode an N-bit floating point bit-pattern for the focal length of the zoom position in millimeters.
- each of the embodiments may be employed alone or in combination with one or more of the other embodiments.
- the optical model or data of the Light Field Data Acquisition Device may be represented using a combination of storing some configuration parameters as well as some information uniquely identifying certain elements or parts of the acquisition device (for example, based on the N-bit key embodiment discussed above).
- post-processing circuitry may read or interpret the data file(s) of the image data, the configuration data and the N-bit key, to create or recreate an optical model of the Light Field Data Acquisition Device which was employed to acquire and/or capture the Light Field Data (i.e., the optical model of the acquisition system which is associated with the Light Field Data).
- the translation and rotation configuration parameters of the microlens array relative to the image sensor are stored, recorded or saved to a file as well as the camera model number and zoom position.
- a suitable geometric and/or optical model may be constructed by determining or looking up the optical system of the camera model at the particular zoom location (based on the N-bit key), and then applying the translation and rotation parameters of the microlens array to more fully express the geometric and/or optical model of the acquisition system which is associated with the Light Field Data.
- the data processing, analyses, computations, generations and/or manipulations may be implemented in or with circuitry disposed (in part or in whole) in/on the camera or in/on an external post-processing system.
- Such circuitry may include one or more microprocessors, Application-Specific Integrated Circuits (ASICs), digital signal processors (DSPs), and/or programmable gate arrays (for example, field-programmable gate arrays (FPGAs)).
- ASICs Application-Specific Integrated Circuits
- DSPs digital signal processors
- FPGAs field-programmable gate arrays
- the circuitry may be any type or form of circuitry whether now known or later developed.
- the post-processing circuitry may include a single component or a multiplicity of components (microprocessors, FPGAs, ASICs and DSPs), either active and/or passive, which are coupled together to implement, provide and/or perform a desired operation/function/application; all of which are intended to fall within the scope of the present invention.
- circuit may mean, among other things, a single component (for example, electrical/electronic) or a multiplicity of components (whether in integrated circuit form, discrete form or otherwise), which are active and/or passive, and which are coupled together to provide or perform a desired function.
- circuitry may mean, among other things, a circuit (whether integrated, discrete or otherwise), a group of such circuits, one or more processors, one or more state machines, one or more processors implementing software, or a combination of one or more circuits (whether integrated, discrete or otherwise), one or more state machines, one or more processors, and/or one or more processors implementing software.
- opticals means a system comprising a plurality of components used to affect the propagation of light, including but not limited to lens elements, windows, apertures and mirrors.
- the post-processing circuitry may perform or execute one or more applications, routines, programs and/or data structures that implement particular methods, techniques, tasks or operations described and illustrated herein.
- the functionality of the applications, routines or programs may be combined or distributed.
- the applications, routines or programs may be implementing by the post-processing circuitry using any programming language whether now known or later developed, including, for example, assembly, FORTRAN, C, C++, and BASIC, whether compiled or uncompiled code; all of which are intended to fall within the scope of the present invention.
- a Light Field Data File is an electronic data file which includes one or more sets of Light Field Data. (See, for example, FIGS. 15A and 15B ).
- the Light Field Data File may include one or more sets of Light Field Data which, in whole or in part, is compressed or uncompressed and/or processed or unprocessed.
- a set of Light Field Data may be data of a scene, image or “exposure” acquired, captured and/or sampled via a Light Field Data Acquisition Device.
- the Light Field Data File may include any file format or structure, whether the data contained therein is, in whole or in part, in compressed or uncompressed form, and/or whether the data contained therein is, in whole or in part, processed or unprocessed.
- file format or structure of the Light Field Data file includes a start code and/or end code to indicate the beginning and/or end, respectively, of a set of a Light Field Data.
- the set of Light Field Data may include a predetermined or predefined amount of data. (See, for example, FIG. 15A ).
- the start or end of a given set of Light Field Data may be indirectly based on an amount of data (with or without start and/or end codes).
- any file format or file structure of the Light Field Data file is intended to fall within the scope of the present invention.
- the file format or structure of the Light Field Data may include metadata (for example, as a header section). (See, for example, FIG. 15B wherein in this exemplary embodiment, such header is located in the beginning of the file—although it need not).
- the metadata may be definitional type data that provides information regarding the Light Field Data and/or the environment or parameters in which it was acquired, captured and/or sampled.
- the metadata may include and/or consist of Light Field Configuration Data.
- the Light Field Data File may include one or more sets of Light Field Data of an image or “exposure” acquired, captured and/or sampled via a Light Field Data Acquisition Device.
- the file includes a plurality of sets of Light Field Data
- such sets may be a series of images or exposures (temporally contiguous) or a plurality of images that were acquired using the same or substantially the same acquisition settings.
- the header section may include and/or consist of Light Field Configuration Data that is applicable to the plurality of sets of Light Field Data.
- the Light Field Data File may be stored and/or maintained in memory (for example, DRAM, SRAM, Flash memory, conventional-type hard drive, tape, CD and/or DVD). (See, for example, FIGS. 16A-16C ).
- memory for example, DRAM, SRAM, Flash memory, conventional-type hard drive, tape, CD and/or DVD.
- Such memory may be accessed by post-processing circuitry to perform Light Field Processing (for example, generating, manipulating and/or editing the image data corresponding to the Light Field Data—after acquisition or recording thereof (including, for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field after acquisition of the Light Field Data)).
- the memory may be internal or external to the Light Field Data Acquisition Device. Further, the memory may be discrete or integrated relative to the circuitry in the Light Field Data Acquisition Device. In addition thereto, or in lieu thereof, the memory may be discrete or integrated relative to the post-processing circuitry/system.
- the post-processing circuitry may access the Light Field Data File (having one or more sets of Light Field Data) and, based on or in response to user inputs or instructions, perform Light Field Processing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field of an image after acquisition of Light Field Data associated with such image). Thereafter, the post-processing circuitry may store the image within the Light Field Data File (for example, append such image thereto) and/or overwrite the associated Light Field Data contained in the Light Field Data File. In addition thereto, or in lieu thereof, the post-processing circuitry may create (in response to user inputs/instructions) a separate file containing the image which was generated using the associated Light Field Data.
- Light Field Processing for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field of an image after acquisition of Light Field Data associated with such image.
- the post-processing circuitry may store the image within the Light Field Data File (for example, append such image there
- This process may be repeated to perform further Light Field Processing and generate additional images using the Light Field Data (for example, re-adjust, re-select and/or re-define a second focus and/or a second depth of field of the second image associated with the same Light Field Data as the first image—again after acquisition of Light Field Data associated with such images).
- Light Field Data for example, re-adjust, re-select and/or re-define a second focus and/or a second depth of field of the second image associated with the same Light Field Data as the first image—again after acquisition of Light Field Data associated with such images).
- the Light Field Data Acquisition Device may include a display to allow the user to view an image or video generated using one or more sets of Light Field Data. (See for example, FIGS. 16C and 16D ).
- the display may facilitate the user to implement desired or predetermined Light Field Processing of one or more sets of Light Field Data in a Light Field Data File.
- the Light Field Data Acquisition Device may also couple to an external display as well as, for example, a recording device, memory, printer and/or processor circuitry (See, for example, FIGS. 16E and 16F ).
- the Light Field Data Acquisition Device or post-processing circuitry may output image data to display, processor circuitry (for example, a special purpose or general purpose processor), and/or a video recording device. (See, for example, FIGS. 16E and 16F ).
- processor circuitry for example, a special purpose or general purpose processor
- a video recording device See, for example, FIGS. 16E and 16F .
- such external devices or circuitry may facilitate, for example, storage of Light Field Data Files and Light Field Processing of Light Field Data Files.
- the Light Field Data Acquisition Device (and/or the post-processing system) may communicate with memory (which may store the electronic data files having one or more sets of Light Field Data and/or Light Field Configuration Data) via write circuitry and read circuitry. (See, FIG. 16G ).
- the write and read circuitry may couple to processing circuitry to implement, for example, Light Field Processing which generates, manipulates and/or edits (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field) the image data corresponding to the Light Field Data—after acquisition or recording thereof.
- the processing circuitry may generate electronic data files including Light Field Data (for example, in a compressed or non-compressed form.
- Light Field Data may include the Light Field Data which is interleaved, threaded, watermarked, encoded, multiplexed and/or meshed into the data of the Standard Image Format.
- Light Field Configuration Data may be stored in a header or in an electronic file that is separate from the electronic file(s) containing the associated Light Field Data. (See, for example, FIGS. 15B and 15C ). Where the Light Field Configuration Data is stored in a separate electronic file, such file may be stored and/or maintained in memory (for example, DRAM, SRAM, Flash memory, conventional-type hard drive, tape, CD and/or DVD). (See, for example, FIGS. 16A-16C ).
- such memory may be accessed by post-processing circuitry to perform Light Field Processing (for example, generating, manipulating and/or editing the image data corresponding to the Light Field Data—after acquisition or recording thereof (including, for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field after acquisition of the Light Field Data using the Light Field Configuration Data)).
- the memory may be internal or external to the Light Field Data Acquisition Device and/or the post-processing system.
- the memory may be discrete or integrated relative to the circuitry in the Light Field Data Acquisition Device.
- the memory may be discrete or integrated relative to the post-processing circuitry/system.
- one or more sets of Light Field Data may be appended to or integrated into image data in or having a Standard Image Format. (See, for example, FIGS. 17A-17C ).
- the one or more sets of Light Field Data is/are associated with the image data in a Standard Image Format in that such one or more sets of Light Field Data may be used to generate the image which is represented in the Standard Image Format.
- the Light Field Data may include one or more sets of Light Field Data which, in whole or in part, is compressed or uncompressed and/or processed or unprocessed.
- the Standard Image Format may be an open format or a proprietary format.
- the Light Field Data in the Standard Image Format—Light Field Data File may include any of the attributes and/or characteristics discussed above in conjunction with the Light Field Data File.
- the electronic data file may include metadata.
- the Light Field Data may include metadata (for example, Light Field Configuration Data in a header section). (See, for example, FIG. 17C wherein in this exemplary embodiment, such header is located in the beginning of the file—although it need not).
- the metadata of the Light Field Data may be incorporated into the metadata associated with the Standard Image Format.
- the Light Field Data is interleaved, threaded, watermarked, encoded, multiplexed and/or meshed into the data of the Standard Image Format. (See, for example, FIG. 17D ).
- processing or reading circuitry may extract and/or decode the data of the image in the Standard Image Format relative to the data set(s) of the Light Field Data.
- the non-light field raw image data which was employed to generate the image data in the Standard Image Format may be appended to or integrated into image data in a Standard Image Format.
- the raw image data (which may or may not be Light Field Data) which is associated with the image data in a Standard Image Format is stored in the same file as the image data in a Standard Image Format.
- Such raw image data may be compressed or uncompressed and/or processed (in whole or in part) or unprocessed.
- such raw data is a representation of the original single-channel raw pixel values read off a sensor with a color mosaic filter array (for example, Bayer color filter array).
- such raw pixel values are from a Light Field Data Acquisition Device, and hence the raw data is a representation of Light Field Data recorded by such a device.
- Light Field Configuration Data may be stored in the header or in an electronic file that is separate from the associated Light Field Data. (See, for example, FIGS. 15C , 17 B and 17 C). Where the Light Field Configuration Data is stored in a separate electronic file, such file may be stored and/or maintained in memory and accessed during processing of the image corresponding to or in the Standard Image Format and/or the Light Field Data (for example, as discussed immediately below).
- circuitry may access a Data File illustrated in FIGS. 17A-17F (for example, a Standard Image Format—Light Field Data File) and read or display the image corresponding to the Standard Image Format. Thereafter, and based on or in response to one or more inputs or instructions (for example, user inputs or instructions), the image corresponding to the Standard Image Format may be modified using, for example, the one or more data sets of the Light Field Data associated with the image.
- a Data File illustrated in FIGS. 17A-17F for example, a Standard Image Format—Light Field Data File
- the image corresponding to the Standard Image Format may be modified using, for example, the one or more data sets of the Light Field Data associated with the image.
- circuitry may perform Light Field Processing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field of an image after acquisition of Light Field Data associated with such image (wherein the image during acquisition included an original focus and depth of field)) to modify the image and thereby provide a new image (having, for example, a new focus and/or depth of field).
- the circuitry may store or re-store the image within the Data File (for example, (i) replace or overwrite the previous image by storing data in the Standard Image Format which is representative of the new image or (ii) append data in the Standard Image Format which is representative of the such new image).
- the circuitry may create (in response to, for example, user inputs/instructions) a separate file containing data corresponding to the new image.
- a separate file containing data corresponding to the new image.
- the data in the Standard Image Format
- such new or separate file may or may not contain the Light Field Data associated therewith.
- the circuitry when the circuitry is performing Light Field Processing to generate the modified image, the circuitry may employ the Standard Image Format—Light Field Data File (or a portion thereof) as a frame buffer.
- Standard Image Format Light Field Data File (or a portion thereof)
- Such a technique provides for efficient use of memory resources.
- the present inventions utilize the standard image portion of the Standard Image Format—Light Field Data File as a “File Framebuffer.” Specifically, this File Framebuffer that represents the pixels to be displayed, is displayed on any display via any Standard Display Mechanism (i.e. method or system, whether now known or developed in the future, that may read, interpret and/or display the standard image portion of the Data File).
- Standard Display Mechanism i.e. method or system, whether now known or developed in the future, that may read, interpret and/or display the standard image portion of the Data File.
- the Standard Display Mechanism may, for example, be one of: a web browser; an image viewer that is possibly integrated with an operating system; a third-party piece of software for image organization, viewing editing and/or slideshows; an internet-based photo sharing website or service; a printing service such as a kiosk at a departmental store; and an internet-based printing service that enables upload of Standard Image Formats.
- a web browser an image viewer that is possibly integrated with an operating system
- a third-party piece of software for image organization, viewing editing and/or slideshows an internet-based photo sharing website or service
- a printing service such as a kiosk at a departmental store
- an internet-based printing service that enables upload of Standard Image Formats.
- such Standard Display Mechanisms may not be able to interpret, process and/or display the Light Field Data portion of the Standard Image—Light Field Data File.
- the modify—store/restore component makes use of the Light Field Data portion of the file in order to create a modified image through Light Field Processing, replacing the “File Framebuffer” in order to provide new pixels for Standard Display Mechanisms.
- the “File Framebuffer” serves as a persistent store of pixels for the present invention to store/restore the effect of the “modify” component for potential display on any Standard Display Mechanism.
- the process of read/display—modify—store/re-store may include many permutations and/or combinations. For example, after modifying the image (using the Light Field Data which is associated therewith) to generate the new image, such new image may be re-read or re-displayed. (See, for example, FIG. 18B ).
- the user may instruct the circuitry to perform a re-modify (i.e., modify the original image again or modify the new image).
- a re-modify i.e., modify the original image again or modify the new image.
- All permutations and/or combinations of read/display—modify—store/re-store are intended to fall within the scope of the present inventions (see, for example, FIGS. 18D and 18E ); however, for the sake of brevity, such permutations and/or combinations of read/display—modify—store/re-store will not be discussed separately herein.
- the circuitry may, in response to user inputs or instructions, generate a new Standard Image Format—Light Field Data File (wherein the Light Field Data may be substantially unchanged) or generate a Standard Image File only (i.e., discard the Light Field Image Data).
- the user may also instruct the circuitry to change the standard format of the Standard Image File prior to storing or re-storing the data (in the selected Standard Image Format) which is representative of the modified image.
- the read/display—modify—store/re-store process is also applicable to the Standard Image Format—Raw Image Data File illustrated in FIGS. 17D and 17E .
- the process in connection with the Standard Image Format—Raw Image Data File is substantially similar to the process for the Standard Image Format—Light Field Image Data File (discussed immediately above) and, as such, for the sake of brevity, the discussion will not be repeated.
- the modify portion of the process includes any type of processing that is accessible and/or possible from the Raw Image data.
- such type of processing may not be accessible and/or possible from the Standard Image data alone.
- processing includes, but is not limited to: changing the white-balance information to affect appearance of color; changing the exposure level to brighten or darken the image; applying dynamic range alteration in order to, for example, reducing the dynamic range by raising the brightness of the dark areas and reducing the brightness of the light areas.
- any type of image processing that is applicable to Raw Image data is intended to fall within the scope of the present inventions.
- a user may utilize any Standard Display Mechanism to view the Standard Image portion of a Data File (for example, the exemplary electronic data files of FIGS. 17A-17F ), reading and/or displaying the image corresponding to the Standard Image Format; a user may also utilize the read/display—modify—store/re-store process described above, possibly changing the Standard Image and/or Light Field Data within the Data File.
- a user may subsequently utilize a Standard Display Mechanism on the resulting Data File, for example to view or share the image via the internet, or to print it via a printing service. A user may subsequently repeat this process any number of times.
- the user acquires a Data File in a Standard Image Format—Light Field Data format, for example, via a recording from a Light Field Data Acquisition Device.
- the Light Field Data Acquisition Device in response to inputs/instructions (for example, user inputs/instructions), communicates the Data File to a computer system, which includes a image viewing computer program (for example, Standard Display Mechanisms) to provide and allow viewing of the Data File as an image on a display.
- a computer system which includes a image viewing computer program (for example, Standard Display Mechanisms) to provide and allow viewing of the Data File as an image on a display.
- the Light Field Data Acquisition Device and/or computer system in response to inputs/instructions (for example, user inputs/instructions), may also communicate the Data File to an internet image sharing site (another Standard Display Mechanism), for example, in order to facilitate sharing of the Data File.
- an internet image sharing site another Standard Display Mechanism
- Another user may then download the Data File from the sharing site, and view it on a computer (Standard Display Mechanism which is, for example, local to the second user).
- the second user employing a computer system, may open the Data File with a software program that implements the Read/Display—Modify—Store/Re-store process.
- the second user views the image, and applies or implements Light Field Processing (for example, changes the optical focus of the image on to a closer focal plane), and stores the resulting image into the File Framebuffer comprising the Standard Image portion of the Data File in the Standard Image Format.
- the second user then prints the Data File using a printer (another Standard Display Mechanism).
- the second user may then upload the Data File to an Internet image sharing site (Standard Display Mechanism), which may be the same or a different sharing site.
- an Internet image sharing site (Standard Display Mechanism), which may be the same or a different sharing site.
- Another user i.e., the first user or a third user) downloads the Data File and prints it.
- the preceding scenario illustrates certain aspects and exemplary embodiments of the present invention.
- the Light Field Data Acquisition Device and/or post-processing system may include a user interface to allow a user/operator to monitor, control and/or program acquisition device and/or post-processing system. (See, for example, FIGS. 1B , 1 C, 1 E, 1 F, 16 C, 16 F, and 20 ). With reference to FIG. 1B , 1 C, 1 E, 1 F, 16 C, 16 F, and 20 ). With reference to FIG.
- user interface may include an output device/mechanism (for example, printer and/or display (Standard Display Mechanism) and/or user input device/mechanism (for example, buttons, switches, touch screen, pointing device (for example, mouse or trackball) and/or microphone) to allow a user/operator to monitor, control and/or the program operating parameters and/or characteristics of the Light Field Data Acquisition Devices and/or post-processing circuitry/system (for example, (i) the rates of acquisition, sampling, capture, storing and/or recording of Light Field Data, (ii) the focal plane, field of view or depth of field of acquisition device, and/or (iii) the post-processing operations implemented by the post-processing circuitry/system).
- an output device/mechanism for example, printer and/or display (Standard Display Mechanism) and/or user input device/mechanism (for example, buttons, switches, touch screen, pointing device (for example, mouse or trackball) and/or microphone) to allow a user/operator to
- the Light Field Data Acquisition Device connects to a post-processing system
- such connection may be via wired and/or wireless architectures using any signaling technique now known or later developed.
- the configuration data may be provided and/or communicated to a post-processing system together with or separate from the associated Light Field Data using any format know known or later developed.
- the static-type configuration data may be provided and/or communicated to a post-processing system together with or separate from dynamic-type configuration data. All communication strategies, designs, formats, techniques and/or architectures relating thereto are intended to fall within the scope of the present inventions.
- circuits and circuitry disclosed herein may be described using computer aided design tools and expressed (or represented), as data and/or instructions embodied in various computer-readable media, in terms of their behavioral, register transfer, logic component, transistor, layout geometries, and/or other characteristics. Formats of files and other objects in which such circuit expressions may be implemented include, but are not limited to, formats supporting behavioral languages such as C, Verilog, and HLDL, formats supporting register level description languages like RTL, and formats supporting geometry description languages such as GDSII, GDSIII, GDSIV, CIF, MEBES and any other suitable formats and languages.
- Computer-readable media in which such formatted data and/or instructions may be embodied include, but are not limited to, non-volatile storage media in various forms (for example, optical, magnetic or semiconductor storage media) and carrier waves that may be used to transfer such formatted data and/or instructions through wireless, optical, or wired signaling media or any combination thereof.
- Examples of transfers of such formatted data and/or instructions by carrier waves include, but are not limited to, transfers (uploads, downloads, e-mail, etc.) over the Internet and/or other computer networks via one or more data transfer protocols (for example, HTTP, FTP, SMTP, etc.).
- Such data and/or instruction-based expressions of the above described circuits may be processed by a processing entity (for example, one or more processors) within the computer system in conjunction with execution of one or more other computer programs including, without limitation, net-list generation programs, place and route programs and the like, to generate a representation or image of a physical manifestation of such circuits.
- a processing entity for example, one or more processors
- Such representation or image may thereafter be used in device fabrication, for example, by enabling generation of one or more masks that are used to form various components of the circuits in a device fabrication process.
- light field data means Light Field Data
- light field configuration data means Light Field Configuration Data
- aperture function means Aperture Function
- exit pupil means Exit Pupil
- light field processing means Light Field Processing
- light field data file means Light Field Data File
- optical model means optical and/or geometric model
- standard image format means Standard Image Format
- circuit means, among other things, a single component (for example, electrical/electronic) or a multiplicity of components (whether in integrated circuit form, discrete form or otherwise), which are active and/or passive, and which are coupled together to provide or perform a desired operation.
- circuitry means, among other things, a circuit (whether integrated or otherwise), a group of such circuits, one or more processors, one or more state machines, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays, or a combination of one or more circuits (whether integrated or otherwise), one or more state machines, one or more processors, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays.
- data means, among other things, a current or voltage signal(s) (plural or singular) whether in an analog or a digital form, which may be a single bit (or the like) or multiple bits (or the like).
- opticals means one or more components and/or a system comprising a plurality of components used to affect the propagation of light, including but not limited to lens elements, windows, microlens arrays, apertures and mirrors.
Abstract
Description
- This application claims priority to U.S. Provisional Application Ser. No. 61/170,620, entitled “Light Field Camera Image, File and Configuration Data, and Method of Using, Storing and Communicating Same”, filed Apr. 18, 2009. The contents of U.S. Provisional Application Ser. No. 61/170,620 are incorporated by reference herein, in their entirety.
- In one aspect, the present inventions are directed to, among other things, Light Field Data Acquisition Devices (as defined in the Detailed Description, for example, light field cameras) that acquire Light Field Data (as also defined in the Detailed Description) or information, post-processing systems relating to such devices, and methods of using such cameras and systems. In another aspect, the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device used in post-processing of the image data captured or acquired thereby. In yet another aspect, the present inventions are directed to providing or communicating (i) such characteristics, parameters and/or configurations and/or (ii) information which is representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the image data acquisition device (for example, an optical and/or a geometric model of the image data acquisition device that is associated with certain acquired Light Field Data). Notably, such characteristics, parameters and/or configurations of the light field camera facilitate such cameras and/or systems to generate, manipulate and/or edit Light Field Data (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition or recording of the Light Field Data and/or information) of, for example, a scene. (See, for example, United States Patent Application Publication 2007/0252074, and the provisional applications to which it claims priority (namely, Ser. Nos. 60/615,179 and 60/647,492), and Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006, all of which are incorporated here in their entirety by reference; and the block diagram illustration of a light field camera in
FIGS. 1A and 1B ). - In one embodiment, the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may provide information which is representative of an optical and/or a geometric model of the image data acquisition device (which may include, for example, the camera optics (for example, one or more lenses of any kind or type), imaging sensors to obtain and/or acquire the Light Field Data or information, and relative distances between the elements of the image data acquisition device). In this way, post-processing circuitry (for example, circuitry which is disposed in or integrated into an image data acquisition device (see
FIG. 1B ) or post-processing circuitry which is external to the image data acquisition device (seeFIG. 1C )) may obtain, receive, acquire and/or determine such characteristics, parameters and/or configurations of the Light Field Data Acquisition Device and may determine, analyze and/or interpret the ray-geometry corresponding to one, some or all of imaging sensor pixel values associated with the imaging sensor in order to generate, manipulate and/or edit image data and/or information of, for example, a scene (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition and/or recording of the image data or information). - The data which is representative of the characteristics, parameters and/or configurations (hereinafter collectively “configuration data”) of the Light Field Data Acquisition Device may be obtained, determined and/or recorded before, during and/or after collection or acquisition of Light Field Data by the imaging sensor of the acquisition device (for example, light field camera). Such configuration data may be stored in the same data file and/or file format as the associated Light Field Data or in a different data file and/or different file format as the associated Light Field Data. In certain embodiments, the configuration data file is associated with a plurality of files each containing Light Field Data.
- Where post-processing is performed “off-camera” or in a device or system separate from the Light Field Data Acquisition Device, such configuration data may be transmitted, provided and/or communicated to a external post-processing system together with or separate from the image data. (See, for example,
FIG. 1C ). Indeed, the data may be transmitted serially or in parallel with the electronic data files containing the Light Field Data. - Notably, a characteristic of a Light Field Data Acquisition Device provides the user the ability to compute images that are focused over a range of depths, corresponding to a range of virtual image planes about the physical plane where the light field sensor (which may include a microlens array and a photo sensor array) was positioned during data acquisition. With reference to
FIG. 2A , this range of focusing corresponds to the range of (virtual) image plane depths a distance of c about the physical light field sensor plane. InFIG. 2A : -
- Lens plane may be characterized as the principal plane of the lenses; it may be advantageous to employ thin-lens simplifications of lenses in the illustrative diagrams, although these inventions are applicable to any lens configuration and/or system;
- Far-focus plane may be characterized as the virtual plane optically conjugate to the furthest objects in the world that can be brought into a predetermined focus, for example, sharply into focus) using post image data acquisition focusing techniques of the light field;
- Focal plane may be characterized as the plane in which rays emanating from optical infinity are brought into sharpest focus by the optics.
- Light field sensor plane may be characterized as the plane in the data acquisition device where the principal plane of the microlens array in the light field sensor (for example, microlens array and image sensor assembly) is physically located;
- Close-focus plane may be characterized as the virtual plane optically conjugate to the closest objects in the world that can be brought sharply into focus through software focusing of the light field;
- v is equal to the distance between the lens plane and the light field sensor plane; and
- ε1 and ε2 are equal to the maximum distances from the light field sensor plane that can be focused sharply after exposure—that is, after acquisition of image data.
- With continued reference to
FIG. 2A , the “world” or everything outside of the Light Field Data Acquisition device is to the left of the lens plane, and the device internals are illustrated to the right of the lens plane. Notably,FIG. 2A is not drawn to scale; indeed, ε1 and ε2 are often smaller than v (for example, ε1<0.01*v and ε2<0.01*v). - As intimated herein, although the present inventions are often described in the context of Light Field Data Acquisition Device, which acquire or obtain refocusable data or information and/or processes or methods of acquiring, generating, manipulating and/or editing such refocusable image data, the present inventions are applicable to other systems, devices, processes and/or methods of acquiring, generating, manipulating and/or editing refocusable image data. In this regard, refocusable image data are image data or information, no matter how acquired or obtained, that may be focused and/or re-focused after acquisition or recording of the data or information. For example, in one embodiment, refocusable image data or information is/are Light Field Data or information acquired or obtained, for example, via a Light Field Data Acquisition Device.
- Notably, as discussed in detail below, the techniques of generating, manipulating and/or editing Light Field Data or information may be implemented via circuitry and techniques on/in the Light Field Data Acquisition Device and/or external post-processing system. Importantly, the present inventions are neither limited to any single aspect nor embodiment, nor to any combinations and/or permutations of such aspects and/or embodiments. Moreover, each of the aspects of the present inventions, and/or embodiments thereof, may be employed alone or in combination with one or more of the other aspects and/or embodiments thereof. For the sake of brevity, many of those permutations and combinations will not be discussed and/or illustrated separately herein.
- There are many inventions described and illustrated herein. The present inventions are neither limited to any single aspect nor embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Moreover, each of the aspects of the present inventions, and/or embodiments thereof, may be employed alone or in combination with one or more of the other aspects of the present inventions and/or embodiments thereof. For the sake of brevity, many of those permutations and combinations will not be discussed separately herein.
- In a first principal aspect, certain of the present inventions are directed to a method of generating and outputting image data corresponding to a scene, comprising: (a) acquiring Light Field Data which is representative of a light field from the scene, wherein the Light Field Data is acquired using a data acquisition device; (b) acquiring configuration data which is representative of how light rays optically propagate through the data acquisition device; (c) generating first image data using the Light Field Data and the configuration data, wherein the first image data includes a focus or focus depth that is different from a focus or focus depth of the Light Field Data; (e) generating a first electronic data file including (1) the first image data, (2) the Light Field Data, and (3) the configuration data; and (f) outputting the first electronic data file (for example, to memory, processing circuitry, a Standard Display Mechanism (such as a printer or display)).
- In one embodiment, generating the first electronic data file further includes arranging the first image data of the first electronic data file in a Standard Image Format (for example, JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats). In another embodiment, generating a first electronic data file further includes interleaving, threading, watermarking, encoding, multiplexing and/or meshing the first image data and the Light Field Data. Indeed, generating the first electronic data file may further include generating a header of the first electronic data file, wherein the header includes the configuration data.
- The method of this aspect of the inventions may further include: (g) reading the first electronic data file; (h) displaying the first image data; (i) receiving a user input; (j) generating second image data, in response to the user input, using (1) the Light Field Data of the electronic data file and (2) the configuration data, wherein the second image data is different from the first image data; (k) generating a second electronic data file including (1) the second image data, (2) the Light Field Data, and (3) the configuration data; and (l) outputting the second electronic data file (for example, to memory, processing circuitry, a Standard Display Mechanism (such as a printer or display)).
- The second image data may include a focus or focus depth that is different from the focus or focus depth of the first image data. Moreover, the second image data may be arranged in a Standard Image Format. Notably, generating a second electronic data file may further include interleaving, threading, watermarking, encoding, multiplexing and/or meshing the second image data and the Light Field Data.
- The method of this aspect of the inventions may further include compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the first electronic data file is the compressed Light Field Data.
- In another embodiment, the method may further include: (g) reading the first electronic data file; (h) displaying the first image data; (i) receiving a user input; (j) generating second image data, in response to the user input, using (1) the Light Field Data of the electronic data file and (2) the configuration data, wherein the second image data is different from the first image data; (k) generating a second electronic data file including the second image data; and (l) outputting the second electronic data file.
- In one embodiment, acquiring configuration data includes acquiring an N-bit key; and the method further includes determining optical model data by correlating the N-bit key to predetermined optical model data and wherein generating first image data includes generating first image data using the Light Field Data and the optical model data.
- The configuration data may include data which is representative of an Aperture Function or an Exit Pupil which is associated with the acquisition of the Light Field Data. In addition thereto, or in lieu thereof, the configuration data may include data which is representative of a mapping from a two-dimensional position on a captured 2D array of pixel values of the data acquisition device to a four-dimensional parameterization of the light field from the scene.
- In another principal aspect, certain of the present inventions are directed to a system to generate an image of a scene, comprising read circuitry to read a first electronic data file which is stored in a memory, wherein the first electronic data file includes (i) first image data, (ii) Light Field Data which is representative of a light field from the scene, and (iii) configuration data which is representative of how light rays optically propagate through a Light Field Data acquisition device. The system further includes a display to visually output an image using the first image data, a user interface to receive a user input, and processing circuitry, coupled to the read circuitry, the display and the user interface, to: (i) determine an optical model data using the configuration data, wherein the optical model data is representative of an optical model of the Light Field Data acquisition device, (ii) generate second image data, in response to the user input, using the Light Field Data and the optical model data, wherein the second image data includes a focus or focus depth that is different from a focus or focus depth of the first image data, and (iii) generate a second electronic data file including the second image data. The system of this aspect further includes write circuitry, coupled to the processing circuitry, to write the second electronic data file to the memory.
- In one embodiment, the second electronic data file further includes (i) the Light Field Data which is representative of a light field from the scene, and (ii) the configuration data and/or the optical model data. The configuration data may include data which is representative of an Aperture Function or an Exit Pupil which is associated with the Light Field Data acquisition device that acquired the Light Field Data.
- The processing circuitry may generate the second electronic data file by interleaving, threading, watermarking, encoding, multiplexing and/or meshing the second image data and the Light Field Data. In addition thereto, or in lieu thereof, the second electronic data file includes a header or the processing circuitry may generate a header of the second electronic data file, wherein the header includes the configuration data and/or the optical model data. Indeed, the processing circuitry may generate the first electronic data file by compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the second electronic data file is the compressed Light Field Data.
- In one embodiment, the processing circuitry arranges the first image data and/or the second image data of the second electronic data file in a Standard Image Format (for example, JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats).
- In one embodiment, the configuration data of the first electronic data file includes an N-bit key, wherein the processing circuitry determines the optical model data by correlating the N-bit key to a plurality of different, predetermined optical model data.
- In another principal aspect, certain of the present inventions are directed to a light field acquisition device for acquiring light field image data of a scene, comprising: optics, a light field sensor, located in the optical path of the optics, to acquire light field image data, a user interface to receive a user input, wherein, in response to the user input, the light field sensor acquires the light field image data of the scene, and processing circuitry, coupled the light field sensor and the user interface, to generate and output an electronic data file, the processing circuitry to: (a) determine configuration data which is representative of how light rays optically propagate through the optics and light field sensor, and (b) generate and output the electronic data file, wherein the electronic data file includes (i) image data (which may be arranged in a Standard Image Format), (ii) Light Field Data which is representative of a light field from the scene, and (iii) configuration data (for example, (1) data which is representative of an Aperture Function or Exit Pupil of the light field acquisition device and/or (2) data which is representative of a mapping from a two-dimensional position on a captured 2D array of pixel values to a four-dimensional parameterization of a light field from the scene). The light field acquisition device of this aspect of the inventions further includes memory, coupled to the processing circuitry, to store the electronic data file therein.
- In one embodiment, the processing circuitry generates the electronic data file by interleaving, threading, watermarking, encoding, multiplexing and/or meshing the image data and the Light Field Data. In another embodiment, the processing circuitry generates the electronic data file by forming a header, wherein the header includes the configuration data. Indeed, in another embodiment, the processing circuitry generates the electronic data file by compressing the Light Field Data to generate compressed Light Field Data, and wherein the Light Field Data of the electronic data file is the compressed Light Field Data.
- The configuration data of the electronic data file may include an N-bit key which is representative of predetermined optical model data.
- In another embodiment, the processing circuitry may generate a header of the electronic data file, wherein the header includes the configuration data and/or the optical model data.
- Again, there are many inventions, and aspects of the inventions, described and illustrated herein. This Summary is not exhaustive of the scope of the present inventions. Indeed, this Summary may not be reflective of or correlate to the inventions protected by the claims in this or in continuation/divisional applications hereof.
- Moreover, this Summary is not intended to be limiting of the inventions or the claims (whether the currently presented claims or claims of a divisional/continuation application) and should not be interpreted in that manner. While certain embodiments have been described and/or outlined in this Summary, it should be understood that the present inventions are not limited to such embodiments, description and/or outline, nor are the claims limited in such a manner (which should also not be interpreted as being limited by this Summary).
- Indeed, many other aspects, inventions and embodiments, which may be different from and/or similar to, the aspects, inventions and embodiments presented in this Summary, will be apparent from the description, illustrations and claims, which follow. In addition, although various features, attributes and advantages have been described in this Summary and/or are apparent in light thereof, it should be understood that such features, attributes and advantages are not required whether in one, some or all of the embodiments of the present inventions and, indeed, need not be present in any of the embodiments of the present inventions.
- In the course of the detailed description to follow, reference will be made to the attached drawings. These drawings show different aspects of the present inventions and, where appropriate, reference numerals illustrating like structures, components, materials and/or elements in different figures are labeled similarly. It is understood that various combinations of the structures, components, materials and/or elements, other than those specifically shown, are contemplated and are within the scope of the present inventions.
- Moreover, there are many inventions described and illustrated herein. The present inventions are neither limited to any single aspect nor embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Moreover, each of the aspects of the present inventions, and/or embodiments thereof, may be employed alone or in combination with one or more of the other aspects of the present inventions and/or embodiments thereof. For the sake of brevity, many of those permutations and combinations will not be discussed and/or illustrated separately herein.
-
FIG. 1A is a block diagram representation of an exemplary Light Field Data Acquisition Device; -
FIG. 1B is a block diagram representation of an exemplary Light Field Data Acquisition Device including, among other things, post-processing circuitry integrated therein; -
FIGS. 1C and 1F are block diagram representations of exemplary Light Field Data acquisition systems including a Light Field Data Acquisition Device and post-processing circuitry; -
FIG. 1D is a block diagram representation of an exemplary Light Field Data Acquisition Device including memory (integrated therein) to store Light Field Data; -
FIG. 1E is a block diagram representation of an exemplary Light Field Data Acquisition Device including, among other things, post-processing circuitry and memory integrated therein; -
FIG. 1G is a block diagram of an exemplary Light Field Data Acquisition Device including optics, a coded aperture, and sensor to record, acquire, sample and/or capture Light Field Data, including memory integrated therein; -
FIG. 1H is a block diagram representation of an exemplary Light Field Data Acquisition Device including a plurality of optics and sensors to record, acquire, sample and/or capture Light Field Data, including memory integrated therein; -
FIG. 2A is an illustrative diagram representation of certain optical characteristics of an exemplary Light Field Data Acquisition Device including certain focus planes such as a far-focus plane, a physical light field sensor plane, and the close-focus plane; -
FIG. 2B is an illustrative diagram representation of an exemplary light field sensor including, among other things, a microlens array and imaging sensor, which may be separated by (or substantially separated by) the focal length of the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions; -
FIG. 2C is an illustrative diagram representation of the light field sensor plane, which may be disposed at the principal plane of the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions; -
FIG. 2D is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, a main lens (representing the optics), a microlens array and an imaging sensor, illustrating two exit pupil locations which provide or result in different locations of the centers of projected lenslets in the microlens array, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions; notably, the positioning the exit pupil at the location corresponding to theExit Pupil 2 results in larger disk images projected onto the surface of the imaging sensor relative to the location corresponding to theExit Pupil 1; -
FIG. 3A is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil is recorded and/or stored as a single number that is the distance of the center of the exit pupil from the microlens array (or imaging sensor surface in an alternative embodiment); -
FIG. 3B is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil may be a location (for example, the center of the exit pupil) in three-dimensional space; -
FIG. 3C is an illustrative diagram representation of an exemplary light field sensor architecture including, among other things, main lens (representing the optics), a microlens array and an imaging sensor, wherein the exit pupil may be a location and shape (in the illustrative embodiment, the location of the center of the exit pupil and a disk of a specified radius) in three-dimensional space; -
FIG. 4 is an illustrative diagram representation of the propagation of an exemplary light ray from the world, though a lens into a light field acquisition device and impinging on the plane of the light field sensor; wherein for a given light ray (represented by a 3D position and 3D direction vector) that enters the acquisition device, the post-processing circuitry/system may calculate or determine how the rays propagate within the acquisition device between the last lens element of the optics and the microlens array of the light field sensor array by “tracing” the light ray through the lens elements of the optics according to the way the ray would physically refract and propagate through each element of the optics based on physical laws given the glass type, curvature and thickness of each lens element of the optics; -
FIG. 5 illustrates a magnified view of a set of projected lenslet disks of the microlens array onto the surface of an imaging sensor (or portion thereof); notably, the locations, size and shape of the projected disks are overlayed onto the captured image; determining the centers and sizes of microlens disks may be performed based on the key optical parameters detailed herein; -
FIG. 6 illustrates a magnified view of the surface of an exemplary imaging sensor (or portion thereof) highlighting/outlining the radius of projected disks lenslet disks of the microlens array, the spacing between neighboring centers (pitch) of the lenslet disks of the microlens array, X and Y translation offsets and rotation; the X and Y offset values in this exemplary illustration are the spatial distance between the center pixel on the sensor and the center of a central projected microlens disk; and the spacing between neighboring disk centers is the pitch of the projected microlens array. Notably, although the diameter of the projected disks appears approximately the same size in the illustration as the pitch, the numbers are different and may be used for different purposes; -
FIGS. 7A and 7B are block diagram representations of exemplary grid architectures of the microlens array, including a hexagonal grid (FIG. 7A ) and a square grid (FIG. 7B ) wherein the pitch of the lenslets of the array of such architectures is highlighted/outlined in conjunction therewith; -
FIGS. 8A-8C are block diagram representations of exemplary grid architectures of the microlens array, including a hexagonal grid (FIG. 8A ), a square grid (FIG. 8B ) and square and octagonal grid (FIG. 8C ); notably, the pattern of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device; -
FIG. 9 is a block diagram representation of sensor pixels of a sensor array (of, for example, a light field sensor) wherein the pitch of the pixels of the sensor array of such architecture is highlighted/outlined in conjunction therewith; the pitch of the pixels/sensors of the sensor may be characterized as the distance between the centers of neighboring sensor pixels and such pitch may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device; -
FIG. 10 is an illustrative diagram representation of a collimated light source, microlens array, and image sensor to create an image of points of light or small disks of light; in this illustrative embodiment the sensor, at any time in the manufacturing process after the microlens array has been fastened to the sensor, samples the light rays of a collimated light source wherein all the light rays are perpendicular to the surface of the light field sensor; -
FIG. 11 is an exemplary illustration of the resulting image provides a grid of points of light or small images of disks, one per lenslet in the microlens array for microlens array to imaging sensor registration; the registration may employ an image of point-lights or small disks (for example, as produced via the architecture ofFIGS. 10 and/or 12); the X and Y offsets are these distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels; -
FIG. 12 is an illustrative diagram representation of an aperture, microlens array and image sensor for registration of the microlens array to the image sensor wherein the small aperture provides a near-uniform light source; an image may be captured from the fully or near fully assembled light field acquisition device of uniform or near uniform field of light (for example, a white wall) when the acquisition device is “stopped down” (i.e. has its optical lens aperture reduced in size) to the minimum available aperture; notably the resulting image will be a grid of points of light or small images of disks, one per lenslet in the microlens array; the X and Y offsets are this distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See,FIG. 11 ); -
FIGS. 13A and 13B are block diagram representations of an exemplary Light Field Data Acquisition Devices including, among other things, sensor (for example, linear or rotary potentiometers, encoders and/or piezo-electric or MEMS transducers, and/or image sensors such as CCDs or CMOS—notably, any sensor whether now known or later developed is intended to fall within the scope of the present inventions) to sense, detect and/or determine (i) the configuration of the lens system of the acquisition device, and/or (ii) determine the Exit Pupil or Aperture Function of the lens system of the Light Field Data Acquisition Device relative to the microlens array (for example, one or more of the size and/or shape and/or other characteristics of the Exit Pupil (relative to the microlens array); notably, the sensors may be employed in any of the acquisition devices described and/or illustrated herein, including those of FIGS. 1A-1H—for the sake of conciseness, such sensors will not be illustrated therewith; -
FIGS. 14A-14C are illustrative diagram representations of a microlens array and image sensor highlighting disks of light projected by a lenslet onto the microlens array; notably, the spacing between neighboring disk centers is the pitch of the projected microlens array; the radius of each projected lenslet disk may be considered extent of the disk of light projected by a lenslet in the microlens array, wherein (i) the size of projected disks may be smaller than spacing between disks (FIG. 14A ), (ii) the size of projected disks may be nearly the same as the spacing between disks (FIG. 14B ), and (iii) the size of projected disks may be larger than spacing between disks (FIG. 14C ); -
FIGS. 15A and 15B are block diagram representations of exemplary electronic light field data files having one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the file format or structure of the Light Field Data file may include a start code and/or end code to indicate the beginning and/or end, respectively, of a set of a Light Field Data; notably, the electronic data file format or structure may have a header section containing metadata which may include and/or consist of Light Field Configuration Data (see,FIG. 15B ); -
FIG. 15C is a bock diagram of exemplary electronic file having Light Field Configuration Data which is associated with one or more electronic light field data files having one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions; -
FIGS. 16A and 16B are block diagram representations of memory (which may store, among other things, the electronic data files having one or more sets of Light Field Data) in communication with post-processing circuitry, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the memory may be separate from or integrated with the post-processing circuitry (FIGS. 16A and 16B , respectively); -
FIGS. 16C and 16D are block diagram representations of exemplary Light Field Data Acquisition Devices, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the exemplary Light Field Data Acquisition Devices include a display (Standard Display Mechanism) to allow the user to view an image or video generated using one or more sets of Light Field Data in a Light Field Data File; -
FIGS. 16E and 16F are block diagram representations of exemplary Light Field Data Acquisition Devices, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein the Light Field Data Acquisition Device couples to external systems/devices (for example, external storage, video display, printer, recording device and/or processor circuitry) including an external display to allow the user to view an image or video generated using one or more sets of Light Field Data in a Light Field Data File; such external devices or circuitry may facilitate, for example, storage of electronic data files that include light field image data, electronic files that include Light Field Configuration Data and/or electronic files that include a combination thereof; -
FIG. 16G is a block diagram representation of memory (which may store the electronic data files having one or more sets of Light Field Data and/or Light Field Configuration Data) in communication with post-processing circuitry, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the post-processing circuitry includes write circuitry and read circuitry to communicate with the memory, and the processing circuitry to implement, for example, Light Field Processing that includes generating, manipulating and/or editing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field) the image data corresponding to the Light Field Data—after acquisition or recording thereof; -
FIG. 17A is a block diagram representation of exemplary electronic data files having a image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format, as defined in the Detailed Description, and one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions; -
FIGS. 17B-17D are block diagram representations of exemplary electronic data files having image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of Light Field Data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files may include one or more headers having metadata which includes, for example, one or more sets of Light Field Configuration Data; -
FIGS. 17E and 17F are block diagram representations of exemplary electronic data files having image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of “raw” image data, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files may include one or more headers having metadata; -
FIGS. 18A-18E are exemplary processing flows for post-processing the exemplary electronic data files having data (for example, one or more sets of Light Field Data), according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain aspects of certain embodiments of the present inventions, wherein the exemplary post-processing flows may be employed in conjunction with the electronic data files ofFIGS. 17A-17F ; -
FIG. 19 is a block diagram representation of exemplary electronic data files in conjunction with exemplary processing flows for post-processing data contained therein, according to at least certain aspects of certain embodiments of the present inventions and/or which may implement certain embodiments of the present inventions, wherein such electronic data files include image data (which is representative of an image) arranged, organized and/or stored in a Standard Image Format and one or more sets of Light Field Data, and the processing may utilize any Standard Display Mechanism to view the Standard Image portion of the electronic data file; notably, such electronic files may include one or more headers (not illustrated) having metadata (which includes, for example, one or more sets of Light Field Configuration Data); moreover, in the processing flows; and -
FIG. 20 is a block diagram representation of an exemplary user interface of, for example, the Light Field Data Acquisition Device and/or post-processing system, according to certain aspects of the present invention; notably, in one embodiment, the user interface may include an output device/mechanism (for example, display and/or speaker) and/or user input device/mechanism (for example, buttons, switches, touch screens, pointing device (for example, mouse or trackball) and/or microphone) to allow a user/operator to monitor, control and/or program the operation of the Light Field Data Acquisition Devices and/or post-processing circuitry/system. - Again, there are many inventions described and illustrated herein. The present inventions are neither limited to any single aspect nor embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Each of the aspects of the present inventions, and/or embodiments thereof, may be employed alone or in combination with one or more of the other aspects of the present inventions and/or embodiments thereof. For the sake of brevity, many of those combinations and permutations are not discussed separately herein.
- There are many inventions described and illustrated herein, as well as many aspects and embodiments of those inventions. In one aspect, the present inventions are directed to, among other things, Light Field Data Acquisition Devices (for example, light field cameras), post-processing systems relating thereto, and methods of using such devices and systems. In another aspect, the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device used to implement post-processing of the image data captured or acquired thereby (or example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition and/or recording of the image data). In yet another aspect, the present inventions are directed to transmitting, providing or communicating such characteristics, parameters and/or configurations to post-processing circuitry—whether such post-processing circuitry is disposed in/on the Light Field Data Acquisition Device (see
FIGS. 1B and 1E ) or external thereto (seeFIGS. 1C and 1F ). The data which is representative of the characteristics, parameters and/or configurations (collectively “configuration data”) of the Light Field Data Acquisition Device may be obtained, determined and/or recorded before, during and/or after collection or acquisition of Light Field Data by the imaging sensor of the acquisition device (for example, light field camera). - Notably, such configuration data may be employed by post-processing circuitry to generate, derive, calculate, estimate and/or determine an optical and/or geometric model of the Light Field Data Acquisition Device (for example, an optical and/or geometric model of the particular device which is associated with specific acquired Light Field Data). The post-processing circuitry may employ the optical and/or geometric model of the Light Field Data Acquisition Device to generate, manipulate and/or edit (for example, define and/or redefine the focus of the light field image data) the light field image data which is associated with or corresponds to the optical and/or geometric model of the Light Field Data Acquisition Device employed to acquire or collect such Light Field Data.
- DEFINITIONS: The inventions described in this detailed description are introduced in terms of exemplary embodiments that are in some cases discussed in terms of the following terms.
- Light Field Data means, among other things, a set of values, where each value represents the light traveling along each geometric light ray (or bundle of rays approximating a geometric light ray) within a corresponding set of light rays. In an exemplary embodiment, Light Field Data represents the 2D image data sampled by and read from the image sensor pixel array in a light field acquisition device (for example, a light field camera comprising a main lens, microlens array and a photo sensor, such as the one shown in United States Patent Application Publication 2007/0252074, and/or the provisional application to which it claims priority, and/or Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006, all of which are incorporated here in their entirety by reference; and/or the block diagram illustration of a light field camera in
FIGS. 1A and 1B ). The Light Field Data may be represented as a function L(x,y,u,v) where L is the amount of light (for example, radiance) traveling along a ray (x,y,u,v) that passes through the optical aperture of the camera lens at 2D position (u,v) and the sensor at 2D position (x,y)—see, for example, the Patent Application Publication 2007/0252074 and PhD dissertation mentioned above. In addition, Light Field Data may mean the image data collected with a coded aperture system. (SeeFIG. 1G ) and/or data encoded and/or recorded in the frequency spectrum of the light field. Indeed, Light Field Data may be a collection of images focused at different depths and/or a collection of images from different viewpoints. (SeeFIG. 1H ). Notably, Light Field Data may mean any collection of images or lighting data that may be used to generate, derive, calculate, estimate and/or determine a full or partial representation or approximation of a light field function L(x,y,u,v) as described above. - Light Field Configuration Data means data that may be used to interpret (in whole or in part) Light Field Data. For example, Light Field Configuration Data are data that may be used to interpret how the values in the Light Field Data relate to or map the characteristics of light flowing on particular light rays or sets of light rays in the scene pertaining to the light field. Such characteristics may include or depend upon, for example, the intensity, color, wavelength, polarization, etc. of the light in the scene. The Light Field Configuration Data may be representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the image data acquisition device (for example, an optical and/or a geometric model of the image data acquisition device that is associated with certain acquired Light Field Data). Light Field Configuration Data may include one, some or all of the following, and/or data representative of and/or used in generating, deriving, calculating, estimating and/or determining one, some or all of the following:
-
- One or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device
- A geometric and/or optical model of the Light Field Data Acquisition Device, that may, for example, be sufficient to enable computation, estimation, determination, representation of how light rays optically propagate (for example, refract, reflect, attenuate, scatter and/or disperse) through the acquisition device. The geometric and/or optical model may be and/or include data which is representative of a mapping from a two-dimensional (x,y) position on the surface of an image sensor in the Light Field Data Acquisition Device to a four-dimensional (x,y,u,v) parameterization of a ray, as described above, in the light field), and correspondingly, from the two-dimensional (x′,y′) position on a captured 2D array of pixel values read from the image sensor, to the four-dimensional (x,y,u,v) parameterization of the light field from the scene.
- An Aperture Function, as described below.
- An Exit Pupil, as described below.
- The relative pitch of microlenses and pixels in a light field camera used to record a light field.
- The zoom and/or focus position of the lens system
- Characteristics, properties, the geometry of and/or parameters of the microlens array (for example, the grid pattern, lens size, focal range, and/or lens formulae of the microlens array).
- The location of the microlens array relative to the imaging sensor (for example, the vertical separation, X and Y offsets, and/or rotation of the microlens array relative to the sensor).
- The characteristics, properties, geometry of and/or parameters of the images that appear on the imaging sensor by light passing from the world through the microlens array onto the imaging sensor surface (for example, in some embodiments the pattern, separation, X and Y offsets, and/or rotation of the array of image disks that appear on the imaging sensor surface)
- The main lens system (for example, lens formulae, f/number, and/or data representing the Exit Pupil).
- A Ray Correction Function (see, e.g., Ren Ng's PhD thesis referenced above) that maps (possibly aberrated) rays within the acquisition device to world rays.
- Notably, Light Field Configuration Data is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, in some exemplary embodiments, Light Field Configuration Data may encompass any information now known or later developed which is representative of and/or used in generating, deriving, calculating, estimating and/or determining an optical and/or a geometric model of the Light Field Data Acquisition Device.
- Aperture Function is a term for data that relates to and/or represents the transfer of light through an optical system. In an exemplary embodiment, Aperture Function is a function that specifies for geometric light rays striking a sensor, how much light passes from the outside of the acquisition device, through the lens (or plurality of lenses) and strikes the sensor along that ray trajectory. The Aperture Function may be represented by a 4D function, A(x,y,u,v), where A represents the fraction of light transferred through the lens (or plurality of lenses) along a ray (x,y,u,v) that strikes the sensor at 2D position (x,y) and from a direction (u,v) on the hemisphere of incoming directions. Notably, other embodiments using alternative parameterizations of the rays that strike the sensor are included in the present inventions. Notably, the Aperture Function corresponding to such a function may be represented or approximated by an Exit Pupil, as described below.
-
- Exit Pupil is term for data relating to or representing the optical exit pupil of a lens system that may be used to describe, generate, represent, construct and/or reconstruct a model and/or function of the optical exit pupil. Exit Pupil may also mean an actual or approximate representation of the optical exit pupil including one, some or all of its size, shape and/or 3D position. For example, the Exit Pupil may be represented as a disk of specified radius, at a specific perpendicular distance relative to a sensor plane and/or microlens array. In other exemplary embodiments, Exit Pupil is a representation where the shape and/or distance and/or 3D position varies depending on the position on the sensor from which the Exit Pupil is viewed. The data representing the Exit Pupil may be a compact parameter or set of parameters.
- In one exemplary embodiment, data representing the Exit Pupil is recorded and/or stored as a single number that is the distance of the center of the exit pupil from the microlens array or imaging sensor surface (see
FIG. 3A ). In another embodiment, data representing the Exit Pupil may be a location (for example, the center of the exit pupil) in 3 dimensional space (seeFIG. 3B ). In another exemplary embodiment, data representing the Exit Pupil may be a location and shape (for example, the location of the center of the exit pupil and a disk of a specified radius) in 3 dimensional space (seeFIG. 3C ). Notably, data representing the Exit Pupil may be recorded and/or stored in many different forms, and data representing the Exit Pupil is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, data representing the Exit Pupil may be in any form now known or later developed. - The terms Aperture Function and Exit Pupil may be used synonymously herein.
- Light Field Processing means processing Light Field Data to, for example, compute an output result, for example, an image. In certain aspects of the present inventions, Light Field Processing encompasses generating, manipulating and/or editing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field relative to the relative to focus and/or depth of field provided by the optics of the acquisition device during acquisition, sampling and/or capture of the Light Field Data) the image data corresponding to the Light Field Data—after acquisition or recording thereof. Light Field Processing may use Light Field Configuration Data in interpreting Light Field Data in order to implement a particular processing to produce a particular output result. Different types of Light Field Processing may be used in different embodiments of the present invention. In one exemplary embodiment, Light Field Processing may include refocusing—that is, processing the Light Field Data to compute an image in which at least part of the image is refocused (relative to the optical focus of the acquisition system) at a desired or virtual focus plane in the scene. In another exemplary embodiment, Light Field Processing may include aberration correction, in which the light rays in the light field are processed in order to reduce the effects of optical aberration in the optical system used to record the Light Field Data. In exemplary embodiments, such aberration correction is implemented according to the methods of processing L(x,y,u,v) light field functions with the geometric and/or optical model of the Light Field Data Acquisition Device as shown in U.S. patent application Ser. No. 12/278,708 filed on Aug. 7, 2008 and entitled “Correction of Optical Aberrations”, which is incorporated in its entirety herein by reference, and/or Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006. In another exemplary embodiment, Light Field Processing may include changing (increasing or decreasing) the depth field, in which the light rays are processed in order to compute an image in which the depth of field is changed (increased or decreased) to, for example, provide a different range of depths in the world into focus or a predetermined focus. These examples are not intended to limit the scope or types of processing associated with Light Field Processing in the description of embodiments below.
- As noted above, Light Field Processing may include processing to correct for inherent lens aberrations in the recorded Light Field Data—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene. In another exemplary embodiment, Light Field Processing includes simulating novel lens systems—after initial acquisition or recording of the Light Field Data and/or information) of, for example, a scene. In another exemplary embodiment, Light Field Processing includes changing the viewing perspective—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene. In another exemplary embodiment, Light Field Processing includes creating holographic images from Light Field Data—after initial acquisition or recording of the Light Field Data and/or information—of, for example, a scene. Notably, Light Field Processing is not limited to any single aspect or embodiment thereof, nor to any combinations and/or permutations of such aspects and/or embodiments. Indeed, Light Field Processing encompasses any act of act of generating, manipulating, editing and/or processing Light Field Data now known or later developed.
- Standard Image Format is a term used to denote images or image data arranged, organized and/or stored (hereinafter, in this context, “stored”) in a standard encoding for storage, display or transmission. Exemplary embodiments include JPEG, EXIF, BMP, PNG, PDF, TIFF and/or HD Photo data formats.
- Light Field Data Acquisition Device means any device or system for acquiring, recording, measuring, estimating, determining and/or computing Light Field Data. Briefly, with reference to
FIGS. 1A-1F , 2B and 2C, the Light Field Data Acquisition Device (in exemplary embodiments illustrated as light field data acquisition device 10) may include optics 12 (including, for example, a main lens),light field sensor 14 includingmicrolens array 15 and sensor 16 (for example, a photo sensor). Themicrolens array 15 is incorporated into the optical path to facilitate acquisition, capture, sampling of, recording and/or obtaining Light Field Data viasensor 16. Such Light Field Data may be stored inmemory 18. Notably, the discussions set forth in United States Patent Application Publication 2007/0252074, the provisional applications to which it claims priority (namely, U.S. Provisional Patent Application Ser. Nos. 60/615,179 and 60/647,492), and Ren Ng's PhD dissertation, “Digital Light Field Photography”) for acquiring Light Field Data are incorporated herein by reference. - The light field
data acquisition device 10 may also include control circuitry to manage or control (automatically or in response to user inputs) the acquisition, sampling, capture, recording and/or obtaining of Light Field Data. The light fielddata acquisition device 10 may store the Light Field Data (for example, output by sensor 16) in external data storage and/or in on-system data storage. All permutations and combinations of data storage formats of the Light Field Data and/or a representation thereof are intended to fall within the scope of the present inventions. - Notably, light field
data acquisition device 10 of the present inventions may be a stand-alone acquisition system/device (see,FIGS. 1A , 1C, 1D and 1F) or may be integrated with post-processing circuitry 20 (see,FIGS. 1B and 1E ). That is, light fielddata acquisition device 10 may be integrated (or substantially integrated) withpost-processing circuitry 20 which may perform Light Field Processing (for example, be employed to generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field—after initial acquisition or recording of the Light Field Data) Light Field Image Data and/or information of, for example, a scene); and, in other exemplary embodiments, light fielddata acquisition device 10 is separate frompost-processing circuitry 20. Thepost-processing circuitry 20 includes processing circuitry (for example, one or more processors, one or more state machines, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays) to implement or perform Light Field Processing. - Notably, a Light Field Data Acquisition Device, during capture and/or acquisition, may have a light field sensor located such that the “optical depth of field” with respect to the light field sensor does not include the location of a subject. (See,
FIG. 2A ). The “optical depth of field” may be characterized as depth of field the device would have if used as a conventional imaging device containing a conventional imaging sensor. In this regard, with reference toFIG. 2C , the location of lightfield sensor plane 22 may be considered the same as the principal plane of the elements in themicrolens array 15. Herein, the location of lightfield sensor plane 22 may be referred to as the location and/or placement of the light field sensor 14 (for example, when describing the location and/or placement relative to other components and/or modules in light field data acquisition device 10 (for example, optics 12)). - The exemplary embodiments of Light Field Data Acquisition Devices above are described to illustrate the underlying principles. Indeed, any device now known or later developed for acquiring, recording, measuring, estimating, determining, and/or computing Light Field Data is intended to fall within the scope of the term Light Field Data Acquisition Device and to fall within the scope of the present inventions.
- Recording and Communicating Data: In one aspect, the present inventions are directed to obtaining, deriving, calculating, estimating, determining, storing and/or recording Light Field Configuration Data (for example, one or more characteristics, parameters and/or configurations of a Light Field Data Acquisition Device, for example, the Light Field Data Acquisition Device illustrated in
FIGS. 1A-2C ). The Light Field Configuration Data may provide information which is representative of an optical and/or a geometric model of the image data acquisition device (which may include, for example, the camera optics (for example, one or more lenses of any kind or type), imaging sensors to obtain and/or acquire the Light Field Data or information, and relative distances between the elements of the image data acquisition device). Indeed, the Light Field Configuration Data may include data which enables or facilitates computation, estimation, determination, representation of how light rays optically propagate (for example, refract, reflect, attenuate, scatter and/or disperse) through the Light Field Data Acquisition Device and to acquisition, capture, sampling of and/or recording by the sensor (for example,sensor 16 ofFIGS. 2A-2C ). - Thereafter, post-processing circuitry (which may be integrated into the image data acquisition system (see, for example,
FIGS. 1A , 1B and 1E) or separate therefrom (see, for example,FIGS. 1C and 1F ) may obtain, receive and/or acquire (i) Light Field Data and/or (ii) the Light Field Configuration Data (which may be stored in memory 18). The post-processing circuitry, using the image data and associated Light Field Configuration Data, may determine, analyze and/or interpret the ray-geometry corresponding to one, some or all of imaging sensor pixel values associated with the imaging sensor of the image data acquisition system and thereby perform Light Field Processing (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof.) - As noted above, Light Field Configuration Data may be obtained, determined and/or recorded before, during and/or after collection, acquisition and/or sampling of image data by the imaging sensor of the acquisition device (for example, light
field acquisition device 10 ofFIGS. 1A-2C ). Such Light Field Configuration Data may be stored in the same data file and/or file format as the associated image data or in a different data file and/or different file format. Where the post-processing is performed “off-camera” or in a device separate from the acquisition device (for example, light field camera), such Light Field Configuration Data may be provided and/or communicated to a separate post-processing system together with (for example, concurrently, serially or in parallel) or separate from the associated image data (for example, before during and/or after collection, acquisition and/or sampling of image data). (See, for example,FIGS. 1C and 1F ). - Thus, in one embodiment, the Light Field Data Acquisition Device and/or post-processing circuitry/system stores, records and/or determines the data or information to construct an optical and/or geometric model of the Light Field Data Acquisition Device. The Light Field Data Acquisition Device and/or post-processing circuitry/system may, in conjunction with the Light Field Data, store, record and/or determine predetermined and/or selected characteristics, parameters and/or configurations of the Light Field Data Acquisition Device. In one embodiment, the Light Field Data Acquisition Device stores or records the predetermined or selected Light Field Configuration Data during, concurrently and/or immediately after collection, acquisition and/or sampling of Light Field Data. The Light Field Data Acquisition Device may store Light Field Configuration Data in, for example, a header of an electronic data file that contains the associated image data. In addition thereto, or in lieu thereof, the Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a separate electronic data file which is different from the electronic data file that contains the associated Light Field Data and/or image data. The Light Field Configuration Data may be associated with one or more electronic data files which include the associated Light Field Data and/or image data (i.e., the data which was acquired by the device that was configured in accordance with the data of the associated Light Field Configuration Data).
- In an exemplary embodiment, the Light Field Data Acquisition Device determines, records and/or stores Light Field Configuration Data, immediately prior to, concurrently, and/or immediately after certain light field acquisition parameters may change or vary between successive or multiple acquisitions (for example, the zoom and focus position of the optics are determined, acquired, recorded, and/or stored prior to, at the time of acquisition, after acquisition, for example, before one or more of certain parameters of the Light Field Configuration Data change or vary). The Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a header of an electronic data file that includes the associated Light Field and/or image data. In addition thereto, or in lieu thereof, the Light Field Data Acquisition Device may store the Light Field Configuration Data in, for example, a separate electronic data file which is different from the data file that contains the associated Light Field and/or image data.
- In one embodiment, the post-processing system, using (i) the Light Field Data and/or image data and (ii) associated Light Field Configuration Data, may perform Light Field Processing on (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field)) the image data—after acquisition or recording thereof—to generate or display a predetermined, selected and/or desired image. In one embodiment, the post-processing system employs the Light Field Configuration Data to construct or re-construct an optical and/or geometric model of the Light Field Data Acquisition Device used to acquire or capture the image data. As noted herein, the post-processing system may obtain the Light Field Configuration Data with or separately from the Light Field Data and/or image data.
- Notably, Light Field Configuration Data may include a representation of an optical and/or geometric model for a Light Field Data capture system that includes or provides information to convert or correlate data from an image sensor pixel to a representation of incoming light rays. In one embodiment, the optical and/or geometric model takes as input a location on the imaging sensor (for example, the X and Y offsets of a pixel), and provides a 4 dimensional representation of the set of light rays captured by that pixel location (for example, a set of rays in (x,y,u,v) ray-space as described above. See, for example, United States Patent Application Publication 2007/0252074, and the provisional application to which it claims priority (namely, Ser. Nos. 60/615,179 and 60/647,492, and Ren Ng's PhD dissertation, “Digital Light Field Photography”).
- The optical and/or geometric model may include (i) the number, shape (for example curvature and thickness), absolute and/or relative position of the optical elements within the device (including but not necessarily limited to lens elements, mirror elements, microlens array elements, and image sensor elements); (ii) characteristics, parameters, configurations and/or properties of one, some or all of the elements (for example, glass type, index of refraction, Abbe number); (iii) manufacturing tolerances; (iv) measured manufacturing deviations; (v) tilts and/or decenters of optical elements in a lens stack; (vi) coating information, etc. In an exemplary embodiment, the geometric and/or optical model is/are sufficient to enable computation, estimation, determination, representation and/or derivation of the trajectory of at least one light ray that enters the device (for example, enters the first lens element of the device), providing the location and direction of the light ray within the device (for example, within the body of the camera between a lens and a microlens array) and/or the termination position of the ray on the device's image sensor and/or the amount, color and/or dispersion of light propagated through the optics of the system.
- In one exemplary embodiment, the geometric and optical model comprises a representation of one or more (or all) of the following: (i) the curvature and thickness of each lens element, (ii) the spacing and orientation between lens elements, (iii) the type of glass for each element, (iv) the spacing and orientation between the last lens element and the microlens array, (v) the geometric shape of the microlens array, including a hexagonal pattern with given pitch and curvature of lenslets, (vi) the relative spacing and orientation between the microlens array and image sensor array, and (vii) the pitch, pattern and relative orientation of the image sensor array. This geometric and optical model may be used to determine the Ray Transfer Function of the acquisition device according to a computational simulation of the optical effect of the acquisition device on the rays that enter the device. In particular, for a given light ray (represented by a 3D position and 3D direction vector) that enters the acquisition device, the post-processing circuitry/system may calculate the ray that propagates within the body of the acquisition device between the last lens element of the optics and the microlens array of the light field sensor by tracing the ray through the lens elements of the optics according to the way the ray would physically refract and propagate through each element based on physical laws given the glass type, curvature and thickness of each lens element (see, for example,
FIG. 4 ). - Recording Characteristics, Parameters and/or Configurations of Device: In one exemplary embodiment, one, some or all of the following characteristics, parameters and/or configurations of the Light Field Data Acquisition Device is/are acquired, stored or recorded:
-
- Data or information representing the Aperture Function or Exit Pupil (for example, the size and/or shape and/or 3D position of the optical exit pupil of the optics or lens system) of the Light Field Data Acquisition Device (or data/information which is representative thereof) relative to the microlens array. In one embodiment, the Exit Pupil may vary with each configuration of the lens or optics system of the Light Field Data Acquisition Device. Indeed, the size and/or shape of the Exit Pupil may change on a shot-by-shot basis. (See,
FIGS. 2D , 5 and 6); and/or - The Light Field Sensor Geometry Model, which is defined generally as the optical and/or geometric model of the sensor that records Light Field Data. In an exemplary embodiment, the light field sensor includes a microlens array disposed or located in front of an image sensor (See, for example,
FIGS. 2A-2C ), and the Light Field Sensor Geometry Model may include one, some or all of the following characteristics, parameters and/or configurations:- The geometry of the microlens array. In the context of the present inventions, “microlens array” is a term that may generally mean any window with a micro-optical patterning. Thus, the geometry of the microlens array would include the surface geometry of the micro-optical patterning; and/or
- The pitch of the lenslets in a microlens array. The pitch of the lenslets in the microlens array may be characterized as the distance between the centers of neighboring microlenses and may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. In some embodiments, the pitch may be a single number that is constant and valid for all lenslets on the microlens array (See,
FIGS. 7A and 7B ). In other exemplary embodiments, the pitch may vary based on the spatial location in the microlens array. In other exemplary embodiments, the term “pitch” may be used generally to refer to the pattern of the microlens array, which may be regular, irregular, repeating or non-repeating; and/or - The distance between the microlens array and the surface of the imaging sensor. In certain embodiments, it may be preferred that this distance is the same as (or substantially the same as) the focal length of the microlens array. (See,
FIG. 2D ) and/or - The offsets and rotation of the microlens array relative to the imaging sensor. The relative offsets and rotation of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device or may vary between Light Field Data Acquisition Devices (even between models, versions or pieces thereof). (See,
FIG. 6 ); and/or - The pattern of the microlens array (for example, hex or square). The pattern of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. (See,
FIGS. 8A , 8B and 8C); and/or - The pitch of the pixels/sensors of the imaging sensor. The pitch of the pixels/sensors of the sensor (for example, photo sensor array) may be characterized as the distance between the centers of neighboring sensor pixels and may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. (See,
FIG. 9 ).
- Data or information representing the Aperture Function or Exit Pupil (for example, the size and/or shape and/or 3D position of the optical exit pupil of the optics or lens system) of the Light Field Data Acquisition Device (or data/information which is representative thereof) relative to the microlens array. In one embodiment, the Exit Pupil may vary with each configuration of the lens or optics system of the Light Field Data Acquisition Device. Indeed, the size and/or shape of the Exit Pupil may change on a shot-by-shot basis. (See,
- As noted above, one, some or all of the Light Field Configuration Data may be determined, stored and/or recorded—and associated with Light Field Data acquired or collected using such exposure characteristics, parameters and/or configurations. While in certain embodiments, all of the exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are employed to perform Light Field Processing (for example, generate, manipulate and/or edit (for example, adjust, select, define and/or redefine the focus and/or depth of field) the image data—after acquisition or recording thereof), less than all may be determined, stored and/or recorded with the associated or corresponding Light Field Data. Accordingly, in certain embodiments, one or more (and, as such, less than all) may be determined, stored and/or recorded for the associated or corresponding Light Field Data. All permutations and combinations of such Light Field Configuration Data (for example, including exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device) may be determined, stored and/or recorded with the associated or corresponding Light Field Data. For the sake of brevity, those permutations and combinations will not be discussed separately herein. As such, the present invention is not limited to any single aspect or embodiment thereof nor to any combinations and/or permutations of such aspects and/or embodiments of determining, storing and/or recording such Light Field Configuration Data with the associated or corresponding Light Field Data.
- Notably, from the perspective of the imaging sensor, the Aperture Function or Exit Pupil may be characterized in some embodiments as a three dimensional image of the aperture of the main lens. The relative location of the exit pupil on the imaging sensor location may, at least in part, be determined by the optics of the Light Field Data Acquisition Device. As such, the relative location of the exit pupil on the imaging sensor location may depend on, for example, the lens, the zoom, and the focus. (See, for example,
FIG. 2D wherein by modifying, adjusting and/or changing the size and location (for example, via zoom) “Exit Pupil 1” projects a first set of rays on the lenslets of the microlens array and “Exit Pupil 2” projects a second set of rays on the lenslets of the microlens array—which impacts the projection of the disks of light from each lenslet onto the surface of the imaging sensor). - Moreover, the size of the projected microlens pitch (for example, the distance between projected lenslet centers on the surface of the imaging sensor, or in other exemplary embodiments the distance across a lenslet along a line between the centers of neighboring lenslets on opposing sides when projected onto) may be characterized or determined using the following relationship:
-
Projected Pitch=(MLA_Pitch*(D+FL m))/D - where:
-
- D is the distance between the sensing array and the exit pupil;
- FLm is the focal length of the microlens array; and
- MLA_Pitch is the lateral separation between centers of neighboring microlenses on the microlens array.
- Further, the X-offset, Y-offset, pattern, sensor pixel pitch and rotation of the microlens array relative to the sensor may determine how the disks align with the sensor pixels. The sensor pitch allows the model to map geometric coordinates (generally measured in millimeters or microns) to pixel locations. The sensor pitch and microlens array grid pattern may be known values based on the manufacturing specifications. In one exemplary embodiment, the x-offset, y-offset, and rotation of the microlens array relative to the imaging sensor can be determined through a registration procedure. In one embodiment, an image may be acquired using the combination of the microlens array with the imaging sensor, at any time in the manufacturing process after the microlens array has been fastened to the imaging sensor, of a collimated light source with all light rays perpendicular to the surface of the light field sensor. In this exemplary embodiment, the resulting image will be a grid of points of light or small images of disks, one per lenslet in the microlens array. The X and Y offsets are this distances from the center of the recorded image to a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference in angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See
FIGS. 10 and 11 ). - In another exemplary embodiment, an image may be captured from the fully or near fully assembled light field acquisition device of uniform or near uniform field of light (for example, a white wall) when the acquisition device is “stopped down” (i.e. has its optical lens aperture reduced in size) to the minimum available aperture. In this embodiment, the resulting image may be a grid of small disks or points of light, one per lenslet in the microlens array. The X and Y offsets are this distances from the center of the recorded image to the center of a nearby (for example, the nearest) point of light/small disk image, and the rotation is the difference is angles between the line determined by a row of points of light and the line determined by a row of sensor pixels (See
FIGS. 11 and 12 ). - It is important to realize that the preceding descriptions of exemplary embodiments are only intended to illustrate the general principles of measuring, determining, registering and/or calibrating characteristics, parameters, configurations and/or properties of the Light Field Data Acquisition Device for incorporation in Light Field Configuration Data, and any relevant procedures for measuring, determining, registering and/or calibrating that are now known or invented in the future are intended to fall within this aspect of the scope of the present inventions.
- In certain embodiments, one of the measured or known values may be left unspecified and other measured or known values may be stored in units relative to the unspecified parameter. For example, in one embodiment the sensor pixel pitch may not be specified and some or all of the distance parameters (for example, the separation of the microlens array from the sensor, the x-offset and y-offset of the microlens array relative to the imaging sensor, and/or the pitch of the microlens array) may then have units that are relative to pitch of the sensor pixels.
- In certain embodiments, all of the characteristics, parameters and/or configurations may be employed to model the projection of the microlens disk images onto the sensor surface. This notwithstanding, certain of the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be fixed or nearly fixed, constant or nearly constant, predetermined and/or implicit for a given or predetermined model, version or series of Light Field Data Acquisition Device. For example, a lens may be designed such that the Exit Pupil does not vary from picture to picture. As a result, the Exit Pupil may be implicit. Moreover, the placement of the microlens array for a particular device model, version or series may be considered fixed (particularly in those situations where the manufacturing is within certain tolerances) and, as such, these characteristics, parameters and/or configurations may be predetermined or implied. Similarly, the microlens pitch, focal length, sensor pitch, and microlens pattern may also be constant across the focal plane for a particular device model, version or series of a particular model and, as such, these characteristics, parameters and/or configurations may be predetermined or implied.
- Notably, the Light Field Configuration Data may for some exemplary embodiments be categorized into three categories. The first of these categories may be referred to as “model static light field configuration data”, and is Light Field Configuration Data that is identical or nearly identical for all light field acquisition devices of a particular model, series or version of a particular model (for example, the pitch of a sensor pixel may be model static). The second of these categories may be referred to as “device static light field configuration data”, and may be Light Field Configuration Data that is fixed or nearly fixed for all light fields acquired by that device (for example, the x-offset, y-offset and rotation of the microlens array relative to the sensor surface may in some instances be device static), excluding model static light field configuration data. The third category may be referred to as “dynamic light field configuration data” and is Light Field Configuration Data that may vary between successive or a plurality of acquisitions from a given or particular Light Field Data Acquisition Device (for example, the zoom and/or optical focus position when acquisition via a given or particular device is performed).
- In certain embodiments, those characteristics, parameters and/or configurations of the Light Field Data Acquisition Device which are fixed, constant, predetermined and/or implicit (the model and/or device static light field configuration data) may be determined (i) on an individual basis during and/or after manufacture, (ii) using empirical data of one or more Light Field Data Acquisition Devices, (iii) using statistical approximations, for example, based on one or more empirical data of one or more Light Field Data Acquisition Devices, and/or using computer-based modeling. Indeed, such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device may be determined using any technique or device whether now known or later developed.
- In addition, data which is representative of such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) may be stored in memory in or on the Light Field Data Acquisition Device. In addition thereto, or in lieu thereof, model, version or series of a particular model of the light field acquisition device may be stored in memory and such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) may be determined therefrom. As such, in one embodiment, the one, some or all of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device is/are stored or recorded (in the same and/or a different data file) in memory in or on the Light Field Data Acquisition Device. (See, for example,
FIGS. 1D and 1E ). Thus, the one, some or all of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) may be stored in resident memory, in a data file with the associated or corresponding Light Field Data and/or in a data file which is different from the file of the associated or corresponding Light Field Data. - Such model and/or static light field configuration data may be stored or recorded in memory in or on the Light Field Data Acquisition Device before, during, concurrently with or after exposure (i.e., acquisition or sampling of the Light Field Data). In one embodiment, the model and/or device static light field configuration data may be appended to the associated or corresponding Light Field Data prior to (for example, immediately prior to) communicating the associated or corresponding Light Field Data to a post-processing circuitry/system. In this way, the post-processing system may acquire the data which is representative of such constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device (the model and/or device static light field configuration data) and the associated or corresponding Light Field Data (in the same or different data file)—and generate, manipulate and/or edit one or more images using the Light Field Data (for example, adjust the depth of focus) after acquisition or recording of such Light Field Data.
- In another embodiment, one, some or all of the model and/or device static light field configuration data is/are stored in memory in or on the post-processing system. (See, for example,
FIG. 1F ). In this embodiment, the post-processing system may determine one or more of the constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device based on, for example, data which is representative of the model, version or series of a particular model of the light field acquisition device. Such data which is representative of the model, version or series of a particular model of the light field acquisition device (the model static light field configuration data) may be stored in a data file that is communicated to the post-processing system via the user (for example, via the user interface) and/or via the Light Field Data Acquisition Device (for example, in a data file containing (i) Light Field Data and (ii) the model static light field configuration data, or in a data file which is different from the Light Field Data). - The memory in or on the post-processing system may include a look-up table or the like providing such fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device. For example, the user may input data, via the user interface, to indicate the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device which is associated with the Light Field Data. The post-processing system, in addition thereto or in lieu thereof, may correlate the Light Field Data with the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device via data stored in a data file and/or data provided by the Light Field Data Acquisition Device to the post-processing system (for example, in those instances where the Light Field Data Acquisition Device is connected to post-processing system). As noted above, the data which is representative of the fixed, constant, predetermined and/or implicit characteristics, parameters and/or configurations of the Light Field Data Acquisition Device data may be data of the model, version or series of a particular model of the light field acquisition device.
- In another embodiment, one, some or all of the model and/or static light field configuration data is/are made available to the post-processing system through a predetermined retrieval system. For example, in one embodiment, the post-processing system may query a database from a local, networked and/or internet source or sources to recover one, some or all of the model and/or static light field configuration data. In another embodiment, the post-processing system may check for and/or install software updates from a local, networked and/or external (for example, Internet) source or sources. Such data which is representative of the model, version or series of a particular model of the light field acquisition device (the model static light field configuration data) may be stored in a data file that is communicated to the post-processing system via the user (for example, via the user interface) and/or via the Light Field Data Acquisition Device (for example, in a data file containing (i) Light Field Data and (ii) the model static light field configuration data, or in a data file which is different from the Light Field Data).
- In those instances where the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are not fixed, constant, predetermined and/or implicit, such characteristics, parameters and/or configurations may be determined using any technique or device whether now known or later developed. For example, in one embodiment, one or more sensors are employed to determine the Exit Pupil or Aperture Function of the lens system of the Light Field Data Acquisition Device relative to the microlens array. In some exemplary embodiment, one or more sensors (for example, linear or rotary potentiometers, encoders and/or piezo-electric or MEMS transducers, and/or image sensors such as CCDs or CMOS—notably, any sensor whether now known or later developed is intended to fall within the scope of the present inventions) may sense, detect and/or determine one or more of the size and/or shape and/or other characteristics of the Exit Pupil (relative to the microlens array) by sensing, detecting and/or determining the configuration of the lens system of the Light Field Data Acquisition Device. (See, for example,
FIGS. 13A and 13B ). - In one exemplary embodiment, an image sensor array with known microlens array between it and the optical system is used to sense the exit pupil of the optical system. Based on the image signal that appears on the image sensor array, the shape and/or location of the exit pupil is deduced from the separation between the microlens disk images that appear under each microlens. The shape of the exit pupil may be determined by the shape of the individual microlens images (which may overlap)—for example, a circular image indicates a circular exit pupil, a hexagonal image indicates a hexagonal exit pupil and a square image indicates a square exit pupil. In some embodiments, the shape may vary across the image sensor, indicating a change in the shape of the exit pupil from that apparent viewpoint on the sensor. The distance of the exit pupil from the microlens array and sensor may be determined by the pitch (distance between the relative centers) of the microlens images. As shown in
FIG. 2D , a smaller pitch indicates a further distance, according to the linear geometric relationship shown in the Figure. - In one particular example, the distance L between the optical exit pupil and the microlens array may be characterized the following equation:
-
L=F*X/(Y−X) - where:
-
- F is the separation between the microlens array and the sensor;
- X is the separation between two given (for example, neighboring) microlens centers (A and B); and
- Y is the separation between the centers of microlens images appearing on the image sensor below A and B.
- In another exemplary embodiment, a sensor or other mechanism is used to detect, determine, measure, or keep track of the configuration of a zoom lens. For example, a sensor may detect, determine and/or measure the position of a stepper motor used to drive the zoom lens, and this position may be used as an indicator of the zoom lens configuration. In these exemplary embodiments, the configuration of the zoom lens may be combined with a database or table that maps the configuration to a pre-determined exit pupil configuration. In some embodiments, the number of stepper motor positions may be discrete and finite, and an N-bit key may be used to uniquely denote each position, with each N-bit key corresponding to an entry in the database or table that corresponds to the pre-determined exit pupil configuration that relates to the corresponding stepper motor position.
- Notably, in those embodiments where the Light Field Data Acquisition Device connects to a post-processing system, such connection may be via wired and/or wireless architectures using any signaling technique now known or later developed. In addition, the configuration data may be provided and/or communicated to a post-processing system together with or separate from the associated Light Field Data using any format now known or later developed. Indeed, the model and/or device static light field configuration data may be provided and/or communicated to a post-processing system together with or separate from dynamic light field configuration data (i.e., characteristics, parameters and/or configurations of the Light Field Data Acquisition Device are not fixed, constant, predetermined and/or implicit). For example, in one embodiment, model and/or device static light field configuration data may be provided and/or communicated to a post-processing system upon initial connection and thereafter dynamic light field configuration data may be communicated to a post-processing system together with or separate from associated Light Field Data. All communication strategies, formats, techniques and/or architectures relating thereto are intended to fall within the scope of the present inventions.
- In one embodiment, the Light Field Data Acquisition Device acquires, determines, stores and/or records data which is representative of the Exit Pupil and sensor pitch. In this regard, in one embodiment, the light field acquisition device acquires, determines, stores and/or records data or information pertaining to the lens configuration (for example, zoom position and range) when the Light Field Data is acquired, collected, sampled and/or obtained (i.e., at the time of “exposure” or when the “shot” is taken). Circuitry in the light field acquisition device may calculate, determine and/or estimate the location of the exit pupil using, for example, the lens configuration.
- Notably, at a high level, an optical and/or geometric model for a light field capture system may include or provide information to convert or correlate data from an image sensor pixel to a representation of incoming light rays. The post-processing circuitry, having a model to convert or correlate pixel values to the incoming light rays from the light field acquisition device, may perform Light Field Processing (for example, compute images including, for example, images having different focal planes, as well as computing images which correct for, capture or address artifacts). The present inventions, in certain aspects, record, store and/or determine the optical parameters of the main lens system and the light field capture sensor which facilitates determining an optical and/or geometric model of certain aspects of the Light Field Data Acquisition Device.
- Once characteristics, parameters and/or configurations of the lens system and the light field sensor are recorded, stored and/or determined, post-processing circuitry may generate an optical and/or geometric model that “maps” or correlates sensor pixels to geometric rays of light or sets of geometric rays.
- Under certain circumstances, the Exit Pupil or Aperture Function may be considered a compact parameter of the Light Field Data Acquisition Device that describes or characterizes the lens system (which may include one or more lenses of any kind or type) of the acquisition device. In this way, post-processing circuitry may employ data which is representative of the Exit Pupil or Aperture Function (for example, size and/or shape of the exit pupil in some embodiments) to facilitate and/or allow Light Field Processing, including, for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device.
- In those instances where the system performs correction of lens system aberrations, a characterization or representation of the lens system and/or Ray Correction Function may be employed. In one exemplary embodiment, the lens system may be characterized or represented by a set of lens formulas that describe the shape, refraction, and/or spacing between each of the lens elements. Such formulas or relationships may describe how light rays are determined to traverse or pass through the optical system before acquisition by the image sensor. Indeed, a characterization or representation of the how light rays will traverse or pass through the optical system facilitates ray-tracing computation of the ray distortion function which may be employed for correction of optical aberrations. (See, for example, Ren Ng's PhD dissertation, “Digital Light Field Photography”, Stanford University 2006, page 135). In another exemplary embodiment, the lens system may be described by formulas and discrete approximation of the Ray Correction Function (or ray distortion function) itself.
- In those instances where vignetting of the lens affects light captured on the sensor surface, it may be advantageous to employ techniques and circuitry to correct, reduce, minimize, and/or eliminate vignetting. An example of such vignetting is darkening of photographs towards the corner, due to eclipsing and/or reduction of the area and/or occlusion (for example, due to internal blockages by boundaries of lens elements or by apertures or by other opaque elements within the barrel of the lens) of the exit pupil from oblique views. Light fields captured with some lens systems may encounter artifacts around the edge of the image if the vignetting of the lens system is not characterized and modeled. In one exemplary embodiment, the lens system is characterized, described and/or represented by the exit pupil parameter and a formula that characterizes the eclipsing of the exit pupil based on the pixel location. In this way, vignetting may be corrected, reduced, minimized, and/or eliminated by normalizing by the area of the eclipsed exit pupil at each pixel location.
- In one exemplary embodiment, a lookup table or the like may be used to test if rays are subject to vignetting. In one specific embodiment, a binary lookup table, accessed using the “discretized” X, Y, U, and V components of a geometric ray or set of rays may be checked when a system is performing Light Field Processing. If the binary lookup table stores a false or zero value for the geometric ray parameters, the information (for example, a pixel value) associated with that geometric ray may be discarded. In another specific exemplary embodiment, a numeric lookup table with values ranging from 0.0 to 1.0 may be checked when a system is performing Light Field Processing, accessed using the X, Y, U and V components of a geometric ray or set of rays. The information (for example, a pixel value) associated with the geometric ray parameters may be modified and/or adjusted (for example, by adjusting the pixel value to account for the occlusion or by using the lookup up value to normalize the pixel value) when a system is performing Light Field Processing.
- The lookup table may be obtained empirically, for example during a calibration step during the manufacture of the Light Field Data Acquisition Device, by using the device to acquire Light Field Data of a pre-determined Light Field (for example, a scene with constant and even (or nearly constant and nearly even) illumination, or otherwise predetermined and known scene or light field), and storing a lookup table of values normalized by dividing for each value in the Light Field Data, the empirically recorded value by the pre-determined scene or light field. In the Light Field Processing aspect of this exemplary embodiment, the lookup table is used during Light Field Processing by normalizing each value in the Light Field Data by scaling it by the inverse of the matching value in the lookup table. For example, for a Light Field Data Acquisition Device with an image sensor, the lookup table may be stored as a normalized sensor image in the Light Field Configuration Data that is supplied to Light Field Processing. During Light Field Processing on a given input Light Field Data set comprising the image sensor values, each value is weighted proportional to the inverse of the corresponding image sensor value in the normalized sensor image (lookup table).
- In yet another exemplary embodiment, the lookup table is represented by an analytic function that approximates the normalized sensor image (for example, for compactness, efficiency and/or optimization). For example, the analytic function and/or approximation used may be a stored subset of the sensor image (for example, the values under one microlens) combined with a process or procedure to map or correlate sensor image pixels in other parts of the image to a corresponding location in the stored subset. In one exemplary embodiment, the mapping or correlation process or procedure is to determine the 2D offset from a predetermined location, for example, the center of the closest microlens, and use the value in the stored subset at the same or nearly the same 2D offset from the center of the microlens in the stored subset. Indeed, methods for determining the 2D offset depend on the pattern of the microlens array, and mathematics are discussed below for exemplary embodiments that utilize data in the Light Field Configuration Data regarding the location of centers and radii of the microlens images in the sensor image.
- Notably, the preceding descriptions of exemplary embodiments are only intended to illustrate the general principles of measuring, determining, registering and/or calibrating a lookup table-like function for correcting aspects of vignetting or other undesirable characteristics of the Light Field Data Acquisition Device, and for incorporation in Light Field Configuration Data. Any suitable procedures for measuring, determining, registering, calibrating, approximating, representing and/or storing such lookup table functions as part of Light Field Configuration Data, whether now known or later developed, are intended to fall within this aspect of the scope of the present inventions.
- Communicating Optical Representation of Characteristics, Parameters and/or Configurations of the Light Field Data Acquisition Device: As noted above, the optical model for converting from recorded pixel data to geometric light ray information may be constructed based on or using one or more of the characteristics, parameters and/or configurations of the Light Field Data Acquisition Device that describe the light field sensor and the main lens system or optical system of the of the Light Field Data Acquisition Device. The model and/or device static light field configuration data, for example, may be stored in memory on the Light Field Data Acquisition Device, for example, non-volatile memory (such as, a ROM-like memory—for example, electrically programmable read-only memory (“EPROM”), electrically erasable programmable read-only memory (“EEPROM”) and/or Flash memory (for example, NOR or NAND)). The data which is representative of such static characteristics, parameters and/or configurations may be written to memory at any time, for example, during the manufacturing process. In certain embodiments, such model and/or device static light field configuration data include the microlens pitch, microlens pattern, and/or sensor pixel pitch. The model and/or device static light field configuration data may also include the spacing between the microlens array and the sensor surface.
- Notably, in those embodiments where the Light Field Data Acquisition Device includes a “fixed” lens system (for example, a light field acquisition device may be manufactured with an attached lens system), the size and shape of the exit pupil may be determined based on, for example, zoom and/or focus position of the optical system of the Light Field Data Acquisition Device. The Exit Pupil or Aperture Function presentation, as related to one or more of the zoom and focus positions, may be predetermined and/or stored in memory (in the form of a look-up table, formula, or the like) on the Light Field Data Acquisition Device and/or in memory on an external post-processing system.
- Certain device static light field configuration data (for example, offset and rotation of the microlens array relative to the sensor surface) may vary for each individual Light Field Data Acquisition Device. As such, where certain characteristics, parameters and/or configurations of the Light Field Data Acquisition Device vary on a device-by-device basis, such characteristics, parameters and/or configurations may be stored and/or updated during, for example, a registration procedure, after construction of the Light Field Data Acquisition Device. This data or information may be stored in non-volatile memory (for example, SRAM, NOR or NAND Flash or EEPROM) on or in the Light Field Data Acquisition Device, and, indeed, may be set as part of the device calibration process after construction of the acquisition device.
- Communication and Storage of Characteristics, Parameters and/or Configurations in an Interchangeable Lens System: In certain embodiments, the Light Field Data Acquisition Device may include one or more interchangeable lenses. In these embodiments, the Light Field Data Acquisition Device may be provided with (for example, by the user via the user interface) and/or detect (for example, via data acquired from the interchangeable lens) details and/or changes to the optical system thereof. In one embodiment, the Light Field Data Acquisition Device retrieves information from the interchangeable lens to determine the characteristics, parameters and/or configurations and/or changes thereto of the optical system. In another embodiment, the user may input, via the user interface, the characteristics, parameters and/or configurations and/or changes thereto of the optical system. Such information may be passed using any communication techniques, circuitry, (electrical or mechanical) interfaces and/or architectures whether now known or later developed.
- In certain exemplary embodiments, the interchangeable lens (i.e., the lens which is incorporated into or on the optical system of the Light Field Data Acquisition Device) and/or the Light Field Data Acquisition Device contains a lookup table in memory that correlates or “maps” (i) the zoom and focus of the optical system to (ii) representation of an Exit Pupil (or Aperture Function). This Exit Pupil may be provided to post-processing circuitry (disposed on the Light Field Data Acquisition Device and/or a stand-alone post-processing system) to facilitate and/or enable Light Field Processing. In another embodiment, the Exit Pupil may be determined by a mathematical relationship based on the zoom and focus of the optical system. Notably, in embodiments having a fixed focus or zoom position, the determination of the size and shape of the exit pupil may depend on different parameters (for example, in an embodiment with a fixed zoom position, the exit pupil may vary only with changes in the focus position).
- In another exemplary embodiment, a firmware update is applied to the Light Field Data Acquisition Device in the event that an interchangeable lens is incorporated into optical system. This update may be implemented as a “patch” and may be installed by the user or may be installed automatically when the lens is first coupled to the Light Field Data Acquisition Device. The firmware update may provide a mechanism for determining certain optical parameters of the optical or lens system based on information the lens may provide at exposure or Light Field Data collection, acquisition and/or capture. For example, the firmware update may allow the Light Field Data Acquisition Device to look up data representing the Exit Pupil or Aperture Function of the lens based on the configuration of the lens (for example, one or more predetermined zoom and focus positions of the lens).
- In yet another embodiment, memory in the Light Field Data Acquisition Device includes data of one or more interchangeable lenses that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device. In this embodiment, the memory (for example, non-volatile memory) includes data which is representative of the characteristics, parameters and/or configurations of a plurality of interchangeable lenses that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device. As such, memory resident in the Light Field Data Acquisition Device may contain a lookup table that “maps” (i) the zoom and focus to (ii) data representing the Exit Pupil. In one embodiment, the resident memory includes a plurality of mathematical relationships wherein a selected one of the predetermined mathematical relationships, based on a particular interchangeable lens implemented or incorporated into the optical system of the Light Field Data Acquisition Device, is employed to determine the exit pupil size and/or shape based on, for example, a particular zoom and focus of the optical system.
- In one exemplary embodiment, the memory of the Light Field Data Acquisition Device contains a database of all available interchangeable lenses that will fit the body. Each entry may be keyed by information made available to the camera by the lens system. The key may be unique in the form of a lens model number, or unique by a combination of available parameters such as min zoom, max zoom, and f-number. Indeed, this key may be used to lookup a particular or predetermined mathematical relationship for determining the exit pupil size and/or shape—for example, by converting from the current zoom and focus position of the lens to the exit pupil location.
- In another exemplary embodiment, when an interchangeable lens is attached to a Light Field Data Acquisition Device, that device may query an external source or sources (for example, an Internet-capable camera may query a networked database) for updates using a wired (for example, a USB cable connection to a local computer) or wireless (for example, a Wi-Fi enabled device) connection. In this embodiment, the Light Field Data Acquisition Device may query the external source or sources the first time, each time or any time an interchangeable lens is attached to the device to check for and/or update data which is representative of the characteristics, parameters and/or configurations of a plurality of interchangeable lens that may be implemented or incorporated into the optical system of the Light Field Data Acquisition Device.
- In the preceding discussion, “zoom and focus” are often used as exemplary characteristics of a lens' optical configuration. Wherever “zoom and focus” are used herein in this context, it should be understood that any subset of the characteristics of an optical systems' configuration, and indeed any of the representations of the optical and/or geometric models of the acquisition device, may be substituted in place of zoom and focus characteristics, or in addition thereto, and such substitutions and generalizations are intended to fall within the scope of the present inventions.
- Optical Representation of Microlens Array: In certain embodiments, data which is representative of the microlens array may be recorded or stored to allow for images to be processed via post-processing circuitry. Such data may be stored in a separate configuration data file or together with the Light Field Data file. The configuration data file may be associated with one or more files each containing Light Field Data. For example, the configuration data file may be stored in a header of the electronic file that stores the Light Field Data. Alternatively, the configuration data file may be stored in a separate electronic file relative to the electronic file including the associated Light Field Data. Moreover, the electronic file including the configuration data may be associated with and separate from a plurality of electronic files each containing different Light Field Data.
- In one exemplary embodiment, the relevant/associated Light Field Configuration Data may be stored in a Standard Image Format, in a header in the Light Field Data file. In this embodiment, the header includes, among other things, Light Field Configuration Data (for example, including data which is representative of the characteristics, parameters and/or configurations of the optical system). Post-processing circuitry in, for example, a stand-alone post-processing system, may read or interpret the header of the data file to facilitate construction of a model to use for processing the associated or corresponding Light Field Data. In this way, the post-processing circuitry may convert or interpret data from the image sensor to perform Light Field Processing, for example, generate, manipulate and/or edit one or more images using the Light Field Data (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device). For example, the post-processing circuitry may convert or interpret Light Field Data from image sensor pixel locations to a representation of incoming light rays.
- Center Locations and Shapes of Projected Disks: In one embodiment, the system acquires or determines data which is representative of the center locations and sizes of the projected microlens disks on the surface of the imaging sensor. With reference to
FIG. 5 , the locations, size and shape of the projected disks are overlayed onto the captured image. Information of the locations, size and shapes of the projected image of the lenslets may be employed by post-processing circuitry for Light Field Processing. Indeed, determining the centers and sizes of microlens disks may be determined based on the key optical parameters listed previously, for example using the calculation procedures described in an exemplary embodiment below. - Projected microlens disk locations and shapes: In one embodiment, the system may store or record data which is representative of the (i) X and Y offset of center lenslet projection to the center of the image sensor (or any other offsets to represent translation of microlens array relative to the image sensor), (ii) rotation of microlens array relative to imaging sensor, (iii) microlens grid pattern (for example, hexagonal or square), (iv) radius of the projected lenslets, and/or (v) spacing between neighboring centers of projected lenslets. Such data may be employed by the post-processing circuitry to determine an optical and/or geometric model that may be used for Light Field Processing (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the Light Field Data by the Light Field Data Acquisition Device). (See, for example,
FIG. 6 ). Notably, the X and Y offset values are the spatial distance between the center pixel on the sensor and the center of a central projected microlens disk (in those situations where the microlens include a disk shape). In addition, the spacing between neighboring disk centers is the pitch of the projected microlens array. The radius of each projected lenslet disk may be considered the extent of the disk of light projected by a lenslet in the microlens array (SeeFIGS. 14A-14C ). Note that although the diameter of the projected disks appears approximately the same size in the illustration as the pitch, the numbers are different and are used for differing purposes. - In this embodiment, circuitry may construct a geometric or optical model which converts sensor locations (for example, the X and Y location of pixel coordinates) into information representing a set of incoming light rays in the following manner:
- The X and Y offsets specify the location of the image formed by a central microlens on the surface of the sensor, referred to as MLXOnSensor and MLYOnSensor, respectively. The size of the image formed by the microlens is specified by the radius of the projected lenslet, referred to as MLROnSensor.
- For each pixel, centered at PXOnSensor and PYOnSensor, if:
-
Sqrt((PXOnSensor−MLXOnSensor)2+(PYOnSensor−MLYOnSensor)2)<MLROnSensor, - then the pixel may be considered as in the projected image of specified microlens.
- The locations of the centers of all projected microlens images onto the sensor surface may be determined by adding the spacing between neighboring lenslets, accounting for rotation. In an exemplary embodiment with a square microlens grid pattern, rotation of Theta relative to the sensor surface, and a spacing of MLSpacing between microlens images on the sensor surface, the 4 neighboring microlenses are centered at the following locations:
- MLXOnSensor+MLSpacing*COS(Theta), MLYOnSensor+MLSpacing*SIN(Theta)
- MLXOnSensor−MLSpacing*COS(Theta), MLYOnSensor+MLSpacing*SIN(Theta)
- MLXOnSensor−MLSpacing*SIN(Theta), MLYOnSensor+MLSpacing*COS(Theta)
- MLXOnSensor+MLSpacing*SIN(Theta), MLYOnSensorMLSpacing*COS(Theta)
- The location of and spacing of the projection of the microlens disk on the sensor surface may determine the location and extents in X and Y components/coordinates of the 4 dimensional set of geometric rays. In an embodiment that converts locations on the imaging sensor to sets of values in X, Y, U, and V, the X and Y components of all pixels contained in a projected microlens image may be considered centered at the center of the microlens projection and have the same size as the entire area of the microlens. The U and V components may be considered angular information and may in some embodiments be determined in the following manner for a pixel centered at PXOnSensor, PYOnSensor under a microlens centered at MLXOnSensor, MLYOnSensor:
-
U=(PXOnSensor−MLXOnSensor)/MLSpacing -
V=(PYOnSensor−MLYOnSensor)/MLSpacing - In these embodiments, U and V are in a normalized coordinate space ranging between −0.5 and 0.5.
- Notably, in this embodiment any sensor pixels not illuminated (there is no projected microlens image with a center less than or equal to one projected disk radius from the location of the pixel) may not be used for Light Field Processing.
- In certain embodiments, it may be advantageous that the locations and sizes of the projected lenslet disks onto the captured image are regular or near regular in appearance.
- Compact Optical Specification of Capture System: In one embodiment, the system may store or record data which is representative of the (i) location and orientation of microlens array relative to the imaging sensor in 3-space, (ii) microlens grid pattern (for example, hexagonal or square), (iii) lens formulas or approximations for microlens array, (iv) lens formulas and spacings, or approximations for the main lens system, and/or (v) location and orientation of the light field sensor relative to the main lens system. (See, for example,
FIGS. 5 and 6 ). Such data may be employed by the post-processing circuitry to determine a model of the optical path of the Light Field Data Acquisition Device. - The model may be employed by post-processing circuitry to perform Light Field Processing (for example, generate, manipulate and/or edit one or more images using the light field image data (for example, focusing or refocusing one or more images at different depths—post-data acquisition or after acquisition of the light field image data by the Light Field Data Acquisition Device)).
- In one exemplary embodiment, the post-processing system acquires and/or determines (i) the x-offset of a central lenslet in the microlens array relative to the center of the imaging sensor, (ii) the y-offset of a central lenslet in the microlens array relative to the center of the imaging sensor, (iii) the rotation of the microlens array (iv) the separation of the microlens array from the imaging sensor, (v) the pitch of the microlens array, (vi) the pattern of the microlens array, and (vii) the location of the center of the exit pupil relative to the microlens array.
- Characteristics, Parameters and/or Configurations Optical System and Lookup: In one embodiment, the system provides system, electronic, scene-dependent and/or optical characteristics, parameters, properties, models, and/or configurations via a lookup system. In a lookup system, according to one embodiment, the Light Field Data Acquisition Device stores or saves the Light Field Data with one or more keys identifying the optical system or components of the optical system of the acquisition device. For example, in one embodiment, a Light Field Data Acquisition Device may store a plurality of keys (for example, two keys), wherein one key may uniquely identify the Light Field Data Acquisition Device, and another key may uniquely identify the characteristics, parameters and/or configurations of the acquisition device when the Light Field Data was taken, acquired, and/or captured. Indeed, in one specific exemplary embodiment, the Light Field Data Acquisition Device according to this aspect of the inventions may have a fixed number of zoom configurations and a unique identifier for each of the zoom configurations—wherein each correspond to a predetermined key.
- Notably, in one embodiment, the Light Field Data Acquisition Device may (in addition thereto or in lieu thereof) store or save an N-bit key to identify the characteristics, parameters and/or configurations of the acquisition device associated with or corresponding to the acquired or captured Light Field Data. In one exemplary embodiment, the Light Field Data Acquisition Device includes a key uniquely identifying one, some or all of the (dynamic and/or static) exposure characteristics, parameters and/or configurations of the Light Field Data Acquisition Device, including:
-
- Data or information representing Aperture Function or Exit Pupil (for example, the size and/or shape and/or 3D position of the optical exit pupil of the lens system) of the Light Field Data acquisition system (or data/information which is representative thereof) relative to the microlens array. In one embodiment, the Exit Pupil may vary with each configuration of the lens system of the Light Field Data Acquisition Device. Indeed, size and/or shape of the Exit Pupil may change on a shot-by-shot basis. (See,
FIGS. 2D , 5 and 6); and/or - The Light Field Sensor Geometry Model, which is defined generally as the optical and/or geometric model of the sensor that records Light Field Data. In a specific exemplary embodiment, this sensor is a microlens array in front of an image sensor array, and the Light Field Sensor Geometry Model may include one, some or all of the following characteristics, parameters and/or configurations:
- The geometry of the microlens array. In the context of the present inventions, “microlens array” is a term that may generally mean any window with a micro-optical patterning. Thus, the geometry of the microlens array would include the surface geometry of the micro-optical patterning; and/or
- The pitch of the lenslets in a microlens array. The pitch of the lenslets in the microlens array may be characterized as the distance between the centers of neighboring microlenses and may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. In some embodiments, the pitch may be a single number that is constant and valid for all lenslets on the microlens array (See,
FIGS. 7A and 7B ). In other exemplary embodiments, the pitch may vary based on the spatial location in the microlens array. In other exemplary embodiments, the term “pitch” may be used generally to refer to the pattern of the microlens array, which may be regular, irregular, repeating or non-repeating; and/or - The distance between the microlens array and the surface of the imaging sensor. In certain embodiments, it may be preferred that this distance is the same as (or substantially the same as) the focal length of the microlens array. (See,
FIGS. 2B-2D ) and/or - The offsets and rotation of the microlens array relative to the imaging sensor.
- Data or information representing Aperture Function or Exit Pupil (for example, the size and/or shape and/or 3D position of the optical exit pupil of the lens system) of the Light Field Data acquisition system (or data/information which is representative thereof) relative to the microlens array. In one embodiment, the Exit Pupil may vary with each configuration of the lens system of the Light Field Data Acquisition Device. Indeed, size and/or shape of the Exit Pupil may change on a shot-by-shot basis. (See,
- The relative offsets and rotation of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device or may vary between Light Field Data Acquisition Devices (even between models, versions or pieces thereof). (See,
FIG. 6 ); and/or -
-
- The pattern of the microlens array (for example, hex or square). The pattern of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. (See,
FIGS. 8A , 8B and 8C); and/or - The pitch of the pixels/sensors of the imaging sensor. The pitch of the pixels/sensors of the imaging sensor may be characterized as the distance between the centers of neighboring sensor pixels and may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. (See,
FIG. 9 ).
- The pattern of the microlens array (for example, hex or square). The pattern of the microlens array may be fixed or constant for a predetermined model, series or version of Light Field Data Acquisition Device. (See,
-
- The N-bit key may be provided to post-processing circuitry which, using a look-up table or the like, may construct or reconstruct optical properties of the Light Field Data Acquisition Device. In one embodiment, the post-processing circuitry may access a resident memory, local database or query an external source (for example, an Internet source) for the optical information associated with the N-bit key.
- In other exemplary embodiments employing one or multiple keys, one some or all of the keys may be encodings or representations of values for specific characteristics, parameters, models and or configurations within the Light Field Configuration Data. As a specific example, in some embodiments, the focal length of the zoom configuration may be represented or stored as an N-bit key, where the value of the N-bits encode an N-bit floating point bit-pattern for the focal length of the zoom position in millimeters. Notably, as mentioned above, each of the embodiments may be employed alone or in combination with one or more of the other embodiments. For example, in one embodiment, the optical model or data of the Light Field Data Acquisition Device may be represented using a combination of storing some configuration parameters as well as some information uniquely identifying certain elements or parts of the acquisition device (for example, based on the N-bit key embodiment discussed above). In this way, post-processing circuitry may read or interpret the data file(s) of the image data, the configuration data and the N-bit key, to create or recreate an optical model of the Light Field Data Acquisition Device which was employed to acquire and/or capture the Light Field Data (i.e., the optical model of the acquisition system which is associated with the Light Field Data).
- In one exemplary embodiment, the translation and rotation configuration parameters of the microlens array relative to the image sensor are stored, recorded or saved to a file as well as the camera model number and zoom position. When this data is read by the post-processing circuitry of, for example, a post-processing system, a suitable geometric and/or optical model may be constructed by determining or looking up the optical system of the camera model at the particular zoom location (based on the N-bit key), and then applying the translation and rotation parameters of the microlens array to more fully express the geometric and/or optical model of the acquisition system which is associated with the Light Field Data.
- Notably, in the exemplary embodiments hereof, the data processing, analyses, computations, generations and/or manipulations may be implemented in or with circuitry disposed (in part or in whole) in/on the camera or in/on an external post-processing system. Such circuitry may include one or more microprocessors, Application-Specific Integrated Circuits (ASICs), digital signal processors (DSPs), and/or programmable gate arrays (for example, field-programmable gate arrays (FPGAs)). Indeed, the circuitry may be any type or form of circuitry whether now known or later developed. For example, the post-processing circuitry may include a single component or a multiplicity of components (microprocessors, FPGAs, ASICs and DSPs), either active and/or passive, which are coupled together to implement, provide and/or perform a desired operation/function/application; all of which are intended to fall within the scope of the present invention.
- The term “circuit” may mean, among other things, a single component (for example, electrical/electronic) or a multiplicity of components (whether in integrated circuit form, discrete form or otherwise), which are active and/or passive, and which are coupled together to provide or perform a desired function. The term “circuitry” may mean, among other things, a circuit (whether integrated, discrete or otherwise), a group of such circuits, one or more processors, one or more state machines, one or more processors implementing software, or a combination of one or more circuits (whether integrated, discrete or otherwise), one or more state machines, one or more processors, and/or one or more processors implementing software. Moreover, the term “optics” means a system comprising a plurality of components used to affect the propagation of light, including but not limited to lens elements, windows, apertures and mirrors.
- Further, as mentioned above, in operation, the post-processing circuitry may perform or execute one or more applications, routines, programs and/or data structures that implement particular methods, techniques, tasks or operations described and illustrated herein. The functionality of the applications, routines or programs may be combined or distributed. Further, the applications, routines or programs may be implementing by the post-processing circuitry using any programming language whether now known or later developed, including, for example, assembly, FORTRAN, C, C++, and BASIC, whether compiled or uncompiled code; all of which are intended to fall within the scope of the present invention.
- Exemplary File and File Structure of Light Field Data: A Light Field Data File is an electronic data file which includes one or more sets of Light Field Data. (See, for example,
FIGS. 15A and 15B ). The Light Field Data File may include one or more sets of Light Field Data which, in whole or in part, is compressed or uncompressed and/or processed or unprocessed. A set of Light Field Data may be data of a scene, image or “exposure” acquired, captured and/or sampled via a Light Field Data Acquisition Device. - The Light Field Data File may include any file format or structure, whether the data contained therein is, in whole or in part, in compressed or uncompressed form, and/or whether the data contained therein is, in whole or in part, processed or unprocessed. In one exemplary embodiment, file format or structure of the Light Field Data file includes a start code and/or end code to indicate the beginning and/or end, respectively, of a set of a Light Field Data. In addition thereto, or in lieu thereof, the set of Light Field Data may include a predetermined or predefined amount of data. (See, for example,
FIG. 15A ). As such, the start or end of a given set of Light Field Data may be indirectly based on an amount of data (with or without start and/or end codes). Notably, any file format or file structure of the Light Field Data file, whether now known or later developed, is intended to fall within the scope of the present invention. - In one exemplary embodiment, the file format or structure of the Light Field Data may include metadata (for example, as a header section). (See, for example,
FIG. 15B wherein in this exemplary embodiment, such header is located in the beginning of the file—although it need not). In one embodiment, the metadata may be definitional type data that provides information regarding the Light Field Data and/or the environment or parameters in which it was acquired, captured and/or sampled. For example, the metadata may include and/or consist of Light Field Configuration Data. - As noted above, the Light Field Data File may include one or more sets of Light Field Data of an image or “exposure” acquired, captured and/or sampled via a Light Field Data Acquisition Device. Where the file includes a plurality of sets of Light Field Data, such sets may be a series of images or exposures (temporally contiguous) or a plurality of images that were acquired using the same or substantially the same acquisition settings. Under these circumstances, the header section may include and/or consist of Light Field Configuration Data that is applicable to the plurality of sets of Light Field Data.
- The Light Field Data File may be stored and/or maintained in memory (for example, DRAM, SRAM, Flash memory, conventional-type hard drive, tape, CD and/or DVD). (See, for example,
FIGS. 16A-16C ). Such memory may be accessed by post-processing circuitry to perform Light Field Processing (for example, generating, manipulating and/or editing the image data corresponding to the Light Field Data—after acquisition or recording thereof (including, for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field after acquisition of the Light Field Data)). The memory may be internal or external to the Light Field Data Acquisition Device. Further, the memory may be discrete or integrated relative to the circuitry in the Light Field Data Acquisition Device. In addition thereto, or in lieu thereof, the memory may be discrete or integrated relative to the post-processing circuitry/system. - In one embodiment, the post-processing circuitry may access the Light Field Data File (having one or more sets of Light Field Data) and, based on or in response to user inputs or instructions, perform Light Field Processing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field of an image after acquisition of Light Field Data associated with such image). Thereafter, the post-processing circuitry may store the image within the Light Field Data File (for example, append such image thereto) and/or overwrite the associated Light Field Data contained in the Light Field Data File. In addition thereto, or in lieu thereof, the post-processing circuitry may create (in response to user inputs/instructions) a separate file containing the image which was generated using the associated Light Field Data. This process may be repeated to perform further Light Field Processing and generate additional images using the Light Field Data (for example, re-adjust, re-select and/or re-define a second focus and/or a second depth of field of the second image associated with the same Light Field Data as the first image—again after acquisition of Light Field Data associated with such images).
- Notably, the Light Field Data Acquisition Device may include a display to allow the user to view an image or video generated using one or more sets of Light Field Data. (See for example,
FIGS. 16C and 16D ). The display may facilitate the user to implement desired or predetermined Light Field Processing of one or more sets of Light Field Data in a Light Field Data File. - The Light Field Data Acquisition Device may also couple to an external display as well as, for example, a recording device, memory, printer and/or processor circuitry (See, for example,
FIGS. 16E and 16F ). In this way, the Light Field Data Acquisition Device or post-processing circuitry may output image data to display, processor circuitry (for example, a special purpose or general purpose processor), and/or a video recording device. (See, for example,FIGS. 16E and 16F ). Moreover, such external devices or circuitry may facilitate, for example, storage of Light Field Data Files and Light Field Processing of Light Field Data Files. - The Light Field Data Acquisition Device (and/or the post-processing system) may communicate with memory (which may store the electronic data files having one or more sets of Light Field Data and/or Light Field Configuration Data) via write circuitry and read circuitry. (See,
FIG. 16G ). The write and read circuitry may couple to processing circuitry to implement, for example, Light Field Processing which generates, manipulates and/or edits (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field) the image data corresponding to the Light Field Data—after acquisition or recording thereof. The processing circuitry (for example, one or more processors, one or more state machines, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays) may generate electronic data files including Light Field Data (for example, in a compressed or non-compressed form. As discussed in detail below, such files may include the Light Field Data which is interleaved, threaded, watermarked, encoded, multiplexed and/or meshed into the data of the Standard Image Format. - Notably, as discussed herein, Light Field Configuration Data may be stored in a header or in an electronic file that is separate from the electronic file(s) containing the associated Light Field Data. (See, for example,
FIGS. 15B and 15C ). Where the Light Field Configuration Data is stored in a separate electronic file, such file may be stored and/or maintained in memory (for example, DRAM, SRAM, Flash memory, conventional-type hard drive, tape, CD and/or DVD). (See, for example,FIGS. 16A-16C ). As noted above, such memory may be accessed by post-processing circuitry to perform Light Field Processing (for example, generating, manipulating and/or editing the image data corresponding to the Light Field Data—after acquisition or recording thereof (including, for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field after acquisition of the Light Field Data using the Light Field Configuration Data)). Again, the memory may be internal or external to the Light Field Data Acquisition Device and/or the post-processing system. Further, the memory may be discrete or integrated relative to the circuitry in the Light Field Data Acquisition Device. In addition thereto, or in lieu thereof, the memory may be discrete or integrated relative to the post-processing circuitry/system. - Additional Exemplary File and File Structure including Light Field Data and/or Raw Image Data: In another set of embodiments, one or more sets of Light Field Data may be appended to or integrated into image data in or having a Standard Image Format. (See, for example,
FIGS. 17A-17C ). In these embodiments, the one or more sets of Light Field Data is/are associated with the image data in a Standard Image Format in that such one or more sets of Light Field Data may be used to generate the image which is represented in the Standard Image Format. The Light Field Data may include one or more sets of Light Field Data which, in whole or in part, is compressed or uncompressed and/or processed or unprocessed. Notably, the Standard Image Format may be an open format or a proprietary format. - The Light Field Data in the Standard Image Format—Light Field Data File may include any of the attributes and/or characteristics discussed above in conjunction with the Light Field Data File. For example, in one exemplary embodiment, the electronic data file may include metadata. (See, for example,
FIG. 17B ). In one exemplary embodiment, the Light Field Data may include metadata (for example, Light Field Configuration Data in a header section). (See, for example,FIG. 17C wherein in this exemplary embodiment, such header is located in the beginning of the file—although it need not). Indeed, the metadata of the Light Field Data may be incorporated into the metadata associated with the Standard Image Format. Although the attributes and/or characteristics of the Light Field Data File discussed above are applicable to the Light Field Data in the Standard Image Format—Light Field Data File, for the sake of brevity, such discussion will not be repeated here. - In one embodiment, the Light Field Data is interleaved, threaded, watermarked, encoded, multiplexed and/or meshed into the data of the Standard Image Format. (See, for example,
FIG. 17D ). In this embodiment, processing or reading circuitry may extract and/or decode the data of the image in the Standard Image Format relative to the data set(s) of the Light Field Data. - Notably, in another set of embodiments, the non-light field raw image data which was employed to generate the image data in the Standard Image Format, may be appended to or integrated into image data in a Standard Image Format. (See, for example,
FIGS. 17E and 17F ). In these embodiments, the raw image data (which may or may not be Light Field Data) which is associated with the image data in a Standard Image Format is stored in the same file as the image data in a Standard Image Format. Such raw image data may be compressed or uncompressed and/or processed (in whole or in part) or unprocessed. In one exemplary embodiment, such raw data is a representation of the original single-channel raw pixel values read off a sensor with a color mosaic filter array (for example, Bayer color filter array). In another specific exemplary embodiment, such raw pixel values are from a Light Field Data Acquisition Device, and hence the raw data is a representation of Light Field Data recorded by such a device. - In these embodiments, Light Field Configuration Data may be stored in the header or in an electronic file that is separate from the associated Light Field Data. (See, for example,
FIGS. 15C , 17B and 17C). Where the Light Field Configuration Data is stored in a separate electronic file, such file may be stored and/or maintained in memory and accessed during processing of the image corresponding to or in the Standard Image Format and/or the Light Field Data (for example, as discussed immediately below). - Exemplary Processing of Files and File Structures including Light Field Data and/or Raw Image Data: With reference to
FIG. 18A , in one embodiment, circuitry may access a Data File illustrated inFIGS. 17A-17F (for example, a Standard Image Format—Light Field Data File) and read or display the image corresponding to the Standard Image Format. Thereafter, and based on or in response to one or more inputs or instructions (for example, user inputs or instructions), the image corresponding to the Standard Image Format may be modified using, for example, the one or more data sets of the Light Field Data associated with the image. In one embodiment, based on or in response to one or more inputs or instructions, circuitry may perform Light Field Processing (for example, adjusting, selecting, defining and/or redefining the focus and/or depth of field of an image after acquisition of Light Field Data associated with such image (wherein the image during acquisition included an original focus and depth of field)) to modify the image and thereby provide a new image (having, for example, a new focus and/or depth of field). Thereafter, the circuitry may store or re-store the image within the Data File (for example, (i) replace or overwrite the previous image by storing data in the Standard Image Format which is representative of the new image or (ii) append data in the Standard Image Format which is representative of the such new image). In addition thereto, or in lieu thereof, the circuitry may create (in response to, for example, user inputs/instructions) a separate file containing data corresponding to the new image. Indeed, in addition to the data (in the Standard Image Format) which is representative of the image, such new or separate file may or may not contain the Light Field Data associated therewith. - Notably, when the circuitry is performing Light Field Processing to generate the modified image, the circuitry may employ the Standard Image Format—Light Field Data File (or a portion thereof) as a frame buffer. Such a technique provides for efficient use of memory resources.
- In another exemplary embodiment, the present inventions utilize the standard image portion of the Standard Image Format—Light Field Data File as a “File Framebuffer.” Specifically, this File Framebuffer that represents the pixels to be displayed, is displayed on any display via any Standard Display Mechanism (i.e. method or system, whether now known or developed in the future, that may read, interpret and/or display the standard image portion of the Data File). In order to illustrate the principles, the Standard Display Mechanism may, for example, be one of: a web browser; an image viewer that is possibly integrated with an operating system; a third-party piece of software for image organization, viewing editing and/or slideshows; an internet-based photo sharing website or service; a printing service such as a kiosk at a departmental store; and an internet-based printing service that enables upload of Standard Image Formats. Notably, such Standard Display Mechanisms may not be able to interpret, process and/or display the Light Field Data portion of the Standard Image—Light Field Data File. In this exemplary embodiment, the modify—store/restore component makes use of the Light Field Data portion of the file in order to create a modified image through Light Field Processing, replacing the “File Framebuffer” in order to provide new pixels for Standard Display Mechanisms. Notably, the “File Framebuffer” serves as a persistent store of pixels for the present invention to store/restore the effect of the “modify” component for potential display on any Standard Display Mechanism. The process of read/display—modify—store/re-store may include many permutations and/or combinations. For example, after modifying the image (using the Light Field Data which is associated therewith) to generate the new image, such new image may be re-read or re-displayed. (See, for example,
FIG. 18B ). Indeed, prior to storing/re-storing data which is representative of the new image (in the Standard Image Format), the user may instruct the circuitry to perform a re-modify (i.e., modify the original image again or modify the new image). (See, for example,FIG. 18C ). All permutations and/or combinations of read/display—modify—store/re-store are intended to fall within the scope of the present inventions (see, for example,FIGS. 18D and 18E ); however, for the sake of brevity, such permutations and/or combinations of read/display—modify—store/re-store will not be discussed separately herein. - Notably, when storing or re-storing the image within the Data File (for example, (i) replace or overwrite the previous image by storing data in the Standard Image Format which is representative of the new image or (ii) append data in the Standard Image Format which is representative of the such new image), the circuitry may, in response to user inputs or instructions, generate a new Standard Image Format—Light Field Data File (wherein the Light Field Data may be substantially unchanged) or generate a Standard Image File only (i.e., discard the Light Field Image Data). The user may also instruct the circuitry to change the standard format of the Standard Image File prior to storing or re-storing the data (in the selected Standard Image Format) which is representative of the modified image.
- As indicated above, the read/display—modify—store/re-store process is also applicable to the Standard Image Format—Raw Image Data File illustrated in
FIGS. 17D and 17E . The process in connection with the Standard Image Format—Raw Image Data File is substantially similar to the process for the Standard Image Format—Light Field Image Data File (discussed immediately above) and, as such, for the sake of brevity, the discussion will not be repeated. - In an exemplary embodiment of the read/display—modify—store/re-store process for the Standard Image—Raw Image Data File, the modify portion of the process includes any type of processing that is accessible and/or possible from the Raw Image data. Notably, such type of processing may not be accessible and/or possible from the Standard Image data alone. For example, such processing includes, but is not limited to: changing the white-balance information to affect appearance of color; changing the exposure level to brighten or darken the image; applying dynamic range alteration in order to, for example, reducing the dynamic range by raising the brightness of the dark areas and reducing the brightness of the light areas. Notably, any type of image processing that is applicable to Raw Image data, whether now known or developed in the future, is intended to fall within the scope of the present inventions.
- Exemplary Mixed-Mode Display, Processing and Communication. With reference to
FIG. 19 , in one embodiment, a user may utilize any Standard Display Mechanism to view the Standard Image portion of a Data File (for example, the exemplary electronic data files ofFIGS. 17A-17F ), reading and/or displaying the image corresponding to the Standard Image Format; a user may also utilize the read/display—modify—store/re-store process described above, possibly changing the Standard Image and/or Light Field Data within the Data File. A user may subsequently utilize a Standard Display Mechanism on the resulting Data File, for example to view or share the image via the internet, or to print it via a printing service. A user may subsequently repeat this process any number of times. - The following is an exemplary scenario utilizing an embodiment of the present inventions and, as such, is not intended to be limiting to the permutations and/or combinations of read/display—modify—store/re-store embodiments. With that in mind, in one exemplary embodiment, the user acquires a Data File in a Standard Image Format—Light Field Data format, for example, via a recording from a Light Field Data Acquisition Device. The Light Field Data Acquisition Device, in response to inputs/instructions (for example, user inputs/instructions), communicates the Data File to a computer system, which includes a image viewing computer program (for example, Standard Display Mechanisms) to provide and allow viewing of the Data File as an image on a display. The Light Field Data Acquisition Device and/or computer system, in response to inputs/instructions (for example, user inputs/instructions), may also communicate the Data File to an internet image sharing site (another Standard Display Mechanism), for example, in order to facilitate sharing of the Data File.
- Another user may then download the Data File from the sharing site, and view it on a computer (Standard Display Mechanism which is, for example, local to the second user). The second user, employing a computer system, may open the Data File with a software program that implements the Read/Display—Modify—Store/Re-store process. The second user views the image, and applies or implements Light Field Processing (for example, changes the optical focus of the image on to a closer focal plane), and stores the resulting image into the File Framebuffer comprising the Standard Image portion of the Data File in the Standard Image Format. The second user then prints the Data File using a printer (another Standard Display Mechanism). The second user may then upload the Data File to an Internet image sharing site (Standard Display Mechanism), which may be the same or a different sharing site. Another user (i.e., the first user or a third user) downloads the Data File and prints it. The preceding scenario illustrates certain aspects and exemplary embodiments of the present invention.
- Notably, the Light Field Data Acquisition Device and/or post-processing system may include a user interface to allow a user/operator to monitor, control and/or program acquisition device and/or post-processing system. (See, for example,
FIGS. 1B , 1C, 1E, 1F, 16C, 16F, and 20). With reference toFIG. 20 , in one embodiment, user interface may include an output device/mechanism (for example, printer and/or display (Standard Display Mechanism) and/or user input device/mechanism (for example, buttons, switches, touch screen, pointing device (for example, mouse or trackball) and/or microphone) to allow a user/operator to monitor, control and/or the program operating parameters and/or characteristics of the Light Field Data Acquisition Devices and/or post-processing circuitry/system (for example, (i) the rates of acquisition, sampling, capture, storing and/or recording of Light Field Data, (ii) the focal plane, field of view or depth of field of acquisition device, and/or (iii) the post-processing operations implemented by the post-processing circuitry/system). - There are many inventions described and illustrated herein. While certain embodiments, features, attributes and advantages of the inventions have been described and illustrated, it should be understood that many others, as well as different and/or similar embodiments, features, attributes and advantages of the present inventions, are apparent from the description and illustrations. As such, the above embodiments of the inventions are merely exemplary. They are not, nor are they intended to be exhaustive or to limit the inventions to the precise forms, techniques, materials and/or configurations disclosed. It is to be understood that such other, similar, as well as different, embodiments, features, materials, configurations, attributes, structures and advantages of the present inventions are within the scope of the present invention. It is to be further understood that other embodiments may be utilized and operational changes may be made without departing from the scope of the present inventions. The scope of the inventions is not limited solely to the description above because the description of the above embodiments has been presented for the purposes of exemplary illustration and description.
- For example, in those embodiments where the Light Field Data Acquisition Device connects to a post-processing system, such connection may be via wired and/or wireless architectures using any signaling technique now known or later developed. In addition, the configuration data may be provided and/or communicated to a post-processing system together with or separate from the associated Light Field Data using any format know known or later developed. Indeed, the static-type configuration data may be provided and/or communicated to a post-processing system together with or separate from dynamic-type configuration data. All communication strategies, designs, formats, techniques and/or architectures relating thereto are intended to fall within the scope of the present inventions.
- It should be further noted that the various circuits and circuitry disclosed herein may be described using computer aided design tools and expressed (or represented), as data and/or instructions embodied in various computer-readable media, in terms of their behavioral, register transfer, logic component, transistor, layout geometries, and/or other characteristics. Formats of files and other objects in which such circuit expressions may be implemented include, but are not limited to, formats supporting behavioral languages such as C, Verilog, and HLDL, formats supporting register level description languages like RTL, and formats supporting geometry description languages such as GDSII, GDSIII, GDSIV, CIF, MEBES and any other suitable formats and languages. Computer-readable media in which such formatted data and/or instructions may be embodied include, but are not limited to, non-volatile storage media in various forms (for example, optical, magnetic or semiconductor storage media) and carrier waves that may be used to transfer such formatted data and/or instructions through wireless, optical, or wired signaling media or any combination thereof. Examples of transfers of such formatted data and/or instructions by carrier waves include, but are not limited to, transfers (uploads, downloads, e-mail, etc.) over the Internet and/or other computer networks via one or more data transfer protocols (for example, HTTP, FTP, SMTP, etc.).
- Indeed, when received within a computer system via one or more computer-readable media, such data and/or instruction-based expressions of the above described circuits may be processed by a processing entity (for example, one or more processors) within the computer system in conjunction with execution of one or more other computer programs including, without limitation, net-list generation programs, place and route programs and the like, to generate a representation or image of a physical manifestation of such circuits. Such representation or image may thereafter be used in device fabrication, for example, by enabling generation of one or more masks that are used to form various components of the circuits in a device fabrication process.
- As noted above, there are many inventions described and illustrated herein. Importantly, each of the aspects of the present invention, and/or embodiments thereof, may be employed alone or in combination with one or more of such aspects and/or embodiments. For the sake of brevity, those permutations and combinations will not be discussed separately herein. As such, the present invention is not limited to any single aspect or embodiment thereof nor to any combinations and/or permutations of such aspects and/or embodiments.
- In the claims, the terms (i) “light field data” means Light Field Data, (ii) “light field configuration data” means Light Field Configuration Data, (iii) “aperture function” means Aperture Function, (iv) “exit pupil” means Exit Pupil, (v) “light field processing” means Light Field Processing, (vi) “light field data file” means Light Field Data File, (vi) “optical model” means optical and/or geometric model, (vii) “standard image format” means Standard Image Format.
- Further, in the claims, the term “circuit” means, among other things, a single component (for example, electrical/electronic) or a multiplicity of components (whether in integrated circuit form, discrete form or otherwise), which are active and/or passive, and which are coupled together to provide or perform a desired operation. The term “circuitry”, in the claims, means, among other things, a circuit (whether integrated or otherwise), a group of such circuits, one or more processors, one or more state machines, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays, or a combination of one or more circuits (whether integrated or otherwise), one or more state machines, one or more processors, one or more processors implementing software, one or more gate arrays, programmable gate arrays and/or field programmable gate arrays. The term “data” means, among other things, a current or voltage signal(s) (plural or singular) whether in an analog or a digital form, which may be a single bit (or the like) or multiple bits (or the like). Moreover, the term “optics”, means one or more components and/or a system comprising a plurality of components used to affect the propagation of light, including but not limited to lens elements, windows, microlens arrays, apertures and mirrors.
Claims (34)
Priority Applications (8)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/703,367 US20100265385A1 (en) | 2009-04-18 | 2010-02-10 | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
EP10764914A EP2419884A4 (en) | 2009-04-18 | 2010-04-05 | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
PCT/US2010/030015 WO2010120591A1 (en) | 2009-04-18 | 2010-04-05 | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
JP2012506066A JP2012524467A (en) | 2009-04-18 | 2010-04-05 | Light field camera images, files and configuration data, and methods for using, storing and communicating them |
CN2010800048439A CN102282590A (en) | 2009-04-18 | 2010-04-05 | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
US13/155,882 US8908058B2 (en) | 2009-04-18 | 2011-06-08 | Storage and transmission of pictures including multiple frames |
US13/523,776 US20120249550A1 (en) | 2009-04-18 | 2012-06-14 | Selective Transmission of Image Data Based on Device Attributes |
US13/664,938 US20130113981A1 (en) | 2006-12-01 | 2012-10-31 | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17062009P | 2009-04-18 | 2009-04-18 | |
US12/703,367 US20100265385A1 (en) | 2009-04-18 | 2010-02-10 | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
Related Child Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/948,901 Continuation US8559705B2 (en) | 2006-12-01 | 2007-11-30 | Interactive refocusing of electronic images |
US13/155,882 Continuation-In-Part US8908058B2 (en) | 2009-04-18 | 2011-06-08 | Storage and transmission of pictures including multiple frames |
Publications (1)
Publication Number | Publication Date |
---|---|
US20100265385A1 true US20100265385A1 (en) | 2010-10-21 |
Family
ID=42980728
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/703,367 Abandoned US20100265385A1 (en) | 2006-12-01 | 2010-02-10 | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
Country Status (5)
Country | Link |
---|---|
US (1) | US20100265385A1 (en) |
EP (1) | EP2419884A4 (en) |
JP (1) | JP2012524467A (en) |
CN (1) | CN102282590A (en) |
WO (1) | WO2010120591A1 (en) |
Cited By (138)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100194921A1 (en) * | 2009-02-05 | 2010-08-05 | Sony Corporation | Image pickup apparatus |
CN102692791A (en) * | 2011-03-25 | 2012-09-26 | 卡西欧计算机株式会社 | Image capturing device with micro-lens array |
US20120301048A1 (en) * | 2011-05-25 | 2012-11-29 | Sony Corporation | Image processing apparatus and method |
US20130044234A1 (en) * | 2011-08-19 | 2013-02-21 | Canon Kabushiki Kaisha | Image capturing apparatus, image processing apparatus, and image processing method for generating auxiliary information for captured image |
WO2013033442A1 (en) | 2011-08-30 | 2013-03-07 | Digimarc Corporation | Methods and arrangements for identifying objects |
WO2013049699A1 (en) * | 2011-09-28 | 2013-04-04 | Pelican Imaging Corporation | Systems and methods for encoding and decoding light field image files |
US20130107102A1 (en) * | 2011-11-02 | 2013-05-02 | Sony Mobile Communications Ab | Displaying of images with lighting on the basis of captured auxiliary images |
US20130229532A1 (en) * | 2012-03-01 | 2013-09-05 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
GB2501936A (en) * | 2012-05-11 | 2013-11-13 | Canon Kk | Micro-lens array with micro-lens subsets displaced from a regular lattice pattern |
US20130308035A1 (en) * | 2012-05-21 | 2013-11-21 | Canon Kabushiki Kaisha | Image pickup apparatus |
US20130329120A1 (en) * | 2012-06-11 | 2013-12-12 | Canon Kabushiki Kaisha | Image processing apparatus, image processing method, image pickup apparatus, method of controlling image pickup apparatus, and non-transitory computer-readable storage medium |
US8619179B2 (en) | 2011-03-28 | 2013-12-31 | Canon Kabushiki Kaisha | Multi-modal image capture apparatus with a tunable spectral response |
EP2708019A1 (en) * | 2011-05-11 | 2014-03-19 | Pelican Imaging Corporation | Systems and methods for transmitting and receiving array camera image data |
GB2505954A (en) * | 2012-09-18 | 2014-03-19 | Canon Kk | Micro lens array with displaced micro-lenses suitable for a light-field colour camera |
GB2505955A (en) * | 2012-09-18 | 2014-03-19 | Canon Kk | Micro lens array with a colour filter set and imaging apparatus suitable for a light-field colour camera |
US8698953B1 (en) * | 2009-08-28 | 2014-04-15 | Marvell International Ltd. | Field programmable digital image capture device |
US20140146132A1 (en) * | 2010-10-29 | 2014-05-29 | Ecole Polytechnique Federale De Lausanne (Epfl) | Omnidirectional sensor array system |
US20140176592A1 (en) * | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
WO2014110484A2 (en) | 2013-01-11 | 2014-07-17 | Digimarc Corporation | Next generation imaging methods and systems |
US20140226039A1 (en) * | 2013-02-14 | 2014-08-14 | Canon Kabushiki Kaisha | Image capturing apparatus and control method thereof |
US8861089B2 (en) | 2009-11-20 | 2014-10-14 | Pelican Imaging Corporation | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
US8866920B2 (en) | 2008-05-20 | 2014-10-21 | Pelican Imaging Corporation | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
US8878950B2 (en) | 2010-12-14 | 2014-11-04 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using super-resolution processes |
US8885059B1 (en) | 2008-05-20 | 2014-11-11 | Pelican Imaging Corporation | Systems and methods for measuring depth using images captured by camera arrays |
US8928793B2 (en) | 2010-05-12 | 2015-01-06 | Pelican Imaging Corporation | Imager array interfaces |
US20150054982A1 (en) * | 2013-08-21 | 2015-02-26 | Canon Kabushiki Kaisha | Image processing apparatus, control method for same, and program |
US8978984B2 (en) | 2013-02-28 | 2015-03-17 | Hand Held Products, Inc. | Indicia reading terminals and methods for decoding decodable indicia employing light field imaging |
US9100635B2 (en) | 2012-06-28 | 2015-08-04 | Pelican Imaging Corporation | Systems and methods for detecting defective camera arrays and optic arrays |
US9100586B2 (en) | 2013-03-14 | 2015-08-04 | Pelican Imaging Corporation | Systems and methods for photometric normalization in array cameras |
US9106784B2 (en) | 2013-03-13 | 2015-08-11 | Pelican Imaging Corporation | Systems and methods for controlling aliasing in images captured by an array camera for use in super-resolution processing |
US20150235476A1 (en) * | 2012-02-21 | 2015-08-20 | Pelican Imaging Corporation | Systems and Method for Performing Depth Based Image Editing |
US9123118B2 (en) | 2012-08-21 | 2015-09-01 | Pelican Imaging Corporation | System and methods for measuring depth using an array camera employing a bayer filter |
US9124864B2 (en) | 2013-03-10 | 2015-09-01 | Pelican Imaging Corporation | System and methods for calibration of an array camera |
US9128228B2 (en) | 2011-06-28 | 2015-09-08 | Pelican Imaging Corporation | Optical arrangements for use with an array camera |
US20150262424A1 (en) * | 2013-01-31 | 2015-09-17 | Google Inc. | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System |
US9143711B2 (en) | 2012-11-13 | 2015-09-22 | Pelican Imaging Corporation | Systems and methods for array camera focal plane control |
US9153026B2 (en) | 2012-11-26 | 2015-10-06 | Ricoh Co., Ltd. | Calibration of plenoptic imaging systems |
WO2015157769A1 (en) * | 2014-04-11 | 2015-10-15 | The Regents Of The University Of Colorado, A Body Corporate | Scanning imaging for encoded psf identification and light field imaging |
US20150312553A1 (en) * | 2012-12-04 | 2015-10-29 | Lytro, Inc. | Capturing and relighting images using multiple devices |
US9185276B2 (en) | 2013-11-07 | 2015-11-10 | Pelican Imaging Corporation | Methods of manufacturing array camera modules incorporating independently aligned lens stacks |
US9210392B2 (en) | 2012-05-01 | 2015-12-08 | Pelican Imaging Coporation | Camera modules patterned with pi filter groups |
US9214013B2 (en) | 2012-09-14 | 2015-12-15 | Pelican Imaging Corporation | Systems and methods for correcting user identified artifacts in light field images |
WO2016011087A1 (en) * | 2014-07-15 | 2016-01-21 | Ostendo Technologies, Inc. | Preprocessor for full parallax light field compression |
US9247117B2 (en) | 2014-04-07 | 2016-01-26 | Pelican Imaging Corporation | Systems and methods for correcting for warpage of a sensor array in an array camera module by introducing warpage into a focal plane of a lens stack array |
US9253380B2 (en) | 2013-02-24 | 2016-02-02 | Pelican Imaging Corporation | Thin form factor computational array cameras and modular array cameras |
EP2940981A3 (en) * | 2014-04-29 | 2016-03-23 | Xiaomi Inc. | Method and device for synchronizing photographs |
US9417121B1 (en) * | 2013-06-04 | 2016-08-16 | James E. Spencer | Methods and apparatuses using optics with aperture for passing optical signals between input and output stages |
US9426361B2 (en) | 2013-11-26 | 2016-08-23 | Pelican Imaging Corporation | Array camera configurations incorporating multiple constituent array cameras |
US9438888B2 (en) | 2013-03-15 | 2016-09-06 | Pelican Imaging Corporation | Systems and methods for stereo imaging with camera arrays |
US9438778B2 (en) | 2014-08-08 | 2016-09-06 | Industrial Technology Research Institute | Image pickup device and light field image pickup lens |
US20160261795A1 (en) * | 2015-03-03 | 2016-09-08 | Canon Kabushiki Kaisha | Image display apparatus, image capturing apparatus, image display method, and storage medium |
US9445003B1 (en) | 2013-03-15 | 2016-09-13 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using image deconvolution based on motion and depth information |
US9462164B2 (en) | 2013-02-21 | 2016-10-04 | Pelican Imaging Corporation | Systems and methods for generating compressed light field representation data using captured light fields, array geometry, and parallax information |
US20160307368A1 (en) * | 2015-04-17 | 2016-10-20 | Lytro, Inc. | Compression and interactive playback of light field pictures |
WO2016172385A1 (en) * | 2015-04-23 | 2016-10-27 | Ostendo Technologies, Inc. | Methods for full parallax compressed light field synthesis utilizing depth information |
US9497380B1 (en) | 2013-02-15 | 2016-11-15 | Red.Com, Inc. | Dense field imaging |
US9497370B2 (en) | 2013-03-15 | 2016-11-15 | Pelican Imaging Corporation | Array camera architecture implementing quantum dot color filters |
US9497429B2 (en) | 2013-03-15 | 2016-11-15 | Pelican Imaging Corporation | Extended color processing on pelican array cameras |
US9516222B2 (en) | 2011-06-28 | 2016-12-06 | Kip Peli P1 Lp | Array cameras incorporating monolithic array camera modules with high MTF lens stacks for capture of images used in super-resolution processing |
US9519972B2 (en) | 2013-03-13 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for synthesizing images from image data captured by an array camera using restricted depth of field depth maps in which depth estimation precision varies |
US9521319B2 (en) | 2014-06-18 | 2016-12-13 | Pelican Imaging Corporation | Array cameras and array camera modules including spectral filters disposed outside of a constituent image sensor |
US9521416B1 (en) | 2013-03-11 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for image data compression |
US9544583B2 (en) | 2015-01-09 | 2017-01-10 | Ricoh Company, Ltd. | Object space calibration of plenoptic imaging systems |
EP3116216A4 (en) * | 2014-03-03 | 2017-02-15 | Panasonic Intellectual Property Management Co., Ltd. | Image pickup apparatus |
US9578259B2 (en) | 2013-03-14 | 2017-02-21 | Fotonation Cayman Limited | Systems and methods for reducing motion blur in images or video in ultra low light with array cameras |
US9600904B2 (en) | 2013-12-30 | 2017-03-21 | Samsung Electronics Co., Ltd. | Illuminating a virtual environment with camera light data |
US9633442B2 (en) | 2013-03-15 | 2017-04-25 | Fotonation Cayman Limited | Array cameras including an array camera module augmented with a separate camera |
US9638883B1 (en) | 2013-03-04 | 2017-05-02 | Fotonation Cayman Limited | Passive alignment of array camera modules constructed from lens stack arrays and sensors based upon alignment information obtained during manufacture of array camera modules using an active alignment process |
WO2017104111A1 (en) * | 2015-12-17 | 2017-06-22 | Canon Kabushiki Kaisha | Data recording apparatus, image capturing apparatus, data recording method, and storage medium |
US9691149B2 (en) | 2014-11-27 | 2017-06-27 | Thomson Licensing | Plenoptic camera comprising a light emitting device |
US9741118B2 (en) | 2013-03-13 | 2017-08-22 | Fotonation Cayman Limited | System and methods for calibration of an array camera |
US20170256059A1 (en) * | 2016-03-07 | 2017-09-07 | Ricoh Company, Ltd. | Object Segmentation from Light Field Data |
US9766380B2 (en) | 2012-06-30 | 2017-09-19 | Fotonation Cayman Limited | Systems and methods for manufacturing camera modules using active alignment of lens stack arrays and sensors |
US9774789B2 (en) | 2013-03-08 | 2017-09-26 | Fotonation Cayman Limited | Systems and methods for high dynamic range imaging using array cameras |
EP2709352A3 (en) * | 2012-09-12 | 2017-09-27 | Canon Kabushiki Kaisha | Image pickup apparatus, image pickup system, image processing device, and method of controlling image pickup apparatus |
US9794476B2 (en) | 2011-09-19 | 2017-10-17 | Fotonation Cayman Limited | Systems and methods for controlling aliasing in images captured by an array camera for use in super resolution processing using pixel apertures |
US9797716B2 (en) | 2015-01-09 | 2017-10-24 | Ricoh Company, Ltd. | Estimating surface properties using a plenoptic camera |
US9813616B2 (en) | 2012-08-23 | 2017-11-07 | Fotonation Cayman Limited | Feature based high resolution motion estimation from low resolution images captured using an array source |
DE102016208210A1 (en) * | 2016-05-12 | 2017-11-16 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | 3D MULTI-PAPER PICTURE DEVICES, MULTI-PAPER IMAGING DEVICE, METHOD FOR PROVIDING AN OUTPUT SIGNAL OF A 3D MULTI-PAPER IMAGING DEVICE AND METHOD FOR DETECTING A TOTAL FACE |
US20170374388A1 (en) * | 2016-06-22 | 2017-12-28 | Thomson Licensing | Method and a device for encoding a signal representative of a light-field content |
US9888194B2 (en) | 2013-03-13 | 2018-02-06 | Fotonation Cayman Limited | Array camera architecture implementing quantum film image sensors |
US9898856B2 (en) | 2013-09-27 | 2018-02-20 | Fotonation Cayman Limited | Systems and methods for depth-assisted perspective distortion correction |
US9942474B2 (en) | 2015-04-17 | 2018-04-10 | Fotonation Cayman Limited | Systems and methods for performing high speed video capture and depth estimation using array cameras |
DE112014005866B4 (en) | 2013-12-24 | 2018-08-02 | Lytro, Inc. | Improvement of plenoptic camera resolution |
US20180260969A1 (en) * | 2015-09-17 | 2018-09-13 | Thomson Licensing | An apparatus and a method for generating data representing a pixel beam |
US10089740B2 (en) | 2014-03-07 | 2018-10-02 | Fotonation Limited | System and methods for depth regularization and semiautomatic interactive matting using RGB-D images |
US10110303B2 (en) | 2014-11-10 | 2018-10-23 | Beijing Zhigu Rui Tuo Tech Co., Ltd. | Light-communication sending methods and apparatus, light-communication receiving methods and apparatus, and light communication systems |
US10119808B2 (en) | 2013-11-18 | 2018-11-06 | Fotonation Limited | Systems and methods for estimating depth from projected texture using camera arrays |
US10122993B2 (en) | 2013-03-15 | 2018-11-06 | Fotonation Limited | Autofocus system for a conventional camera that uses depth information from an array camera |
US10205896B2 (en) | 2015-07-24 | 2019-02-12 | Google Llc | Automatic lens flare detection and correction for light-field images |
US20190080434A1 (en) * | 2017-09-08 | 2019-03-14 | Ricoh Company, Ltd. | Reducing Color Artifacts In Plenoptic Imaging Systems |
US10244223B2 (en) | 2014-01-10 | 2019-03-26 | Ostendo Technologies, Inc. | Methods for full parallax compressed light field 3D imaging systems |
US10250871B2 (en) | 2014-09-29 | 2019-04-02 | Fotonation Limited | Systems and methods for dynamic calibration of array cameras |
US20190124318A1 (en) * | 2015-05-26 | 2019-04-25 | Google Llc | Capturing light-field images with uneven and/or incomplete angular sampling |
US10275892B2 (en) | 2016-06-09 | 2019-04-30 | Google Llc | Multi-view scene segmentation and propagation |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US10310450B2 (en) | 2015-04-23 | 2019-06-04 | Ostendo Technologies, Inc. | Methods and apparatus for full parallax light field display systems |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US10390005B2 (en) | 2012-09-28 | 2019-08-20 | Fotonation Limited | Generating images from light fields utilizing virtual viewpoints |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US10448030B2 (en) | 2015-11-16 | 2019-10-15 | Ostendo Technologies, Inc. | Content adaptive light field compression |
US10453431B2 (en) | 2016-04-28 | 2019-10-22 | Ostendo Technologies, Inc. | Integrated near-far light field display systems |
US10469873B2 (en) | 2015-04-15 | 2019-11-05 | Google Llc | Encoding and decoding virtual reality video |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10482618B2 (en) | 2017-08-21 | 2019-11-19 | Fotonation Limited | Systems and methods for hybrid depth regularization |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US10545215B2 (en) | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US10552947B2 (en) | 2012-06-26 | 2020-02-04 | Google Llc | Depth-based image blurring |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10656836B2 (en) * | 2017-11-20 | 2020-05-19 | Hitachi, Ltd. | Storage system |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
RU2729698C2 (en) * | 2015-09-17 | 2020-08-11 | ИНТЕРДИДЖИТАЛ ВиСи ХОЛДИНГЗ, ИНК. | Apparatus and method for encoding an image captured by an optical system for acquiring data |
US10776995B2 (en) | 2017-10-17 | 2020-09-15 | Nvidia Corporation | Light fields as better backgrounds in rendering |
US10805589B2 (en) | 2015-04-19 | 2020-10-13 | Fotonation Limited | Multi-baseline camera array system architectures for depth augmentation in VR/AR applications |
US10873693B2 (en) * | 2018-12-03 | 2020-12-22 | Samsung Electronics Co.. Ltd. | Calibration method and apparatus |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
US11182872B2 (en) * | 2018-11-02 | 2021-11-23 | Electronics And Telecommunications Research Institute | Plenoptic data storage system and operating method thereof |
US11270110B2 (en) | 2019-09-17 | 2022-03-08 | Boston Polarimetrics, Inc. | Systems and methods for surface modeling using polarization cues |
US11290658B1 (en) | 2021-04-15 | 2022-03-29 | Boston Polarimetrics, Inc. | Systems and methods for camera exposure control |
US11302012B2 (en) | 2019-11-30 | 2022-04-12 | Boston Polarimetrics, Inc. | Systems and methods for transparent object segmentation using polarization cues |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US11525906B2 (en) | 2019-10-07 | 2022-12-13 | Intrinsic Innovation Llc | Systems and methods for augmentation of sensor systems and imaging systems with polarization |
US11580667B2 (en) | 2020-01-29 | 2023-02-14 | Intrinsic Innovation Llc | Systems and methods for characterizing object pose detection and measurement systems |
US11689813B2 (en) | 2021-07-01 | 2023-06-27 | Intrinsic Innovation Llc | Systems and methods for high dynamic range imaging using crossed polarizers |
US11792538B2 (en) | 2008-05-20 | 2023-10-17 | Adeia Imaging Llc | Capturing and processing of images including occlusions focused on an image sensor by a lens stack array |
US11797863B2 (en) | 2020-01-30 | 2023-10-24 | Intrinsic Innovation Llc | Systems and methods for synthesizing data for training statistical models on different imaging modalities including polarized images |
US11953700B2 (en) | 2020-05-27 | 2024-04-09 | Intrinsic Innovation Llc | Multi-aperture polarization optical systems using beam splitters |
US11954886B2 (en) | 2021-04-15 | 2024-04-09 | Intrinsic Innovation Llc | Systems and methods for six-degree of freedom pose estimation of deformable objects |
US11982775B2 (en) | 2022-12-12 | 2024-05-14 | Intrinsic Innovation Llc | Systems and methods for augmentation of sensor systems and imaging systems with polarization |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6168794B2 (en) * | 2012-05-31 | 2017-07-26 | キヤノン株式会社 | Information processing method and apparatus, program. |
JP2014099696A (en) * | 2012-11-13 | 2014-05-29 | Toshiba Corp | Solid state image pickup device |
US9092890B2 (en) | 2012-12-20 | 2015-07-28 | Ricoh Company, Ltd. | Occlusion-aware reconstruction of three-dimensional scenes from light field images |
JP6210687B2 (en) * | 2013-01-11 | 2017-10-11 | キヤノン株式会社 | Image processing apparatus, image processing method, and program |
CN104184936B (en) * | 2013-05-21 | 2017-06-23 | 吴俊辉 | Image focusing processing method and system based on light field camera |
JP6406804B2 (en) * | 2013-08-27 | 2018-10-17 | キヤノン株式会社 | Image processing apparatus, image processing method and program, and imaging apparatus |
WO2015137635A1 (en) | 2014-03-13 | 2015-09-17 | Samsung Electronics Co., Ltd. | Image pickup apparatus and method for generating image having depth information |
KR102228456B1 (en) * | 2014-03-13 | 2021-03-16 | 삼성전자주식회사 | Image pickup apparatus and image pickup method of generating image having depth information |
EP3065394A1 (en) * | 2015-03-05 | 2016-09-07 | Thomson Licensing | Light field metadata |
EP3088954A1 (en) * | 2015-04-27 | 2016-11-02 | Thomson Licensing | Method and device for processing a lightfield content |
EP3113478A1 (en) * | 2015-06-30 | 2017-01-04 | Thomson Licensing | Plenoptic foveated camera |
JP6546474B2 (en) * | 2015-07-31 | 2019-07-17 | キヤノン株式会社 | Image pickup apparatus and control method thereof |
EP3144890A1 (en) * | 2015-09-17 | 2017-03-22 | Thomson Licensing | An apparatus and a method for calibrating an optical acquisition system |
EP3145195A1 (en) * | 2015-09-17 | 2017-03-22 | Thomson Licensing | An apparatus and a method for encoding an image captured by an optical acquisition system |
JP2018537877A (en) * | 2015-09-17 | 2018-12-20 | トムソン ライセンシングＴｈｏｍｓｏｎ Ｌｉｃｅｎｓｉｎｇ | How to encode light field content |
EP3144887A1 (en) * | 2015-09-17 | 2017-03-22 | Thomson Licensing | A method and an apparatus for generating data representative of a pixel beam |
EP3182697A1 (en) * | 2015-12-15 | 2017-06-21 | Thomson Licensing | A method and apparatus for correcting vignetting effect caused on an image captured by lightfield cameras |
CN107347129B (en) * | 2016-05-05 | 2020-02-14 | 中强光电股份有限公司 | Light field camera |
CN107490627B (en) * | 2016-06-12 | 2019-10-29 | 中国航发商用航空发动机有限责任公司 | Focus ultrasonic probe parameter scaling method |
CN109242900B (en) * | 2018-08-01 | 2021-09-21 | 曜科智能科技（上海）有限公司 | Focal plane positioning method, processing device, focal plane positioning system and storage medium |
CN111427166B (en) * | 2020-03-31 | 2022-07-05 | 京东方科技集团股份有限公司 | Light field display method and system, storage medium and display panel |
KR102403361B1 (en) * | 2020-11-30 | 2022-05-27 | 원광대학교산학협력단 | Generating device of data format structure of plenoptic video content |
CN113610696B (en) * | 2021-08-16 | 2024-01-30 | 齐鲁工业大学 | Thirty-two digital moment-based light field image watermarking method and system |
Citations (51)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US725567A (en) * | 1902-09-25 | 1903-04-14 | Frederic E Ives | Parallax stereogram and process of making same. |
US4383170A (en) * | 1979-11-19 | 1983-05-10 | Tokyo Shibaura Denki Kabushiki Kaisha | Image input device |
US4661986A (en) * | 1983-06-27 | 1987-04-28 | Rca Corporation | Depth-of-focus imaging process method |
US4694185A (en) * | 1986-04-18 | 1987-09-15 | Eastman Kodak Company | Light sensing devices with lenticular pixels |
US4920419A (en) * | 1988-05-31 | 1990-04-24 | Eastman Kodak Company | Zoom lens focus control device for film video player |
US5076687A (en) * | 1990-08-28 | 1991-12-31 | Massachusetts Institute Of Technology | Optical ranging apparatus |
US5282045A (en) * | 1990-04-27 | 1994-01-25 | Hitachi, Ltd. | Depth-of-field control apparatus and image pickup apparatus having the same therein |
US5748371A (en) * | 1995-02-03 | 1998-05-05 | The Regents Of The University Of Colorado | Extended depth of field optical systems |
US5757423A (en) * | 1993-10-22 | 1998-05-26 | Canon Kabushiki Kaisha | Image taking apparatus |
US5949433A (en) * | 1996-04-11 | 1999-09-07 | Discreet Logic, Inc. | Processing image data |
US6023523A (en) * | 1996-02-16 | 2000-02-08 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US6028606A (en) * | 1996-08-02 | 2000-02-22 | The Board Of Trustees Of The Leland Stanford Junior University | Camera simulation system |
US6097394A (en) * | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US6201899B1 (en) * | 1998-10-09 | 2001-03-13 | Sarnoff Corporation | Method and apparatus for extended depth of field imaging |
US6320979B1 (en) * | 1998-10-06 | 2001-11-20 | Canon Kabushiki Kaisha | Depth of field enhancement |
US20020159030A1 (en) * | 2000-05-08 | 2002-10-31 | Frey Rudolph W. | Apparatus and method for objective measurement of optical systems using wavefront analysis |
US6483535B1 (en) * | 1999-12-23 | 2002-11-19 | Welch Allyn, Inc. | Wide angle lens system for electronic imagers having long exit pupil distances |
US20030103670A1 (en) * | 2001-11-30 | 2003-06-05 | Bernhard Schoelkopf | Interactive images |
US6577342B1 (en) * | 1998-09-25 | 2003-06-10 | Intel Corporation | Image sensor with microlens material structure |
US20030117511A1 (en) * | 2001-12-21 | 2003-06-26 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
US6597859B1 (en) * | 1999-12-16 | 2003-07-22 | Intel Corporation | Method and apparatus for abstracting video data |
US20030156077A1 (en) * | 2000-05-19 | 2003-08-21 | Tibor Balogh | Method and apparatus for displaying 3d images |
US20040114176A1 (en) * | 2002-12-17 | 2004-06-17 | International Business Machines Corporation | Editing and browsing images for virtual cameras |
US6842297B2 (en) * | 2001-08-31 | 2005-01-11 | Cdm Optics, Inc. | Wavefront coding optics |
US20050080602A1 (en) * | 2003-10-10 | 2005-04-14 | Microsoft Corporation | Systems and methods for all-frequency relighting using spherical harmonics and point light distributions |
US6900841B1 (en) * | 1999-01-11 | 2005-05-31 | Olympus Optical Co., Ltd. | Image processing system capable of applying good texture such as blur |
US6927922B2 (en) * | 2001-12-18 | 2005-08-09 | The University Of Rochester | Imaging using a multifocal aspheric lens to obtain extended depth of field |
US7034866B1 (en) * | 2000-11-22 | 2006-04-25 | Koninklijke Philips Electronics N.V. | Combined display-camera for an image processing system |
US20070071316A1 (en) * | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US20070252074A1 (en) * | 2004-10-01 | 2007-11-01 | The Board Of Trustees Of The Leland Stanford Junio | Imaging Arrangements and Methods Therefor |
US20080018668A1 (en) * | 2004-07-23 | 2008-01-24 | Masaki Yamauchi | Image Processing Device and Image Processing Method |
US7336430B2 (en) * | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US20080131019A1 (en) * | 2006-12-01 | 2008-06-05 | Yi-Ren Ng | Interactive Refocusing of Electronic Images |
US20080180792A1 (en) * | 2007-01-25 | 2008-07-31 | Georgiev Todor G | Light Field Microscope With Lenslet Array |
US20080193026A1 (en) * | 2007-02-09 | 2008-08-14 | Kenichi Horie | Decoding method, decoding apparatus, storage medium in which decoding program is stored, and electronic camera |
US20080226274A1 (en) * | 2005-11-03 | 2008-09-18 | Spielberg Anthony C | Systems For Improved Autofocus in Digital Imaging Systems |
US20080266655A1 (en) * | 2005-10-07 | 2008-10-30 | Levoy Marc S | Microscopy Arrangements and Approaches |
US20080266688A1 (en) * | 2004-04-27 | 2008-10-30 | Fico Mirrors, Sa | Folding Mechanism for Exterior Rear-View Mirrors in Automotive Vehicles |
US20090041448A1 (en) * | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Capture by Multiplexing in the Frequency Domain |
US20090102956A1 (en) * | 2007-10-18 | 2009-04-23 | Georgiev Todor G | Fast Computational Camera Based On Two Arrays of Lenses |
US20090128669A1 (en) * | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US20090140131A1 (en) * | 2005-06-23 | 2009-06-04 | Nikon Corporation | Image input apparatus, photodetection apparatus, and image synthesis method |
US20090185801A1 (en) * | 2008-01-23 | 2009-07-23 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090268970A1 (en) * | 2008-04-29 | 2009-10-29 | Sevket Derin Babacan | Method and Apparatus for Block-Based Compression of Light-field Images |
US7620309B2 (en) * | 2006-04-04 | 2009-11-17 | Adobe Systems, Incorporated | Plenoptic camera |
US7623726B1 (en) * | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US20090295829A1 (en) * | 2008-01-23 | 2009-12-03 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20100128145A1 (en) * | 2008-11-25 | 2010-05-27 | Colvin Pitts | System of and Method for Video Refocusing |
US20100141802A1 (en) * | 2008-12-08 | 2010-06-10 | Timothy Knight | Light Field Data Acquisition Devices, and Methods of Using and Manufacturing Same |
US20100277629A1 (en) * | 2009-05-01 | 2010-11-04 | Samsung Electronics Co., Ltd. | Photo detecting device and image pickup device and method thereon |
US7949252B1 (en) * | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8103111B2 (en) * | 2006-12-26 | 2012-01-24 | Olympus Imaging Corp. | Coding method, electronic camera, recording medium storing coded program, and decoding method |
-
2010
- 2010-02-10 US US12/703,367 patent/US20100265385A1/en not_active Abandoned
- 2010-04-05 CN CN2010800048439A patent/CN102282590A/en active Pending
- 2010-04-05 WO PCT/US2010/030015 patent/WO2010120591A1/en active Application Filing
- 2010-04-05 EP EP10764914A patent/EP2419884A4/en not_active Withdrawn
- 2010-04-05 JP JP2012506066A patent/JP2012524467A/en active Pending
Patent Citations (54)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US725567A (en) * | 1902-09-25 | 1903-04-14 | Frederic E Ives | Parallax stereogram and process of making same. |
US4383170A (en) * | 1979-11-19 | 1983-05-10 | Tokyo Shibaura Denki Kabushiki Kaisha | Image input device |
US4661986A (en) * | 1983-06-27 | 1987-04-28 | Rca Corporation | Depth-of-focus imaging process method |
US4694185A (en) * | 1986-04-18 | 1987-09-15 | Eastman Kodak Company | Light sensing devices with lenticular pixels |
US4920419A (en) * | 1988-05-31 | 1990-04-24 | Eastman Kodak Company | Zoom lens focus control device for film video player |
US5282045A (en) * | 1990-04-27 | 1994-01-25 | Hitachi, Ltd. | Depth-of-field control apparatus and image pickup apparatus having the same therein |
US5076687A (en) * | 1990-08-28 | 1991-12-31 | Massachusetts Institute Of Technology | Optical ranging apparatus |
US5757423A (en) * | 1993-10-22 | 1998-05-26 | Canon Kabushiki Kaisha | Image taking apparatus |
US5748371A (en) * | 1995-02-03 | 1998-05-05 | The Regents Of The University Of Colorado | Extended depth of field optical systems |
US6023523A (en) * | 1996-02-16 | 2000-02-08 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US5949433A (en) * | 1996-04-11 | 1999-09-07 | Discreet Logic, Inc. | Processing image data |
US6028606A (en) * | 1996-08-02 | 2000-02-22 | The Board Of Trustees Of The Leland Stanford Junior University | Camera simulation system |
US6097394A (en) * | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US6577342B1 (en) * | 1998-09-25 | 2003-06-10 | Intel Corporation | Image sensor with microlens material structure |
US6320979B1 (en) * | 1998-10-06 | 2001-11-20 | Canon Kabushiki Kaisha | Depth of field enhancement |
US6201899B1 (en) * | 1998-10-09 | 2001-03-13 | Sarnoff Corporation | Method and apparatus for extended depth of field imaging |
US6900841B1 (en) * | 1999-01-11 | 2005-05-31 | Olympus Optical Co., Ltd. | Image processing system capable of applying good texture such as blur |
US6597859B1 (en) * | 1999-12-16 | 2003-07-22 | Intel Corporation | Method and apparatus for abstracting video data |
US6483535B1 (en) * | 1999-12-23 | 2002-11-19 | Welch Allyn, Inc. | Wide angle lens system for electronic imagers having long exit pupil distances |
US20020159030A1 (en) * | 2000-05-08 | 2002-10-31 | Frey Rudolph W. | Apparatus and method for objective measurement of optical systems using wavefront analysis |
US20030156077A1 (en) * | 2000-05-19 | 2003-08-21 | Tibor Balogh | Method and apparatus for displaying 3d images |
US7034866B1 (en) * | 2000-11-22 | 2006-04-25 | Koninklijke Philips Electronics N.V. | Combined display-camera for an image processing system |
US6842297B2 (en) * | 2001-08-31 | 2005-01-11 | Cdm Optics, Inc. | Wavefront coding optics |
US20030103670A1 (en) * | 2001-11-30 | 2003-06-05 | Bernhard Schoelkopf | Interactive images |
US6927922B2 (en) * | 2001-12-18 | 2005-08-09 | The University Of Rochester | Imaging using a multifocal aspheric lens to obtain extended depth of field |
US20030117511A1 (en) * | 2001-12-21 | 2003-06-26 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
US20040114176A1 (en) * | 2002-12-17 | 2004-06-17 | International Business Machines Corporation | Editing and browsing images for virtual cameras |
US20050080602A1 (en) * | 2003-10-10 | 2005-04-14 | Microsoft Corporation | Systems and methods for all-frequency relighting using spherical harmonics and point light distributions |
US20080266688A1 (en) * | 2004-04-27 | 2008-10-30 | Fico Mirrors, Sa | Folding Mechanism for Exterior Rear-View Mirrors in Automotive Vehicles |
US20080018668A1 (en) * | 2004-07-23 | 2008-01-24 | Masaki Yamauchi | Image Processing Device and Image Processing Method |
US7336430B2 (en) * | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US20070252074A1 (en) * | 2004-10-01 | 2007-11-01 | The Board Of Trustees Of The Leland Stanford Junio | Imaging Arrangements and Methods Therefor |
US20090140131A1 (en) * | 2005-06-23 | 2009-06-04 | Nikon Corporation | Image input apparatus, photodetection apparatus, and image synthesis method |
US20070071316A1 (en) * | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US20080266655A1 (en) * | 2005-10-07 | 2008-10-30 | Levoy Marc S | Microscopy Arrangements and Approaches |
US20080226274A1 (en) * | 2005-11-03 | 2008-09-18 | Spielberg Anthony C | Systems For Improved Autofocus in Digital Imaging Systems |
US7623726B1 (en) * | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US20090128669A1 (en) * | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US20100026852A1 (en) * | 2006-02-07 | 2010-02-04 | Yi-Ren Ng | Variable imaging arrangements and methods therefor |
US7620309B2 (en) * | 2006-04-04 | 2009-11-17 | Adobe Systems, Incorporated | Plenoptic camera |
US20080131019A1 (en) * | 2006-12-01 | 2008-06-05 | Yi-Ren Ng | Interactive Refocusing of Electronic Images |
US20080180792A1 (en) * | 2007-01-25 | 2008-07-31 | Georgiev Todor G | Light Field Microscope With Lenslet Array |
US20080193026A1 (en) * | 2007-02-09 | 2008-08-14 | Kenichi Horie | Decoding method, decoding apparatus, storage medium in which decoding program is stored, and electronic camera |
US20090041381A1 (en) * | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Processing by Demultiplexing in the Frequency Domain |
US20090041448A1 (en) * | 2007-08-06 | 2009-02-12 | Georgiev Todor G | Method and Apparatus for Radiance Capture by Multiplexing in the Frequency Domain |
US20090102956A1 (en) * | 2007-10-18 | 2009-04-23 | Georgiev Todor G | Fast Computational Camera Based On Two Arrays of Lenses |
US20090185801A1 (en) * | 2008-01-23 | 2009-07-23 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090295829A1 (en) * | 2008-01-23 | 2009-12-03 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
US20090268970A1 (en) * | 2008-04-29 | 2009-10-29 | Sevket Derin Babacan | Method and Apparatus for Block-Based Compression of Light-field Images |
US20100128145A1 (en) * | 2008-11-25 | 2010-05-27 | Colvin Pitts | System of and Method for Video Refocusing |
US20100129048A1 (en) * | 2008-11-25 | 2010-05-27 | Colvin Pitts | System and Method for Acquiring, Editing, Generating and Outputting Video Data |
US20100141802A1 (en) * | 2008-12-08 | 2010-06-10 | Timothy Knight | Light Field Data Acquisition Devices, and Methods of Using and Manufacturing Same |
US7949252B1 (en) * | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US20100277629A1 (en) * | 2009-05-01 | 2010-11-04 | Samsung Electronics Co., Ltd. | Photo detecting device and image pickup device and method thereon |
Cited By (316)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US9049390B2 (en) | 2008-05-20 | 2015-06-02 | Pelican Imaging Corporation | Capturing and processing of images captured by arrays including polychromatic cameras |
US9094661B2 (en) | 2008-05-20 | 2015-07-28 | Pelican Imaging Corporation | Systems and methods for generating depth maps using a set of images containing a baseline image |
US9077893B2 (en) | 2008-05-20 | 2015-07-07 | Pelican Imaging Corporation | Capturing and processing of images captured by non-grid camera arrays |
US9060120B2 (en) | 2008-05-20 | 2015-06-16 | Pelican Imaging Corporation | Systems and methods for generating depth maps using images captured by camera arrays |
US9485496B2 (en) | 2008-05-20 | 2016-11-01 | Pelican Imaging Corporation | Systems and methods for measuring depth using images captured by a camera array including cameras surrounding a central camera |
US10027901B2 (en) | 2008-05-20 | 2018-07-17 | Fotonation Cayman Limited | Systems and methods for generating depth maps using a camera arrays incorporating monochrome and color cameras |
US10142560B2 (en) | 2008-05-20 | 2018-11-27 | Fotonation Limited | Capturing and processing of images including occlusions focused on an image sensor by a lens stack array |
US9712759B2 (en) | 2008-05-20 | 2017-07-18 | Fotonation Cayman Limited | Systems and methods for generating depth maps using a camera arrays incorporating monochrome and color cameras |
US9041829B2 (en) | 2008-05-20 | 2015-05-26 | Pelican Imaging Corporation | Capturing and processing of high dynamic range images using camera arrays |
US8902321B2 (en) | 2008-05-20 | 2014-12-02 | Pelican Imaging Corporation | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
US9191580B2 (en) | 2008-05-20 | 2015-11-17 | Pelican Imaging Corporation | Capturing and processing of images including occlusions captured by camera arrays |
US9188765B2 (en) | 2008-05-20 | 2015-11-17 | Pelican Imaging Corporation | Capturing and processing of images including occlusions focused on an image sensor by a lens stack array |
US8896719B1 (en) | 2008-05-20 | 2014-11-25 | Pelican Imaging Corporation | Systems and methods for parallax measurement using camera arrays incorporating 3 x 3 camera configurations |
US9124815B2 (en) | 2008-05-20 | 2015-09-01 | Pelican Imaging Corporation | Capturing and processing of images including occlusions captured by arrays of luma and chroma cameras |
US11412158B2 (en) | 2008-05-20 | 2022-08-09 | Fotonation Limited | Capturing and processing of images including occlusions focused on an image sensor by a lens stack array |
US8866920B2 (en) | 2008-05-20 | 2014-10-21 | Pelican Imaging Corporation | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
US9576369B2 (en) | 2008-05-20 | 2017-02-21 | Fotonation Cayman Limited | Systems and methods for generating depth maps using images captured by camera arrays incorporating cameras having different fields of view |
US9060142B2 (en) | 2008-05-20 | 2015-06-16 | Pelican Imaging Corporation | Capturing and processing of images captured by camera arrays including heterogeneous optics |
US9749547B2 (en) | 2008-05-20 | 2017-08-29 | Fotonation Cayman Limited | Capturing and processing of images using camera array incorperating Bayer cameras having different fields of view |
US8885059B1 (en) | 2008-05-20 | 2014-11-11 | Pelican Imaging Corporation | Systems and methods for measuring depth using images captured by camera arrays |
US9060124B2 (en) | 2008-05-20 | 2015-06-16 | Pelican Imaging Corporation | Capturing and processing of images using non-monolithic camera arrays |
US9060121B2 (en) | 2008-05-20 | 2015-06-16 | Pelican Imaging Corporation | Capturing and processing of images captured by camera arrays including cameras dedicated to sampling luma and cameras dedicated to sampling chroma |
US9055213B2 (en) | 2008-05-20 | 2015-06-09 | Pelican Imaging Corporation | Systems and methods for measuring depth using images captured by monolithic camera arrays including at least one bayer camera |
US9055233B2 (en) | 2008-05-20 | 2015-06-09 | Pelican Imaging Corporation | Systems and methods for synthesizing higher resolution images using a set of images containing a baseline image |
US9049411B2 (en) | 2008-05-20 | 2015-06-02 | Pelican Imaging Corporation | Camera arrays incorporating 3×3 imager configurations |
US11792538B2 (en) | 2008-05-20 | 2023-10-17 | Adeia Imaging Llc | Capturing and processing of images including occlusions focused on an image sensor by a lens stack array |
US9041823B2 (en) | 2008-05-20 | 2015-05-26 | Pelican Imaging Corporation | Systems and methods for performing post capture refocus using images captured by camera arrays |
US9049367B2 (en) | 2008-05-20 | 2015-06-02 | Pelican Imaging Corporation | Systems and methods for synthesizing higher resolution images using images captured by camera arrays |
US9049391B2 (en) | 2008-05-20 | 2015-06-02 | Pelican Imaging Corporation | Capturing and processing of near-IR images including occlusions using camera arrays incorporating near-IR light sources |
US9049381B2 (en) | 2008-05-20 | 2015-06-02 | Pelican Imaging Corporation | Systems and methods for normalizing image data captured by camera arrays |
US8325241B2 (en) * | 2009-02-05 | 2012-12-04 | Sony Corporation | Image pickup apparatus that stores adjacent and contiguous pixel data before integration of same |
US20100194921A1 (en) * | 2009-02-05 | 2010-08-05 | Sony Corporation | Image pickup apparatus |
US8698953B1 (en) * | 2009-08-28 | 2014-04-15 | Marvell International Ltd. | Field programmable digital image capture device |
US8861089B2 (en) | 2009-11-20 | 2014-10-14 | Pelican Imaging Corporation | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
US9264610B2 (en) | 2009-11-20 | 2016-02-16 | Pelican Imaging Corporation | Capturing and processing of images including occlusions captured by heterogeneous camera arrays |
US10306120B2 (en) | 2009-11-20 | 2019-05-28 | Fotonation Limited | Capturing and processing of images captured by camera arrays incorporating cameras with telephoto and conventional lenses to generate depth maps |
US8928793B2 (en) | 2010-05-12 | 2015-01-06 | Pelican Imaging Corporation | Imager array interfaces |
US10455168B2 (en) | 2010-05-12 | 2019-10-22 | Fotonation Limited | Imager array interfaces |
US9936148B2 (en) | 2010-05-12 | 2018-04-03 | Fotonation Cayman Limited | Imager array interfaces |
US10362225B2 (en) | 2010-10-29 | 2019-07-23 | Ecole Polytechnique Federale De Lausanne (Epfl) | Omnidirectional sensor array system |
US9876953B2 (en) * | 2010-10-29 | 2018-01-23 | Ecole Polytechnique Federale De Lausanne (Epfl) | Omnidirectional sensor array system |
US20140146132A1 (en) * | 2010-10-29 | 2014-05-29 | Ecole Polytechnique Federale De Lausanne (Epfl) | Omnidirectional sensor array system |
US11875475B2 (en) | 2010-12-14 | 2024-01-16 | Adeia Imaging Llc | Systems and methods for synthesizing high resolution images using images captured by an array of independently controllable imagers |
US8878950B2 (en) | 2010-12-14 | 2014-11-04 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using super-resolution processes |
US11423513B2 (en) | 2010-12-14 | 2022-08-23 | Fotonation Limited | Systems and methods for synthesizing high resolution images using images captured by an array of independently controllable imagers |
US10366472B2 (en) | 2010-12-14 | 2019-07-30 | Fotonation Limited | Systems and methods for synthesizing high resolution images using images captured by an array of independently controllable imagers |
US9041824B2 (en) | 2010-12-14 | 2015-05-26 | Pelican Imaging Corporation | Systems and methods for dynamic refocusing of high resolution images generated using images captured by a plurality of imagers |
US9047684B2 (en) | 2010-12-14 | 2015-06-02 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using a set of geometrically registered images |
US9361662B2 (en) | 2010-12-14 | 2016-06-07 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using images captured by an array of independently controllable imagers |
US20140176592A1 (en) * | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
US20120243101A1 (en) * | 2011-03-25 | 2012-09-27 | Casio Computer Co., Ltd. | Image capturing device with micro-lens array |
CN102692791A (en) * | 2011-03-25 | 2012-09-26 | 卡西欧计算机株式会社 | Image capturing device with micro-lens array |
US8619179B2 (en) | 2011-03-28 | 2013-12-31 | Canon Kabushiki Kaisha | Multi-modal image capture apparatus with a tunable spectral response |
US9866739B2 (en) | 2011-05-11 | 2018-01-09 | Fotonation Cayman Limited | Systems and methods for transmitting and receiving array camera image data |
US10218889B2 (en) | 2011-05-11 | 2019-02-26 | Fotonation Limited | Systems and methods for transmitting and receiving array camera image data |
EP2708019A1 (en) * | 2011-05-11 | 2014-03-19 | Pelican Imaging Corporation | Systems and methods for transmitting and receiving array camera image data |
US10742861B2 (en) | 2011-05-11 | 2020-08-11 | Fotonation Limited | Systems and methods for transmitting and receiving array camera image data |
US9197821B2 (en) | 2011-05-11 | 2015-11-24 | Pelican Imaging Corporation | Systems and methods for transmitting and receiving array camera image data |
CN107404609A (en) * | 2011-05-11 | 2017-11-28 | Fotonation开曼有限公司 | For transmitting the system and method with receiving array camera image data |
EP2708019A4 (en) * | 2011-05-11 | 2014-10-08 | Pelican Imaging Corp | Systems and methods for transmitting and receiving array camera image data |
CN103765864A (en) * | 2011-05-11 | 2014-04-30 | 派力肯影像公司 | Systems and methods for transmitting and receiving array camera image data |
US8693799B2 (en) * | 2011-05-25 | 2014-04-08 | Sony Corporation | Image processing apparatus for emphasizing details of an image and related apparatus and methods |
US20120301048A1 (en) * | 2011-05-25 | 2012-11-29 | Sony Corporation | Image processing apparatus and method |
US9578237B2 (en) | 2011-06-28 | 2017-02-21 | Fotonation Cayman Limited | Array cameras incorporating optics with modulation transfer functions greater than sensor Nyquist frequency for capture of images used in super-resolution processing |
US9516222B2 (en) | 2011-06-28 | 2016-12-06 | Kip Peli P1 Lp | Array cameras incorporating monolithic array camera modules with high MTF lens stacks for capture of images used in super-resolution processing |
US9128228B2 (en) | 2011-06-28 | 2015-09-08 | Pelican Imaging Corporation | Optical arrangements for use with an array camera |
US20130044234A1 (en) * | 2011-08-19 | 2013-02-21 | Canon Kabushiki Kaisha | Image capturing apparatus, image processing apparatus, and image processing method for generating auxiliary information for captured image |
US9456118B2 (en) * | 2011-08-19 | 2016-09-27 | Canon Kabushiki Kaisha | Image capturing apparatus, image processing apparatus, and image processing method for generating auxiliary information for captured image |
WO2013033442A1 (en) | 2011-08-30 | 2013-03-07 | Digimarc Corporation | Methods and arrangements for identifying objects |
US9794476B2 (en) | 2011-09-19 | 2017-10-17 | Fotonation Cayman Limited | Systems and methods for controlling aliasing in images captured by an array camera for use in super resolution processing using pixel apertures |
US10375302B2 (en) | 2011-09-19 | 2019-08-06 | Fotonation Limited | Systems and methods for controlling aliasing in images captured by an array camera for use in super resolution processing using pixel apertures |
US9031335B2 (en) | 2011-09-28 | 2015-05-12 | Pelican Imaging Corporation | Systems and methods for encoding light field image files having depth and confidence maps |
US9042667B2 (en) | 2011-09-28 | 2015-05-26 | Pelican Imaging Corporation | Systems and methods for decoding light field image files using a depth map |
JP2014535191A (en) * | 2011-09-28 | 2014-12-25 | ペリカン イメージング コーポレイション | System and method for encoding and decoding bright-field image files |
KR102002165B1 (en) * | 2011-09-28 | 2019-07-25 | 포토내이션 리미티드 | Systems and methods for encoding and decoding light field image files |
CN107230236A (en) * | 2011-09-28 | 2017-10-03 | Fotonation开曼有限公司 | System and method for coding and decoding light field image file |
US9036931B2 (en) | 2011-09-28 | 2015-05-19 | Pelican Imaging Corporation | Systems and methods for decoding structured light field image files |
US9129183B2 (en) | 2011-09-28 | 2015-09-08 | Pelican Imaging Corporation | Systems and methods for encoding light field image files |
US9025895B2 (en) | 2011-09-28 | 2015-05-05 | Pelican Imaging Corporation | Systems and methods for decoding refocusable light field image files |
US9025894B2 (en) | 2011-09-28 | 2015-05-05 | Pelican Imaging Corporation | Systems and methods for decoding light field image files having depth and confidence maps |
US9811753B2 (en) | 2011-09-28 | 2017-11-07 | Fotonation Cayman Limited | Systems and methods for encoding light field image files |
US10984276B2 (en) | 2011-09-28 | 2021-04-20 | Fotonation Limited | Systems and methods for encoding image files containing depth maps stored as metadata |
US9031342B2 (en) | 2011-09-28 | 2015-05-12 | Pelican Imaging Corporation | Systems and methods for encoding refocusable light field image files |
CN104081414A (en) * | 2011-09-28 | 2014-10-01 | 派力肯影像公司 | Systems and methods for encoding and decoding light field image files |
US9536166B2 (en) | 2011-09-28 | 2017-01-03 | Kip Peli P1 Lp | Systems and methods for decoding image files containing depth maps stored as metadata |
US9864921B2 (en) | 2011-09-28 | 2018-01-09 | Fotonation Cayman Limited | Systems and methods for encoding image files containing depth maps stored as metadata |
US9031343B2 (en) | 2011-09-28 | 2015-05-12 | Pelican Imaging Corporation | Systems and methods for encoding light field image files having a depth map |
US10275676B2 (en) * | 2011-09-28 | 2019-04-30 | Fotonation Limited | Systems and methods for encoding image files containing depth maps stored as metadata |
US8831367B2 (en) | 2011-09-28 | 2014-09-09 | Pelican Imaging Corporation | Systems and methods for decoding light field image files |
US11729365B2 (en) | 2011-09-28 | 2023-08-15 | Adela Imaging LLC | Systems and methods for encoding image files containing depth maps stored as metadata |
US10019816B2 (en) | 2011-09-28 | 2018-07-10 | Fotonation Cayman Limited | Systems and methods for decoding image files containing depth maps stored as metadata |
KR20140067156A (en) * | 2011-09-28 | 2014-06-03 | 펠리칸 이매징 코포레이션 | Systems and methods for encoding and decoding light field image files |
US9036928B2 (en) | 2011-09-28 | 2015-05-19 | Pelican Imaging Corporation | Systems and methods for encoding structured light field image files |
US10430682B2 (en) | 2011-09-28 | 2019-10-01 | Fotonation Limited | Systems and methods for decoding image files containing depth maps stored as metadata |
US20180197035A1 (en) * | 2011-09-28 | 2018-07-12 | Fotonation Cayman Limited | Systems and Methods for Encoding Image Files Containing Depth Maps Stored as Metadata |
WO2013049699A1 (en) * | 2011-09-28 | 2013-04-04 | Pelican Imaging Corporation | Systems and methods for encoding and decoding light field image files |
EP2761534A4 (en) * | 2011-09-28 | 2015-03-04 | Pelican Imaging Corp | Systems and methods for encoding and decoding light field image files |
US20130107102A1 (en) * | 2011-11-02 | 2013-05-02 | Sony Mobile Communications Ab | Displaying of images with lighting on the basis of captured auxiliary images |
US9036070B2 (en) * | 2011-11-02 | 2015-05-19 | Sony Corporation | Displaying of images with lighting on the basis of captured auxiliary images |
EP2590398B1 (en) * | 2011-11-02 | 2021-07-07 | Sony Group Corporation | Displaying of images with lighting on the basis of captured auxiliary images |
EP3933782A1 (en) * | 2011-11-02 | 2022-01-05 | Sony Group Corporation | Displaying of images with lighting on the basis of captured auxiliary images |
US20150235476A1 (en) * | 2012-02-21 | 2015-08-20 | Pelican Imaging Corporation | Systems and Method for Performing Depth Based Image Editing |
US9754422B2 (en) * | 2012-02-21 | 2017-09-05 | Fotonation Cayman Limited | Systems and method for performing depth based image editing |
US20170365104A1 (en) * | 2012-02-21 | 2017-12-21 | Fotonation Cayman Limited | Systems and Method for Performing Depth Based Image Editing |
US9412206B2 (en) | 2012-02-21 | 2016-08-09 | Pelican Imaging Corporation | Systems and methods for the manipulation of captured light field image data |
US10311649B2 (en) * | 2012-02-21 | 2019-06-04 | Fotonation Limited | Systems and method for performing depth based image editing |
US20130229532A1 (en) * | 2012-03-01 | 2013-09-05 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
JP2013183278A (en) * | 2012-03-01 | 2013-09-12 | Canon Inc | Image processing apparatus, image processing method and program |
US8937662B2 (en) * | 2012-03-01 | 2015-01-20 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
EP2635019A3 (en) * | 2012-03-01 | 2014-01-22 | Canon Kabushiki Kaisha | Image processing device, image processing method, and program |
US9210392B2 (en) | 2012-05-01 | 2015-12-08 | Pelican Imaging Coporation | Camera modules patterned with pi filter groups |
US9706132B2 (en) | 2012-05-01 | 2017-07-11 | Fotonation Cayman Limited | Camera modules patterned with pi filter groups |
GB2501936B (en) * | 2012-05-11 | 2016-11-30 | Canon Kk | Micro lens array and imaging apparatus |
GB2501936A (en) * | 2012-05-11 | 2013-11-13 | Canon Kk | Micro-lens array with micro-lens subsets displaced from a regular lattice pattern |
US9338336B2 (en) * | 2012-05-21 | 2016-05-10 | Canon Kabushiki Kaisha | Image pickup apparatus |
US20130308035A1 (en) * | 2012-05-21 | 2013-11-21 | Canon Kabushiki Kaisha | Image pickup apparatus |
US9628696B2 (en) | 2012-06-11 | 2017-04-18 | Canon Kabushiki Kaisha | Image processing apparatus, image processing method, image pickup apparatus, method of controlling image pickup apparatus, and non-transitory computer-readable storage medium |
US9451147B2 (en) * | 2012-06-11 | 2016-09-20 | Canon Kabushiki Kaisha | Image processing apparatus, image processing method, image pickup apparatus, method of controlling image pickup apparatus, and non-transitory computer-readable storage medium |
US20130329120A1 (en) * | 2012-06-11 | 2013-12-12 | Canon Kabushiki Kaisha | Image processing apparatus, image processing method, image pickup apparatus, method of controlling image pickup apparatus, and non-transitory computer-readable storage medium |
US10552947B2 (en) | 2012-06-26 | 2020-02-04 | Google Llc | Depth-based image blurring |
US10334241B2 (en) | 2012-06-28 | 2019-06-25 | Fotonation Limited | Systems and methods for detecting defective camera arrays and optic arrays |
US9100635B2 (en) | 2012-06-28 | 2015-08-04 | Pelican Imaging Corporation | Systems and methods for detecting defective camera arrays and optic arrays |
US9807382B2 (en) | 2012-06-28 | 2017-10-31 | Fotonation Cayman Limited | Systems and methods for detecting defective camera arrays and optic arrays |
US9766380B2 (en) | 2012-06-30 | 2017-09-19 | Fotonation Cayman Limited | Systems and methods for manufacturing camera modules using active alignment of lens stack arrays and sensors |
US10261219B2 (en) | 2012-06-30 | 2019-04-16 | Fotonation Limited | Systems and methods for manufacturing camera modules using active alignment of lens stack arrays and sensors |
US11022725B2 (en) | 2012-06-30 | 2021-06-01 | Fotonation Limited | Systems and methods for manufacturing camera modules using active alignment of lens stack arrays and sensors |
US9129377B2 (en) | 2012-08-21 | 2015-09-08 | Pelican Imaging Corporation | Systems and methods for measuring depth based upon occlusion patterns in images |
US9240049B2 (en) | 2012-08-21 | 2016-01-19 | Pelican Imaging Corporation | Systems and methods for measuring depth using an array of independently controllable cameras |
US9123118B2 (en) | 2012-08-21 | 2015-09-01 | Pelican Imaging Corporation | System and methods for measuring depth using an array camera employing a bayer filter |
US9147254B2 (en) | 2012-08-21 | 2015-09-29 | Pelican Imaging Corporation | Systems and methods for measuring depth in the presence of occlusions using a subset of images |
US9123117B2 (en) | 2012-08-21 | 2015-09-01 | Pelican Imaging Corporation | Systems and methods for generating depth maps and corresponding confidence maps indicating depth estimation reliability |
US9858673B2 (en) | 2012-08-21 | 2018-01-02 | Fotonation Cayman Limited | Systems and methods for estimating depth and visibility from a reference viewpoint for pixels in a set of images captured from different viewpoints |
US10380752B2 (en) | 2012-08-21 | 2019-08-13 | Fotonation Limited | Systems and methods for estimating depth and visibility from a reference viewpoint for pixels in a set of images captured from different viewpoints |
US9235900B2 (en) | 2012-08-21 | 2016-01-12 | Pelican Imaging Corporation | Systems and methods for estimating depth and visibility from a reference viewpoint for pixels in a set of images captured from different viewpoints |
US10462362B2 (en) | 2012-08-23 | 2019-10-29 | Fotonation Limited | Feature based high resolution motion estimation from low resolution images captured using an array source |
US9813616B2 (en) | 2012-08-23 | 2017-11-07 | Fotonation Cayman Limited | Feature based high resolution motion estimation from low resolution images captured using an array source |
EP2709352A3 (en) * | 2012-09-12 | 2017-09-27 | Canon Kabushiki Kaisha | Image pickup apparatus, image pickup system, image processing device, and method of controlling image pickup apparatus |
US9214013B2 (en) | 2012-09-14 | 2015-12-15 | Pelican Imaging Corporation | Systems and methods for correcting user identified artifacts in light field images |
GB2505954A (en) * | 2012-09-18 | 2014-03-19 | Canon Kk | Micro lens array with displaced micro-lenses suitable for a light-field colour camera |
GB2505955B (en) * | 2012-09-18 | 2017-05-10 | Canon Kk | Light field imaging device with micro lens array with a colour filter set |
GB2505954B (en) * | 2012-09-18 | 2017-05-24 | Canon Kk | Light field imaging device with a micro lens array integrating a sensor mounted with a colour-filter-array |
GB2505955A (en) * | 2012-09-18 | 2014-03-19 | Canon Kk | Micro lens array with a colour filter set and imaging apparatus suitable for a light-field colour camera |
US10390005B2 (en) | 2012-09-28 | 2019-08-20 | Fotonation Limited | Generating images from light fields utilizing virtual viewpoints |
US9143711B2 (en) | 2012-11-13 | 2015-09-22 | Pelican Imaging Corporation | Systems and methods for array camera focal plane control |
US9749568B2 (en) | 2012-11-13 | 2017-08-29 | Fotonation Cayman Limited | Systems and methods for array camera focal plane control |
US9153026B2 (en) | 2012-11-26 | 2015-10-06 | Ricoh Co., Ltd. | Calibration of plenoptic imaging systems |
US20150312553A1 (en) * | 2012-12-04 | 2015-10-29 | Lytro, Inc. | Capturing and relighting images using multiple devices |
WO2014110484A2 (en) | 2013-01-11 | 2014-07-17 | Digimarc Corporation | Next generation imaging methods and systems |
US20150262424A1 (en) * | 2013-01-31 | 2015-09-17 | Google Inc. | Depth and Focus Discrimination for a Head-mountable device using a Light-Field Display System |
US20140226039A1 (en) * | 2013-02-14 | 2014-08-14 | Canon Kabushiki Kaisha | Image capturing apparatus and control method thereof |
US10547828B2 (en) | 2013-02-15 | 2020-01-28 | Red.Com, Llc | Dense field imaging |
US10939088B2 (en) | 2013-02-15 | 2021-03-02 | Red.Com, Llc | Computational imaging device |
US9769365B1 (en) | 2013-02-15 | 2017-09-19 | Red.Com, Inc. | Dense field imaging |
US10277885B1 (en) | 2013-02-15 | 2019-04-30 | Red.Com, Llc | Dense field imaging |
US9497380B1 (en) | 2013-02-15 | 2016-11-15 | Red.Com, Inc. | Dense field imaging |
US10009538B2 (en) | 2013-02-21 | 2018-06-26 | Fotonation Cayman Limited | Systems and methods for generating compressed light field representation data using captured light fields, array geometry, and parallax information |
US9462164B2 (en) | 2013-02-21 | 2016-10-04 | Pelican Imaging Corporation | Systems and methods for generating compressed light field representation data using captured light fields, array geometry, and parallax information |
US9774831B2 (en) | 2013-02-24 | 2017-09-26 | Fotonation Cayman Limited | Thin form factor computational array cameras and modular array cameras |
US9253380B2 (en) | 2013-02-24 | 2016-02-02 | Pelican Imaging Corporation | Thin form factor computational array cameras and modular array cameras |
US9374512B2 (en) | 2013-02-24 | 2016-06-21 | Pelican Imaging Corporation | Thin form factor computational array cameras and modular array cameras |
US9743051B2 (en) | 2013-02-24 | 2017-08-22 | Fotonation Cayman Limited | Thin form factor computational array cameras and modular array cameras |
US9235741B2 (en) | 2013-02-28 | 2016-01-12 | Hand Held Products, Inc. | Indicia reading terminals and methods employing light field imaging |
US8978984B2 (en) | 2013-02-28 | 2015-03-17 | Hand Held Products, Inc. | Indicia reading terminals and methods for decoding decodable indicia employing light field imaging |
US9638883B1 (en) | 2013-03-04 | 2017-05-02 | Fotonation Cayman Limited | Passive alignment of array camera modules constructed from lens stack arrays and sensors based upon alignment information obtained during manufacture of array camera modules using an active alignment process |
US9774789B2 (en) | 2013-03-08 | 2017-09-26 | Fotonation Cayman Limited | Systems and methods for high dynamic range imaging using array cameras |
US9917998B2 (en) | 2013-03-08 | 2018-03-13 | Fotonation Cayman Limited | Systems and methods for measuring scene information while capturing images using array cameras |
US9124864B2 (en) | 2013-03-10 | 2015-09-01 | Pelican Imaging Corporation | System and methods for calibration of an array camera |
US10225543B2 (en) | 2013-03-10 | 2019-03-05 | Fotonation Limited | System and methods for calibration of an array camera |
US9986224B2 (en) | 2013-03-10 | 2018-05-29 | Fotonation Cayman Limited | System and methods for calibration of an array camera |
US10958892B2 (en) | 2013-03-10 | 2021-03-23 | Fotonation Limited | System and methods for calibration of an array camera |
US11272161B2 (en) | 2013-03-10 | 2022-03-08 | Fotonation Limited | System and methods for calibration of an array camera |
US11570423B2 (en) | 2013-03-10 | 2023-01-31 | Adeia Imaging Llc | System and methods for calibration of an array camera |
US9521416B1 (en) | 2013-03-11 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for image data compression |
US9519972B2 (en) | 2013-03-13 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for synthesizing images from image data captured by an array camera using restricted depth of field depth maps in which depth estimation precision varies |
US9106784B2 (en) | 2013-03-13 | 2015-08-11 | Pelican Imaging Corporation | Systems and methods for controlling aliasing in images captured by an array camera for use in super-resolution processing |
US10127682B2 (en) | 2013-03-13 | 2018-11-13 | Fotonation Limited | System and methods for calibration of an array camera |
US9888194B2 (en) | 2013-03-13 | 2018-02-06 | Fotonation Cayman Limited | Array camera architecture implementing quantum film image sensors |
US9800856B2 (en) | 2013-03-13 | 2017-10-24 | Fotonation Cayman Limited | Systems and methods for synthesizing images from image data captured by an array camera using restricted depth of field depth maps in which depth estimation precision varies |
US9741118B2 (en) | 2013-03-13 | 2017-08-22 | Fotonation Cayman Limited | System and methods for calibration of an array camera |
US9733486B2 (en) | 2013-03-13 | 2017-08-15 | Fotonation Cayman Limited | Systems and methods for controlling aliasing in images captured by an array camera for use in super-resolution processing |
US10547772B2 (en) | 2013-03-14 | 2020-01-28 | Fotonation Limited | Systems and methods for reducing motion blur in images or video in ultra low light with array cameras |
US10091405B2 (en) | 2013-03-14 | 2018-10-02 | Fotonation Cayman Limited | Systems and methods for reducing motion blur in images or video in ultra low light with array cameras |
US9787911B2 (en) | 2013-03-14 | 2017-10-10 | Fotonation Cayman Limited | Systems and methods for photometric normalization in array cameras |
US10412314B2 (en) | 2013-03-14 | 2019-09-10 | Fotonation Limited | Systems and methods for photometric normalization in array cameras |
US9100586B2 (en) | 2013-03-14 | 2015-08-04 | Pelican Imaging Corporation | Systems and methods for photometric normalization in array cameras |
US9578259B2 (en) | 2013-03-14 | 2017-02-21 | Fotonation Cayman Limited | Systems and methods for reducing motion blur in images or video in ultra low light with array cameras |
US9602805B2 (en) | 2013-03-15 | 2017-03-21 | Fotonation Cayman Limited | Systems and methods for estimating depth using ad hoc stereo array cameras |
US9800859B2 (en) | 2013-03-15 | 2017-10-24 | Fotonation Cayman Limited | Systems and methods for estimating depth using stereo array cameras |
US9497370B2 (en) | 2013-03-15 | 2016-11-15 | Pelican Imaging Corporation | Array camera architecture implementing quantum dot color filters |
US9633442B2 (en) | 2013-03-15 | 2017-04-25 | Fotonation Cayman Limited | Array cameras including an array camera module augmented with a separate camera |
US9438888B2 (en) | 2013-03-15 | 2016-09-06 | Pelican Imaging Corporation | Systems and methods for stereo imaging with camera arrays |
US10455218B2 (en) | 2013-03-15 | 2019-10-22 | Fotonation Limited | Systems and methods for estimating depth using stereo array cameras |
US9955070B2 (en) | 2013-03-15 | 2018-04-24 | Fotonation Cayman Limited | Systems and methods for synthesizing high resolution images using image deconvolution based on motion and depth information |
US10542208B2 (en) | 2013-03-15 | 2020-01-21 | Fotonation Limited | Systems and methods for synthesizing high resolution images using image deconvolution based on motion and depth information |
US9445003B1 (en) | 2013-03-15 | 2016-09-13 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using image deconvolution based on motion and depth information |
US10182216B2 (en) | 2013-03-15 | 2019-01-15 | Fotonation Limited | Extended color processing on pelican array cameras |
US10674138B2 (en) | 2013-03-15 | 2020-06-02 | Fotonation Limited | Autofocus system for a conventional camera that uses depth information from an array camera |
US10638099B2 (en) | 2013-03-15 | 2020-04-28 | Fotonation Limited | Extended color processing on pelican array cameras |
US10122993B2 (en) | 2013-03-15 | 2018-11-06 | Fotonation Limited | Autofocus system for a conventional camera that uses depth information from an array camera |
US9497429B2 (en) | 2013-03-15 | 2016-11-15 | Pelican Imaging Corporation | Extended color processing on pelican array cameras |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US9417121B1 (en) * | 2013-06-04 | 2016-08-16 | James E. Spencer | Methods and apparatuses using optics with aperture for passing optical signals between input and output stages |
US9621797B2 (en) * | 2013-08-21 | 2017-04-11 | Canon Kabushiki Kaisha | Image processing apparatus, control method for same, and program |
US10109036B2 (en) * | 2013-08-21 | 2018-10-23 | Canon Kabushiki Kaisha | Image processing apparatus, control method for same, and program that performs image processing for image data having a focus state that is changeable |
US20170169542A1 (en) * | 2013-08-21 | 2017-06-15 | Canon Kabushiki Kaisha | Image processing apparatus, control method for same, and program |
US20150054982A1 (en) * | 2013-08-21 | 2015-02-26 | Canon Kabushiki Kaisha | Image processing apparatus, control method for same, and program |
US9898856B2 (en) | 2013-09-27 | 2018-02-20 | Fotonation Cayman Limited | Systems and methods for depth-assisted perspective distortion correction |
US10540806B2 (en) | 2013-09-27 | 2020-01-21 | Fotonation Limited | Systems and methods for depth-assisted perspective distortion correction |
US9426343B2 (en) | 2013-11-07 | 2016-08-23 | Pelican Imaging Corporation | Array cameras incorporating independently aligned lens stacks |
US9924092B2 (en) | 2013-11-07 | 2018-03-20 | Fotonation Cayman Limited | Array cameras incorporating independently aligned lens stacks |
US9185276B2 (en) | 2013-11-07 | 2015-11-10 | Pelican Imaging Corporation | Methods of manufacturing array camera modules incorporating independently aligned lens stacks |
US9264592B2 (en) | 2013-11-07 | 2016-02-16 | Pelican Imaging Corporation | Array camera modules incorporating independently aligned lens stacks |
US10767981B2 (en) | 2013-11-18 | 2020-09-08 | Fotonation Limited | Systems and methods for estimating depth from projected texture using camera arrays |
US10119808B2 (en) | 2013-11-18 | 2018-11-06 | Fotonation Limited | Systems and methods for estimating depth from projected texture using camera arrays |
US11486698B2 (en) | 2013-11-18 | 2022-11-01 | Fotonation Limited | Systems and methods for estimating depth from projected texture using camera arrays |
US10708492B2 (en) | 2013-11-26 | 2020-07-07 | Fotonation Limited | Array camera configurations incorporating constituent array cameras and constituent cameras |
US9426361B2 (en) | 2013-11-26 | 2016-08-23 | Pelican Imaging Corporation | Array camera configurations incorporating multiple constituent array cameras |
US9813617B2 (en) | 2013-11-26 | 2017-11-07 | Fotonation Cayman Limited | Array camera configurations incorporating constituent array cameras and constituent cameras |
US9456134B2 (en) | 2013-11-26 | 2016-09-27 | Pelican Imaging Corporation | Array camera configurations incorporating constituent array cameras and constituent cameras |
DE112014005866B4 (en) | 2013-12-24 | 2018-08-02 | Lytro, Inc. | Improvement of plenoptic camera resolution |
US9600904B2 (en) | 2013-12-30 | 2017-03-21 | Samsung Electronics Co., Ltd. | Illuminating a virtual environment with camera light data |
US10244223B2 (en) | 2014-01-10 | 2019-03-26 | Ostendo Technologies, Inc. | Methods for full parallax compressed light field 3D imaging systems |
EP3116216A4 (en) * | 2014-03-03 | 2017-02-15 | Panasonic Intellectual Property Management Co., Ltd. | Image pickup apparatus |
US10089740B2 (en) | 2014-03-07 | 2018-10-02 | Fotonation Limited | System and methods for depth regularization and semiautomatic interactive matting using RGB-D images |
US10574905B2 (en) | 2014-03-07 | 2020-02-25 | Fotonation Limited | System and methods for depth regularization and semiautomatic interactive matting using RGB-D images |
US9247117B2 (en) | 2014-04-07 | 2016-01-26 | Pelican Imaging Corporation | Systems and methods for correcting for warpage of a sensor array in an array camera module by introducing warpage into a focal plane of a lens stack array |
US20170031151A1 (en) * | 2014-04-11 | 2017-02-02 | The Regents Of The University Of Colorado, A Body Corporate | Scanning Imaging For Encoded PSF Identification and Light Field Imaging |
US10613312B2 (en) * | 2014-04-11 | 2020-04-07 | The Regents Of The University Of Colorado, A Body | Scanning imaging for encoded PSF identification and light field imaging |
WO2015157769A1 (en) * | 2014-04-11 | 2015-10-15 | The Regents Of The University Of Colorado, A Body Corporate | Scanning imaging for encoded psf identification and light field imaging |
EP2940981A3 (en) * | 2014-04-29 | 2016-03-23 | Xiaomi Inc. | Method and device for synchronizing photographs |
US9521319B2 (en) | 2014-06-18 | 2016-12-13 | Pelican Imaging Corporation | Array cameras and array camera modules including spectral filters disposed outside of a constituent image sensor |
TWI691197B (en) * | 2014-07-15 | 2020-04-11 | 美商傲思丹度科技公司 | Preprocessor for full parallax light field compression |
WO2016011087A1 (en) * | 2014-07-15 | 2016-01-21 | Ostendo Technologies, Inc. | Preprocessor for full parallax light field compression |
US9438778B2 (en) | 2014-08-08 | 2016-09-06 | Industrial Technology Research Institute | Image pickup device and light field image pickup lens |
US11546576B2 (en) | 2014-09-29 | 2023-01-03 | Adeia Imaging Llc | Systems and methods for dynamic calibration of array cameras |
US10250871B2 (en) | 2014-09-29 | 2019-04-02 | Fotonation Limited | Systems and methods for dynamic calibration of array cameras |
US10110303B2 (en) | 2014-11-10 | 2018-10-23 | Beijing Zhigu Rui Tuo Tech Co., Ltd. | Light-communication sending methods and apparatus, light-communication receiving methods and apparatus, and light communication systems |
US9691149B2 (en) | 2014-11-27 | 2017-06-27 | Thomson Licensing | Plenoptic camera comprising a light emitting device |
US9918077B2 (en) | 2015-01-09 | 2018-03-13 | Ricoh Company, Ltd. | Object space calibration of plenoptic imaging systems |
US9544583B2 (en) | 2015-01-09 | 2017-01-10 | Ricoh Company, Ltd. | Object space calibration of plenoptic imaging systems |
US9797716B2 (en) | 2015-01-09 | 2017-10-24 | Ricoh Company, Ltd. | Estimating surface properties using a plenoptic camera |
US9832376B2 (en) * | 2015-03-03 | 2017-11-28 | Canon Kabushiki Kaisha | Image display apparatus, image capturing apparatus, image display method, and storage medium for distinguishing reconstructable image data from normal image data |
US10382676B2 (en) | 2015-03-03 | 2019-08-13 | Canon Kabushiki Kaisha | Image display apparatus, image capturing apparatus, image display method, and storage medium |
US20160261795A1 (en) * | 2015-03-03 | 2016-09-08 | Canon Kabushiki Kaisha | Image display apparatus, image capturing apparatus, image display method, and storage medium |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US10469873B2 (en) | 2015-04-15 | 2019-11-05 | Google Llc | Encoding and decoding virtual reality video |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US9942474B2 (en) | 2015-04-17 | 2018-04-10 | Fotonation Cayman Limited | Systems and methods for performing high speed video capture and depth estimation using array cameras |
US20160307368A1 (en) * | 2015-04-17 | 2016-10-20 | Lytro, Inc. | Compression and interactive playback of light field pictures |
US11368662B2 (en) | 2015-04-19 | 2022-06-21 | Fotonation Limited | Multi-baseline camera array system architectures for depth augmentation in VR/AR applications |
US10805589B2 (en) | 2015-04-19 | 2020-10-13 | Fotonation Limited | Multi-baseline camera array system architectures for depth augmentation in VR/AR applications |
US10310450B2 (en) | 2015-04-23 | 2019-06-04 | Ostendo Technologies, Inc. | Methods and apparatus for full parallax light field display systems |
US10528004B2 (en) | 2015-04-23 | 2020-01-07 | Ostendo Technologies, Inc. | Methods and apparatus for full parallax light field display systems |
WO2016172385A1 (en) * | 2015-04-23 | 2016-10-27 | Ostendo Technologies, Inc. | Methods for full parallax compressed light field synthesis utilizing depth information |
CN107430782A (en) * | 2015-04-23 | 2017-12-01 | 奥斯坦多科技公司 | Method for being synthesized using the full parallax squeezed light field of depth information |
US10070115B2 (en) | 2015-04-23 | 2018-09-04 | Ostendo Technologies, Inc. | Methods for full parallax compressed light field synthesis utilizing depth information |
US20190124318A1 (en) * | 2015-05-26 | 2019-04-25 | Google Llc | Capturing light-field images with uneven and/or incomplete angular sampling |
US10897608B2 (en) * | 2015-05-26 | 2021-01-19 | Google Llc | Capturing light-field images with uneven and/or incomplete angular sampling |
US10205896B2 (en) | 2015-07-24 | 2019-02-12 | Google Llc | Automatic lens flare detection and correction for light-field images |
RU2729698C2 (en) * | 2015-09-17 | 2020-08-11 | ИНТЕРДИДЖИТАЛ ВиСи ХОЛДИНГЗ, ИНК. | Apparatus and method for encoding an image captured by an optical system for acquiring data |
US10872442B2 (en) | 2015-09-17 | 2020-12-22 | Interdigital Vc Holdings, Inc. | Apparatus and a method for encoding an image captured by an optical acquisition system |
US10902624B2 (en) * | 2015-09-17 | 2021-01-26 | Interdigital Vc Holdings, Inc. | Apparatus and a method for generating data representing a pixel beam |
US20180260969A1 (en) * | 2015-09-17 | 2018-09-13 | Thomson Licensing | An apparatus and a method for generating data representing a pixel beam |
US11019347B2 (en) | 2015-11-16 | 2021-05-25 | Ostendo Technologies, Inc. | Content adaptive light field compression |
US10448030B2 (en) | 2015-11-16 | 2019-10-15 | Ostendo Technologies, Inc. | Content adaptive light field compression |
WO2017104111A1 (en) * | 2015-12-17 | 2017-06-22 | Canon Kabushiki Kaisha | Data recording apparatus, image capturing apparatus, data recording method, and storage medium |
US20170256059A1 (en) * | 2016-03-07 | 2017-09-07 | Ricoh Company, Ltd. | Object Segmentation from Light Field Data |
US10136116B2 (en) * | 2016-03-07 | 2018-11-20 | Ricoh Company, Ltd. | Object segmentation from light field data |
US11145276B2 (en) | 2016-04-28 | 2021-10-12 | Ostendo Technologies, Inc. | Integrated near-far light field display systems |
US10453431B2 (en) | 2016-04-28 | 2019-10-22 | Ostendo Technologies, Inc. | Integrated near-far light field display systems |
DE102016208210A1 (en) * | 2016-05-12 | 2017-11-16 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | 3D MULTI-PAPER PICTURE DEVICES, MULTI-PAPER IMAGING DEVICE, METHOD FOR PROVIDING AN OUTPUT SIGNAL OF A 3D MULTI-PAPER IMAGING DEVICE AND METHOD FOR DETECTING A TOTAL FACE |
US10708570B2 (en) | 2016-05-12 | 2020-07-07 | Fraunhofer-Gesellschaft zur Föderung der angewandten Forschung e.V. | 3D multi-aperture imaging devices, multi-aperture imaging device, method for providing an output signal of a 3D multi-aperture imaging device and method for capturing a total field of view |
US10275892B2 (en) | 2016-06-09 | 2019-04-30 | Google Llc | Multi-view scene segmentation and propagation |
US11665369B2 (en) * | 2016-06-22 | 2023-05-30 | Interdigital Ce Patent Holdings, Sas | Method and a device for encoding a signal representative of a light-field content |
US20170374388A1 (en) * | 2016-06-22 | 2017-12-28 | Thomson Licensing | Method and a device for encoding a signal representative of a light-field content |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US10818026B2 (en) | 2017-08-21 | 2020-10-27 | Fotonation Limited | Systems and methods for hybrid depth regularization |
US11562498B2 (en) | 2017-08-21 | 2023-01-24 | Adela Imaging LLC | Systems and methods for hybrid depth regularization |
US10482618B2 (en) | 2017-08-21 | 2019-11-19 | Fotonation Limited | Systems and methods for hybrid depth regularization |
US20190080434A1 (en) * | 2017-09-08 | 2019-03-14 | Ricoh Company, Ltd. | Reducing Color Artifacts In Plenoptic Imaging Systems |
US10552942B2 (en) * | 2017-09-08 | 2020-02-04 | Ricoh Company, Ltd. | Reducing color artifacts in plenoptic imaging systems |
US10545215B2 (en) | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US10776995B2 (en) | 2017-10-17 | 2020-09-15 | Nvidia Corporation | Light fields as better backgrounds in rendering |
US11604584B2 (en) * | 2017-11-20 | 2023-03-14 | Hitachi, Ltd. | Storage system |
US10656836B2 (en) * | 2017-11-20 | 2020-05-19 | Hitachi, Ltd. | Storage system |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
US11182872B2 (en) * | 2018-11-02 | 2021-11-23 | Electronics And Telecommunications Research Institute | Plenoptic data storage system and operating method thereof |
US10873693B2 (en) * | 2018-12-03 | 2020-12-22 | Samsung Electronics Co.. Ltd. | Calibration method and apparatus |
US11699273B2 (en) | 2019-09-17 | 2023-07-11 | Intrinsic Innovation Llc | Systems and methods for surface modeling using polarization cues |
US11270110B2 (en) | 2019-09-17 | 2022-03-08 | Boston Polarimetrics, Inc. | Systems and methods for surface modeling using polarization cues |
US11525906B2 (en) | 2019-10-07 | 2022-12-13 | Intrinsic Innovation Llc | Systems and methods for augmentation of sensor systems and imaging systems with polarization |
US11302012B2 (en) | 2019-11-30 | 2022-04-12 | Boston Polarimetrics, Inc. | Systems and methods for transparent object segmentation using polarization cues |
US11842495B2 (en) | 2019-11-30 | 2023-12-12 | Intrinsic Innovation Llc | Systems and methods for transparent object segmentation using polarization cues |
US11580667B2 (en) | 2020-01-29 | 2023-02-14 | Intrinsic Innovation Llc | Systems and methods for characterizing object pose detection and measurement systems |
US11797863B2 (en) | 2020-01-30 | 2023-10-24 | Intrinsic Innovation Llc | Systems and methods for synthesizing data for training statistical models on different imaging modalities including polarized images |
US11953700B2 (en) | 2020-05-27 | 2024-04-09 | Intrinsic Innovation Llc | Multi-aperture polarization optical systems using beam splitters |
US11290658B1 (en) | 2021-04-15 | 2022-03-29 | Boston Polarimetrics, Inc. | Systems and methods for camera exposure control |
US11683594B2 (en) | 2021-04-15 | 2023-06-20 | Intrinsic Innovation Llc | Systems and methods for camera exposure control |
US11954886B2 (en) | 2021-04-15 | 2024-04-09 | Intrinsic Innovation Llc | Systems and methods for six-degree of freedom pose estimation of deformable objects |
US11689813B2 (en) | 2021-07-01 | 2023-06-27 | Intrinsic Innovation Llc | Systems and methods for high dynamic range imaging using crossed polarizers |
US11982775B2 (en) | 2022-12-12 | 2024-05-14 | Intrinsic Innovation Llc | Systems and methods for augmentation of sensor systems and imaging systems with polarization |
US11983893B2 (en) | 2023-01-12 | 2024-05-14 | Adeia Imaging Llc | Systems and methods for hybrid depth regularization |
US11985293B2 (en) | 2023-01-30 | 2024-05-14 | Adeia Imaging Llc | System and methods for calibration of an array camera |
Also Published As
Publication number | Publication date |
---|---|
EP2419884A4 (en) | 2013-01-02 |
EP2419884A1 (en) | 2012-02-22 |
CN102282590A (en) | 2011-12-14 |
JP2012524467A (en) | 2012-10-11 |
WO2010120591A1 (en) | 2010-10-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20100265385A1 (en) | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same | |
US20130113981A1 (en) | Light field camera image, file and configuration data, and methods of using, storing and communicating same | |
CN103221975B (en) | Three-dimensional imaging system | |
US8749620B1 (en) | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same | |
CN102246505B (en) | Image processing apparatus and image processing method, and data processing apparatus and data processing method | |
Birklbauer et al. | Panorama light‐field imaging | |
CN103051833B (en) | Picture pick-up device and manufacture method, image processing apparatus and image processing method | |
CN102957863B (en) | Picture pick-up device, image processing equipment and image processing method | |
CN102682440B (en) | Image processing apparatus, image capturing apparatus, and image processing method | |
KR102583723B1 (en) | A method and an apparatus for generating data representative of a light field | |
CN103297683B (en) | Image processing equipment and image processing method | |
KR20190076998A (en) | Apparatus and method for obtaining distance information from a view | |
WO2011158498A1 (en) | Image capture device and image capture method | |
CN103595979A (en) | Image processing device, image capturing device, and image processing method | |
US20030142877A1 (en) | Imaging using a multifocal aspheric lens to obtain extended depth of field | |
Dansereau et al. | A wide-field-of-view monocentric light field camera | |
CN101795361A (en) | Two-dimensional polynomial model for depth estimation based on two-picture matching | |
CN101241235A (en) | Decoding method, decoding apparatus and electronic camera | |
CN103685920A (en) | Image processing apparatus and method and an imaging apparatus having image processing apparatus | |
CN104604215A (en) | Image capture apparatus, image capture method and program | |
KR20170042226A (en) | Application programming interface for multi-aperture imaging systems | |
WO2014011182A1 (en) | Convergence/divergence based depth determination techniques and uses with defocusing imaging | |
TW201518847A (en) | Method and electrical device for taking 3D image and computer readable storage medium for storing the method | |
CN105190229A (en) | Three-dimensional shape measurement device, three-dimensional shape measurement method, and three-dimensional shape measurement program | |
KR20180053668A (en) | Method and apparatus for generating data representing a pixel beam |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: REFOCUS IMAGING, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KNIGHT, TIMOTHY JAMES;PITTS, COLVIN;NG, YI-REN;AND OTHERS;REEL/FRAME:024119/0029Effective date: 20100317 |
|
AS | Assignment |
Owner name: TRIPLEPOINT CAPITAL LLC, CALIFORNIAFree format text: SECURITY AGREEMENT;ASSIGNOR:LYTRO, INC.;REEL/FRAME:029732/0787Effective date: 20130123 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LYTRO, INC.;REEL/FRAME:050131/0664Effective date: 20180325Owner name: LYTRO, INC., CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:REFOCUS IMAGING, INC.;REEL/FRAME:050131/0582Effective date: 20110228 |