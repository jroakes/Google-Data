CN115858601A - Conducting collaborative search sessions through automated assistant - Google Patents
Conducting collaborative search sessions through automated assistant Download PDFInfo
- Publication number
- CN115858601A CN115858601A CN202211617950.XA CN202211617950A CN115858601A CN 115858601 A CN115858601 A CN 115858601A CN 202211617950 A CN202211617950 A CN 202211617950A CN 115858601 A CN115858601 A CN 115858601A
- Authority
- CN
- China
- Prior art keywords
- user
- query
- search results
- client device
- automated assistant
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9536—Search customisation based on social or collaborative filtering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2455—Query execution
- G06F16/24553—Query execution of query operations
- G06F16/24558—Binary matching operations
- G06F16/2456—Join operations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
Abstract
The present disclosure relates to conducting collaborative search sessions through automated assistants. Techniques for conducting a collaborative search session with an automated assistant are described herein. One method comprises the following steps: receiving a first query in a query session from a first user of a first client device; providing a first set of search results to the first user; determining that the first query is relevant to a second user of the first client device based on at least one term in the first query; providing the second user with a selectable option to join the query session; in response to receiving an indication from the second user to accept the selectable selection, adding the second user to the query session; receiving further input from the second user; generating a set of modified search results based on the additional input received from the second user; and providing the modified set of search results to the first user and the second user.
Description
Technical Field
The present disclosure relates to conducting collaborative search sessions through automated assistants.
Background
Humans may participate in human-computer conversations through an interactive software application, also referred to herein as an "automated assistant" (also referred to as a "digital assistant," "digital agent," "interactive personal assistant," "intelligent personal assistant," "assistant application," "session agent," etc.). For example, a human being (which may be referred to as a "user" when interacting with the automatic assistant) may provide commands and/or requests to the automatic assistant using spoken natural language input (i.e., utterances), and in some cases, may convert the spoken natural language input to text by providing textual (e.g., typed) natural language input and/or by touch and/or non-verbal physical motions (e.g., gestures, eye gaze, facial movements, etc.) and then process. The automated assistant responds to the request by providing responsive user interface output (e.g., audible and/or visual user interface output), controlling one or more smart devices, and/or controlling one or more functions of a device implementing the automated assistant (e.g., controlling other applications of the device).
An automated assistant may be a software application executing on a client device. The client device may be a standalone interactive speaker, a standalone interactive display device (which may also include a speaker and/or camera), a smart appliance such as a smart television (or a standard television with automatic assistant capabilities equipped with a web dongle), a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), and/or a wearable device of a user that includes a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device, a virtual or augmented reality computing device).
In some cases, the automated assistant may be used by multiple users (e.g., multiple members of a particular family or family), particularly where the automated assistant is executed on a client device such as a separate interactive speaker, a separate interactive display device, and a smart appliance that may be shared between users. In other cases, the automated assistant may be used by only a single user, particularly where the automated assistant executes on a less commonly shared client device such as a mobile telephone computing device.
Multiple users may conduct similar or redundant searches using automated assistants executing on client devices shared by the users. For example, two members of a particular household that are purchasing a new television may each perform a separate search for the television using an automated assistant executing on separate interactive display devices. Each of these users may perform multiple searches because they separately refine the respective search with the newly added constraints. Where multiple users separately perform and refine searches on the same topic, such as television, multiple activations of an automated assistant can waste network and/or computing resources. Additionally, the user experience may be negatively impacted when the second user repeats the search that has been performed by the first user.
Disclosure of Invention
Some embodiments disclosed herein relate to conducting collaborative search sessions through automated assistants. As described in greater detail herein, a collaborative search session (e.g., a product search session) may be detected and maintained across multiple users. These search sessions may be longer-term (e.g., lasting hours or days) and may be available to two or more participants on an automated assistant device, and the search sessions may be used as an input method for refining a product search continuously in the search session until the product is selected and the purchase is completed. In some embodiments, these collaborative product search sessions may be used to search for and purchase a single item (e.g., television) or multiple items (e.g., household items).
In various embodiments, a method performed by one or more processors may include: receiving, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device; providing, by a first automated assistant to a first user, a first set of search results for a first query; determining that the first query is relevant to a second user of the first client device based on at least one term in the first query; responsive to determining that the first query is relevant to the second user, providing, by the first automated assistant, a selectable option to join the query session to the second user of the first client device; in response to receiving an indication from the second user to accept the selectable option to join the query session, adding the second user to the query session; receiving further input from the second user 5 to refine the first query; generating a set of modified search results based on additional input received from the second user; and providing, by the first automated assistant, the set of modified search results to the first user and the second user.
In some embodiments, the query session is a shopping session; the first set of search results 0 comprises a first set of products; and the modified set of search results comprises a modified set of products. In some implementations, the first client device is an automated assistant device; providing the first set of search results comprises the first automated assistant causing the first set of search results to be provided on a display of a mobile device of the first user; and providing the set of modified search results comprises the first automated assistant causing the set of modified search results to be provided on a display of the mobile phone of the first user 5 and a display of the mobile device of the second user.
In some embodiments, the method may further include providing, by the first automated assistant, a selectable option to the first user to allow a second user of the first client device to join the query session. Providing the selectable option to join the query session to the second user may further be in response to 0 receiving an indication from the first user to accept the selectable option to allow the second user of the first client device to join the query session.
In some embodiments, the method may further include determining a predicted level of interest of the second user in the query session. Determining that the first query is relevant to the second user may further 5 be based on the predicted level of interest of the second user in the query session satisfying a threshold. The predicted level of interest of the second user in the query session may be based on a query history of the second user.
In some embodiments, the method may further include determining an environmental context. Determining that the first query is relevant to the second user may be further based on the environmental context. In some embodiments 0, the method may further include determining a frequency with which the second user interacts with the first client device. Determining that the first query is relevant to the second user may be further based on a frequency of interaction of the second user with the first client device satisfying a threshold. In some embodiments, determining that the first query is relevant to the second user of the first client device may be based on a score of one of the at least one term in the first query satisfying a threshold.
In some embodiments, the method may further include automatically determining, by the first automatic assistant, the filter term based on the inferred preference of the second user. Generating the set of modified search results may be further based on the filter terms. In some implementations, the second client device receives additional input.
In some embodiments, generating the modified set of search results may include: determining, by the first automated assistant, a second query based on the first query and the additional input; and generating a set of modified search results based on the second set of search results for the second query. In some embodiments, generating the modified set of search results may include filtering the first set of search results based on the additional input.
In some additional or alternative embodiments, the computer program product may include one or more computer-readable storage media having program instructions stored collectively on the one or more computer-readable storage media. The program instructions are executable to: receiving, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device; providing, by a first automated assistant to a first user, a first set of search results for a first query; receiving, by the first automated assistant, additional input from a second user of the first client device to refine the first query; in response to receiving additional input from the second user to refine the first query, adding the second user to the query session; generating a set of modified search results based on additional input received from the second user; and providing, by the first automated assistant, the set of modified search results to the first user and the second user. The modified set of search results may be provided to the second user based on adding the second user to the query session.
In some embodiments, the query session may be a shopping session; the first set of search results may include a first set of products; and the modified set of search results may include a modified set of products.
In some embodiments, the first client device may be an automated assistant device; providing the first set of search results may include the first automated assistant causing the first set of search results to be provided on a display of a mobile device of the first user; and providing the set of modified search results may include the first automated assistant causing the set of modified search results to be provided on a display of the first user's mobile phone and a display of the second user's mobile device.
In some embodiments, the program instructions may be further executable to automatically determine, by the first automatic assistant, the filter term based on the inferred preference of the second user. Generating the set of modified search results may be further based on the filter terms.
In some embodiments, the program instructions may be further executable to: determining an identity of the second user based on a voice of the second user detected by a microphone of the first client device or based on a face of the second user detected by a camera of the first client device; and identifying the mobile device of the second user based on the identity of the second user. Adding the second user to the query session in response to receiving the additional input may include adding a mobile phone of the second user to the query session. Providing the modified set of search results to the second user may include the first automated assistant causing the modified set of search results to be provided on a display of the second user's mobile phone based on adding the second user's mobile phone to the query session.
In some embodiments, the program instructions may be further executable to: determining an identity of the second user based on a voice of the second user detected by a microphone of the first client device or based on a face of the second user detected by a camera of the first client device; and identifying a user account of the second user based on the identity of the second user. Adding the second user to the query session in response to receiving the additional input may include adding a user account of the second user to the query session. Providing the modified set of search results to the second user may be based on subsequently detecting the second user based on the second user's voice or based on the second user's face.
In some additional or alternative embodiments, a system may include a processor, a computer-readable memory, one or more computer-readable storage media, and program instructions collectively stored on the one or more computer-readable storage media. The program instructions are executable to: receiving, by a first automated assistant executing on a first client device at 5, a first query in a query session from a first user of the first client device; providing, by a first automated assistant to a first user, a first set of search results for a first query; determining that the first query is relevant to a second user of the first client device based on at least one term in the first query; providing, by the first automated assistant to the second user of the first client device, a selectable option to join the 0-query session in response to determining that the first query is relevant to the second user; in response to receiving an indication from the second user to accept the selectable option to join the query session, adding the second user to the query session; receiving additional input from the second user to refine the first query; generating a set of modified search results based on further input received from the second user; and providing, by the first automated assistant, the set of modified search results to the first user and the second user.
By utilizing one or more of the techniques described herein, the occurrence of multiple activations of an automated assistant that can waste network and/or computing resources when multiple users conduct similar or redundant searches may be reduced. This results in improved performance by allowing the automated assistant to reduce the number of activations.
0 provides the above description as an overview of some embodiments of the disclosure. Further description of these and other embodiments is described in more detail below.
Various embodiments can include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a central processing unit 5 (CPU), a Graphics Processing Unit (GPU), a Digital Signal Processor (DSP), and/or a Tensor Processing Unit (TPU)) to perform a method, such as one or more of the methods described herein. Other embodiments can include an automated assistant client device (e.g., a client device including at least one automated assistant for interfacing with a cloud-based automated assistant component) comprising a processor operable to 0 execute stored instructions to perform a method, such as one or more methods described herein. Other embodiments can include a system of one or more servers including one or more processors operable to execute stored instructions to perform a method, such as one or more of the methods described herein.
Drawings
Fig. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various embodiments.
FIG. 2 depicts a flowchart illustrating an example method for practicing selected aspects of the present disclosure.
FIG. 3 depicts another flowchart illustrating an example method for practicing selected aspects of the present disclosure.
Fig. 4 depicts an example architecture of a computing device.
Detailed Description
Fig. 1 schematically depicts an example environment 100 in which selected aspects of the present disclosure may be implemented, in accordance with various embodiments. Any of the computing devices depicted in fig. 1 or elsewhere in the figures may include logic, such as one or more microprocessors (e.g., a central processing unit or "CPU", a graphics processing unit or "GPU") executing computer-readable instructions stored in memory, or other types of logic, such as an application specific integrated circuit ("ASIC"), a field programmable gate array ("FPGA"), etc. Some of the systems depicted in fig. 1, such as cloud-based automated assistant component 130, may be implemented using one or more server computing devices that form what is sometimes referred to as a "cloud infrastructure," although this is not required.
In some embodiments, the environment 100 may include a client device 110 (or multiple client devices 110) that implements an automated assistant client 120. Client device 110 is illustrated as having one or more microphones 111, one or more speakers 112, one or more cameras and/or other visual components 113, and a display 114 (e.g., a touch-sensitive display). Client device 110 may further include pressure sensors, proximity sensors, accelerometers, magnetometers, and/or other sensors to generate other sensor data in addition to audio data captured by one or more microphones 111. The client device 110 at least selectively executes the automated assistant client 120. The automated assistant client 120 may include an on-device speech capture engine 121, an on-device visual capture engine 122, an on-device hotword detection engine 123, an on-device speech recognizer 124, an on-device Natural Language Understanding (NLU) engine 125, and/or an on-device fulfillment engine 126. The automated assistant client 120 can include additional and/or alternative engines, such as a Voice Activity Detector (VAD) engine, an endpoint detector engine, and/or other engines.
One or more cloud-based automated assistant components 130 can optionally be implemented on one or more computing systems (collectively "cloud" computing systems) that are communicatively coupled to client device 110 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 150. The cloud-based automated assistant component 130 can be implemented, for example, via a cluster of high performance servers. In various embodiments, an instance of the automated assistant client 120 may form a logical instance that appears to the user from the perspective of the automated assistant with which the user may engage in human-machine interactions (e.g., verbal interactions, gesture-based interactions, and/or touch-based interactions) through its interaction with one or more cloud-based automated assistant components 130.
The client device 110 may be used by two or more users and may be, for example: a standalone interactive speaker, a standalone interactive display device (which may also include a speaker and/or camera), a smart appliance device such as a smart television (or a standard television with automatic assistant capabilities equipped with a web dongle), a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), and/or a wearable device of a user that includes a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.
The one or more vision components 113 can take various forms, such as an integral camera, a stereo camera, a lidar component (or other laser-based component), a radar component, and so forth. The one or more vision components 113 may be used, for example, by vision capture engine 122 to capture visual frames (e.g., image frames, laser-based visual frames) of the environment in which client device 110 is deployed. In some implementations, such visual frames can be used to determine whether the user is near the client device 110 and/or whether the user (e.g., the user's face) is a distance away from the client device 110. For example, such a determination can be used to determine whether to activate various on-device machine learning engines depicted in fig. 1, and/or other engines.
The speech capture engine 121 can be configured to capture the user's speech and/or other audio data captured via the microphone 111. Further, client device 110 may include pressure sensors, proximity sensors, accelerometers, magnetometers, and/or other sensors that are used to generate other sensor data in addition to audio data captured by microphone 111. As described herein, the hotword detection engine 123 and/or other engines can utilize such audio data and other sensor data to determine whether to initiate one or more currently dormant automatic assistant functions, refrain from initiating one or more currently dormant automatic assistant functions, and/or shut down one or more currently active automatic assistant functions. The automated assistant functionality can include an on-device speech recognizer 124, an on-device NLU engine 125, an on-device fulfillment engine 126, and additional and/or alternative engines. For example, the on-device speech recognizer 124 can process audio data that captures the spoken utterance with an on-device speech recognition model to generate recognized text that corresponds to the spoken utterance. On-device NLU engine 125 optionally performs on-device natural language understanding on the recognized text using on-device NLU models to generate NLU data. For example, the NLU data can include an intent corresponding to the spoken utterance and, optionally, parameters of the intent (e.g., a bin value). Further, on-device fulfillment engine 126 optionally generates fulfillment data based on the NLU data using an on-device fulfillment model. The fulfillment data can define local and/or remote responses (e.g., answers) to spoken utterances, interactions with locally installed applications based on the spoken utterances, commands transmitted to internet of things (IoT) devices (directly or via a corresponding remote system) based on the spoken utterances, and/or other analytic actions performed based on the spoken utterances. Fulfillment data is then provided for local and/or remote execution/fulfillment of the determined action to resolve the spoken utterance. Execution can include, for example, rendering local and/or remote responses (e.g., visually and/or audibly rendering (optionally with local text-to-speech models)), interacting with locally installed applications, transmitting commands to IoT devices, and/or other actions.
The display 114 can be used to display the recognized text from the on-device speech recognizer 124, and/or one or more results from execution (e.g., search results responsive to a search query). The display 114 can also be one of the user interface output components through which the visual portion of the response is presented from the automated assistant client 120.
In some embodiments, the cloud-based automated assistant component 130 can include a remote ASR engine 131 that performs speech recognition, a remote NLU engine 132 that performs natural language understanding, and/or a remote fulfillment engine 133 that generates fulfillment. A remote execution module can also optionally be included that executes remote execution based on locally or remotely determined fulfillment data. Additional and/or alternative remote engines can be included. In various implementations, at least on-device speech processing, on-device NLUs, on-device fulfillment, and/or on-device execution can be prioritized due to the delay they provide in resolving the spoken utterance and/or network usage reduction (due to not requiring a client-server round trip to resolve the spoken utterance). However, one or more cloud-based automated assistant components 130 can be at least selectively utilized. For example, such components can be used in parallel with on-device components, and the output of such components used in the event of a local component failure. For example, the on-device fulfillment engine 126 can fail in certain circumstances (e.g., due to the relatively limited resources of the client device 110), and the remote fulfillment engine 133 can utilize the more powerful resources of the cloud to generate fulfillment data in such circumstances. The remote fulfillment engine 133 can operate in parallel with the on-device fulfillment engine 126 and utilize its results when on-device fulfillment fails, or can invoke the remote fulfillment engine in response to determining that the on-device fulfillment engine 126 fails.
In various embodiments, the NLU engine (on-device and/or remote) can generate NLU data that includes one or more annotations of the identified text and one or more (e.g., all) terms of the natural language input. In some embodiments, the NLU engine is configured to identify and annotate various types of grammatical information in the natural language input. For example, the NLU engine may include a morphology module that may, for example, separate individual words into morphemes and/or annotate morphemes, for example, with their categories. The NLU engine may also include a portion of a speech tagger configured to annotate terms with their grammar rules. Also, for example, in some embodiments, the NLU engine may additionally and/or alternatively include a dependency parser configured to determine grammatical relationships between terms in the natural language input.
In some embodiments, the NLU engine may additionally and/or alternatively include an entity annotator configured to annotate entity references in one or more segments, e.g., references to people (including, e.g., literary characters, celebrities, public characters, etc.), organizations, locations (real and virtual), and so forth. In some embodiments, the NLU engine may additionally and/or alternatively include a co-fingered parser (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. In some embodiments, one or more components of the NLU engine may rely on annotations from one or more other components of the NLU engine.
The NLU engine may also include an intent matcher configured to determine the intent of a user participating in the interaction with the automated assistant client 120. The intent matcher can use various techniques to determine the intent of the user. In some embodiments, the intent matcher may access one or more local and/or remote data structures that include, for example, a plurality of mappings between grammars and response intents. For example, the grammars included in the mapping can be selected and/or learned over time and may represent common intentions of the users. For example, a grammar "play < artist >" may be mapped to an intent to invoke a response action that causes music of < artist > to be played on the client device 110. Another grammar "[ weather | forecast ] today" may be compared with, for example, "how is the weather today" and "how is the weather today? "match. In addition to or instead of grammars, in some embodiments, the intent matcher can employ one or more trained machine learning models, either alone or in combination with one or more grammars. These trained machine learning models can be trained to identify an intent, for example, by embedding recognized text from a spoken utterance in a reduced-dimensional space, and then determining which other insertions (and thus intents) are closest, for example, using techniques such as euclidean distance, cosine similarity, and so on. As seen in the "play < artist >" example syntax above, some syntaxes have slots (e.g., < artist >) that can be filled with slot values (or "parameters"). The slot value may be determined in various ways. The user will typically actively provide the slot value. For example, for the grammar "give me a pizza of < ingredients >, the user might say the phrase" give me a pizza of sausage ", in which case the slot < ingredients > is filled automatically. Other slot values can be inferred based on, for example, user location, currently presented content, user preferences, and/or other cues.
The fulfillment engine (local and/or remote) can be configured to receive the predicted/estimated intent output by the NLU engine, as well as any associated slot values, and to fulfill (or "resolve") the intent. In various embodiments, fulfillment (or "resolution") of a user's intent may result in various fulfillment information (also referred to as fulfillment data) being generated/obtained, for example, by a fulfillment engine. This can include determining local and/or remote responses (e.g., answers) to the spoken utterance, interactions with locally installed applications based on the spoken utterance, commands transmitted to internet of things (IoT) devices (directly or via a corresponding remote system) based on the spoken utterance, and/or other analytic actions performed based on the spoken utterance. The on-device fulfillment can then initiate local and/or remote execution/enforcement of the determined action for parsing the spoken utterance.
In embodiments, environment 100 may also include user devices 140-1, \8230 \ 8230;, 140-n, which may include user input engine 141 and presentation engine 142 in various embodiments. User devices 140-1, \8230;, 140-n may communicate with client device 110 and/or cloud-based automated assistant component 130 via computer network 150. Each of user devices 140-1, \8230;, 140-n may be a personal device of a user (e.g., a device typically used by a single user, but not often used by multiple users) and may be, for example: a mobile phone computing device, a desktop computing device, a laptop computing device, a tablet computing device, and/or a wearable apparatus that includes a user of a computing device (e.g., a watch of the user having the computing device, glasses of the user having the computing device, a virtual or augmented reality computing device). Additional and/or alternative user devices may be provided.
In some embodiments, the automated assistant client 1200 on the client device 110 may cause the user device 140-1, \8230;, 140-n to present a visual portion of the response from the automated assistant client 120 (e.g., using the presentation engine 142).
FIG. 2 is a flow diagram illustrating an example method 200 for conducting a collaborative search session by an automated assistant in accordance with embodiments disclosed herein. For convenience, the operations of the flow diagram are described with reference to a system that performs operation 5. The system may include various components of various computer systems, such as one or more components of client device 110. Further, while the operations of method 200 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
0 at block 205, the system receives, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device. In an embodiment, at block 205, the automated assistant client 120 executing on the client device 110 may receive a first query in a query session from a first user of the client device 110. In some embodiments, the query session may be a shopping session. For example, the first user may be purchasing 5 tv, and the automated assistant client 120 may receive the query "computer, show me the most popular 65 inch tv" that may be spoken by the first user.
At block 210, the system provides, by the first automatic assistant, a first set of search results for the first query to the first user. In an embodiment, at block 210, the automated assistant client 120 executing on the client device 1100 may provide the first user with the first set of search results for the first query received at block 205. In some embodiments, the automated assistant client 120 may provide the first set of search results by causing the first set of search results to be presented, for example, by the presentation engine 142, on the display 114 of the client device 110 and/or on a user device of the first user (e.g., one of the user devices 140-1, \8230;, 140-n). In some embodiments, the first set of search results comprises a first set of products. In the above example, the first set of search results may be a first set of televisions (e.g., a set of hot 65 inch televisions).
Still referring to block 210, in some embodiments, the first client device (e.g., client device 110) may be an automated assistant device, and providing the first set of search results may include the first automated assistant (e.g., automated assistant client 120) causing the first set of search results to be provided on a display of a mobile device (e.g., one of user devices 140-1, \ 8230; \8230;, 140-n) of the first user.
At block 215, the system determines whether the first query is relevant to the second user of the first client device based on the at least one term in the first query. In an embodiment, at block 215, the automated assistant client 120 executing on the client device 110 may determine whether the first query is relevant to the second user of the client device 110 based on at least one term in the first query received at block 205. In some embodiments, the automated assistant client 120 may determine that the first query is relevant to the second user of the client device 110 based on a score of one of the at least one term in the first query satisfying a threshold. In some embodiments, a score may be assigned to a term and/or the score may be determined based on an expected relevance of the term to other users of the client device. For example, terms related to commonly shared products such as "television", "dishwasher", etc. may be associated with a relatively high score (e.g., a score that meets a threshold), while terms related to less commonly shared products such as "toothbrush", "socks", etc. may be associated with a relatively low score (e.g., a score that does not meet a threshold).
In some embodiments, the account associated with the first user and the account associated with the second user may belong to the same family of accounts. In other embodiments, both the account associated with the first user and the account associated with the second user may be logged on the client device 110.
In some embodiments, the automated assistant client 120 may determine that the first query is relevant to the second user of the first client device based on the second user having previously performed a search similar to the first query, and/or based on the second user having previously performed a search indicating a high likelihood that the second user is interested in the first query. For example, if the second user previously performed multiple searches related to a new technical product, the automated assistant client 120 may infer that the second user is interested in the new technical product and may determine that the first query related to television is related to the second user. In some embodiments, BERT (bi-directional encoder representation from a transformer) or other machine learning based methods may be used to determine query similarity.
Still referring to block 215, in response to the automated assistant client 120 determining that the first query is not relevant to the second user of the first client device, flow proceeds to block 220 and the method ends. On the other hand, in response to the automated assistant client 120 determining that the first query is relevant to the second user of the first client device, flow proceeds to block 225.
Still referring to block 215, in some embodiments, the automated assistant client 120 executing on the client device 110 may determine a predicted level of interest of the second user in the query session. The automated assistant client 120 may further determine whether the first query is relevant to the second user based on a predicted interest level of the second user in the query session satisfying a threshold. In some embodiments, the automated assistant client 120 may determine a predicted level of interest of the second user in the query session based on the query history of the second user.
Still referring to block 215, in some embodiments, the automated assistant client 120 executing on the client device 110 may determine an environmental context. The automated assistant client 120 may further determine whether the first query is relevant to the second user based on the environmental context.
For example, the environmental context at the time the first query is received at block 205 may include one or more signals that may be used to determine whether the first query is relevant to the second user. In the case where the first user provides the first query in a "private" setting, where no other person is detected in the camera frame at the time the first query is received at block 205 and/or during a threshold period of time before and/or after the first query is received, and/or where the microphone does not detect speech of another person, the automated assistant client 120 executing on the client device 110 may determine that the first query is unrelated to the second user based on one or more identified environmental signals. Likewise, where the automated assistant client 120 executing on the client device 110 determines that no other people are at home and the first query is provided by the first user, the automated assistant client 120 executing on the client device 110 may determine that the first query is unrelated to the second user based on one or more identified environmental signals. Further, in the event that the first query is provided by the first user after the user enters the "private" or "traceless" mode (e.g., by saying "let us keep private"), the automated assistant client 120 executing on the client device 110 may determine that the first query is not relevant to the second user.
On the other hand, where there are other users in the room in which the client device 110 is located, and/or when the automated assistant client 120 executing on the client device 110 determines that the first query is received at block 205 and/or during a threshold period of time before and/or after the first query is received, that another user (e.g., another person, such as a second user) is detected in the camera frame as being nearby, and/or that the speech of another person is detected by the microphone, the automated assistant client 120 executing on the client device 110 may determine that the first query is relevant to the second user. Likewise, where other users frequently use the automated assistant client 120 executing on the client device 110 (e.g., at least a threshold proportion of queries are made by others, such as the second user, and/or at least a threshold number of queries are made by others, such as the second user), the automated assistant client 120 executing on the client device 110 may determine that the first query is relevant to the second user.
In particular, when the first query is received at block 205, the automated assistant client 120 may identify one or more environmental signals associated with the environment in which the first user of the client device 110 is located. The one or more environmental signals can include, for example, location information corresponding to a location of the first user when the first query is received at block 205, audio data that captures environmental noise of the environment when the first query is received at block 205, and/or visual data that captures the environment when the first query is received at block 205.
For example, continuing the above example, assume that the first user of client device 110 submits a search query "computer, show me popular 65 inch televisions". Further, assume that when a user submits a search query, one or more environmental signals indicate that a first user is located in a living room at home and another person is present in the living room in close proximity to the first user. In this example, the automated assistant client 120 may determine that the first query is relevant to the second user based on one or more identified environmental signals, such as proximity of the first location to the first user and location information (living room).
Alternatively, assume that when a user submits a search query, one or more environmental signals indicate that the first user is at home in the office and that no other people are present in the office. In this example, the automated assistant client 120 may determine that the first query is not relevant to the second user based on one or more identified environmental signals, such as the absence of other people and location information (offices) proximate to the first user.
Still referring to block 215, in some embodiments, the automated assistant client 120 executing on the client device 110 may determine a frequency of interaction of the second user with the client device 110. The automated assistant client 120 may further determine whether the first query is relevant to the second user based on the frequency of interaction of the second user with the client device 110 satisfying a threshold.
At block 225, the system provides, by the first automated assistant, a selectable option to the first user to allow a second user of the first client device to join the query session. In an embodiment, at block 225, in response to determining at block 215 that the first query is relevant to the second user of the client device 110, the automated assistant client 120 executing on the client device 110 may provide a selectable option to the first user to allow the second user of the client device 110 to join the query session. In some implementations, the automated assistant client 120 can provide the selectable options, for example, by visually presenting the selectable options on a user interface of the client device 110 (e.g., "do you want to allow the user 2 to participate in a search.
At block 230, the system determines whether an indication has been received to accept the selectable option to allow a second user of the first client device to join the query session. In some embodiments, the first user may provide user input (e.g., a tap or click) via a user interface of the client device 110 that is an indication of acceptance or rejection of the selectable option to allow a second user of the first client device to join the query session. Alternatively, the first user may provide a spoken response (e.g., "yes" or "no") that is an indication of acceptance or rejection of the selectable option to allow the second user of the first client device to join the query session.
Still referring to block 230, in an embodiment, in response to the automated assistant client 120 determining that the indication provided at block 225 to accept the selectable option to allow the second user of the first client device to join the query session has not been received (e.g., a rejection of the selectable option is received), flow proceeds to block 220 and the method ends. In another aspect, in response to the automated assistant client 120 determining that an indication has been received that accepts the selectable option provided at block 225 to allow the second user of the first client device to join the query session, flow proceeds to block 235.
At block 235, in response to determining that the first query is relevant to the second user, the system provides, by the first automatic assistant, a selectable option to join the query session to the second user of the first client device. In an embodiment, at block 235, in response to determining at block 215 that the first query is relevant to the second user of the client device 110, the automated assistant client 120 executing on the client device 110 may provide the second user of the first client device 110 with a selectable option to join the query session. In some embodiments, the providing of the selectable option to join the query session by the automated assistant client 120 may be further responsive to receiving an indication from the first user at block 230 to accept the selectable option to allow the second user of the first client device 110 to join the query session. In some implementations, the automated assistant client 120 can provide the selectable options, for example, by visually presenting the selectable options (e.g., "user 2, do you want to participate in a search") on a user interface of the client device 110, and/or by audibly presenting the selectable options on the client device 110.
At block 240, the system determines whether an indication has been received from the second user to accept the selectable option to join the query session. In some embodiments, the second user may provide user input (e.g., a tap or click) via a user interface of the client device 110 that is an indication of a selectable option to accept or decline to join the query session. Alternatively, the second user may provide a spoken response (e.g., "yes" or "no") that is an indication of an option selectable to accept or decline to join the query session.
Still referring to block 240, in an embodiment, in response to the automated assistant client 120 determining that the indication of the selectable option to accept the join query session provided at block 235 has not been received (e.g., a rejection of the selectable option is received), the flow proceeds to block 220 and the method ends. In another aspect, in response to the automated assistant client 120 determining that an indication of the selectable option provided at block 235 to accept the join query session has been received, flow proceeds to block 245.
At block 245, in response to receiving an indication from the second user to accept the selectable option to join the query session, the system adds the second user to the query session. In an embodiment, at block 245, in response to receiving an indication from the second user at block 240 to accept the selectable option to join the query session, the automated assistant client 120 executing on the client device 110 may add the second user to the query session.
Still referring to block 245, in some embodiments, a member of the query session (e.g., the second user) may be able to send messages or comments (e.g., products or other search results about interests, feedback, desired options/constraints, etc.) to other members of the query session (e.g., the first user) via the automated assistant client 120 executing on the client device or via an automated assistant client executing on one of the user devices 140-1, \8230;, 140-n (e.g., the mobile device of the user), or to tag search results of other members of the query session. In some embodiments, members of the query session may be able to add/remove products in the shared shopping cart. In some embodiments, the automated assistant client 120 may obtain approval from one member (e.g., a designated administrator), multiple members (e.g., two parents), or all members of the query session before allowing addition/removal of products in the shopping cart. In other embodiments, a member may be able to add/remove products in a shopping cart without obtaining approval from other members of the query session.
At block 250, the system receives additional input from the second user to refine the first query. In an embodiment, at block 250, the automated assistant client 120 executing on the client device 110 may receive additional input from the second user to refine the first query received at block 205. Further input may be provided by the second user via a user interface of the client device 110, e.g. via a touch screen. For example, the second user may tap the touch screen to select a filter for the first set of search results (e.g., "price less than $ 1000", "no 0 red", "HDMI 2.1 only", etc.), apply, delete, or modify the filter based on the user input. Alternatively, the further input may be spoken by the second user. For example, the second user may say "i want the 4K model". In other embodiments, the received additional input may be a ranking of the first search result by the second user.
5 still referring to block 250, in some embodiments, additional input provided by the second user may be displayed at a user interface of client device 110 or user device 140-1, \8230;, a,
140-n. For example, additional inputs (e.g., preferences) of the second user may be displayed in a visually separate manner, such as by separate columns or other visual affordances on the display 114 of the client device 110. For example, each user may be represented by a circle on the user interface on display 0 114, and tapping one of the circles may display additional input (e.g., preferences) of the selected user. For example, tapping a circle representing the second user on the user interface may result in an additional input of "I want 4K model" by the second user "
Is displayed on the user interface and annotated with the user (e.g., the second user) of the added additional input.
Still referring to block 250, in some implementations, the second client device receives additional input. The second client device may be another client device 110 or may be one of user devices 140-1, \8230;, 140-n. In particular, in some embodiments, additional input may be provided by a second user via a user interface of another client device 110 or one of user devices 140-1, \8230;, 140-n, which may in turn provide the additional input to automated assistant client 120 on client device 110.
At block 255, the system generates a set of modified search results based on additional input received from the second user. In an embodiment, at block 255, the automated assistant client 120 executing on the client device 110 may generate a set of modified search results based on the additional input received from the second user at block 250.
Still referring to block 255, in some embodiments, generating the set of modified search results includes the system determining, by the first automatic assistant, a second query based on the first query and the additional input; and generating a modified set of search results based on the second set of search results for the second query. In particular, the automated assistant client 120 executing on the client device 110 may determine a second query based on the first query received at block 205 and the additional input received at block 250, and generate a set of modified search results based on a second set of search results for the second query. For example, in the case of the first query "computer, show me hot-marketed 65 inch tv" and the additional input "i want 4K model", the automated assistant client 120 may determine the second query "hot-marketed 65 inch 4K tv".
Still referring to block 255, in other embodiments, generating the modified set of search results includes the system filtering the first set of search results based on additional input. In particular, the automated assistant client 120 executing on the client device 110 may filter the first set of search results from block 210 based on the additional input received at block 250. For example, where the first query "computer, show me a hot-sold 65 inch television" and the additional input "i want a 4K model," the automated assistant client 120 may determine that "4K" is the attribute filtered through, and the automated assistant client 120 may filter the first set of search results from block 210 to identify and display a subset of the first set of search results that are associated with the "4K" attribute, and hide other search results in subsets of the first set of search results that are not associated with the "4K" attribute.
Still referring to block 255, in other embodiments, generating the modified set of search results includes the system reordering the first set of search results based on the ranking provided at block 250, and optionally hiding or removing products not included in the ranking. In some embodiments, when the ranking provided at block 250 includes similar or near-duplicate items, the user may vote for the item they consider most appropriate given the option. In some embodiments, the ranking of the displayed advertisements may be determined based on the rankings provided at block 250.
Still referring to block 255, in other embodiments, instead of generating an improved set of search results (e.g., by incorporating additional input received from the second user into the search session), additional input (e.g., preferences) of the second user may be displayed in a visually separate manner, for example, by separate columns or other visual affordances on the display 114 of the client device 110. For example, each user may be represented by a circle on the user interface on display 114, and tapping one of the circles may display the preferences of the selected user. In some embodiments, the preferences of one or more other users may be temporarily enabled/disabled, or completely deleted, for example, by providing the other users in the session with the option of an explanatory message (e.g., "do i really want to buy red tv, can.
Still referring to block 255, in some embodiments, the system automatically determines, by the first automatic assistant, the filter terms based on the inferred preferences of the second user. The system can further generate a set of modified search results based on the filter terms. In some embodiments, the inferred preferences may be based on past searches of the second user.
At block 260, the system provides, by the first automatic assistant, the set of modified search results to the first user and the second user. In an embodiment, at block 260, the automated assistant client 120 executing on the client device 110 may provide the first user and the second user with the set of modified search results generated at block 255. In some embodiments, the automated assistant client 120 may provide the modified set of search results by causing the modified set of search results to be presented, for example, by the presentation engine 142 on the display 114 of the client device 110 and/or on a user device of a first user (e.g., one of the user devices 140-1, \8230;, 140-n), and/or on a user device of a second user (e.g., one of the user devices 140-1, \8230;, 140-n), for example, by the presentation engine 142. In some embodiments, the modified set of search results includes a modified set of products. In the above example, the set of modified search results may be a set of modified televisions (e.g., a set of hot-marketable 65 inch 4K televisions).
Still referring to block 260, in some embodiments, providing the modified set of search results may include the first automatic assistant (e.g., automatic assistant client 120) causing the modified set of search results to be provided on a display of a mobile phone of the first user (e.g., one of user devices 140-1, \8230;, 140-n) and a display of a mobile device of the second user (e.g., another of user devices 140-1, \8230;, 140-n).
In some embodiments, the operations of some or all of blocks 215-260 may optionally be repeated with respect to another user (e.g., a third user). Additionally, during subsequent iterations, additional input received at block 250 may be used to further refine the first query and may be received from the first user, the second user, and/or additional users.
FIG. 3 is a flow diagram illustrating an example method 300 for conducting a collaborative search session by an automated assistant in accordance with embodiments disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, such as one or more components of client device 110. Further, while the operations of method 300 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 310, the system receives, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device. In an embodiment, at block 310, the automated assistant client 120 executing on the client device 110 may receive a first query in a query session from a first user of the client device 110. In some embodiments, the query session may be a shopping session. For example, the first user may be purchasing a television, and the automated assistant client 120 may receive the query "computer, show me the most popular 65 inch television" that the first user may speak.
At block 320, the system provides, by the first automated assistant, the first set of search results for the first query to the first user. In an embodiment, at block 320, the automated assistant client 120 executing on the client device 110 may provide the first user with the first set of search results for the first query received at block 310. In some embodiments, the automated assistant client 120 may provide the first set of search results by causing the first set of search results to be presented, for example, by the presentation engine 142, on the display 114 of the client device 110 and/or on a user device of the first user (e.g., one of the user devices 140-1, \8230;, 140-n). In some embodiments, the first set of search results comprises a first set of products. In the above example, the first set of search results may be a first set of televisions (e.g., a first set of hot 65 inch televisions).
Still referring to block 320, in some embodiments, the first client device (e.g., client device 110) may be an automated assistant device, and providing the first set of search results may include the first automated assistant (e.g., automated assistant client 120) causing the first set of search results to be provided on a display of a mobile device (e.g., one of user devices 140-1, \ 8230; \8230;, 140-n) of the first user.
At block 330, the system receives, by the first automatic assistant, additional input from the second user of the first client device to refine the first query. In an embodiment, at block 330, the automated assistant client 120 executing on the client device 110 may receive additional input from the second user to refine the first query received at block 310. Further input may be provided by the second user via a user interface of the client device 110, e.g. via a touch screen. For example, the second user may tap the touch screen to select a filter for the first set of search results (e.g., "price less than $ 1000", "no red", "HDMI 2.1 only", etc.). Alternatively, the further input may be spoken by the second user. For example, the second user may say "i want the 4K model".
At block 340, in response to receiving additional input from the second user to refine the first query, the system adds the second user to the query session. In an embodiment, at block 340, the automated assistant client 120 executing on the client device 110 may add the second user to the query session in response to receiving additional input from the second user to refine the first query at block 330.
Still referring to block 340, in some embodiments, a member of the query session (e.g., the second user) may be able to send messages or comments (e.g., products or other search results regarding interests, feedback, desired options/constraints, etc.) to other members of the query session (e.g., the first user) via the automated assistant client 120 executing on the client device or via an automated assistant client executing on one of the user devices 140-1, \8230;, 140-n (e.g., the user's mobile device), or to tag search results of other members of the query session. In some embodiments, members of the query session may be able to add/remove products in the shared shopping cart. In some embodiments, the automated assistant client 120 may obtain approval from one member (e.g., a designated administrator), multiple members (e.g., two parents), or all members of the query session before allowing addition/removal of products from the shopping cart. In other embodiments, a member may be able to add/remove products from a shopping cart without obtaining approval from other members of the query session.
Still referring to block 340, in some embodiments, the system (e.g., the automated assistant client 120 of the client device 110) may determine the identity of the second user, for example, based on the second user's voice detected by the first client device's microphone (e.g., the one or more microphones 111 of the client device 110), based on the second user's face detected by the first client device's camera (e.g., the one or more cameras and/or other visual components 113 of the client device 110), and/or using other wireless signals, such as bluetooth signals, that may be used to detect the presence of the second user's smart watch. The system (e.g., the automated assistant client 120 of the client device 110) may identify the mobile device of the second user based on the identity of the second user. In some embodiments, adding the second user to the query session in response to receiving the additional input may include adding a mobile phone of the second user to the query session. Additionally, in some embodiments, providing the modified set of search results to the second user may include the first automatic assistant (e.g., automatic assistant client 120 of client device 110) causing the modified set of search results to be provided on a display of the second user's mobile phone (e.g., one of user devices 140-1, \8230;, 140-n) based on adding the second user's mobile phone to the query session.
Still referring to block 340, in some implementations, the system (e.g., automated assistant client 120 of client device 110) may determine the identity of the second user, for example, based on the second user's voice detected by a microphone of the first client device (e.g., one or more microphones 111 of client device 110), or based on the second user's face detected by a camera of the first client device (e.g., one or more cameras and/or other visual components 113 of client device 110). The system (e.g., the automated assistant client 120 of the client device 110) may identify a user account of the second user based on the identity of the second user. In some embodiments, adding the second user to the query session in response to receiving the additional input may include adding a user account of the second user to the query session. Additionally, in some embodiments, providing the modified set of search results to the second user may be based on subsequently detecting the second user based on the second user's voice or based on the second user's face (e.g., by the automated assistant client 120 of the client device 110).
At block 350, the system generates a set of modified search results based on additional input received from the second user. In an embodiment, at block 350, the automated assistant client 120 executing on the client device 110 may generate a set of modified search results based on the additional input received from the second user at block 340.
Still referring to block 350, in some embodiments, generating the modified search results includes the system determining, by the first automated assistant, a second query based on the first query and the additional input; and generating a set of modified search results based on the second set of search results for the second query. In particular, the automated assistant client 120 executing on the client device 110 may determine a second query based on the first query received at block 310 and the additional input received at block 330, and generate a set of modified search results based on the second set of search results for the second query. For example, in the case of the first query "computer, show me hot-marketed 65 inch tv" and the additional input "i want 4K model", the automated assistant client 120 may determine the second query "hot-marketed 65 inch 4K tv".
Still referring to block 350, in other embodiments, generating the modified set of search results includes the system filtering the first set of search results based on additional input. In particular, the automated assistant client 120 executing on the client device 110 may filter the first set of search results from block 320 based on the additional input received at block 330. For example, where the first query "computer, display me a hot-sold 65 inch television" and the additional input "i want a 4K model," the automated assistant client 120 may determine that "4K" is the attribute filtered through, and the automated assistant client 120 may filter the first set of search results from block 320 to identify a subset of the first set of search results associated with the "4K" attribute.
Still referring to block 350, in some embodiments, the system automatically determines, by the first automated assistant, the filter terms based on preferences of the second user inferred from one or more searches previously performed by the user. The system can further generate a set of modified search results based on the filter terms.
At block 360, the system provides, by the first automated assistant, the set of modified search results to the first user and the second user. In an embodiment, the modified set of search results may be provided to the second user based on adding the second user to the query session at block 340. In an embodiment, at block 360, the automated assistant client 120 executing on the client device 110 may provide the first user and the second user with the set of modified search results generated at block 350. In some embodiments, the automated assistant client 120 may provide the modified set of search results by causing the modified set of search results to be presented, for example, by the presentation engine 142 on the display 114 of the client device 110 and/or on a user device of a first user (e.g., one of the user devices 140-1, \8230;, 140-n), and/or on a user device of a second user (e.g., one of the user devices 140-1, \8230;, 140-n), for example, by the presentation engine 142. In some embodiments, the modified set of search results includes a modified set of products. In the above example, the set of modified search results may be a set of modified televisions (e.g., a set of hot-marketable 65 inch 4K televisions).
Still referring to block 360, in some embodiments, providing the modified set of search results may include the first automatic assistant (e.g., automatic assistant client 120) causing the modified set of search results to be provided on a display of a mobile phone of the first user (e.g., one of user devices 140-1, \8230;, 140-n) and a display of a mobile device of the second user (e.g., another of user devices 140-1, \8230;, 140-n).
Still referring to block 360, in some embodiments, providing the modified set of search results may include the automated assistant client 120 causing the modified set of search results to be provided on the display 114 of the client device 110 in response to determining that the first user and/or the second user are proximate to (e.g., in the same room as) the client device 110. For example, the automated assistant client 120 may determine that the first user and/or the second user is proximate to the client device 110 based on detecting the first user's and/or the second user's voice by one or more microphones 111 of the client device 110, or based on detecting the first user's and/or the second user's face by one or more cameras and/or other visual components 113 of the client device 110.
Still referring to block 360, in some embodiments, the automated assistant client 120 may refrain from providing the set of modified search results in response to determining (e.g., using one or more microphones 111 and/or one or more cameras and/or other visual components 113 of the client device 110) that a person other than the first user and/or the second user is proximate to the client device 110, e.g., to refrain from destroying surprise elements regarding the gift.
In some embodiments, the operations of some or all of blocks 330 through 360 may optionally be repeated with respect to another user (e.g., a third user). Additionally, during subsequent iterations, additional input received at block 330 may be used to further refine the first query, and may be received from the first user, the second user, and/or additional users.
FIG. 4 is a block diagram of an example computing device 410 that may optionally be used to perform one or more aspects of the techniques described herein. In some embodiments, one or more of the client device, cloud-based automated assistant component, and/or other components may comprise one or more components of the example computing device 410.
The user interface input device 422 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 410 or onto a communication network.
User interface output device 420 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for producing a visible image. The display subsystem may also provide a non-visual display, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 410 to a user or to another machine or computing device.
These software modules are typically executed by the processor 414 alone or in combination with other processors. A memory subsystem 425 included in the storage subsystem 424 may include a number of memories, including a main Random Access Memory (RAM) 430 for storing instructions and data during program execution and a Read Only Memory (ROM) 432 to store fixed instructions. File storage subsystem 426 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 426 in storage subsystem 424, or in other machines accessible to processor 414.
While several embodiments have been described and illustrated herein, various other ways and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be used, and each of these variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend on the particular application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
1. A method implemented by one or more processors, the method comprising:
receiving, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device;
providing, by the first automated assistant to the first user, a first set of search results for the first query;
determining that the first query is relevant to a second user of the first client device based on at least one term in the first query;
providing, by the first automatic assistant, a selectable option to join the query session to the second user of the first client device in response to determining that the first query is relevant to the second user;
in response to receiving an indication from the second user to accept the selectable option to join the query session, adding the second user to the query session;
receiving additional input from the second user to refine the first query;
generating a set of modified search results based on the additional input received from the second user; and
providing, by the first automatic assistant, the set of modified search results to the first user and the second user.
2. The method of claim 1, wherein:
the query session is a shopping session;
the first set of search results comprises a first set of products; and
the modified set of search results includes a modified set of products.
3. The method of claim 1, wherein:
the first client device is an automated assistant device;
providing the first set of search results comprises the first automatic assistant causing the first set of search results to be provided on a display of a mobile device of the first user; and
providing the set of modified search results comprises the first automated assistant causing the set of modified search results to be provided on a display of the first user's mobile device and on a display of the second user's mobile apparatus.
4. The method of claim 1, further comprising providing, by the first automated assistant, a selectable option to the first user to allow the second user of the first client device to join the query session, and
wherein providing the selectable option to join the query session to the second user is further in response to receiving an indication from the first user of: accepting the selectable option to allow the second user of the first client device to join the query session.
5. The method of claim 1, further comprising determining a predicted level of interest of the second user in the query session,
wherein determining that the first query is relevant to the second user is further based on the predicted level of interest of the second user in the query session satisfying a threshold.
6. The method of claim 5, wherein determining the predicted level of interest of the second user in the query session is based on a query history of the second user.
7. The method of claim 1, further comprising determining an environmental context,
wherein determining that the first query is relevant to the second user is further based on the environmental context.
8. The method of claim 1, further comprising determining a frequency with which the second user interacts with the first client device,
wherein determining that the first query is relevant to the second user is further based on the frequency with which the second user interacts with the first client device satisfying a threshold.
9. The method of claim 1, wherein determining that the first query is relevant to the second user of the first client device is based on a score of one of the at least one term in the first query satisfying a threshold.
10. The method of claim 1, further comprising automatically determining, by the first automatic assistant, filter terms based on inferred preferences of the second user,
wherein generating the set of modified search results is further based on the filter term.
11. The method of claim 1, wherein the additional input is received by a second client device.
12. The method of claim 1, wherein generating the set of modified search results comprises:
determining, by the first automated assistant, a second query based on the first query and the additional input; and
generating the set of modified search results based on a second set of search results for the second query.
13. The method of claim 1, wherein generating the modified set of search results comprises filtering the first set of search results based on the additional input.
14. A computer program product comprising one or more computer-readable storage media, the computer program product having program instructions stored collectively on the one or more computer-readable storage media, the program instructions executable to:
receiving, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device;
providing, by the first automated assistant to the first user, a first set of search results for the first query;
receiving, by the first automated assistant, additional input from a second user of the first client device to refine the first query;
in response to receiving the additional input from the second user to refine the first query, adding the second user to the query session;
generating a set of modified search results based on the further input received from the second user; and
providing, by the first automatic assistant to the second user and the second user, the set of modified search results, wherein the set of modified search results is provided to the second user based on adding the second user to the query session.
15. The computer program product of claim 14, wherein:
the query session is a shopping session;
the first set of search results comprises a first set of products; and
the modified set of search results includes a modified set of products.
16. The computer program product of claim 14, wherein:
the first client device is an automated assistant device;
providing the first set of search results comprises the first automatic assistant causing the first set of search results to be provided on a display of a mobile device of the first user; and
providing the set of modified search results comprises the first automatic assistant causing the set of modified search results to be provided on the display of the mobile device of the first user and a display of a mobile device of the second user.
17. The computer program product of claim 14, wherein the program instructions are further executable to automatically determine, by the first automatic assistant, a filter term based on inferred preferences of the second user,
wherein generating the set of modified search results is further based on the filter term.
18. The computer program product of claim 14, wherein the program instructions are further executable to:
determining an identity of the second user based on the second user's voice detected by a microphone of the first client device or based on the second user's face detected by a camera of the first client device; and
identifying a mobile device of the second user based on the identity of the second user,
wherein:
adding the second user to the query session in response to receiving the additional input comprises adding the mobile device of the second user to the query session; and
providing the modified set of search results to the second user comprises the first automatic assistant causing the modified set of search results to be provided on a display of the mobile device of the second user based on adding the mobile device of the second user to the query session.
19. The computer program product of claim 14, wherein the program instructions are further executable to:
determining an identity of the second user based on the second user's voice detected by a microphone of the first client device or based on the second user's face detected by a camera of the first client device; and
identifying a user account of the second user based on the identity of the second user,
wherein:
adding the second user to the query session in response to receiving the additional input comprises adding the user account of the second user to the query session; and
providing the modified set of search results to the second user is based on subsequently detecting the second user based on the voice of the second user or based on the face of the second user.
20. A system, comprising:
a processor, a computer-readable memory, one or more computer-readable storage media, and program instructions collectively stored on the one or more computer-readable storage media, the program instructions executable to:
receiving, by a first automated assistant executing on a first client device, a first query in a query session from a first user of the first client device;
providing, by the first automated assistant to the first user, a first set of search results for the first query;
determining that the first query is relevant to a second user of the first client device based on at least one term in the first query;
responsive to determining that the first query is relevant to the second user, providing, by the first automated assistant, a selectable option to join the query session to the second user of the first client device;
in response to receiving an indication from the second user to accept the selectable option to join the query session, adding the second user to the query session;
receiving additional input from the second user to refine the first query;
generating a set of modified search results based on the further input received from the second user; and
providing, by the first automatic assistant, the set of modified search results to the first user and the second user.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/554,608 | 2021-12-17 | ||
US17/554,608 US11914660B2 (en) | 2021-12-17 | 2021-12-17 | Collaborative search sessions through an automated assistant |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115858601A true CN115858601A (en) | 2023-03-28 |
Family
ID=85673357
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211617950.XA Pending CN115858601A (en) | 2021-12-17 | 2022-12-15 | Conducting collaborative search sessions through automated assistant |
Country Status (3)
Country | Link |
---|---|
US (1) | US11914660B2 (en) |
CN (1) | CN115858601A (en) |
AU (1) | AU2022268339B2 (en) |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8554767B2 (en) * | 2008-12-23 | 2013-10-08 | Samsung Electronics Co., Ltd | Context-based interests in computing environments and systems |
US8849791B1 (en) * | 2011-06-29 | 2014-09-30 | Amazon Technologies, Inc. | Assisted shopping |
US8886630B2 (en) * | 2011-12-29 | 2014-11-11 | Mcafee, Inc. | Collaborative searching |
US20140280294A1 (en) * | 2013-03-13 | 2014-09-18 | Google, Inc. | Connecting users in search services based on received queries |
US10530733B2 (en) * | 2015-11-10 | 2020-01-07 | Hipmunk, Inc. | Inferring preferences from message metadata and conversations |
US11436417B2 (en) * | 2017-05-15 | 2022-09-06 | Google Llc | Providing access to user-controlled resources by automated assistants |
US10650054B2 (en) | 2018-04-24 | 2020-05-12 | Rovi Guides, Inc. | Systems and methods for updating search results based on a conversation |
US11068554B2 (en) * | 2019-04-19 | 2021-07-20 | Microsoft Technology Licensing, Llc | Unsupervised entity and intent identification for improved search query relevance |
US11212035B2 (en) * | 2019-06-14 | 2021-12-28 | Arris Enterprises Llc | Systems and methods for assessing Wi-Fi coverage for client devices in a multi-access point environment |
-
2021
- 2021-12-17 US US17/554,608 patent/US11914660B2/en active Active
-
2022
- 2022-11-09 AU AU2022268339A patent/AU2022268339B2/en active Active
- 2022-12-15 CN CN202211617950.XA patent/CN115858601A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11914660B2 (en) | 2024-02-27 |
AU2022268339A1 (en) | 2023-07-06 |
US20230195815A1 (en) | 2023-06-22 |
AU2022268339B2 (en) | 2024-03-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929072B2 (en) | Using textual input and user state information to generate reply content to present in response to the textual input | |
US11735182B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
JP6437669B2 (en) | Providing suggested voice-based action queries | |
US11347801B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
US11200893B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
CN111033492A (en) | Providing command bundle suggestions to automated assistants | |
CN112292724A (en) | Dynamic and/or context-specific hotwords for invoking automated assistants | |
CN115004190A (en) | Analyzing graphical user interfaces to facilitate automated interactions | |
US20200103978A1 (en) | Selective detection of visual cues for automated assistants | |
WO2022245395A1 (en) | Voice commands for an automated assistant utilized in smart dictation | |
CN115699166A (en) | Detecting approximate matches of hotwords or phrases | |
CN115668361A (en) | Detecting and handling failures in automated voice assistance | |
AU2022268339B2 (en) | Collaborative search sessions through an automated assistant | |
CN115605871A (en) | Recommending actions based on an entity or entity type | |
US20220215179A1 (en) | Rendering content using a content agent and/or stored content parameter(s) | |
US20230343336A1 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
US20240031339A1 (en) | Method(s) and system(s) for utilizing an independent server to facilitate secure exchange of data | |
US20230215422A1 (en) | Multimodal intent understanding for automated assistant | |
WO2024019767A1 (en) | Method(s) and system(s) for utilizing an independent server to facilitate secure exchange of data | |
EP4330850A1 (en) | System(s) and method(s) to enable modification of an automatically arranged transcription in smart dictation | |
CN115735201A (en) | Method and system for presenting privacy-friendly query activity based on ambient signals | |
CN112236739A (en) | Adaptive automated assistant based on detected mouth movement and/or gaze |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |