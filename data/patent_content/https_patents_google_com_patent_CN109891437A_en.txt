CN109891437A - Use Processing with Neural Network text sequence - Google Patents
Use Processing with Neural Network text sequence Download PDFInfo
- Publication number
- CN109891437A CN109891437A CN201780067511.7A CN201780067511A CN109891437A CN 109891437 A CN109891437 A CN 109891437A CN 201780067511 A CN201780067511 A CN 201780067511A CN 109891437 A CN109891437 A CN 109891437A
- Authority
- CN
- China
- Prior art keywords
- neural network
- label
- output
- output position
- sequence
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/47—Machine-assisted translation, e.g. using translation memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
Abstract
It is a kind of for training the method implemented by computer of neural network, which is configurable to generate the score distribution that the collection of multiple output positions closes.The neural network is configured as processing network inputs, to generate the corresponding scores distribution of each output position in multiple output positions, the score distribution of each output position in multiple output position includes the corresponding scores of each label in scheduled tag set, which includes the n-gram of multiple and different sizes.Exemplary method described herein provides trained neural network, the result that the trained neural network generates has higher accuracy compared with prior art, such as translate compared with prior art more acurrate, or speech recognition is more acurrate compared with prior art.
Description
Background technique
This specification is related to the neural network that training generates output sequence.
Machine learning model receives input, and output (for example, output of prediction) is generated based on received input.One
A little machine learning models are parameter models, and the value of the parameter based on received input and model generates output.
Neural network is that the machine learning model of the output of received input is predicted using one or more layers non-linear unit.
Some neural networks further include one or more hidden layers other than output layer.The output of each hidden layer is used as in network
Next layer (that is, next hidden layer or output layer) input.Each layer of network is according to the current value of the set of relevant parameter
It generates and exports from received input.
Summary of the invention
Present specification describes be embodied as computer program on one or more computers of one or more positions
System how to train neural network, the neural network receive network inputs simultaneously handle the network inputs to generate multiple output bits
Corresponding scores distribution on the tag set for each output position set, the set of the label include the n of multiple and different sizes
Metagrammar.Exemplary method described herein provides trained neural network, and the result generated is compared with prior art
It is more more acurrate than the prior art with higher accuracy, such as translation or speech recognition more more acurrate than the prior art.
On the whole, a novel aspects can be embodied in the method for training neural network, wherein the nerve net
Network is configurable to generate the score distribution that the collection of multiple output positions closes, this method comprises: obtaining for training neural network
Training data, wherein the neural network be configured as receive network inputs, and according to the multiple parameters of neural network handle net
Network input is distributed with the corresponding scores for generating each output position in multiple output positions, wherein each output position is corresponding
Score distribution includes the corresponding scores of each label in scheduled tag set, wherein scheduled tag set include it is multiple not
With the n-gram of size, wherein for each output position, the corresponding scores of each label in the score distribution of output position
Indicate a possibility that label is the label at output position in the output sequence of network inputs, and wherein training data packet
Multiple training inputs are included, and each training is inputted, respective objects output sequence includes one or more words；For every
A training input: according to the current value of the parameter of neural network, using Processing with Neural Network training input to generate multiple outputs
The corresponding scores of each output position in position are distributed；From multiple possible effective points of the target output sequence of training input
Target sequence is decomposed in effective decomposition of sampled targets output sequence in solution, the wherein possible effective decomposition of each of target sequence
For the different flags sequence in scheduled tag set；And the current value of the parameter of neural network is adjusted, it has been sampled with increasing
A possibility that label in effectively decomposing is the label in output sequence at corresponding output position.
In some embodiments, it has sampled and has effectively decomposed the n-gram including multiple and different sizes.
In some embodiments, the current value of the parameter of neural network is adjusted to increase and sample the mark in effectively decomposing
A possibility that note is the label in output sequence at corresponding output position includes: the iteration for executing neural network training process,
With increase with it is in the score distribution for having sampled the corresponding output position in the position of label in effectively decomposing, sampled it is effective
The logarithm of the product of the corresponding scores of each label in decomposition.
In some embodiments, effective decomposition of sampled targets output sequence includes, in multiple output positions
Each output position and since initial position successively: it is random from the significant notation in scheduled tag set with probability ε
Significant notation is sampled, wherein the significant notation of output position is the label in scheduled tag set, comes from scheduled mark
Label in note set by be output position target output sequence effective addition for effectively decomposing of current portions；And according to
The score of significant notation in the score distribution of the output position of training input, is sampled effectively with 1-ε of probability from significant notation
Label.
In some embodiments, this method further includes, for each output position in multiple output positions and from
Initial position starts successively: the significant notation of sampling of output position being fed as input to neural network, for generating
The score distribution of next output position of multiple output positions.
In some embodiments, one or more n-grams in scheduled tag set are for scheduled label
The prefix of other n-grams of one or more of set.
In some embodiments, the n-gram in scheduled tag set includes character and fragments of words.
In some embodiments, the n-gram in scheduled tag set further includes word.
In some embodiments, neural network is speech recognition neural network, and network inputs indicate language
Audio data or audio frequency characteristics.
In some embodiments, neural network is neural machine translation neural network, and network inputs are to indicate source
The input marking sequence of the word sequence of language, and wherein the n-gram in scheduled tag set is target natural language
N-gram.
In some embodiments, neural network is that image subtitle generates neural network, and network inputs are images.
In some embodiments, neural network is autocoder neural network, and network inputs are word sequences.
Above-mentioned aspect can by it is any it is convenient in the form of implement.For example, various aspects and embodiment can be by meters appropriate
Calculation machine program is implemented, which can carry on mounting medium appropriate, which can be tangible load
Body medium (such as disk) or invisible mounting medium (such as signal of communication).Various aspects also can be used suitable device and come in fact
It applies, which can take the form of the programmable calculator of operation computer program.
The specific embodiment of theme described in this specification can be implemented, to realize one or more in following advantages
It is a.System described in this specification can train the neural network for generating output sequence, so that neural network is being related to from net
Network input accurately generates the result realized in the various tasks of output sequence and be better than the prior art.Specifically, it is different from wherein false
If the conventional method that fixed and scheduled output sequence decomposes, once it is trained using described technology, neural network
It is exported with regard to generating, which defines the decomposition of the output sequence in different type n-gram in a manner of based on network inputs.
For example, the system can train neural machine translation neural network to generate translation more more accurate than the prior art, training voice
Identifying system come generate it is more more accurate than the prior art transcription, etc..
In addition, system described in this specification is computationally efficient compared with the conventional model of such as character model
, in the conventional model, output sequence is broken down into single character, this leads to long decoder length and calculates upper valuableness
Infer.Therefore, because sequence is resolved into longer n-gram in due course by system, so described in this specification
System can quickly generate output sequence than the conventional model of such as character model, and have less computing resource, together
When still generate high quality output.
The one or more embodiments of the detail of this specification theme illustrate in the accompanying drawings and the description below.From specification,
In drawings and claims, other features, aspects and advantages of this theme be will become obvious.
Detailed description of the invention
Fig. 1 shows the example machine study system for being configurable to generate the score distribution that the collection of multiple output positions closes
System.
Fig. 2 is the instantiation procedure for training machine learning system.
Fig. 3 is the instantiation procedure sampled for the significant notation (token) to output position.
Similar Ref. No. and mark indicate similar element in various attached drawings.
Specific embodiment
Present specification describes be embodied as computer program on one or more computers of one or more positions
System (for example, machine learning system) how to train neural network to receive network inputs and to handle the network inputs to generate
The corresponding scores of each output position in multiple output positions are distributed.
The corresponding scores distribution of each output position includes the corresponding scores of each label in the set of label, the label
Set include natural language multiple and different sizes n-gram (n-grams).To calibration in the score distribution of output position
The given label of the fraction representation of note is the label at the output position in natural language target sequence corresponding with network inputs
Probability.
One is trained, and neural network is used for standard decoding process to generate nature corresponding with network inputs
Language target sequence.
Fig. 1 shows example machine learning system 100.Machine learning system 100 is the one of one or more positions
The example of the system of computer program is embodied as on a or multiple computers, wherein implementing systems described below, component and skill
Art.
Machine learning system 100 includes neural network 1 04, which is configured as receiving network inputs 102,
Network inputs 102 are handled according to the value of the set of neural network model parameter 110, and are generated every in multiple output positions
The corresponding scores distribution 112 of the set of the label of a output position.One is trained, and neural network 1 04 may be used in standard solution
Network inputs 102 are mapped to corresponding natural language target sequence 116 by code process, that is, the sequence in target natural language.
Each label of the score distribution of given output position into the set of multiple labels distributes corresponding score, wherein
The fraction representation of the given label given label should be in natural language target sequence 116 corresponding with network inputs 102
Output position at label a possibility that.
The mode that neural network 1 04 generates score distribution 112 depends on neural network 1 04 and has been configured as the machine executed
Learning tasks and framework depending on neural network 1 04.
In some cases, task is voice recognition tasks.In voice recognition tasks, network inputs indicate language
Audio data or audio frequency characteristics, and natural language output sequence is the transcription of the audio data or audio frequency characteristics.William
" Listen, attend and spell ", arXiv Preprint (preprint) arXiv:1508.01211 of Chan et al.
(2015) it is described in and generates this score distribution and the example of the speech recognition neural network that can be used for voice recognition tasks.
In some cases, task is machine translation task.In machine translation task, network inputs are derived from right language
Word sequence, and natural language output sequence is the word sequence of target natural language, the word sequence of target natural language
Column are the translations of the word sequence of the source natural language." Google ' the s Neural Machine of Yonghui Wu et al.
Translation System:Bridging the Gap between Human and Machine Translation "
It is described in arXiv Preprint arXiv:1609.08144 (2016) and generates this score distribution and can be used for machine translation task
Neural machine translation neural network example.
In some cases, task is image subtitle task.In image subtitle task, network inputs are images, and
Natural language output sequence is the word sequence for describing input picture.In some cases, task is autocoding task.Certainly
In dynamic encoding tasks, network inputs are the word sequences of natural language, and network output is identical as network inputs.Ilya
" the Sequence to sequence learning with of Sutskever, Oriol Vinyals and Quoc V et al.
Neural networks " Advances in neural information processing systems is described in 2014
This score distribution of generation and it can be used for sequence and with the machine learning task to such as autocoding be ranked up or can fit
The example of neural network for image subtitle task.
The set of label includes the n-gram of multiple and different sizes of target natural language.For example, the set of label can be with
Including single character, the fragments of words being made of multiple characters, and optional, entire word.In general, in the set of label
One or more n-grams are the prefixes (prefix) of other n-grams of one or more of the set of label.For example, mark
The set of note may include character " c ", " a ", " t " and 2 metagrammars " ca ", " ct ", " at " and 3 metagrammars " cat ", etc..
It can be by modifying the output layer of neural network (that is, increasing soft maximum (softmax) output layer of neural network
Size, and optionally increases the size of other layers in neural network) any of the above neural networks of modification with generate including
The score of the set of the label of the n-gram of multiple and different sizes, so that output layer is each label life in the set of label
At corresponding score.
One is trained, and neural network 1 04 may be used in standard decoding process --- for example, using William Chan etc.
People " wave beam described in Listen, attend and spell ", arXiv Preprint arXiv:1508.01211 (2015) is searched
Rope decoding process, Yonghui Wu et al. " Google ' s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation " arXiv Preprint arXiv:
Beam search decoding process or another decoding process appropriate in 1609.08144 (2016) --- it is defeated with network to generate
Enter 102 corresponding natural language target sequences 116.
In order to train neural network, system 100 obtains the training data including training input, and defeated for each training
Enter, obtains the respective objects sequence of one or more words including natural language.
For given training input, neural network 1 04 generates more according to the current value of neural network model parameter 110
The corresponding scores distribution 112 of the set of the label of each output position of a output position.
Given training is inputted, label of the significant notation sample engine 108 in system 100 from each output position
Set in sequentially sample significant notation, wherein if given label with by significant notation sample engine 108 to previous output
The orderly series connection of the label of position sampling generates the prefix of target sequence effectively decomposed, then the given label is output position
Significant notation.The effective of target sequence decomposes the flag sequence resolved into target sequence in the set of label, that is, so that different
It is effective decompose may include different number label, and can be by the difference of different size of various n-grams
Group is combined into.For example, target sequence " banana " is effectively decomposed into 1 metagrammar, that is, such as " b ", " a ", " n ", " a ", " n ", " a "
Character, and be effectively decomposed into the combination of 1 metagrammar, 2 metagrammars and 3 metagrammars, such as: " b ", " ana ", " na ".Effectively
The prefix of decomposition is the ordered sequence of the continued labelling effectively decomposed since first label effectively decomposed.As another
Example, if target sequence is " banana ", the set of label is the set of all 1 metagrammars, 2 metagrammars and 3 metagrammars, and
And by the label that significant notation sample engine 108 samples previous output position be " b ", " a " and " n ", then " a ", " an " and
" ana " is all the example for the significant notation that current output bit sets place.
Specifically, for each output position, significant notation sample engine 108 with probability ε stochastical sampling significant notation, and
And in the score distribution of output position that is generated according to neural network 1 04 of significant notation sample engine 108 significant notation score,
Significant notation is sampled with probability (1-ε), wherein ε is the number between 0 and 1.
When the series connection of the significant notation sampled by significant notation sample engine 108 and effective decomposition of target sequence are (that is,
When sampling effectively decomposes 114) corresponding, neural network 1 04 stops the corresponding scores distribution for being sequentially generated output position.
In general, system 100 trains neural network 1 04 by adjusting the current value of neural network model parameter 110, with excellent
Change performance of the neural network 1 04 relative to objective function 106, the set that wherein objective function 106 depends on as training data
In training input generate sampling effectively decompose 114.In some embodiments, system 100 will come from previous output bit
Set (that is, current output bit set before output position) the significant notation of sampling be fed as input to neural network 1 04, and
And neural network 1 04 be at least partially based on previous output position sampled significant notation to generate the score that current output bit is set
Distribution, and objective function 106 is:
Wherein J is the quantity of training input, and I is the quantity of output position, andIt is to markInstruction
Practice input xjProcessing during the score that is generated by neural network 1 04, whereinIt is to be directed to be by significant notation sample engine 108
The x that system 100 generatesjSampled effectively decompose 114 in output position i sampling label, andIt is to be adopted by significant notation
Label sets of the sample engine 108 to the output position sampling before the i of position.
The current value of system call interception neural network model parameter 110, to use standard machine learning training technology (such as sharp
With backpropagation or the stochastic gradient descent of backpropagation at any time) come optimization object function.
Fig. 2 is the flow chart of the instantiation procedure 200 for training machine learning system.For convenience's sake, process 200 will
It is described as being executed by the one or more system for computer for being located at one or more positions.For example, according to this specification
Properly programmed machine learning system, such as the machine learning system 100 of Fig. 1, can be with implementation procedure 200.
System obtains the score that the set of label of generation is set by the current output bit that neural network is given training input
It is distributed (step 202).Corresponding point of each label distribution of the score distribution that current output bit is set into the set of multiple labels
Number, wherein the given label of the fraction representation of given label should be working as in natural language target sequence corresponding with training input
A possibility that label at preceding output position.
The mode that neural network generates score distribution depends on the frame of the machine learning task and neural network that are carrying out
Structure.In some embodiments, system will from previous output position (that is, current output bit set before output position)
Sampling significant notation is fed as input to neural network, and neural network is at least partially based on having adopted for previous output position
Sample significant notation generates the score distribution that current output bit is set.With reference to Ilya Sutskever, Oriol Vinyals and Quoc
" Sequence to sequence learning with neural networks " Advances in neural of V et al.
Information processing systems, 2014.
System is sampled (step 204) to the significant notation that current output bit is set.If given label with to previous defeated
The orderly series connection of the significant notation of out position sampling generates the prefix of target sequence effectively decomposed, then the given label is current
The significant notation of output position.The example sampled to the significant notation that current output bit is set is described with reference to the process 300 of Fig. 3
Technology.
System determine previous output position sampled significant notation and current output bit is set has sampled significant notation
Orderly series connection whether be target sequence effective decomposition (step 206).
In response to the sampling significant notation for having sampled significant notation and current output bit and having set of the previous output position of determination
Orderly series connection be not target sequence effective decomposition, system goes to next output position, return step 202, and repeats front
The step of (step 208).
In response to the sampling significant notation for having sampled significant notation and current output bit and having set of the previous output position of determination
Orderly series connection be target sequence effective decomposition, the current value of the parameter of system call interception neural network sampled with increasing
A possibility that label in effect decomposition is the label at corresponding output position (step 210).In some embodiments, it adjusts
The current value of the parameter of neural network includes executing the iteration of neural network training process, to increase and sample in effectively decomposition
Label position corresponding output position score distribution in, effectively decompose in the corresponding scores of each label multiply
Long-pending logarithm.For example, neural network training process can be backpropagation or backpropagation at any time.
Fig. 3 is the flow chart of the instantiation procedure 300 sampled to the significant notation of output position.For convenience, process
300 will be described as being executed by the one or more system for computer for being located at one or more positions.For example, according to this theory
The properly programmed machine learning system of bright book (such as machine learning system 100 of Fig. 1) can be with implementation procedure 300.
With reference to the digital ε between 0 and 1 come the step of describing process 300.Effectively divide for generating to have sampled for training input
The value of the different operations of the process 200 of solution, ε can be different.In some embodiments, the value of epsilon is initially set to
Non-zero, and as training process 200 is performed a plurality of times in system, the value of epsilon reduces, until it finally is set to zero.
System obtains the score distribution (step 302) for the label that current output bit is set.
System determines the set (step 303) of the significant notation of output position.If given mark and to previous output bit
The orderly series connection for setting the label of sampling generates the prefix of target sequence effectively decomposed, then the given label is having for output position
Criterion note.
With probability ε, system stochastical sampling significant notation (step 304) from the set of all possible significant notation.
With probability (1-ε), system samples significant notation from probability distribution, and the probability distribution is by generating neural network
The score distribution set of current output bit be limited to the set of significant notation to define.
This specification use term " configuration " related with system and computer program component.For being configured as executing tool
One or more system for computer of operation or the movement of body, it is meant that system has had mounted thereto software, firmware, hard
Part or their combination, these softwares, firmware, hardware or their combination cause system to execute operation or movement in operation.
For being configured as executing one or more computer programs of concrete operations or movement, it is meant that one or more programs include
Device is made to execute the instruction of operation or movement when being run by data processing equipment.
Theme described in this specification and the embodiment of feature operation can be implemented in the following: Fundamental Digital Circuit,
Computer software or firmware, the computer hardware (including structure disclosed in this specification and its equivalent structures) of tangible embodiment
Middle implementation or the combination in one or more of which.The embodiment of theme described in this specification can be carried out
For one or more computer programs, i.e., one of computer program instructions of the coding on tangible non-transitory storage medium
Or multiple modules, for being run by data processing equipment or to control the operation of data processing equipment.Computer storage medium
Can be machine readable storage device, machine readable storage substrate, random or serial-access storage equipment or they in
One or more combinations.Alternatively or additionally, program instruction can be coded on manually generated transmitting signal, example
Such as electricity, light or electromagnetic signal that machine generates, which is generated with encoded information, is used for transmission suitable acceptor device
For data processing equipment operation.
Term " data processing equipment " refers to data processing hardware, and including for handling data various devices,
Equipment and machine, for example including programmable processor, computer or multiple processors or computer.Device can also be or into one
Step includes dedicated logic circuit, for example, FPGA (field programmable gate array, field programmable gate array)
Or ASIC (application specific integrated circuit, specific integrated circuit).In addition to hardware, it fills
The code that running environment is created for computer program can be optionally included by setting, such as constitute processor firmware, protocol stack, data
The combined code of base management system, operating system or one or more of which.
It can be referred to as or be described as the meter of program, software, software application, app, module, software module, script or code
Calculation machine program can in any form programming language (including compiling or interpretative code, or declaratively or procedural) write
Enter；And it can in any form (including as stand-alone program or as module, component, subroutine or be suitble to calculate ring
Other units used in border) deployment.Program can with but not necessarily correspond to the file in file system.Program can store
In a part for saving the file (such as the one or more scripts being stored in marking language document) of other programs or data,
Be stored in the single file for being exclusively used in discussed program, or be stored in multiple coordination files (such as storage one or more
The file of a module, subprogram or partial code) in.Computer program can be deployed as positioned at a website or across multiple
It is run on website distribution and the computer or multiple computers that are interconnected by data communication network.
In the present specification, term " engine " is widely used in referring to software-based system, subsystem or is programmed to hold
The process of row one or more concrete function.In general, engine will be implemented as one that is mounted on one or more positions or
One or more software modules or component on multiple computers.In some cases, one or more computers will be specific to
Specific engine；In other cases, can on identical one or more computers the multiple engines of installation and operation.
Process and logic flow described in this specification can by run one or more computer programs with by pair
Input data is operated and generates output and execute one or more programmable calculators of function to execute.Process and logic
Process can also be by dedicated logic circuit (for example, FPGA or ASIC) or by dedicated logic circuit and one or more programmings
The combination of computer executes.
The computer for being suitable for running computer program can be based on general or specialized microprocessor or the two, Huo Zheren
What other kinds of central processing unit.In general, central processing unit will from read-only memory or random access memory or this
The two receives instruction and data.The primary element of computer is for executing or the central processing unit of operating instruction and for depositing
Store up one or more memory devices of instruction and data.Central processing unit and memory can be by supplementeds
Or it is integrated in the dedicated logic circuit.In general, computer will also be deposited including one or more large capacities for storing data
Store up equipment, such as disk, magneto-optic disk or CD, or be operatively coupled to one or more mass-memory units with from
The one or more mass-memory unit receive data or pass data to the one or more mass-memory unit or
Both persons.However, computer does not need such equipment.In addition, computer can be embedded into another equipment, such as mobile electricity
Words, personal digital assistant (personal digital assistant, PDA), Mobile audio frequency or video player, game control
Platform, global positioning system (Global Positioning System, GPS) receiver or portable memory apparatus are (such as logical
With universal serial bus (a universal serial bus, USB) flash drive), only lift several examples.
It is suitable for storing computer program instructions and the computer-readable medium of data including the non-volatile of form of ownership
Memory, medium and memory devices, for example including semiconductor memory devices, such as EPROM, EEPROM and flash memories
Equipment；Disk, such as internal hard drive or moveable magnetic disc；Magneto-optic disk；And CD-ROM and DVD-ROM disk.
In order to provide the interaction with user, the embodiment of theme described in this specification can be implemented on computers,
The computer has the display equipment for showing information to user, such as CRT (cathode-ray tube, cathode ray tube)
Or LCD (liquid crystal display, liquid crystal display) and user can provide input to computer by it
Keyboard and pointing device, such as mouse or trackball.Also other kinds of equipment can be used to provide the interaction with user；Example
Such as, the feedback for being supplied to user may be any type of sense feedback, such as visual feedback, audio feedback or touch feedback；
And input from the user can be received in any form, including sound, voice or tactile input.In addition, computer can be with
Document and the equipment used from the user reception document are sent by the equipment used to user to interact with user；For example, logical
It crosses in response to sending webpage from the received web browser requested on user equipment of web browser.Moreover, computer can
With by personal device (for example, operation message transmission (messaging) apply smart phone) sending information message or its
The message of his form, and receive response message from user in turn and interacted with user.
Data processing equipment for implementing machine learning model can also include such as dedicated hardware accelerators unit, with
For handling machine learning training or the public and computation-intensive part of production, i.e. reasoning, workload.
Machine learning frame can be used to implement and dispose in machine learning model, for example, TensorFlow frame,
Microsoft Cognitive Toolkit frame, Apache Singa frame or Apache MXNet frame.
The embodiment of theme described in this specification can be implemented in computing systems, which includes rear end group
Part (for example, as data server) or including middleware component (for example, application server) or including front end assemblies
(for example, with graphic user interface, web browser or app client computer, user can by the front end assemblies with
Theme described in this specification embodiment interaction) or one or more as rear end, middleware or front end assemblies
Any combination.The component of system can be interconnected by any form or the digital data communications (such as communication network) of medium.
The example of communication network includes the wide area network (wide of local area network (LAN, local area network) and such as internet
Area network, WAN).
Computing system may include client and server.Client and server is generally remote from each other, and typically
Pass through communication network interaction.The relationship of client and server is by running on corresponding computer and having each other
There is the computer program of client-server relation to generate.In some embodiments, server is by data (for example, HTML page
Face) it is transferred to user equipment, for example, with for showing data to the user that interacts with the equipment for serving as client and from the user
Receive the purpose of user's input.At user equipment generate data, such as user interaction as a result, can at server from
Equipment receives.
Although this specification includes many specific implementation details, these are not necessarily to be construed as the range to any invention
Or can be with the limitation of claimed range, but the description of the specific features to the specific embodiment specifically invented.This explanation
Certain features described in the context of individual embodiment can also combine implementation in a single embodiment in book.Anti- mistake
Come, the various features described in the context of single embodiment can also be in various embodiments individually or with any suitable
Sub-portfolio implement.In addition, and even initially being wanted although features described above can be described as working with certain combinations
Such protection is asked, but in some cases, it can be from the combination from claimed combined one or more features
It deletes, and claimed combination can be directed toward the variant of sub-portfolio or sub-portfolio.
Similarly, although depicting operation in the accompanying drawings, and operation is described with specific order in the claims,
It is that this is not construed as requiring these operation specific orders shown in or be executed with sequential order, or requires to execute institute
Operation is shown in having to obtain desired result.In some cases, multitasking and parallel processing can be advantageous.This
Outside, the separation of various system modules and component should not be construed as requiring this point in all embodiments in above-described embodiment
From, and it will be understood that described program assembly and system usually can integrate in single software product or be encapsulated into it is more
In a software product.
The specific embodiment of theme has been described.Other embodiments are in the range of following claims.For example, right
The movement described in it is required that can be executed in different order, and still obtain desired result.As an example, attached drawing
The process of middle description not necessarily require shown in specific order or sequential order to obtain desired result.In some cases,
Multitasking and parallel processing can be advantageous.
Claims (14)
1. a kind of the method implemented by computer, comprising:
The training data for training neural network is obtained,
Wherein, the neural network is configured as receiving network inputs, and handles institute according to the multiple parameters of the neural network
State network inputs with generate the corresponding scores of each output position in multiple output positions distribution,
Wherein, the corresponding scores distribution of each output position includes the corresponding scores of each label in scheduled tag set,
Wherein, the scheduled tag set includes the n-gram of multiple and different sizes,
Wherein, for each output position, the corresponding scores of each label in the score distribution of output position indicate that label is
A possibility that label at output position in the output sequence of the network inputs, and
Wherein, the training data includes multiple training inputs, and each training is inputted, respective objects output sequence packet
Include one or more words；
For each training input:
It is multiple to generate using training input described in the Processing with Neural Network according to the current value of the parameter of the neural network
The corresponding scores of each output position in output position are distributed；
The target output sequence is sampled from multiple possible effective decomposition of the target output sequence of the training input
It effectively decomposes, wherein the target sequence is decomposed into the scheduled label by the possible effective decomposition of each of target sequence
Different flags sequence in set；And
Adjust the current value of the parameter of the neural network, with increase sampled effectively decompose in label be in output sequence it is right
A possibility that label at the output position answered.
2. according to the method described in claim 1, wherein, described sampled effectively decomposes the n member language including multiple and different sizes
Method.
3. method according to claim 1 or 2, wherein adjust the current value of the parameter of the neural network
To increase a possibility that label sampled in effectively decomposition is the label in output sequence at corresponding output position packet
It includes:
The iteration of neural network training process is executed, it is corresponding with the position of label sampled in effectively decomposition to increase
Output position score distribution in, it is described sampled effectively decompose in each label corresponding scores product pair
Number.
4. method according to any one of claim 1-3, wherein sample effective decomposition packet of the target output sequence
It includes, for each output position in the multiple output position and since initial position successively:
With probability ε from the significant notation in the scheduled tag set stochastical sampling significant notation, wherein the output bit
The significant notation set is the label in the scheduled tag set, and the label in scheduled tag set will
It is effective addition that the current portions of the target output sequence of the output position effectively decompose；And
According to the score of the significant notation in the score distribution of the output position of the training input, have with 1-ε of probability from described
Significant notation is sampled in criterion note.
5. according to the method described in claim 4, further include, for each output position in the multiple output position and
Since initial position successively:
The significant notation of sampling of the output position is fed as input to the neural network, with described more for generating
The score distribution of next output position of a output position.
6. method according to any one of claims 1-5, wherein one or more of described scheduled tag set
N-gram is the prefix of other n-grams of one or more of the scheduled tag set.
7. method according to claim 1 to 6, wherein the n-gram packet in the scheduled tag set
Include character and fragments of words.
8. according to the method described in claim 7, wherein, the n-gram in the scheduled tag set further includes word.
9. method according to claim 1 to 8, wherein the neural network is speech recognition neural network,
And the network inputs are the audio data or audio frequency characteristics for indicating language.
10. method according to claim 1 to 8, wherein the neural network is neural machine translation nerve
Network, and the network inputs are to indicate the input marking sequence of the word sequence of original language, and it is wherein described scheduled
N-gram in tag set is the n-gram of target natural language.
11. method according to claim 1 to 8, wherein the neural network is that image subtitle generates nerve
Network, and the network inputs are images.
12. method according to claim 1 to 8, wherein the neural network is autocoder nerve net
Network, and the network inputs are word sequences.
13. a kind of system, one or more storage equipment including one or more computers and store instruction, work as described instruction
When being run by one or more of computers, so that one or more of computers are executed as any in claim 1-12
The corresponding operating of method described in.
14. one or more computer storage mediums of store instruction, described instruction when being run by one or more computers,
So that one or more of computers execute the corresponding operating such as method of any of claims 1-12.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662403615P | 2016-10-03 | 2016-10-03 | |
US62/403,615 | 2016-10-03 | ||
PCT/US2017/054833 WO2018067495A1 (en) | 2016-10-03 | 2017-10-03 | Processing text sequences using neural networks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN109891437A true CN109891437A (en) | 2019-06-14 |
Family
ID=60084127
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780067511.7A Pending CN109891437A (en) | 2016-10-03 | 2017-10-03 | Use Processing with Neural Network text sequence |
Country Status (4)
Country | Link |
---|---|
US (1) | US11182566B2 (en) |
EP (1) | EP3520036B1 (en) |
CN (1) | CN109891437A (en) |
WO (1) | WO2018067495A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111274764A (en) * | 2020-01-23 | 2020-06-12 | 北京百度网讯科技有限公司 | Language generation method and device, computer equipment and storage medium |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200065654A1 (en) * | 2018-08-22 | 2020-02-27 | Electronics And Telecommunications Research Institute | Neural network fusion apparatus and modular neural network fusion method and matching interface generation method for the same |
CN109146064B (en) * | 2018-09-05 | 2023-07-25 | 腾讯科技（深圳）有限公司 | Neural network training method, device, computer equipment and storage medium |
CN111325000B (en) * | 2020-01-23 | 2021-01-26 | 北京百度网讯科技有限公司 | Language generation method and device and electronic equipment |
KR20220010259A (en) * | 2020-07-17 | 2022-01-25 | 삼성전자주식회사 | Natural language processing method and apparatus |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10867597B2 (en) * | 2013-09-02 | 2020-12-15 | Microsoft Technology Licensing, Llc | Assignment of semantic labels to a sequence of words using neural network architectures |
US9971765B2 (en) * | 2014-05-13 | 2018-05-15 | Nuance Communications, Inc. | Revising language model scores based on semantic class hypotheses |
US9378731B2 (en) * | 2014-09-25 | 2016-06-28 | Google Inc. | Acoustic model training corpus selection |
US10332509B2 (en) * | 2015-11-25 | 2019-06-25 | Baidu USA, LLC | End-to-end speech recognition |
US10481863B2 (en) * | 2016-07-06 | 2019-11-19 | Baidu Usa Llc | Systems and methods for improved user interface |
-
2017
- 2017-10-03 CN CN201780067511.7A patent/CN109891437A/en active Pending
- 2017-10-03 WO PCT/US2017/054833 patent/WO2018067495A1/en active Application Filing
- 2017-10-03 EP EP17784512.0A patent/EP3520036B1/en active Active
- 2017-10-03 US US16/338,174 patent/US11182566B2/en active Active
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111274764A (en) * | 2020-01-23 | 2020-06-12 | 北京百度网讯科技有限公司 | Language generation method and device, computer equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US20200026765A1 (en) | 2020-01-23 |
EP3520036B1 (en) | 2020-07-29 |
WO2018067495A1 (en) | 2018-04-12 |
EP3520036A1 (en) | 2019-08-07 |
US11182566B2 (en) | 2021-11-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10635977B2 (en) | Multi-task learning using knowledge distillation | |
CN106997370B (en) | Author-based text classification and conversion | |
US10268671B2 (en) | Generating parse trees of text segments using neural networks | |
US10559300B2 (en) | Generating target sequences from input sequences using partial conditioning | |
KR102534721B1 (en) | Method, apparatus, device and storage medium for training model | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
CN109891437A (en) | Use Processing with Neural Network text sequence | |
ES2714152T3 (en) | Batch Normalization Layers | |
CN109313719B (en) | Dependency resolution for generating text segments using neural networks | |
US20210192288A1 (en) | Method and apparatus for processing data | |
CN110520871A (en) | Training machine learning model | |
CN109074517B (en) | Global normalized neural network | |
US20200364617A1 (en) | Training machine learning models using teacher annealing | |
EP3475890A1 (en) | Reward augmented model training | |
CN110023928A (en) | Forecasting search engine ranking signal value | |
CN111079945B (en) | End-to-end model training method and device | |
US11853861B2 (en) | Generating output examples using bit blocks | |
US20230195998A1 (en) | Sample generation method, model training method, trajectory recognition method, device, and medium | |
CN113779225A (en) | Entity link model training method, entity link method and device | |
CN111008689A (en) | Reducing neural network inference time using SOFTMAX approximation | |
CN115226408A (en) | Speaker-adaptive end of utterance detection for conversational AI applications | |
CN111328416A (en) | Speech patterns for fuzzy matching in natural language processing | |
CN114492456B (en) | Text generation method, model training method, device, electronic equipment and medium | |
CN110851569B (en) | Data processing method, device, equipment and storage medium | |
CN111859981A (en) | Language model acquisition and Chinese semantic understanding method, device and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |