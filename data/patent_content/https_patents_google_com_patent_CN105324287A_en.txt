CN105324287A - Methods and systems for detecting weather conditions using vehicle onboard sensors - Google Patents
Methods and systems for detecting weather conditions using vehicle onboard sensors Download PDFInfo
- Publication number
- CN105324287A CN105324287A CN201480033640.0A CN201480033640A CN105324287A CN 105324287 A CN105324287 A CN 105324287A CN 201480033640 A CN201480033640 A CN 201480033640A CN 105324287 A CN105324287 A CN 105324287A
- Authority
- CN
- China
- Prior art keywords
- vehicle
- environment
- laser data
- laser
- data point
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 251
- 239000003595 mist Substances 0.000 claims description 135
- 238000013500 data storage Methods 0.000 claims description 23
- 238000012360 testing method Methods 0.000 claims description 10
- 230000013011 mating Effects 0.000 claims description 2
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 78
- 230000006870 function Effects 0.000 description 49
- 238000010586 diagram Methods 0.000 description 34
- 230000000875 corresponding effect Effects 0.000 description 33
- 230000008569 process Effects 0.000 description 32
- 238000004891 communication Methods 0.000 description 31
- 238000001514 detection method Methods 0.000 description 30
- 230000033001 locomotion Effects 0.000 description 27
- 238000003860 storage Methods 0.000 description 19
- 230000008447 perception Effects 0.000 description 18
- 230000004438 eyesight Effects 0.000 description 15
- 230000004927 fusion Effects 0.000 description 15
- 238000009826 distribution Methods 0.000 description 14
- 230000007613 environmental effect Effects 0.000 description 13
- 230000009471 action Effects 0.000 description 12
- 230000003287 optical effect Effects 0.000 description 11
- 230000000007 visual effect Effects 0.000 description 10
- 230000008859 change Effects 0.000 description 9
- 238000005259 measurement Methods 0.000 description 9
- 238000002310 reflectometry Methods 0.000 description 9
- 239000007787 solid Substances 0.000 description 9
- 230000003466 anti-cipated effect Effects 0.000 description 7
- 230000004044 response Effects 0.000 description 7
- 241001424688 Enceliopsis Species 0.000 description 6
- 230000035945 sensitivity Effects 0.000 description 6
- 230000001133 acceleration Effects 0.000 description 5
- 230000000712 assembly Effects 0.000 description 5
- 238000000429 assembly Methods 0.000 description 5
- 230000001276 controlling effect Effects 0.000 description 5
- 239000012634 fragment Substances 0.000 description 5
- 239000000446 fuel Substances 0.000 description 5
- 238000012545 processing Methods 0.000 description 5
- 241000287828 Gallus gallus Species 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 4
- 239000000203 mixture Substances 0.000 description 4
- 238000003825 pressing Methods 0.000 description 4
- 230000001953 sensory effect Effects 0.000 description 4
- BQCADISMDOOEFD-UHFFFAOYSA-N Silver Chemical compound [Ag] BQCADISMDOOEFD-UHFFFAOYSA-N 0.000 description 3
- 238000009825 accumulation Methods 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 3
- 230000006399 behavior Effects 0.000 description 3
- 238000012512 characterization method Methods 0.000 description 3
- 230000008878 coupling Effects 0.000 description 3
- 238000010168 coupling process Methods 0.000 description 3
- 238000005859 coupling reaction Methods 0.000 description 3
- 230000005484 gravity Effects 0.000 description 3
- 238000009434 installation Methods 0.000 description 3
- 230000008520 organization Effects 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 229910052709 silver Inorganic materials 0.000 description 3
- 239000004332 silver Substances 0.000 description 3
- 241001269238 Data Species 0.000 description 2
- LFQSCWFLJHTTHZ-UHFFFAOYSA-N Ethanol Chemical compound CCO LFQSCWFLJHTTHZ-UHFFFAOYSA-N 0.000 description 2
- ATUOYWHBWRKTHZ-UHFFFAOYSA-N Propane Chemical compound CCC ATUOYWHBWRKTHZ-UHFFFAOYSA-N 0.000 description 2
- 230000010267 cellular communication Effects 0.000 description 2
- 239000004568 cement Substances 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000012790 confirmation Methods 0.000 description 2
- 238000010276 construction Methods 0.000 description 2
- 238000013480 data collection Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 239000000463 material Substances 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000000149 penetrating effect Effects 0.000 description 2
- 230000001012 protector Effects 0.000 description 2
- 238000012958 reprocessing Methods 0.000 description 2
- 238000001228 spectrum Methods 0.000 description 2
- 238000009736 wetting Methods 0.000 description 2
- 241000931526 Acer campestre Species 0.000 description 1
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 1
- 239000002253 acid Substances 0.000 description 1
- 230000004888 barrier function Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 239000011449 brick Substances 0.000 description 1
- 238000002485 combustion reaction Methods 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000011109 contamination Methods 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 230000002596 correlated effect Effects 0.000 description 1
- 239000002283 diesel fuel Substances 0.000 description 1
- 238000001035 drying Methods 0.000 description 1
- 229920001971 elastomer Polymers 0.000 description 1
- 239000012530 fluid Substances 0.000 description 1
- 230000004313 glare Effects 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 238000007689 inspection Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 229910001416 lithium ion Inorganic materials 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 229910052751 metal Inorganic materials 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 238000012806 monitoring device Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 238000007500 overflow downdraw method Methods 0.000 description 1
- 238000002360 preparation method Methods 0.000 description 1
- 239000001294 propane Substances 0.000 description 1
- 230000005855 radiation Effects 0.000 description 1
- 238000011084 recovery Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000001172 regenerating effect Effects 0.000 description 1
- 230000008929 regeneration Effects 0.000 description 1
- 238000011069 regeneration method Methods 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 229920006395 saturated elastomer Polymers 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W40/00—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models
- B60W40/02—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models related to ambient conditions
- B60W40/06—Road conditions
- B60W40/064—Degree of grip
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W40/00—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models
- B60W40/02—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models related to ambient conditions
- B60W40/06—Road conditions
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S13/00—Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified
- G01S13/86—Combinations of radar systems with non-radar systems, e.g. sonar, direction finder
- G01S13/865—Combination of radar systems with lidar systems
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S13/00—Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified
- G01S13/86—Combinations of radar systems with non-radar systems, e.g. sonar, direction finder
- G01S13/867—Combination of radar systems with cameras
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S13/00—Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified
- G01S13/88—Radar or analogous systems specially adapted for specific applications
- G01S13/93—Radar or analogous systems specially adapted for specific applications for anti-collision purposes
- G01S13/931—Radar or analogous systems specially adapted for specific applications for anti-collision purposes of land vehicles
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/66—Tracking systems using electromagnetic waves other than radio waves
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/86—Combinations of lidar systems with systems other than lidar, radar or sonar, e.g. with direction finders
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/88—Lidar systems specially adapted for specific applications
- G01S17/93—Lidar systems specially adapted for specific applications for anti-collision purposes
- G01S17/931—Lidar systems specially adapted for specific applications for anti-collision purposes of land vehicles
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/88—Lidar systems specially adapted for specific applications
- G01S17/95—Lidar systems specially adapted for specific applications for meteorological use
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S7/00—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00
- G01S7/48—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00 of systems according to group G01S17/00
- G01S7/4802—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00 of systems according to group G01S17/00 using analysis of echo signal for target characterisation; Target signature; Target cross-section
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S7/00—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00
- G01S7/48—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00 of systems according to group G01S17/00
- G01S7/4808—Evaluating distance, position or velocity data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/50—Context or environment of the image
- G06V20/56—Context or environment of the image exterior to a vehicle by using sensors mounted on the vehicle
-
- G—PHYSICS
- G08—SIGNALLING
- G08G—TRAFFIC CONTROL SYSTEMS
- G08G1/00—Traffic control systems for road vehicles
- G08G1/01—Detecting movement of traffic to be counted or controlled
- G08G1/04—Detecting movement of traffic to be counted or controlled using optical or ultrasonic detectors
-
- G—PHYSICS
- G08—SIGNALLING
- G08G—TRAFFIC CONTROL SYSTEMS
- G08G1/00—Traffic control systems for road vehicles
- G08G1/09—Arrangements for giving variable traffic instructions
- G08G1/0962—Arrangements for giving variable traffic instructions having an indicator mounted inside the vehicle, e.g. giving voice messages
- G08G1/09623—Systems involving the acquisition of information from passive traffic signs by means mounted on the vehicle
-
- B60W2420/408—
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/02—Systems using the reflection of electromagnetic waves other than radio waves
- G01S17/06—Systems determining position data of a target
- G01S17/42—Simultaneous measurement of distance and other co-ordinates
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02A—TECHNOLOGIES FOR ADAPTATION TO CLIMATE CHANGE
- Y02A90/00—Technologies having an indirect contribution to adaptation to climate change
- Y02A90/10—Information and communication technologies [ICT] supporting adaptation to climate change, e.g. for weather forecasting or climate simulation
Abstract
Example methods and systems for detecting weather conditions using vehicle onboard sensors are provided. An example method includes receiving laser data collected for an environment of a vehicle, and the laser data includes a plurality of laser data points. The method also includes associating, by a computing device, laser data points of the plurality of laser data points with one or more objects in the environment, and determining given laser data points of the plurality of laser data points that are unassociated with the one or more objects in the environment as being representative of an untracked object. The method also includes based on one or more untracked objects being determined, identifying by the computing device an indication of a weather condition of the environment.
Description
Background technology
Unless separately had instruction in this article, otherwise the material described in this part is not the prior art of the claim in the application, and is not admitted to be prior art because being included in this part yet.
Autonomous vehicle uses various computing system to help passenger to be transported to another position from a position.Some autonomous vehicles may need from operator, such as, navigator, chaufeur or passenger, initial input or input continuously.Other autonomous systems, such as autonavigator, can work as when system is used and use, it allows operator from manual mode (wherein, operator exercises Altitude control for the mobile of vehicle) be switched to autonomous mode (wherein, vehicle is basic oneself drives) and marginal pattern.
This vehicle is usually equipped with various types of sensor so that the object in arround inspection.Such as, autonomous vehicle can comprise laser, sonar, radar (radar), camera and other scanning and record from the equipment of the data arround vehicle.Detected object and respective characteristic (position, shape, working direction, speed etc.) thereof is can be used for from the one or more sensing data in these equipment.The safe in operation of these detection and Identification to autonomous vehicle is useful.
Summary of the invention
In this example, the equipment and the method that use onboard sensor detection to comprise the weather condition on wet surface is provided.
In one example, the method provided comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Method also comprises determines laser data point in described multiple laser data point, that be associated with the one or more objects in environment by computing equipment.Method also comprises based on laser data point uncorrelated with the described one or more object in environment, and the surface travelled thereon by computing equipment identification vehicle is wet instruction.
In another example, provide the non-transitory computer readable storage medium storing instruction, this instruction causes computing equipment n-back test when being run by computing equipment.Function comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Function also comprises determines laser data point in described multiple laser data point, that be associated with the one or more objects in environment, and based on laser data point uncorrelated with the described one or more object in environment, identify that the surface that travels of vehicle is wet instruction thereon.
In another example, provide system, it comprises at least one treater and data storage apparatus, and this data storage apparatus comprises and can be run with the instruction making system n-back test by least one treater described.Function comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Function also comprises determines laser data point in described multiple laser data point, that be associated with the one or more objects in environment, and based on laser data point uncorrelated with the described one or more object in environment, the surface travelled thereon by computing equipment identification vehicle is wet instruction.
In another example, provide equipment, it comprises the device of the laser data that the environment for receiving for vehicle is collected, and laser data comprises multiple laser data point.Equipment also comprises the device for determining laser data point in described multiple laser data point, that be associated with the one or more objects in environment.Equipment also comprises for based on laser data point uncorrelated with the described one or more object in environment, identifies that surface that vehicle travels is the device of wet instruction thereon.
In other examples, provide and use onboard sensor to detect the equipment and the method that comprise the weather condition of mist.
Such as, in another example, the method provided comprises the laser data receiving and collect from the scanning of the environment to vehicle, and laser data comprises multiple laser data point.Method also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point by computing equipment, and the laser data point of the pattern representative with the uncorrelated laser data point of the one or more objects in environment and storage caused due to mist compares.Method also comprises based on described comparison, is comprised the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.
In another example, provide the non-transitory computer readable storage medium storing instruction, this instruction causes computing equipment n-back test when being run by computing equipment.Function comprises the laser data receiving and collect from the scanning of the environment to vehicle, and laser data comprises multiple laser data point.Function also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point, and the laser data point of the pattern representative with the uncorrelated laser data point of the one or more objects in environment and storage caused due to mist compares.Function also comprises based on described comparison, is comprised the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.
In another example, provide system, it comprises at least one treater and data storage apparatus, and this data storage apparatus comprises and can be run with the instruction making system n-back test by least one treater described.Function comprises the laser data receiving and collect from the scanning of the environment to vehicle, and laser data comprises multiple laser data point.Function also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point, and the laser data point of the pattern representative with the uncorrelated laser data point of the one or more objects in environment and storage caused due to mist compares.Function also comprises based on described comparison, is comprised the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.
In another example, provide equipment, it comprises the device of the laser data that the environment for receiving for vehicle is collected, and laser data comprises multiple laser data point.Equipment also comprises the device for determining laser data point in described multiple laser data point, that be associated with the one or more objects in environment.Equipment also comprises for based on laser data point uncorrelated with the described one or more object in environment, is comprised the device of the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.
In other examples other, provide and use onboard sensor to detect the equipment and the method that comprise the weather condition of sunlight.
Such as, the method provided comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Method also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point by computing equipment, and by described multiple laser data point, be defined as representing the object do not tracked in the given position relative to vehicle with the uncorrelated given laser data point of described one or more object in environment.Method also comprises as the vehicle moves, determines that the object do not tracked remains on the roughly the same relative position relative to vehicle, and is sunny instruction by the weather condition of the environment of computing equipment identification vehicle.
In another example, provide the non-transitory computer readable storage medium storing instruction, this instruction causes computing equipment n-back test when being run by computing equipment.Function comprises the laser data receiving and collect for the environment of vehicle, and wherein laser data comprises multiple laser data point, and is associated with the one or more objects in environment by the laser data point in described multiple laser data point.Function also comprise by described multiple laser data point, with environment in the uncorrelated given laser data point of described one or more object be defined as representing the object do not tracked in the given position relative to vehicle, and as the vehicle moves, determine that the object do not tracked remains on the roughly the same relative position relative to vehicle.The weather condition that function also comprises the environment identifying vehicle is sunny instruction.
In another example, provide system, it comprises at least one treater and data storage apparatus, and this data storage apparatus comprises and can be run with the instruction making system n-back test by least one treater described.Function comprises the laser data receiving and collect for the environment of vehicle, and wherein laser data comprises multiple laser data point, and is associated with the one or more objects in environment by the laser data point in described multiple laser data point.Function also comprise by described multiple laser data point, with environment in the uncorrelated given laser data point of described one or more object be defined as representing the object do not tracked in the given position relative to vehicle, and as the vehicle moves, determine that the object do not tracked remains on the roughly the same relative position relative to vehicle.The weather condition that function also comprises the environment identifying vehicle is sunny instruction.
In another example, provide equipment, it comprises the device of the laser data that the environment for receiving for vehicle is collected, and laser data comprises multiple laser data point.Equipment also comprises the device for determining laser data point in described multiple laser data point, that be associated with the one or more objects in environment.Equipment also comprises for based on laser data point uncorrelated with the described one or more object in environment, identifies that the weather condition of the environment of vehicle is the device of sunny instruction.
In other examples other, provide the equipment and the method that use onboard sensor to detect general weather condition.
Such as, in one example, the method provided comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Method also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point by computing equipment.Method also comprise by described multiple laser data point, with environment in the uncorrelated given laser data point of described one or more object be defined as representing the object do not tracked, and determined, by the instruction of the weather condition of computing equipment environment-identification based on one or more object do not tracked.
In another example, provide the non-transitory computer readable storage medium storing instruction, this instruction causes computing equipment n-back test when being run by computing equipment.Function comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point.Function also comprises and being associated with the one or more objects in environment by the laser data point in described multiple laser data point, and by described multiple laser data point, be defined as representing the object do not tracked with the uncorrelated given laser data point of described one or more object in environment.Function also comprises to be determined based on one or more object do not tracked, the instruction of the weather condition of environment-identification.
In another example, provide system, it comprises at least one treater and data storage apparatus, and this data storage apparatus comprises and can be run with the instruction making system n-back test by least one treater described.Function comprises the laser data receiving and collect for the environment of vehicle, and laser data comprises multiple laser data point, and is associated with the one or more objects in environment by the laser data point in described multiple laser data point.Function also comprise by described multiple laser data point, with environment in the uncorrelated given laser data point of described one or more object be defined as representing the object do not tracked, and determined based on one or more object do not tracked, the instruction of the weather condition of environment-identification.
In another example, provide equipment, it comprises the device of the laser data that the environment for receiving for vehicle is collected, and laser data comprises multiple laser data point.Equipment also comprises the device for being associated with the one or more objects in environment by the laser data point in described multiple laser data point.Equipment also comprise for by described multiple laser data point, be defined as representing the device of the object do not tracked with the uncorrelated given laser data point of described one or more object in environment, and for being determined, by the device of the instruction of the weather condition of computing equipment environment-identification based on one or more object do not tracked.
By taking the circumstances into consideration to read following detailed description in detail with reference to accompanying drawing, those of ordinary skill in the art will know these and other side, advantage and alternative.
Accompanying drawing explanation
Fig. 1 describes the functional block diagram according to the vehicle of example embodiment.
Fig. 2 depicts example vehicle, and it can comprise all or part function combined with reference to described by the vehicle of figure 1.
Fig. 3 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising wet surface according at least some embodiment described herein.
Fig. 4 determines the block diagram of the exemplary method of the instruction of the weather condition comprising wet surface further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.
Fig. 5 identifies that the surface that travels of vehicle is the example concept lateral plan of wet instruction thereon.
Fig. 6 identifies that the surface that travels of vehicle is another example concept diagram of wet instruction thereon.
Fig. 7 identifies that the surface that travels of vehicle is the example concept birds-eye view of wet instruction thereon.
Fig. 8 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising mist according at least some embodiment described herein.
Fig. 9 determines the block diagram of the exemplary method of the instruction of the weather condition comprising mist further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.
Figure 10 A is the example concept diagram that the weather condition of environment-identification comprises the instruction of mist.
Figure 10 B is the example concept diagram of the image of being caught by the vehicle in Figure 10 A.
Figure 11 A to Figure 11 B comprises and identifies that the environment of vehicle comprises the example concept lateral plan of the instruction of mist.
Figure 12 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising mist according at least some embodiment described herein.
Figure 13 is the block diagram of the exemplary method for determining the object in environment-identification mistakenly according at least some embodiment described herein.
Figure 14 be according at least some embodiment described herein utilize on-vehicle vehicle sensor to determine further weather condition is the block diagram of the exemplary method of sunny instruction.
Figure 15 A is the example concept diagram of sunny instruction based on the weather condition of camera image environment-identification.
Figure 15 B is the example concept diagram of the image of being caught by the vehicle in Figure 15 A.
Figure 16 comprises and identifies that the environment of vehicle is the example concept lateral plan of sunny instruction.
Figure 17 detects the block diagram of the exemplary method of weather condition according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.
Figure 18 determines the block diagram of the exemplary method of the instruction of weather condition further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.
Detailed description of the invention
Below describe in detail and be described with reference to the various Characteristic and function of accompanying drawing to disclosed system and method.In the drawings, unless context is pointed out in addition, otherwise the assembly that similar symbol logo is similar, and in order to graphic object, the assembly in accompanying drawing or accompanying drawing can not necessarily be drawn in proportion.Demonstrative system described herein and embodiment of the method are not intended to limit.Can easy understand, some aspect of disclosed system and method can be undertaken arranging and combining by multiple different configuration, and all these is susceptible in this article.
In this example, provide use onboard sensor detect weather condition and correspondingly revise the method and system of the behavior of vehicle.In some instances, certainly drive a car or autonomous vehicle under some weather condition, such as heavy rain, wet road, mist, direct sunlight etc., can not drive or also can drive, therefore, the behavior of autonomous vehicle can based on the weather condition detected.
In one example, provide a kind of method, it comprise receive for environment collect laser data.Computing equipment can determine the laser data point be associated with any object in environment.Based on laser data point uncorrelated with the object in environment, computing equipment can identify that surface that vehicle travels is wet instruction thereon.In other examples, based on laser data point uncorrelated with the one or more objects in environment, computing equipment can identify that the weather condition of the environment of vehicle comprises the instruction of mist.In other examples other, based on laser data point uncorrelated with the one or more objects in environment, computing equipment can identify that the weather condition of the environment of vehicle is sunny instruction.In some examples again, determined based on one or more object do not tracked, computing equipment can the instruction of general weather condition of environment-identification.
In concrete example, radar sensor possibly cannot detect rainwater, specifically, and rain/water such as described below: the vehicle travelled with certain speed is from the arc water kicked up on the surface (such as, the water of chicken shape of tail).But laser sensor can collect the laser data relevant with such water condition.Therefore, for any laser data (such as, laser data unmatched with the radar data received) received do not tracked, can determine that road may be wet instruction.
Further information can be used, the speed of such as vehicle, higher confidence level or the confirmation of wet road conditions are provided.Such as, if vehicle is not in mobile or slowly movement, the vehicle of this vehicle front is unlikely to be enough to make water kick up and the speed quick travel detected by laser sensor from road.Other information operable comprise rain detector information, from the information about weather of server or from the view data of camera being coupled to sensor.
In another concrete example, laser sensor may detect object through mist, and in fact, may receive the data point reflected by mist.For any laser data (such as, with tracking in environment or the known unmatched laser data of object) received do not tracked, the instruction of vehicle in the environment with mist weather condition can be determined.
Further information can be used to be higher confidence level or the confirmation of mist to provide weather condition.Such as, information can comprise from the information about weather of server, from the view data of camera or the data from radar sensor that are coupled to vehicle, and radar sensor can be seen through mist and detect not by object that laser is seen.
In another concrete example, sunlight can by causing additional laser data point or changing the wavelength of laser data point and pollute laser data.Therefore, some laser data of collecting may not represent the object in the environment of vehicle.By the data (such as using the object that laser or radar data track) with the object in environment, those contaminated laser data points can be identified, and based on identifying such contamination data, can obtain fair weather condition.Additional details can be accessed, to confirm fair weather condition further, the geographic position of such as vehicle and the time in one day, from the information about weather of server or from the view data of camera being coupled to vehicle.
Road is wet, weather condition is mist, weather condition instruction that is sunny or certain general weather condition can be useful for the safe driving action determining autonomous vehicle.Example action can comprise the instruction providing instruction request to be transformed into manual mode, if or remain on autonomous mode, be switched to specific to wet road pattern (namely, drive with slower speed, thus allow larger distance to complete braking etc.).
The example system will described in more detail within the scope of the disclosure now.Usually, example system can realize with the form of automobile or can adopt the form of automobile.But, example system can also realize with the form of other vehicles or take the form of other vehicle, such as car, truck, motor bike, bus, ship, aircraft, helicopter, lawnmower, Recreational Vehicle, amusement park vehicle, agricultural equipment, preparation of construction, tramway train, golf cart, train and electric car.Other vehicle is also possible.
Fig. 1 describes the functional block diagram according to the vehicle 100 of example embodiment.Vehicle 100 is configured to completely or partially with autonomous mode operation, and therefore can be called as " autonomous vehicle ".Such as, computer system 112 can control via the control command of the control system 106 of vehicle 100 vehicle 100 that is in autonomous mode.Computer system 112 can receive the information from sensing system 104, and carries out one or more control process (such as arranging working direction to avoid the obstacle detected) based on the information received under automated manner.
Vehicle 100 can be completely autonomous or part is autonomous.In part autonomous vehicle, some function can (such as, by chaufeur) Non-follow control within some times or full time alternatively.In addition, part autonomous vehicle can be configured to switch between complete manual operation mode and the autonomous and/or complete autonomous operation pattern of part.
Vehicle 100 can comprise each subsystem, such as propulsion system 102, sensing system 104, control system 106, one or more external equipment 108 and power supply 110, computer system 112 and user interface 116.Vehicle 100 can comprise more or less subsystem, and each subsystem can comprise multiple element.In addition, each subsystem of vehicle 100 and element can be interconnective.Therefore, one or more in the described function of vehicle 100 can be divided into additional function or physical assemblies, or are combined into less function or physical assemblies.In other example, additional function and/or physical assemblies can be added in the example shown in Fig. 1.
Propulsion system 102 can comprise the assembly that can operate to provide dynamic movement to vehicle 100.Depend on embodiment, propulsion system 102 can comprise engine/driving engine 118, energy source 119, driving device 120 and tire/wheel 121.Engine/driving engine 118 can be the engine of combustion engine, electrical motor, steam engine, stirling engine or other type and/or the combination in any of driving engine.In certain embodiments, propulsion system 102 can comprise polytype engine and/or driving engine.Such as, pneumoelectric hybrid vehicle can comprise Gasoline engine and/or electrical motor.Other example is also possible.
Energy source 119 can represent the energy source that can provide all or part of power to engine/driving engine 118.That is, engine/driving engine 118 can be configured to energy source 119 to be converted to mechanical energy with operating and driving device 120.The example of energy source 119 can comprise gasoline, diesel oil, other fuel based on oil, propane, other fuel based on pressure gas, ethanol, solar panel, battery, cond, flywheel, regeneration brake system and/or other electric power source etc.Energy source 119 also can provide energy for other system of automobile 100.
Driving device 120 can comprise the element that can operate mechanical power is sent to tire/wheel 121 from engine/driving engine 118.Such element can comprise change speed gear box, power-transfer clutch, diff, axle drive shaft and/or (one or more) axle, etc.Driving device 120 also can comprise other elements.Axle drive shaft can comprise one or more axles that can be coupled to one or more tire/wheel 121.
Tire/wheel 121 can be arranged with stably support vehicle 100, provides simultaneously and draws with the friction on the surface of the such as road of vehicle 100 movement thereon.Therefore, the tire/wheel 121 of vehicle 100 can be configured to various forms, comprises single wheel cycle, bicycle/motor bike, three-wheel vehicle or car/truck four-wheel form.Other tire/wheel geometric configuration is also possible, such as comprises six or more wheels those.Any combination of the tire/wheel 121 of vehicle 100 can be operable as and rotate relative to other tire/wheel 121 differential.Tire/wheel 124 can represent at least one wheel being fixedly attached to driving device 120 and at least one tire being coupled to the edge of wheel that can contact with running surface.Tire/wheel 121 can comprise the combination in any of metal and rubber, or other combination of materials.
Sensing system 104 generally includes and is configured to detect the one or more sensors about the information of the environment around vehicle 100.Such as, sensing system 104 can comprise global positioning system (GPS) 122, rainfall sensor 123, Inertial Measurement Unit (IMU) 124, RADAR (radio detection and range finding) unit 126, laser rangefinder and/or LIDAR (laser imaging detects and range finding) unit 128, camera 130 and/or microphone 131.Sensing system 104 also can comprise sensor (such as, the O being configured to the built-in system monitoring vehicle 100
2monitoring device, fuel ga(u)ge, oil temperature, vehicle-wheel speed sensor, etc.).One or more can being configured to be included in the sensor in sensing system 104 activates separately and/or jointly, so as to revise one or more sensor position and/or towards.
Sensor in sensing system 104 can be configured to provide the data by computer system 112 process in real time.Such as, sensor constantly can upgrade and export with the environment being reflected in a period of time or sense within a period of time, and continuously or provide the output of renewal on request to computer system 112, to make computer system 112 can in response to sensed environment to determine whether the direction that vehicle is current or speed should be modified.
GPS122 can be any sensor being configured to the geographic position estimating vehicle 100.For this reason, GPS122 can comprise and can operate to provide about the transceiver of vehicle 100 relative to the location information of the earth.
Rainfall sensor 123 can be installed in the below of the Windshield of vehicle 100 or can be incorporated to the Windshield of vehicle 100.Rainfall sensor also can be arranged on other position various, such as in the position of head lamp or the position etc. near head lamp.In one example, rainfall sensor 123 can comprise the set of the photodetector of one or more infrarede emitting diode (LED) and such as photodiode.The light launched by LED can by Windshield reflected light electric diode.The light that photodiode receives is fewer, and the rainfall of vehicle 100 outside can be indicated more.The amount of the light of reflection or some other designators of moisture conditions detected can be passed to computer system 112.
IMU124 can comprise the combination in any being configured to come the position of senses vehicle 100 and the sensor (such as, accelerometer and gyroscope) towards change based on inertial acceleration.
RADAR unit 126 can represent the system utilizing radio signal to carry out the object in the local environment of senses vehicle 100.In certain embodiments, except sensed object, RADAR unit 126 additionally can also be configured to speed and/or the working direction of sensed object.
Similarly, laser rangefinder or LIDAR unit 128 can be configured to use laser to carry out any sensor of the object of the environment that senses vehicle 100 is arranged in.Depend on embodiment, laser rangefinder/LIDAR unit 128 can comprise one or more lasing light emitter, laser scanner and one or more detector, etc.Laser rangefinder/LIDAR unit 128 can be configured to operate with relevant (such as, using Heterodyne detect) or noncoherent detection modes.
Camera 130 can comprise one or more equipment of the multiple images being configured to catch the environment around vehicle 100.Camera 130 can be still camera or video camera.In certain embodiments, camera 130 can mechanically move, and is such as moved by the platform rotated and/or be tiltedly installed with camera.Like this, the control treatment of vehicle 100 may be implemented as the movement controlling camera 130.
Sensing system 104 can also comprise microphone 131.Microphone 131 can be configured to catch the sound of the environment around from vehicle 100.In some cases, multiple microphone can be arranged to microphone array, maybe may be arranged as multiple microphone array.
Control system 106 can be configured to (one or more) operation controlling vehicle 100 and assembly thereof.Therefore, control system 106 can comprise various element, comprises steering unit 132, throttle 134, brake unit 136, sensor fusion algorithm 138, computer vision system 140, navigation/route control system 142 and obstacle-avoidance system 144 etc.
Steering unit 132 can represent the combination in any of the mechanism of the working direction that can be operable as adjustment vehicle 100.Such as, steering unit 132 can adjust (one or more) axle of one or more tire/wheel 121, to affect the turning of vehicle 100.Throttle 134 can be configured to the running velocity controlling such as engine/driving engine 118, and and then the speed of control vehicle 100.Brake unit 136 can comprise the combination in any being configured to the mechanism that vehicle 100 is slowed down.Such as, brake unit 136 can use friction to the tire/wheel 121 that slows down.In other embodiments, brake unit 136 makes tire/wheel 121 inductively slow down by the process of regenerative braking kinetic energy of tire/wheel 121 being converted to electric current.Brake unit 136 also can take other form.
Sensor fusion algorithm 138 can be configured to accept the data of sensor 104 as the algorithm (or computer program of storage algorithm) inputted.Described data can comprise the data such as representing the information sensed at the sensor place of sensing system 104.Sensor fusion algorithm 138 can comprise or be configured to use such as Kalman filter, Bayesian network or other algorithms to perform.Sensor fusion algorithm 138 can provide various evaluation based on the data from sensing system 104.Depend on embodiment, evaluate can comprise the individual subject in the environment of vehicle 100 and/or feature assessment, on the assessment of concrete situation and/or based on concrete situation on the assessment that may affect.Other evaluation is possible.
Computer vision system 140 can be can operate to process and analyze the image that caught by camera 130 to identify any system of object in the environment of vehicle 100 and/or feature, and described object and/or feature can comprise traffic signal, road boundary, other vehicle, pedestrian and/or obstacle etc.Computer vision system 140 can use object recognition algorithm, exercise recovery structure (StructurefromMotion, SFM) algorithm, video frequency tracking and other computer vision technique.In certain embodiments, computer vision system 140 additionally can be configured to the speed of environment being carried out to mapping, tracing object, estimation object, etc.
Navigation and route control system 142 can be any systems of the drive route being configured to determine vehicle 100.Such as, navigation and route control system 142 can determine a series of speed and working direction, so that while making vehicle 100 advance along the route based on track being directed to final destination on the whole, vehicle 100 is moved along the route substantially avoiding perceived obstacle, such as, described final destination can input according to the user via user interface 116 and arrange.Navigation and route control system 142 can additionally be configured to dynamically upgrade drive route when vehicle 100 is in operation.In certain embodiments, navigation and route control system 142 can be configured to merge the data from sensor fusion algorithm 138, GPS122 and one or more predetermined map, to determine the drive route of vehicle 100.
Obstacle-avoidance system 144 can represent the control system being configured to identify, assess and avoid or otherwise cross the potential obstacle in the environment of vehicle 100.Such as, obstacle-avoidance system 144 by the one or more subsystems in operation control system 106 to carry out turning to (swerving) manipulation, turn control, brake operation etc., can realize the change of the navigation of vehicle 100.In certain embodiments, obstacle-avoidance system 144 is configured to automatically determine that feasible (" available ") obstacle is avoided handling based on periphery transit mode, condition of road surface etc.Such as, obstacle-avoidance system 144 can be configured to when other sensing system vehicle 100 will turn in the adjacent domain entered vehicle, architectural barriers, other obstacles etc. detected time, do not carry out handling maneuver.In certain embodiments, obstacle-avoidance system 144 can automatically select not only can with but also the manipulation maximizing vehicle occupant safety is provided.Such as, obstacle-avoidance system 144 can select to be predicted to be the avoidance manipulation of the minimum amount of acceleration causing the main cabin of vehicle 100.
Control system 106 additionally or alternatively can comprise the assembly except shown and described those.
Vehicle 100 also comprises the mutual external equipment 108 between the user being configured to allow vehicle 100 and external sensor, other vehicle, other computer systems and/or the such as occupant of vehicle 100.Such as, the external equipment 108 for receiving information from occupant, external system etc. can comprise wireless communication system 146, touch-screen 148, microphone 150 and/or loud speaker 152.
In certain embodiments, external equipment 108 is for the mutual input of the user and user interface 116 that receive vehicle 100.For this reason, touch-screen 148 both can provide information to the user of vehicle 100, again user is sent to user interface 116 via the information that touch-screen 148 indicates.Touch-screen 148 can be configured to via the sensing such as capacitance sensing, resistance sensing, optics sensing, surface acoustic wave process from the touch location of the finger (or writing pencil etc.) of user and touch gestures.Touch-screen 148 can sense in or direction in its plane parallel with touch screen surface, move in the direction perpendicular to touch screen surface or the finger in this both direction, and also can sense the level of the pressure being applied to touch screen surface.The occupant of vehicle 100 also can utilize voice command interface.Such as, microphone 150 can be configured to receive the audio frequency (such as, voice command or the input of other audio frequency) from the occupant of vehicle 100.Similarly, loud speaker 152 can be configured to occupant audio frequency being exported to vehicle 100.
In certain embodiments, external equipment 108 for allow the external system of vehicle 100 and the such as equipment in its surrounding environment, sensor, other vehicles etc. and/or provide useful information (such as traffic information, Weather information etc.) around about vehicle physically away from the communication between controller, server etc. of vehicle 100.Such as, wireless communication system 146 can directly or via communication network and one or more equipment radio communication.Wireless communication system 146 can use 3G cellular communication alternatively, such as CDMA, EVDO, GSM/GPRS and/or 4G cellular communication, such as WiMAX or LTE.Additionally or alternatively, wireless communication system 146 can such as use WiFi to communicate with WLAN (WLAN).In certain embodiments, wireless communication system 146 such as can use infrared link, short range links etc. and equipment direct communication.Wireless communication system 146 can comprise one or more Dedicated Short Range Communications, (DSRC) equipment, and it can comprise the public and/or private data communication between vehicle and/or road side station.For sending and receive other wireless protocols of embedding information in the signal, such as various automobile-used communication system, also can be adopted by the wireless communication system 146 in context of the present disclosure.
Electric power can be supplied to the assembly of vehicle 100 by power supply 110, the electronics package in such as external equipment 108, computer system 112, sensing system 104 etc.Power supply 110 can comprise for storage of electrical energy and the rechargable lithium ion from electric energy to each Power Supply Assembly or the lead-acid battery that discharge.In certain embodiments, one or more battery pack can be configured to provide electric power.In certain embodiments, power supply 110 and energy source 119 can together with realize, as in some all-electric cars.
Many or all functions of vehicle 100 can control via computer system 112, this computer system 112 receives the input from sensing system 104, external equipment 108 etc., and suitable control signal is communicated to propulsion system 102, control system 106, external equipment 108 etc., to realize the autonomous operation of vehicle 100 based on the surrounding environment of vehicle 100.Computer system 112 can comprise at least one treater 113 (it can comprise at least one microprocessor), and it performs the instruction 115 be stored in the non-transitory computer-readable medium of such as data storage apparatus 114.Computer system 112 can also represent and can be used for controlling in a distributed fashion each assembly of vehicle 100 or multiple computing equipments of subsystem.
In certain embodiments, data storage apparatus 114 can comprise instruction 115 (such as, programming logic), and instruction 115 can perform various automobile function by treater 113, comprises above those functions described in conjunction with Figure 1.Data storage apparatus 114 can also comprise additional instruction, comprise for to/from the one or more sending/receiving data in propulsion system 102, sensing system 104, control system 106 and external equipment 108, mutual and/or control their instruction with them.
Except instruction 115, data store 114 can also store data, such as road-map, route information and other information.Such information can at vehicle 100 independently, partly independently and/or during operate in manual mode to be used by vehicle 100 and computer system 112.
The input that vehicle 100 and the computer system 112 that is associated provide information to the user (occupant in the main cabin of such as vehicle 100) of vehicle 100 and/or receive from the user of vehicle 100.Therefore, vehicle 100 can comprise user interface 116, for providing information to the user of vehicle 100 or receiving input from it.The content of the interaction figure picture that user interface 116 can control or enable control can show on touch-screen 148 and/or layout.In addition, user interface 116 can comprise the one or more input-output apparatus in one group of external equipment 108 of such as wireless communication system 146, touch-screen 148, microphone 150 and loud speaker 152.
The input of computer system 112 based on the instruction vehicle received from each subsystem (such as, propulsion system 102, sensing system 104 and/or control system 106) and/or environmental conditions and the input from the indicating user preference of user interface 116 are to control the operation of vehicle 100.Such as, computer system 112 can utilize input from control system 106 to control steering unit 132, to avoid the obstacle detected by sensing system 104 and obstacle-avoidance system 144.Computer system 112 can be configured to the many aspects controlling vehicle 100 and subsystem thereof.But, usually, regulation manual priority auto mat driver behavior, such as in case of emergency, or only in response to user activated priority scheduling.
The mode that the assembly of vehicle 100 described herein can be configured to interconnect works together with other assembly in or beyond they separately system.Such as, camera 130 when vehicle 100 operates in autonomous mode, can catch the multiple images represented about the information of the environment of vehicle 100.Environment can comprise other vehicles, traffic lights, traffic sign, pavement marker, pedestrian etc.Computer vision system 140 can based on the Object identifying model be stored in advance in data storage apparatus 114 and/or by other technologies, classifies synergistically and/or the various aspects of environment-identification with sensor fusion algorithm 138, computer system 112 etc.
Although Fig. 1 shows the various assemblies of vehicle 100, namely, wireless communication system 146, computer system 112, data storage apparatus 114 and user interface 116 are integrated in vehicle 100, but one or more can being installed on vehicle 100 or discretely in these assemblies is associated with vehicle 100.Such as, data storage apparatus 114 partly or wholly can separate with vehicle 100 and exists.Therefore, vehicle 100 can provide with the form of the equipment component that can place discretely or together.The equipment component forming vehicle 100 can be coupled in wired and/or wireless mode usually communicatedly.
Fig. 2 depicts example vehicle 200, and it can comprise with reference to figure 1 in conjunction with all or some functions described by vehicle 100.Although for graphic object, example vehicle 200 is shown as four-wheel car type automobile in fig. 2, and the disclosure is not limited thereto.Such as, example vehicle 200 can represent the vehicle of any type.
Example vehicle 200 comprises sensor unit 202, wireless communication system 204, LIDAR unit 206, laser ranging unit 208 and camera 210.In addition, example vehicle 200 can comprise any assembly described by the vehicle 100 of composition graphs 1.
Sensor unit 202 is arranged on the top of example vehicle 200, and comprises the information that is configured to detect about the environment around example vehicle 200 and export one or more sensors of the instruction of this information.Such as, sensor unit 202 can comprise the combination in any of camera, RADAR, LIDAR, apomecometer and acoustic sensor.Sensor unit 202 can comprise one or more movable supporting frame, its can operate to regulate the one or more sensors in sensor unit 202 towards.In one embodiment, movable supporting frame can comprise rotation platform, its can scanning sensor to obtain information from each direction around example vehicle 200.In another embodiment, the movable supporting frame of sensor unit 202 can move with scan mode in specific angle and/or bearing range.Such as, sensor unit 202 can be installed on the top at the top of automobile, but other installation sites are possible.In addition, the sensor of sensor unit 202 can be distributed in different positions, and need not be configured in single position.Some possible sensor types and installation site comprise RADAR unit 206 and laser ranging unit 208.In addition, each sensor of sensor unit 202 can be configured to move independent of other sensor of sensor unit 202 or scan.
As depicted in Figure 2, wireless communication system 204 can be positioned at the roof of example vehicle 200.Alternatively, wireless communication system 204 can be positioned at other places whole or in part.Wireless communication system 204 can comprise radio transmitters and receptor, and it can be configured to the devices communicating with the outside or inside of example vehicle 200.Specifically, wireless communication system 204 can comprise the transceiver be configured to the computing device communication in other vehicles and/or such as automobile-used communication system or road side station.The example of this automobile-used communication system comprises Dedicated Short Range Communications, (DSRC), RF identification (RFID) and other communication standards of advising for intelligent transportation system.
Camera 210 can be light activated instrument, such as still camera, video camera etc., and it is configured to multiple images of the environment of catching example vehicle 200.For this reason, camera 210 can be configured to detect visible ray, and can addition or alternatively be configured to detect the light from other parts of frequency spectrum, such as infrared light or ultraviolet light.Camera 210 can be two-dimensional detector, and can have the susceptibility of three dimensional space distance alternatively.In certain embodiments, camera 210 can comprise, and such as, distance detector, it is configured to generate instruction from camera 210 to the two dimensional image of the distance of the some points environment.For this reason, camera 210 can use one or more distance detection technique.
Such as, camera 210 can provide range information by using structured light technique, and wherein example vehicle 200 utilizes predetermined pattern of light, such as grid or gridiron pattern pattern, object in environment is irradiated, and uses camera 210 to detect the reflection of the predetermined pattern of light from ambient environment.Based on the distortion in the light pattern of reflection, example vehicle 200 can determine the distance of the point on object.Predetermined pattern of light can comprise infrared light, or other are applicable to the radiation of the wavelength of such measurement.
The front windshield that camera 210 can be installed in example vehicle 200 is inner.Particularly, camera 210 can be positioned as catching relative to example vehicle 200 towards the image in the forward direction visual field.Also other installation site and the visual angle of camera 210 can be used, in the inside of example vehicle 200 or outside at it.
Camera 210 can have the optics be associated, and it can operate to provide adjustable visual field.In addition, camera 210 can utilize moveable support to be installed to example vehicle 200, such as to change the sensing angle of camera 210 via shake/leaning device.
Fig. 3 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising wet surface according at least some embodiment described herein.The embodiment of method 300 put forward the methods shown in Fig. 3, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200, or is used by the assembly of vehicle 100 or vehicle 200.Such as, process described herein can perform by being installed on autonomous vehicle (such as, vehicle 200), that communicate with computer system 112, sensor fusion algorithm 138 and/or computer vision system 140 RADAR unit 126, LIDAR unit 128 or camera 130.Method 300 can comprise one or more operation, function or action, as shown in by one or more square frame 302-306.Although these square frames illustrate sequentially, these square frames can executed in parallel in some cases, and/or performs with the order being different from order described herein.And based on the implementation expected, each square frame can be combined into less square frame, is divided into additional square frame, and/or deleted.
In addition, for method 300 and other processes disclosed herein and method, diagram of circuit shows a function in the cards and the operation of these embodiments.In this, each square frame can the module of representation program code, fragment or part, and it comprises and can perform one or more instructions (such as, machine readable code) for the specific logical function in implementation procedure or step by treater.Program code can be stored in the computer-readable medium of any type, such as such as comprises the storage equipment of dish or hard disk drive.Computer-readable medium can comprise non-transitory computer-readable medium, and such as such as register memory, processor cache and random access storage device (RAM), the short time stores the computer-readable medium of data.Computer-readable medium also can comprise the non-state medium of such as secondary or permanent long-term storage apparatus and so on, such as, as read-only memory (ROM) (ROM), CD or disk, compact disk read-only memory (ROM) (CD-ROM).Computer-readable medium also can be other volatibility any or Nonvolatile memory system.Computer-readable medium can be counted as such as computer-readable recording medium, tangible storage equipment, computer program or other goods.
Non-transitory computer-readable medium also can be distributed between multiple data storage elements, and the position of these data storage elements can be away from each other.Run the computing equipment of some or all stored in instruction can be vehicle, all example vehicle 200 as shown in Figure 2.Alternatively, run the computing equipment of some or all stored in instruction can be other computing equipments, such as server.
In addition, for method 300 disclosed herein and other process and method, each square frame in Fig. 3 can represent by line with the circuit of the specific logical function in implementation.
Exemplary method, the method 300 of such as Fig. 3, can be performed by vehicle and subsystem thereof whole or in part.Therefore, exemplary method can be described to be realized by vehicle in this article in an illustrative manner.But, be to be understood that exemplary method can whole or in part by other computing equipment of vehicle or realize with other computing equipment of Vehicles separation.Such as, exemplary method can be realized by server system whole or in part, and described server system is from equipment, and those equipment be such as associated with vehicle, receive data.Can other example of the computing equipment of realization example method or the combination of computing equipment be possible.
At square frame 302, method 300 comprises the laser data receiving and collect for the environment of vehicle.Laser data comprises the multiple laser data points based on the object in environment, and described object is perceived as physics to be existed due to reflection or rear orientation light.The assembly of vehicle or vehicle, such as computing equipment or treater, can be configured to the laser data collected by receiving.
As an example, vehicle can have LIDAR unit, its irradiate the surrounding of vehicle, surrounding environment, front, rear, side or any near or relevant region, and detection of reflected light.In operation, LIDAR finite element rotation also (such as, periodically) Emission Lasers bundle.Then received by the reflection of the object in environment to the laser beam launched by suitable sensor.The reception of reflected signal is added timestamp and allows each reflected signal (if receiving) to be associated with the laser pulse launched recently, and measure the time delay between the transmitting of laser pulse and the reception of reflected light.By converting according to the speed of light in middle air, time delay provides the estimation of the distance of reflectance signature.By the range information of each reflected signal and the position allowing to determine in three dimensions reflectance signature towards combining of LIDAR equipment being used for each impulse ejection.For illustrative purposes, can in conjunction with the single sweep operation of the LIDAR equipment of the position for estimating the series of points being arranged in x-y plane, describe environment scene in two dimensional x-y plane.But, it should be noted that, by its to scene upper once scanning time adjustment light beam turn to optics to be guided up or down from x-y plane by laser beam, or the additional laser of sampling by providing the some position in the plane that is exclusively used in above and below to x-y plane and the beam steering optics be associated thereof, or by the combination of these two kinds of modes, provide more complete three-dimensional sample.
In square frame 304, method 300 comprises the laser data point be associated with the one or more objects in environment determined by computing equipment in multiple laser data point.As an example, follow-up system can be used to tracing object, and can determine the laser data point that is associated with the object tracked.
In other examples, the some cloud corresponding with the object in environment can be generated.Each point in some cloud can pass through azimuth (such as, when launching corresponding with this some pulse LIDAR equipment towards, its by LIDAR anglec of rotation mirror (rotatingangledmirror) towards determining) and sight line (LOS) distance (such as, by distance that the time delay between impulse ejection and reflected light receive indicates) mark.For causing the pulse returning reflected signal, the distance in point diagram can be set to the ultimate range sensitivity of LIDAR equipment alternatively.Ultimate range sensitivity can postpone to determine according to the maximum time of the optical pickocff the be associated reflected signal to be returned such as after each impulse ejection, this maximum delay itself can according to when the prediction reflectivity etc. of given ambient lighting conditions, exomonental intensity, environmental characteristic, and reflected signal is arranged in the expection signal strength of specified distance.In some instances, ultimate range can be approximately 60 meters, 80 meters, 100 meters or 150 meters, but for the concrete configuration of LIDAR equipment with the optical pickocff be associated, other example is also possible.
In certain embodiments, the laser data collected by the sensor fusion algorithm 138 shown in Fig. 1, computer vision system 140 and/or computer system 112 can be configured to explain separately and/or in conjunction with the instruction of other sensor information and/or explain collected laser data based on the pattern match point cloud of memory device and/or the baseline chart of environment so that by the classification of the group of point or be identified as corresponding to the object in environment.
In addition, each spatial point can be associated to from the corresponding laser in one group of laser and corresponding timestamp.That is, comprise at LIDAR in the embodiment of multiple laser, each spatial point received accordingly can to according to the corresponding spatial point detection received to particular laser be associated.In addition, each corresponding spatial point can be associated with corresponding timestamp (such as, laser be launched or receive time).By this way, the spatial point received can be organized, be identified or otherwise be sorted based on space (laser mark) and/or time (timestamp).Such sequence can be that significant order is helped or improves the analysis to spatial point data by allowing spatial point Organization of Data.
In some instances, object detection is provided in conjunction with example LIDAR equipment.LIDAR equipment can be configured to use one or more laser to catch laser point cloud image.Laser point cloud comprises the many points for each pulse of launching from LIDAR equipment; Reflected signal can indicate the actual position of reflective object, and fails to receive reflected signal instruction and in specific range, there is not actv. reflective object along the direction of visual lines of laser.Depend on various factors, comprise laser pulse rate, scene refresh rate, total solid angle of being sampled by each LIDAR equipment (or when only using a LIDAR equipment, only total solid angle of scene), the number of the sample point in each some cloud can be determined.Some embodiments can provide the some cloud of the point, the point of 80000 laser designations, the point of 100000 laser designations etc. with nearly 50000 laser designations.Usually, angle resolution in as one of the number of the point of the laser designation in each some cloud and as refresh rate on the other hand between compromise.LIDAR equipment is driven to determine that relevant sufficiently high refresh rate is to provide angle resolution to the real-time navigation of autonomous vehicle.Therefore, LIDAR equipment can be configured to predetermined time interval, such as 100 milliseconds (to realize the refresh rate of 10 frames per second), 33 milliseconds (to realize the refresh rate of 30 frames per second), 1 millisecond, 1 second etc., catch one or more laser point clouds of scanning area.
With reference to figure 1, the data storage apparatus 114 of the computer system 112 of vehicle 100 can storage object detector software, code or other programmed instruction.Such object detector software can comprise that above-described to comprise in the control system 106 of sensor fusion algorithm 138, computer vision system 140 and/or obstacle-avoidance system 144 one or more, or their part.Object detector can be the arbitrary disposition of software and/or hardware, it is configured to by classify to object based on the one or more sensors in the laser point cloud of being caught by LIDAR128 and/or sensor based system 104 and/or identify, with the feature in perception environment scene.When laser point cloud be catch via LIDAR128 time, indicate the data of some cloud of catching to be communicated to object detector, object detector analyzes data to determine whether there is object in laser point cloud.The object indicated by a cloud can be, such as, and vehicle, pedestrian, road sign, traffic lights, cone etc.
In order to determine whether there is object in laser point cloud image, the classification of the arrangement of the point of laser designation with pattern match object, environmental characteristic and/or object or feature can be associated by object detector software and/or module.Object detector can by pre-loaded (or dynamically being indicated) to associate arrangement according to one or more parameters corresponding with the physical object/feature in the environment around vehicle 100.Such as, object detector can by pre-loaded to indicate the length of the exemplary height of pedestrian, typical automotive, information to the confidence threshold value that object of suspicion is classified etc.
When object in object detector identification point cloud, object detector can define the Bounding Box surrounding this object.Such as, Bounding Box can correspond to the outside face of the prediction of the object of some cloud instruction.Certainly, border " box " form that the multiaspect of the exterior boundary of the prediction of defining objects generally can be taked close-shaped.
For each some cloud of catching, position and the corresponding borders thereof of the object of perception are associated with frame number or frame time.Therefore, the object appearing at the similar shape of roughly analogous location in the continuous scanning of scene can be associated with each other, so that tracing object in time.For the object of the perception appeared in multiple somes cloud frames (such as, the scanning completely of scanning area), occur each frame thereon for object, object can be associated with the clear boundary shape of dimensional extent of the object defining perception.
To move relative to vehicle when vehicle 100 drives through the environment around it and/or when object thus through the scanning area of LIDAR equipment 128, the object of perception can be followed the trail of.Combine the mobile message that two or more some clouds of catching continuously can allow the object determining to detect thus.Such as by acceleration/accel and/or the speed of the object of observation (such as together with vehicle 100 along the automobile of road movement), the prediction of in the future position can be made for the object of the motion outline with characterization, so as in follow up scan the position of forecasting object.In certain embodiments, the object of movement is aloft assumed that the track along being affected by gravity moves.
In order to help to provide Object identifying, vehicle 100 also can communicate with object recognition server (such as, via wireless communication system 146).Object recognition server can verify the object that uses object detector to detect by vehicle 100 and/or to its classification.In addition, object recognition server can promote that one or more parameters of the object being used for object detector detecting in the laser point cloud of catching based on the data of the accumulation from other similar systems, local condition are optimized.In one embodiment, object bounds and their corresponding image parameters can communicate to object recognition server for verifying that the object of institute's perception is correctly validated by vehicle 100, such as by indicating the assessment of the correct statistical likelihood identified.
Referring back to Fig. 3, at square frame 306, method 300 comprises based on laser data point uncorrelated with the one or more objects in environment, and the surface travelled thereon by computing equipment identification vehicle is wet instruction.In one example, laser data can be relevant with the existence of water on the surface, and water be the follow-up system of vehicle follow the trail of less than project.But collected laser data point may caused by water.Such as, for detecting the LIDAR equipment of also positioning optical reflectance signature, the water on road surface or the water kicked up by vehicle are similar to reflecting cloud, because light pulse is reflected from the particulate of such as water droplet.Therefore, for any laser data (such as, laser data unmatched with the object tracked) received do not tracked, can determine that surface or road may be wet instructions.
In addition (or alternatively), at square frame 306, method 300 can comprise determines that the number of the uncorrelated laser data point with the one or more objects in environment exceedes predetermined threshold, and the surface then travelled thereon by computing equipment identification vehicle is wet instruction.Such as, if there is the laser data point uncorrelated with object of minority, then this may cause due to many factors; But for the laser data point of certain number of thresholds, higher possibility can be, these laser data points are that the laser beam owing to launching is reflected by water.
In other example, method 300 can also comprise the quantity of the laser data point be associated with the position below the expection height on the surface travelled at vehicle determined in multiple laser data point, and identifies that surface that vehicle travels is wet instruction based on the quantity of laser data point thereon higher than number of thresholds further.Such as, when road is wet, the surface of road can be revealed as minute surface to laser beam, thus causes the laser beam reflected to be revealed as and to be associated with the position below the height of road surface.Thus some water can as the minute surface of reflects laser, and some laser can through water and by road surface reflection simultaneously.The angle of the reflected beam received can be determined, and if angle is higher than threshold value (relative to LIDAR equipment), then such laser data received can be represented as in ground level " below ".When collecting such laser data, computing equipment can use the mark of this laser data to be wet instruction as surface.
In this example; consider and usually data point can be detected at and lower slightly place slightly higher than the expection road surface in additional detailed map information; therefore can service range threshold value; such as above and below surperficial 15-20 centimetre, the cloud of data point be associated with dry roads surface and the cloud sector be associated with puddle are separated.Therefore, for the cloud of the laser data point close to (as determined from detailed map) expection road surface, by laser data point with in position higher than expect road surface first threshold and in position lower than expecting that the Second Threshold of road surface compares.When cloud comprises at least one the laser data point higher than first threshold and at least one the laser data point lower than Second Threshold, can obtain road is wet instruction.
In one example, when one or more lasers of vehicle move forward, vehicle can collect the data point comprising scope and strength information for same position (point or region) from some directions and/or in the different time.Such as, each data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it.With high reverse--bias surface, such as lane markings, the strength rating be associated can higher than low reflecting surface, such as pitchy road, cement road or other road surface.Similarly, the strength rating be associated with the dark objects (black, Dark Blue, brown etc.) absorbing more light can lower than the light object (white, cream-coloured, silver color etc.) that can reflect more light.In this respect, when to as if wet time, it can become more dark, and therefore, water can reduce the strength rating of object, instead of value of gaining in strength.
In some instances, the strength rating of laser spots data can be determined scale, and such as, from 0 to 250, wherein 0 is dark and 250 is bright.Therefore, specularity more by force, brighter surface can be associated with the strength rating closer to 250, and more weak, the darker surface of specularity can be associated with the strength rating closer to 0.Laser scanning data can be received and process to generate geographical position coordinates.These geographical position coordinates can comprise the GPS latitude and longitude coordinate (x, y) with altitude component (z), or can be associated with other system of axess.The result of this process is one group of data point.Each data point in this group data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it: (x, y, z).
The mean intensity of the laser data point of road can compare with threshold value, to identify that whether road is wet instruction further.Such as, as mentioned above, the water of the wet or ice field of covering path can be reduced in the intensity of the laser data that road is collected.Therefore, the average intensity value of the laser data that wet road has can slightly lower than dry roads (such as cement road, pitchy road, brick road etc.) and other expection roadway characteristics of appearing in cartographic information, the strength rating of the laser data of such as lane markings etc.If the laser data the checked point of certain percentum has the strength rating lower than certain threshold value, then can determine that road is wet.Such as, use the scale of as above 0 to 250, if there are at least 850 points (or at least 85% of these laser data points) in 1000 laser data points and this 1000 points to have intensity lower than threshold value 10 on road, then road is dark-coloured.As mentioned above, low intensity value can indicate road to be wet high probability.Also can use such as based on the expection intensity of road or other threshold values of composition and percentum.
In further example, method 300 can comprise the position determining vehicle, position based on vehicle determines the distribution of the anticipated value of the laser data on the surface travelled for vehicle in dry conditions thereon, and the laser data that determined distribution and the environment for vehicle are collected is compared.The distribution of the anticipated value of the laser data on the surface travelled thereon for vehicle in dry conditions can such as based on the position of vehicle, from data storage apparatus retrieval or receive from server.For drying condition and wet both conditions, the map of the intensity of the laser data by road distribution can be generated and stored.The distribution of the anticipated value of laser data can based on the composition on surface.Based on the difference between determined distribution and the laser data of collecting for the environment of vehicle higher than threshold value, can identify surface that vehicle travels thereon is the second wet instruction.Such as, if described difference is very high, then the laser data that this laser data and expection are seen for desiccated surface is not mated.Therefore, such can indication surface be relatively another wet instruction.
In further example, method 300 can comprise and such as compares the laser data point of the lane markings in the environment of the instruction vehicle determined in multiple laser data point by object detection technique or with map datum, and based on the number of the lane markings indicated by laser data point lower than desired amount, the surface that cause computing equipment identification vehicle travels thereon is the second wet instruction.Such as, road surface generally includes lane markings or other bright mark, and when road surface is wet, the intensity of reflected beam can lower than the intensity when road surface is dry.The intensity of the reduction of reflected beam possibly cannot be treated to lane markings on instruction road surface or bright mark.On typical road, the number of lane markings can be determined from stored data, and when identifying lane markings fewer than expection, and this can be used as surface is the second wet instruction.The data stored of example can comprise identify lane markings position, height and shape the additional detailed map information of lane markings.Lane markings can comprise such as solid line or discontinuous two-lane line or bicycle diatom, solid line or discontinuous lane mark, reverberation etc.Given track can be associated with other lane markings on left and right lane mark or the border limiting track.Therefore, track can with the right hand edge of the left hand edge of a lane mark and another lane mark for boundary.
In this example, using method 300, vehicle can be configured to operate in autonomous mode, and the surface that vehicle can be utilized thereon to travel is that wet instruction is to determine that the driving of vehicle determines.May expect to control vehicle in a different manner according to various weather condition or road conditions, therefore, when road is wet, vehicle can operate according to " wet surface " driving technique (such as, allow more braked space, underspeed).Vehicle also possibly cannot operate under some weather condition, therefore, in some instances, during instruction based on wet surface, vehicle can provide instruction to be transformed into manual mode with instruction request, such as by providing alarm to be started the operation to vehicle by forbidding autonomous mode to chaufeur.
In further example, can determine based on the number of uncorrelated laser data point the degree that road is wet.In the example that road is very wet, the reflection that can there is a large amount of uncorrelated laser data points and see in camera data.The operation mode of vehicle can further based on the degree that road is wet, such as autonomous mode can be adopted for moist road, and road being considered to the example that wets very much, vehicle can be transformed into manual operation mode or provide the request being transformed into manual operation mode.
Except make drive determine except, or as to make drive decision replacement, road weather condition and/or pavement conditions can trigger the alarm being sent to chaufeur.Alarm can ask chaufeur to control vehicle, or also can provide the alarm about described condition to chaufeur simply.Alarm can be any other signal of audible signal, visual signal, sense of touch or sensory signal and/or the attention obtaining chaufeur.In this illustration, after alerting driver, vehicle can receive input from chaufeur, such as steering wheel rotation, applying braking, applying accelerator, pressing emergency cut-off etc.
In some instances, (except with the object tracked uncorrelated laser data point except) additional data can be considered to provide the further instruction on wet surface, or provide surface to be more high probability or the confidence of wet (such as, uncorrelated laser data point is caused by water).
Fig. 4 determines the block diagram of the exemplary method of the instruction of the weather condition comprising wet surface further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.Method 400 shown in Fig. 4 proposes the embodiment of method, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200 (or being used by the assembly of vehicle 100 or vehicle 200), and additionally can perform outside the method 300 shown in Fig. 3.Method 400 can represent the module of program code, fragment or part, and this program code comprises and can perform one or more instructions (such as, machine readable code) for the specific logical function in implementation procedure or step by treater.
As shown in square frame 402, method 400 can comprise determine vehicle speed whether higher than threshold value, if like this, then method 400 is included in frame 404 and identifies that surface that vehicle travels is wet instruction thereon.Such as, the laser data received can be cause owing to travelling the water kicked up at the vehicle of this vehicle front or side, and such water can at vehicle to be kicked up when at least threshold velocity travels.Also can receive the laser data that the tail gas due to automobile causes, this speed usually occurring in automobile lower than during threshold value or idle running time.Therefore, when the speed of vehicle is higher than threshold value, uncorrelated laser data can be considered to caused by the water owing to kicking up from automobile, because automobile is can cause water by the speeds kicked up.
Its speed can be the vehicle being configured to autonomous operation and having the LIDAR unit for collecting laser data by the vehicle determined.In other example, the speed of such as, vehicle in autonomous vehicle front (or substantially in the front of vehicle, the direct front at autonomous vehicle or the adjacent lane in its front) can be determined, to determine the speed of the vehicle of " water of kicking up ".When the speed (or this vehicle follow the speed of automobile) of vehicle is higher than threshold value, for the uncorrelated laser data point substantially in the region of the environment of vehicle front can by this vehicle the automobile of the following instruction of water of kicking up.
If the speed of vehicle is not higher than threshold value, then method 400 may be advanced to square frame 406, wherein method 400 comprise determine laser data whether with RADAR data consistent.Such as, autonomous vehicle can collect the RADAR data of the environment of vehicle, and the instruction of RADAR data exists object in vehicle environmental.For the water kicked up, RADAR incites somebody to action not return data, but RADAR data will be identified in the automobile in the front of autonomous vehicle.Laser data is also used to tracing object as mentioned above, when the existence of the object indicated by RADAR data and the object indicated by LIDAR data exist inconsistent time, LIDAR data can be considered to off-square.But the instruction of the water indicate any extra object do not indicated by RADAR data can be water on road by LIDAR data, being kicked up by automobile etc.
In some instances, but by LIDAR data tracing to any object of not seeing in RADAR data can be classified as being because weather condition causes.As illustrated by method 400, due to RADAR data and LIDAR data inconsistent, can obtain surface that vehicle travels is wet instruction.
Method 400 may be advanced to square frame 408, and wherein method 400 comprises and determines that whether laser data is consistent with camera data.Such as, autonomous vehicle can comprise camera, and can receive the view data that the environment for vehicle collects.Large in the rain, water can gather on the way, and can be visible in the picture.In addition, identify that rain is also possible in the picture.Object detection technique can be adopted to process image, such as, by image being supplied to object detection server and receiving the response of the object in indicating image.
In some instances, can image data processing to identify wet surface, and when the object indicated by the object indicated by camera and LIDAR data is inconsistent, also can obtain surface that vehicle travels is wet instruction.
In further example, can image data processing to determine the amount of the view data of the reflection of the object be included in the environment of vehicle.Such as, when rainy, other object on brake lamp and road provides the reflection in image.The reflection that many technology are come in detected image can be used, such as, reflection is modeled as the region comprising two different layers moved relative to each other by the path of motion of analytical characteristic point.Based on the amount of view data of the reflection of the one or more objects in the environment of instruction vehicle higher than threshold value, can obtain surface that vehicle travels is the second wet instruction.
Method 400 may be advanced to square frame 410, and wherein method 400 comprises and determines that whether laser data is consistent with other sensing datas.Such as, autonomous vehicle can have rainfall sensor or " rain detector ", and when the existing of the wet surface of LIDAR data instruction and rainfall sensor instruction rainfall, and can obtain surface that vehicle travels is the second wet instruction.The output possibly cannot depending merely on rainfall sensor determines whether surface is wet, because stopping a period of time inside face after raining can remain wet, or surface can be gathered due to puddle or other water but wet when not raining.
In further example, the data that processing is provided by various sensor can comprise process previous time point obtain and expect the environmental data that is continuously present in environment.Such as, detailed cartographic information (such as, identify the very detailed map of the shape of road and height, four corners, cross walk, speed restriction, traffic signal, building, label, real-time traffic information or other such object and information) can be provided or via database access, and the expection brightness of different sections of highway or the data of laser intensity data value of description road can be comprised.When laser data is not mated with expection brightness or laser intensity data value, the instruction on the surface of wetting can be determined.
Method 400 may be advanced to square frame 412, and wherein method 400 comprises and determines that whether laser data is consistent with weather data.Such as, the Weather information of the position of vehicle can be received from server by network, and when Weather information instruction is being rained or when raining recently, can obtain road based on Weather information is the second wet instruction.
Method 400 may be advanced to square frame 414, and wherein method 400 comprises reprocessing laser data to attempt identified object and to be compared by the object of follow-up system identification.Such as, method 400 can also comprise based on to the access by the determined information of other automobiles on road, confirms that whether surface is wet determination.
In these examples, the surface that method 400 can be performed to provide vehicle to travel is wet further instruction.Except the method 300 of Fig. 3, can also manner of execution 400 alternatively.Such as, uncorrelated laser spots (such as, represent the water of chicken shape of tail) can be uncorrelated relative to the vehicle of laser detection or may be uncorrelated relative to the vehicle of detections of radar, and be wet according to being verified by cross reference other sensing data, Weather informations etc. for identified surface.
Fig. 5 identifies that the surface that travels of vehicle is the example concept lateral plan of wet instruction thereon.In Figure 5, vehicle 502 travels on surface 504, and another vehicle 506 travels in the front of vehicle 502.Vehicle 502 can comprise LIDAR unit 508, and it is configured to receive the laser data of collecting for the front region (such as, as illustrated by arrows 5) of vehicle 502.Vehicle 506 can travel puddle on surface 504 or other fluid/water gathered, and can at the afterbody 510 of vehicle 506 and/or water of kicking up in the side 512 of vehicle 506.When laser beam is reflected by water, water can be detected by LIDAR unit 508.But water is not the object that the follow-up system of vehicle 502 tracks, because other sensor (such as, RADAR) possibly cannot detect that As time goes on water or LIDAR unit 508 may can not track water always.When the laser data received not match tracing arrive object time, uncorrelated laser data can be regarded as the instruction of water and surface 504 is wet instructions.Laser data can compare with the chicken shape of tail point cloud that can produce due to the water kicked up below at the object of movement that is predefined or that store.Such as, chicken shape of tail water can be detected as laser reflection, but is not object tracked in environment.
Therefore, whether vehicle 502 can detect water by the uncorrelated or random laser spots of collecting from the afterbody of vehicle 506 via process and be kicked up by the tire of vehicle, thus identified surface 504 is wet instructions.In this respect, vehicle 502 can detect the one or more random number strong points cloud after the rear tyre of vehicle 506, and as the result of water movement, the position of the data point in cloud and quantity can constantly change.Therefore, the cloud from the data point of water can not have clear and definite structure, and a part for solid object, the tail end of such as vehicle, is associated with the data point defining clear surface.Similar point cloud data also can find at the rear of other vehicles.Such discovery can indicate rainy or ground to be wet.
Fig. 6 identifies that the surface that travels of vehicle is another example concept diagram of wet instruction thereon.In figure 6, vehicle 602 travels on surface 604, and comprises the sensor of the data of the peripheral region collecting vehicle 602.Sensor can comprise LIDAR unit, and it can receive from the region environment, such as from region 606 and 608, and the laser beam of reflection.Region 606 can be tracked and be considered to the instruction travelling another vehicle 610 in vehicle 602 right front.Region 608 can be tracked and be considered to the instruction travelling vehicle 612 in vehicle 602 front.The laser data of collecting for region 608 also can be caused by other in region 608, the water track 614 that surface 604 such as, produce because vehicle 612 drives through water.Water track 614 can also be in sight in the view data collected by the sensor by vehicle 602.Vehicle 602 can also comprise radar (radar) unit, with the measurement of the existence and speed of determining vehicle 612.Radar possibly cannot detect water track 614.Because radar determination vehicle 612 exist and speed higher than threshold value, so now laser data is the instruction of a certain project (such as, water track 614), thus vehicle 602 can be reached a conclusion: be wet substantially on the surface 604 in vehicle 602 front.But if the speed of vehicle 612 is lower, such as several miles per hour, so water may not be kicked up, or may not produce water track, therefore false data can be considered to the uncorrelated laser data of other objects in region 608.
Fig. 7 identifies that the surface that travels of vehicle is the example concept birds-eye view of wet instruction thereon.In the figure 7, vehicle 702 travels on surface 704, and comprises the sensor unit 706 for collecting the data in area-of-interest 708, and area-of-interest 708 can be defined as the region in vehicle 702 front.Sensor unit 706 can use the method described in Fig. 3 to Fig. 4, identifies the water spots 710 caused by water track of water or the vehicle 712 kicked up due to vehicle 712.Sensor unit 706 can also by the reflection of such as laser beam, in camera image appreciiable reflection or other sensor fusion method of describing in Fig. 3 to Fig. 4, determine do not have vehicle to travel other water spots of process, such as water spots 714 recently.Vehicle 702 can receive the data from sensor unit 706, and to determine whether the surface 704 that vehicle 702 travels is wet thereon, or whether the surface 704 in vehicle 702 front is wet.
In other example, Fig. 8 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising mist according at least some embodiment described herein.The embodiment of method 800 put forward the methods shown in Fig. 8, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200, or is used by the assembly of vehicle 100 or vehicle 200.Such as, process described herein can perform by being installed on autonomous vehicle (such as, vehicle 200), that communicate with computer system 112, sensor fusion algorithm 138 and/or computer vision system 140 RADAR unit 126, LIDAR unit 128 or camera 130.Method 800 can comprise one or more operation, function or action, as shown in by one or more in square frame 802-808.Although these square frames illustrate sequentially, these square frames can executed in parallel in some cases, and/or performs with the order being different from order described herein.And based on the implementation expected, each square frame can be combined into less square frame, is divided into additional square frame, and/or deleted.
Exemplary method, the method 800 of such as Fig. 8, can be performed by vehicle and subsystem thereof whole or in part.Therefore, exemplary method can be described to be realized by vehicle in this article in an illustrative manner.
At square frame 802, method 800 comprises the laser data receiving and collect from the scanning of the environment to vehicle.Laser data comprises the multiple laser data points based on the object in environment, and described object is perceived as physics to be existed due to reflection or rear orientation light.The assembly of vehicle or vehicle, such as computing equipment or treater, can be configured to the laser data collected by receiving.Such as, laser data can to receive with the similar mode described above with reference to Fig. 3.
In square frame 804, method 800 comprises and being associated with the one or more objects in environment by the laser data point in multiple laser data point by computing equipment.Exemplarily, follow-up system can be used to tracing object, and can determine the laser data point that is associated with the object tracked.The laser data received due to the reflectance signature in environment can be considered to caused by the object (such as, making laser by the object reflected) of physics existence in environment.Computing equipment can be configured to the position be associated storing the such object seen by laser data, and stores the association between laser data and object.
In some instances, the some cloud corresponding with the object in environment can be generated.Each point in some cloud can pass through azimuth (such as, when launching corresponding with this some pulse LIDAR equipment towards, its by LIDAR anglec of rotation mirror towards determining) and sight line (LOS) distance (such as, impulse ejection and reflected light receive between the distance that indicates of time delay) mark.For causing the pulse returning reflected signal, the distance in point diagram can be set to the ultimate range sensitivity of LIDAR equipment alternatively.Ultimate range sensitivity can postpone to determine according to the maximum time of the optical pickocff the be associated reflected signal to be returned such as after each impulse ejection, and this maximum time delay itself can according to arranging in the expection signal strength of the reflected signal of specified distance when the prediction reflectivity etc. of given ambient lighting conditions, exomonental intensity, environmental characteristic.In some instances, ultimate range can be approximately 60 meters, 80 meters, 100 meters or 150 meters, but for the concrete configuration of LIDAR equipment with the optical pickocff be associated, other example is also possible.
In certain embodiments, laser data collected by sensor fusion algorithm 138 shown in Fig. 1, computer vision system 140 and/or computer system 112 can be configured to explain separately and/or in conjunction with the instruction of other sensor information and/or explain collected laser data based on the pattern match point cloud of memory device and/or the baseline chart of environment, so that by the classification of the group of point or be identified as corresponding to the object in environment.
In addition, each spatial point can be associated to from the corresponding laser in one group of laser and corresponding timestamp.That is, comprise at LIDAR in the embodiment of multiple laser, each spatial point received accordingly can to according to the corresponding spatial point detection received to particular laser be associated.In addition, each corresponding spatial point can be associated with corresponding timestamp (such as, laser be launched or receive time).By this way, the spatial point received can be organized, be identified or otherwise be sorted based on space (laser mark) and/or time (timestamp).Such sequence can be that significant order is helped or improves the analysis to spatial point data by allowing spatial point Organization of Data.
In some instances, object detection is provided in conjunction with example LIDAR equipment.LIDAR equipment can be configured to use one or more laser to catch laser point cloud image.Laser point cloud comprises the many points for each pulse of launching from LIDAR equipment; Reflected signal can indicate the actual position of reflective object, and fails to receive reflected signal instruction and in specific range, there is not actv. reflective object along the direction of visual lines of laser.Depend on various factors, comprise laser pulse rate, scene refresh rate, total solid angle of being sampled by each LIDAR equipment (or when only using a LIDAR equipment, only total solid angle of scene), the number of the sample point in each some cloud can be determined.Some embodiments can provide the some cloud of the point, the point of 80000 laser designations, the point of 100000 laser designations etc. with nearly 50000 laser designations.Usually, angle resolution in as one of the number of the point of the laser designation in each some cloud and as refresh rate on the other hand between compromise.LIDAR equipment is driven to determine that relevant sufficiently high refresh rate is to provide angle resolution to the real-time navigation of autonomous vehicle.Therefore, LIDAR equipment can be configured to predetermined time interval, such as 100 milliseconds (to realize the refresh rate of 10 frames per second), 33 milliseconds (to realize the refresh rate of 30 frames per second), 1 millisecond, 1 second etc., catch one or more laser point clouds of scanning area.
With reference to figure 1, the data storage apparatus 114 of the computer system 112 of vehicle 100 can storage object detector software, code or other programmed instruction.Such object detector software can comprise that above-described to comprise in the control system 106 of sensor fusion algorithm 138, computer vision system 140 and/or obstacle-avoidance system 144 one or more, or their part.Object detector can be the arbitrary disposition of software and/or hardware, it is configured to by classify to object based on the one or more sensors in the laser point cloud of being caught by LIDAR128 and/or sensor based system 104 and/or identify, with the feature in perception environment scene.When laser point cloud be catch via LIDAR128 time, indicate the data of some cloud of catching to be communicated to object detector, object detector analyzes data to determine whether there is object in laser point cloud.The object indicated by a cloud can be, such as, and vehicle, pedestrian, road sign, traffic lights, cone etc.
In order to determine whether there is object in laser point cloud image, the classification of the arrangement of the point of laser designation with pattern match object, environmental characteristic and/or object or feature can be associated by object detector software and/or module.Object detector can by pre-loaded (or dynamically being indicated) to associate arrangement according to one or more parameters corresponding with the physical object/feature in the environment around vehicle 100.Such as, object detector can by pre-loaded to indicate the length of the exemplary height of pedestrian, typical automotive, information to the confidence threshold value that object of suspicion is classified etc.
When object in object detector identification point cloud, object detector can define the Bounding Box surrounding this object.Such as, Bounding Box can correspond to the outside face of the prediction of the object of some cloud instruction.Certainly, border " box " form that the multiaspect of the exterior boundary of the prediction of defining objects generally can be taked close-shaped.
For each some cloud of catching, position and the corresponding borders thereof of the object of perception are associated with frame number or frame time.Therefore, the object appearing at the similar shape of roughly analogous location in the continuous scanning of scene can be associated with each other, so that tracing object in time.For the object of the perception appeared in multiple somes cloud frames (such as, the scanning completely of scanning area), occur each frame thereon for object, object can be associated with the clear boundary shape of dimensional extent of the object defining perception.
To move relative to vehicle when vehicle 100 drives through the environment around it and/or when object thus through the scanning area of LIDAR equipment 128, the object of perception can be followed the trail of.Combine the mobile message that two or more some clouds of catching continuously can allow the object determining to detect thus.Such as by acceleration/accel and/or the speed of the object of observation (such as together with vehicle 100 along the automobile of road movement), the prediction of in the future position can be made for the object of the motion outline with characterization, so as in follow up scan the position of forecasting object.In certain embodiments, the object of movement is aloft assumed that the track along being affected by gravity moves.
In order to help to provide Object identifying, vehicle 100 also can communicate with object recognition server (such as, via wireless communication system 146).Object recognition server can verify the object that uses object detector to detect by vehicle 100 and/or to its classification.In addition, object recognition server can promote that one or more parameters of the object being used for object detector detecting in the laser point cloud of catching based on the data of the accumulation from other similar systems, local condition are optimized.In one embodiment, object bounds and their corresponding image parameters can communicate to object recognition server for verifying that the object of institute's perception is correctly validated by vehicle 100, such as by indicating the assessment of the correct statistical likelihood identified.
Referring back to Fig. 8, at square frame 806, method 800 comprises and comparing with the uncorrelated laser data point of the one or more objects in environment and the laser data point of the pattern of representative caused by mist stored.In this example, for the laser data being considered to representative object, laser data can be associated with the object in environment, and residue receive or collect laser data can be considered to uncorrelated with the object in environment.
In this example, method 800 can be performed to determine that but object is expected and exists not detected by laser or unexpected object is detected the example of (that is, laser measurement is lost due to mist or changed).As mentioned above, for the reflects laser data received, can determine that object exists thus causes laser to be reflected.But under the condition having mist, mist can cause reflection, thus cause determining the mistake of object.In order to the laser data determined when is based on the real object be present in environment instead of based on the mist of reflection causing Laser emission, computing equipment can be configured to by the laser data of laser data and known representative object and/or known be compare because mist reflects the laser data caused.Therefore, the pattern of the laser data of the laser data received and known representative object can be compared to identify may be laser data because other factors except object cause.
The pattern of the laser data of any number can be stored, and carry out more uncorrelated laser data with reference to described pattern, so that the laser data uncorrelated laser data being categorized as indicated object or the laser data caused due to weather condition (such as mist).
As another example distinguished in the laser data caused by object and carrying out between the laser data caused by other condition (such as, mist), computing equipment can be configured to the movement of following the trail of the object represented by laser data.Therefore, all laser data all can be used as the laser data representing mobile object to follow the trail of, and can think in sight with the object of strange mode movement or at short notice appearing and subsiding to as if the laser data that causes due to weather condition reflect the error object caused.
In this example, method 800 can comprise and determines the uncorrelated laser data point with the one or more objects in environment.LIDAR can be configured to perform the first scanning to environment, and is associated with the one or more objects in environment by the laser data point in multiple laser data point.Then, LIDAR can be configured to perform the second scanning to environment, and determines the laser data point with one or more object matching based on the position of the object represented by laser data point.Then, lack based on the one or more objects in scanning with first and mate, the laser data point uncorrelated with the one or more objects in environment in the second scanning can be determined.In further example, second scanning can represent after the first scan or after multiple scanning to the scanning of environment, to make object tracing there occurs a period of time, so that compare the second scan-data.
At square frame 808, method 800 comprises and comparing based on this, is comprised the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.In one example, when uncorrelated laser data coupling or when substantially mating the pattern of the laser data caused by mist, computing equipment can identify that this laser data causes due to mist.
In one example, laser data can to there is water in air or usually have the Cloudy conditions of mist relevant, and such water droplet or cloud be vehicle follow-up system follow the trail of less than project.Such as, for detecting the LIDAR equipment of also positioning optical reflectance signature, the water droplet in air or the cloud feature class in air are similar to object cloud, because light pulse is reflected from particulate.LIDAR equipment possibly cannot penetrating fog, and therefore, when there is mist, many laser spots can return due to mist.Therefore, for any laser data (such as, laser data unmatched with the object tracked) received do not tracked, can determine that the weather condition of the environment of vehicle comprises the instruction of mist.
In addition (or alternatively), at square frame 806, method 800 can comprise determines that the number of the uncorrelated laser data point with the one or more objects in environment exceedes predetermined threshold, is then comprised the instruction of mist by the weather condition of the environment of computing equipment identification vehicle.Such as, if there is the laser data point uncorrelated with object of minority, then this may be because many factors (such as, such as false reflects laser) cause; But for the laser data point of certain number of thresholds, higher probability can be, these laser data points due to launch laser beam by air water droplet reflection cause.
In another example, when one or more lasers of vehicle move forward, vehicle can collect the data point comprising scope and strength information for same position (point or region) from some directions and/or in the different time.Such as, each data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it.The strength rating be associated with high reverse--bias surface can higher than low reflecting surface.Similarly, the strength rating be associated with the dark objects (black, Dark Blue, brown etc.) absorbing more light can lower than the light object (white, cream-coloured, silver color etc.) that can reflect more light.In this respect, cloud and mist can represent dark situation, and therefore, mist can reduce the strength rating of the object perceived, instead of value of gaining in strength.
In some instances, the strength rating of laser data point can be determined scale, and such as, from 0 to 250, wherein 0 is dark and 250 is bright.Therefore, specularity more by force, brighter surface can be associated with the strength rating closer to 250, and more weak, the darker surface of specularity can be associated with the strength rating closer to 0.Laser scanning data can be received and process to generate geographical position coordinates.These geographical position coordinates can comprise the GPS latitude and longitude coordinate (x, y) with altitude component (z), or can be associated with other system of axess.The result of this process is one group of data point.Each data point in this group data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it: (x, y, z).
The mean intensity of the laser data point of environment can compare with threshold value, to identify the instruction of mist further.Such as, as mentioned above, water droplet can reduce the intensity of the laser data of collection.Therefore, the average intensity value of the laser data having the environment of mist to have can slightly lower than the average intensity value of the laser data of sunny environment.If the laser data the checked point of certain percentum has the strength rating lower than certain threshold value, then can determine that weather condition comprises mist.Such as, use the scale of as above 0 to 250, if there are at least 850 points (or at least 85% of these laser data points) in 1000 laser data points and this 1000 points to have intensity lower than threshold value 10 on road, then environment is dark.As mentioned above, low intensity value can indicate the high probability that mist exists.Such as, also other threshold values and percentum can be used.Such as, the laser data representing sunny environment can be stored and access for comparing.
In further example, method 800 can comprise the position determining vehicle, determines the distribution of the anticipated value of the laser data of the environment of the position of vehicle, and the laser data that determined distribution and the environment for vehicle are collected is compared.Distribution for the anticipated value of the laser data of the position of sunny condition can such as be retrieved from data storage apparatus based on the position of vehicle or receive from server.For having mist condition and sunny both conditions, can generate and store the map of the intensity of the laser data of opsition dependent.Based on the difference between determined distribution and the laser data of collecting for the environment of vehicle higher than threshold value, can identify that weather condition comprises the second instruction of mist.Such as, if described difference is very high, then the laser data that this laser data and expection are seen at fair weather is not mated.Therefore, such another instruction that mist relatively can be indicated to exist.
In further example, method 800 can also comprise the form of the shape determining the uncorrelated laser data point with the one or more objects in environment.Mist can have unique shape, and can create and store shape and the radius of the some cloud produced due to general mist, and itself and the laser data received is compared subsequently.When laser data is mapped to the shape of the mist of storage, the further instruction of mist can be obtained.
In this example, using method 800, vehicle can be configured to operate in autonomous mode, and the surface that vehicle can be utilized thereon to travel is that wet instruction is to determine that the driving of vehicle determines.May expect to control vehicle in a different manner according to various weather condition or road conditions, therefore, when mist exists, vehicle can operate according to " mist " driving technique (such as, allow more braked space, underspeed).Vehicle also possibly cannot operate under some weather condition, therefore, in some instances, based on the instruction of mist, vehicle can provide instruction to be transformed into manual mode with instruction request, such as by providing alarm to be started the operation to vehicle by forbidding autonomous mode to chaufeur.
Except make drive determine except, or as to make drive decision replacement, road weather condition and/or pavement conditions can trigger the alarm being sent to chaufeur.Alarm can ask chaufeur to control vehicle, or also can provide the alarm about described condition to chaufeur simply.Alarm can be any other signal of audible signal, visual signal, sense of touch or sensory signal and/or the attention obtaining chaufeur.In this illustration, after alerting driver, vehicle can receive input from chaufeur, such as steering wheel rotation, applying braking, applying accelerator, pressing emergency cut-off etc.
In some instances, (except with the object tracked uncorrelated laser data point except) additional data can be considered to determine that the weather condition of the environment that vehicle is resident comprises mist, or the more high probability providing mist to exist or confidence (such as, uncorrelated laser data point be by the water particulate in air but not the object tracked causes).
Fig. 9 determines the block diagram of the exemplary method of the instruction of the weather condition comprising mist further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.The embodiment of the method that the method 900 shown in Fig. 9 proposes such as can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200 (or being used by the assembly of vehicle 100 or vehicle 200), and can also perform the method 900 shown in Fig. 9 except performing the method 800 shown in Fig. 8.Method 900 can represent the module of program code, fragment or part, and it comprises and can perform one or more instructions (such as, machine readable code) for the specific logical function in implementation procedure or step by treater.
As shown in square frame 902, method 900 can comprise determine laser data whether with RADAR data consistent.Such as, autonomous vehicle can collect the RADAR data of the environment of vehicle, and the instruction of RADAR data exists object in vehicle environmental.But for the water particulate existed in air or cloudy composition, radar generally incites somebody to action not return data, and RADAR data can pass through cloud identifying object.Laser data is also used to tracing object as mentioned above, and when the existence of the object indicated by RADAR data and the object indicated by LIDAR data exist inconsistent time, LIDAR data can be considered to off-square.But the instruction of other laser data returned indicating any extra objects do not indicated by RADAR data can be water particulate or cause due to mist by LIDAR data.In such an example, described object detected when the object seen according to LIDAR data at certain distance at a certain threshold time period, but when not seeing the object of coupling according to RADAR, the instruction that road has mist can be obtained.
In some instances, but by LIDAR data tracing to any object of not seeing in RADAR data can be classified as being because weather condition causes.As shown in method 900, based on RADAR data and LIDAR data inconsistent, at square frame 904, method 900 comprises determines that the mistake of laser data returns.At square frame 906, can determine that whether number that mistake returns is higher than threshold value as mentioned above.Such as, a small amount of mistake is returned, such as false reflects laser data or the reflects laser data of mistake, do not take further action or again can process laser data (as shown in square frame 914).But when the number that mistake returns is higher than threshold value, method 900 comprises and identifies that the weather condition of environment of vehicle comprises the instruction of mist, as shown in frame 908.
In further example, the uncorrelated and round-shaped laser data point formed around the object indicated by RADAR of the object indicated with RADAR can be confirmed as because mist causes.In some instances, the concentration of mist also can be estimated.Such as, in environment with the uncorrelated object of laser data point (such as, mist) and (and may being associated with laser data point) environment of being indicated by RADAR data in given object between distance can be determined, and can based on the concentration of this distance estimations mist.Use two sensors (LIDAR and RADAR), can the existence of detected object within the specific limits, and detect the position of mist to object position between distance the visbility of mist or the measurement of concentration can be provided.Concentration can be expressed as the distance of visbility, is such as such as less than the visbility of 5 kilometers, and this distance can be detect the position of mist and object position between distance determined.In further example, the determination of the concentration of mist can based on the number of uncorrelated laser data point.In the example that heavy fog or thick fog exist, a large amount of uncorrelated laser data points can be there is.
Method 900 may be advanced to square frame 910, and wherein method 900 comprises and determines that whether laser data is consistent with camera data.Such as, autonomous vehicle can comprise camera, and can receive the view data that the environment for vehicle collects.Mist can be visible in the picture, therefore, object detection technique can be used to process image, such as, by image being supplied to object detection server and the response receiving the object in indicating image processes image.
Figure 10 A is the example concept diagram that the weather condition of environment-identification comprises the instruction of mist.In Figure 10 A, vehicle 1002 travels from the teeth outwards, and comprises the sensor of the data of collecting vehicle 1002 peripheral region.Sensor can comprise LIDAR unit, and it can receive the reflected beam from the region in environment, and can comprise camera, and it can be configured to the image collecting environment.Another vehicle 1004 can travel in the front of vehicle 1002.Camera can the image of capture region 1006.
Figure 10 B is the example concept diagram of the image of being caught by the vehicle 1002 in Figure 10 A.The part that the image show the afterbody of vehicle is covered by mist or covers.This image can be processed to determine that in view data, color is the number of the pixel of grey substantially.Be the percentum of the pixel of grey substantially based on color, the second instruction of mist weather condition can be obtained.In fig. 1 ob, about 40-45% of image is covered by mist, and therefore, threshold value can be met.Also lower percentum can be used.But, the percentum of about 40% or higher can be used, to obtain the more high probability of the image comprising mist.
In some instances, view data can be processed with the object in recognition image, and when the object indicated by camera image and the object indicated by LIDAR data are inconsistent, also can obtain the instruction that mist exists.Such as, the image in Figure 10 B can indicate at covering coordinate x
1-x
nregion in the object that exists, and laser data can only indicate at covering coordinate x
2-x
nregion in the object that exists.Therefore, camera data and laser data inconsistent, and so inconsistent instruction that can be used as mist and exist.
Referring back to Fig. 9, inconsistent based on camera data and laser data, the environment that can obtain vehicle comprises the instruction of mist, as shown in square frame 908.
Method 900 may be advanced to square frame 912, and wherein method 900 comprises and determines that whether laser data is consistent with weather data.Such as, can receive for the Weather information of the position of vehicle via network from server, and when Weather information instruction may have mist or have mist condition, can obtain that mist exists second indicates.Alternatively or additionally, can from on-vehicle vehicle sensor, such as rainfall sensor or " rain detector " receive weather data, and when rainfall sensor instruction rainfall exists when LIDAR data misdirection returns, can obtain the second instruction that mist exists.The output possibly cannot depending merely on rainfall sensor determines whether comprise the amount of moisture being enough to cause mist in air, and therefore, sensor exports and can be used as auxiliary tolerance.
In further example, additional weather data, such as temperature or dew point, can to determine or by determining with server communication from on-vehicle vehicle sensor, and the weather condition that can obtain the environment of vehicle based on temperature or dew point comprises the second instruction of mist.Such as, when the difference between temperature and dew point is approximately less than 2.5 degrees Celsius or 4 Degrees Fahrenheit, mist can be formed.Under mist usually occurs in the relative humidity close to 100%, but mist can be formed at a lower humidity, and mist can not be formed sometimes under the relative humidity of 100%.The relative humidity readings of 100% means that air can not hold extra moisture again; If extra moisture is added into, air will become oversaturation.Mist can be formed suddenly, and can dissipate rapidly equally, and this depends on which side of temperature at dew point.
In this example, method 900 can be performed to provide the environment of vehicle to comprise the further instruction of mist.Except the method 800 of Fig. 8, can also manner of execution 900 alternatively.Such as, uncorrelated laser spots (such as, uncorrelated laser spots because the moisture in water droplet, air or mist under normal circumstances generate) can be uncorrelated relative to the vehicle of laser detection or may be uncorrelated relative to the vehicle of detections of radar, and for identify mist according to being verified by cross reference other sensing data, Weather informations etc.Any combination of the function described in Fig. 9 can be combined with Fig. 3 with the confidence level of the raising providing mist to exist.
Further, except above-mentioned sensing data, the input from other sensors on vehicle also can be used to make weather condition additionally determine.Such as, these sensors can comprise tyre pressure sensor, engine temperature sensor, brake heat sensor, slipper state sensor, tire protector sensor, fuel sensor, oil level and mass sensor, air mass sensor (particulate in detected temperatures, humidity or air) etc.
In addition, the many sensors on vehicle provide the data be processed in real-time, and that is, sensor constantly can upgrade and export with the environment being reflected in certain hour continuously or on request or sense within the scope of certain hour.
Figure 11 A to Figure 11 B comprises and identifies that the environment of vehicle comprises the example concept lateral plan of the instruction of mist.In Figure 11 A, vehicle 1102 travels from the teeth outwards, and another vehicle 1104 is at the traveling ahead of vehicle 1102.Vehicle 1102 can comprise LIDAR unit 1106, its be configured to receive for vehicle 1102 front region (such as, as shown in the arrow in Figure 11 A) laser data of collecting, and also the distance that laser data can indicate vehicle 1104 to be positioned at apart from vehicle 1102 is d
1place.Vehicle 1104 can drive through mist, as shown in Figure 11 B.When laser beam is reflected by water, the water in mist can be detected by LIDAR unit 1106, and due to mist, laser data can indicate vehicle 1104 and vehicle 1102 distance d now
2.But water is not the object that the follow-up system of vehicle 1102 tracks, because other sensor (such as, radar) possibly cannot detect that As time goes on water or LIDAR unit 1108 may can not track water always.When the laser data received not match tracing arrive object time, uncorrelated laser data can be regarded as the instruction of the water in air, and mist exist instruction.
Laser data can compare with the shape point cloud that can be produced by mist that is predefined or that store.Such as, roughly at the data collection tracked indicated by RADAR data and (when comparing with the some cloud stored) represent the laser data point of the object of the substantially rounded shape of the radius with mist, can process further mist is verified by comparing with the some cloud of storage.Represent roughly at all laser data points of the object of the data collection tracked for the object onrelevant in the environment indicated with radar data, such laser data point can indicate mist.In this respect, vehicle 1102 can detect the cloud at the one or more random number strong points at vehicle 1104 afterbody, and as the result of the water movement in air, the position of the data point in cloud and number can constantly change.Therefore, the cloud from the data point of mist can not have clear and definite structure, and a part for solid object, the tail end of such as vehicle, is associated with the data point defining clear surface.Also similar point cloud data can be found at the rear of other vehicles.Such discovery can indicate mist to exist.
In other example, Figure 12 is the block diagram utilizing on-vehicle vehicle sensor to detect the exemplary method of the weather condition comprising mist according at least some embodiment described herein.The embodiment of method 1200 put forward the methods shown in Figure 12, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200, or is used by the assembly of vehicle 100 or vehicle 200.Such as, process described herein can perform by being installed on autonomous vehicle (such as, vehicle 200), that communicate with computer system 112, sensor fusion algorithm 138 and/or computer vision system 140 RADAR unit 126, LIDAR unit 128 or camera 130.Method 1200 can comprise one or more operation, function or action, as shown in one or more in square frame 1202-1206.Although these square frames illustrate sequentially, these square frames can executed in parallel in some cases, and/or performs with the order being different from order described herein.And based on the implementation expected, each square frame can be combined into less square frame, be divided into additional square frame and/or deleted.
Exemplary method, the method 1200 of such as Figure 12, can be performed by vehicle and subsystem thereof whole or in part.Therefore, exemplary method can be described to be realized by vehicle in this article in an illustrative manner.
At square frame 1202, method 1200 comprises the laser data receiving and collect for the environment of vehicle.Laser data comprises the multiple laser data points based on the object in environment, and described object is perceived as physics to be existed due to reflection or rear orientation light.The assembly of vehicle or vehicle, such as computing equipment or treater, can be configured to the laser data collected by receiving.
At square frame 1204, method 1200 comprises and being associated with the one or more objects in environment by the laser data point in multiple laser data point by computing equipment.Exemplarily, follow-up system can be used to tracing object, and can determine the laser data point that is associated with the object tracked.The laser data received due to the reflectance signature in environment can be considered to caused by the object (such as, making laser by the object reflected) of physics existence in environment.Computing equipment can be configured to the position be associated storing the such object seen by laser data, and stores the association between laser data and object.
In other examples, the some cloud corresponding with the object in environment can be generated.Each point in some cloud can pass through azimuth (such as, when launching corresponding with this some pulse LIDAR equipment towards, its by LIDAR anglec of rotation mirror towards determining) and sight line (LOS) distance (such as, impulse ejection and reflected light receive between the distance that indicates of time delay) mark.For causing the pulse returning reflected signal, the distance in point diagram can be set to the ultimate range sensitivity of LIDAR equipment alternatively.Ultimate range sensitivity can postpone to determine according to the maximum time of the optical pickocff the be associated reflected signal to be returned such as after each impulse ejection, and this maximum time delay itself can according to arranging in the expection signal strength of the reflected signal of specified distance when the prediction reflectivity etc. of given ambient lighting conditions, exomonental intensity, environmental characteristic.In some instances, ultimate range can be approximately 60 meters, 80 meters, 100 meters or 150 meters, but for the concrete configuration of LIDAR equipment with the optical pickocff be associated, other example is also possible.
In certain embodiments, laser data collected by sensor fusion algorithm 138 shown in Fig. 1, computer vision system 140 and/or computer system 112 can be configured to explain separately and/or in conjunction with the instruction of other sensor information and/or explain collected laser data based on the pattern match point cloud of memory device and/or the baseline chart of environment, so that by the classification of the group of point or be identified as corresponding to the object in environment.
In addition, each spatial point can be associated to from the corresponding laser in one group of laser and corresponding timestamp.That is, comprise at LIDAR in the embodiment of multiple laser, each spatial point received accordingly can to according to the corresponding spatial point detection received to particular laser be associated.In addition, each corresponding spatial point can be associated with corresponding timestamp (such as, laser be launched or receive time).By this way, the spatial point received can be organized, be identified or otherwise be sorted based on space (laser mark) and/or time (timestamp).Such sequence can be that significant order is helped or improves the analysis to spatial point data by allowing spatial point Organization of Data.
In some instances, object detection is provided in conjunction with example LIDAR equipment.LIDAR equipment can be configured to use one or more laser to catch laser point cloud image.Laser point cloud comprises the many points for each pulse of launching from LIDAR equipment; Reflected signal can indicate the actual position of reflective object, and fails to receive reflected signal instruction and in specific range, there is not actv. reflective object along the direction of visual lines of laser.Depend on various factors, comprise laser pulse rate, scene refresh rate, total solid angle of being sampled by each LIDAR equipment (or when only using a LIDAR equipment, only total solid angle of scene), the number of the sample point in each some cloud can be determined.Some embodiments can provide the some cloud of the point, the point of 80000 laser designations, the point of 100000 laser designations etc. with nearly 50000 laser designations.Usually, angle resolution in as one of the number of the point of the laser designation in each some cloud and as refresh rate on the other hand between compromise.LIDAR equipment is driven to determine that relevant sufficiently high refresh rate is to provide angle resolution to the real-time navigation of autonomous vehicle.Therefore, LIDAR equipment can be configured to predetermined time interval, such as 100 milliseconds (to realize the refresh rate of 10 frames per second), 33 milliseconds (to realize the refresh rate of 30 frames per second), 1 millisecond, 1 second etc., catch one or more laser point clouds of scanning area.
With reference to figure 1, the data storage apparatus 114 of the computer system 112 of vehicle 100 can storage object detector software, code or other programmed instruction.Such object detector software can comprise that above-described to comprise in the control system 106 of sensor fusion algorithm 138, computer vision system 140 and/or obstacle-avoidance system 144 one or more, or their part.Object detector can be the arbitrary disposition of software and/or hardware, it is configured to by classify to object based on the one or more sensors in the laser point cloud of being caught by LIDAR128 and/or sensor based system 104 and/or identify, with the feature in perception environment scene.When laser point cloud be catch via LIDAR128 time, indicate the data of some cloud of catching to be communicated to object detector, object detector analyzes data to determine whether there is object in laser point cloud.The object indicated by a cloud can be, such as, and vehicle, pedestrian, road sign, traffic lights, cone etc.
In order to determine whether there is object in laser point cloud image, the classification of the arrangement of the point of laser designation with pattern match object, environmental characteristic and/or object or feature can be associated by object detector software and/or module.Object detector can by pre-loaded (or dynamically being indicated) to associate arrangement according to one or more parameters corresponding with the physical object/feature in the environment around vehicle 100.Such as, object detector can by pre-loaded to indicate the length of the exemplary height of pedestrian, typical automotive, information to the confidence threshold value that object of suspicion is classified etc.
When object in object detector identification point cloud, object detector can define the Bounding Box surrounding this object.Such as, Bounding Box can correspond to the outside face of the prediction of the object of some cloud instruction.Certainly, border " box " form that the multiaspect of the exterior boundary of the prediction of defining objects generally can be taked close-shaped.
For each some cloud of catching, position and the corresponding borders thereof of the object of perception are associated with frame number or frame time.Therefore, the object appearing at the similar shape of roughly analogous location in the continuous scanning of scene can be associated with each other, so that tracing object in time.For the object of the perception appeared in multiple somes cloud frames (such as, the scanning completely of scanning area), occur each frame thereon for object, object can be associated with the clear boundary shape of dimensional extent of the object defining perception.
To move relative to vehicle when vehicle 100 drives through the environment around it and/or when object thus through the scanning area of LIDAR equipment 128, the object of perception can be followed the trail of.Combine the mobile message that two or more some clouds of catching continuously can allow the object determining to detect thus.Such as by acceleration/accel and/or the speed of the object of observation (such as together with vehicle 100 along the automobile of road movement), the prediction of in the future position can be made for the object of the motion outline with characterization, so as in follow up scan the position of forecasting object.In certain embodiments, the object of movement is aloft assumed that the track along being affected by gravity moves.
In order to help to provide Object identifying, vehicle 100 also can communicate with object recognition server (such as, via wireless communication system 146).Object recognition server can verify the object that uses object detector to detect by vehicle 100 and/or to its classification.In addition, object recognition server can promote that one or more parameters of the object being used for object detector detecting in the laser point cloud of catching based on the data of the accumulation from other similar systems, local condition are optimized.In one embodiment, object bounds and their corresponding image parameters can communicate to object recognition server for verifying that the object of institute's perception is correctly validated by vehicle 100, such as by indicating the assessment of the correct statistical likelihood identified.
Referring back to Figure 12, at square frame 1206, method 1200 comprises and will be defined as representing the object do not tracked in the given position relative to vehicle with the uncorrelated given laser data point of the one or more objects in environment in multiple laser data point.
In this example, for the laser data being considered to representative object, laser data can be associated with the object in environment, and remaining reception or collect laser data can be considered to uncorrelated with the object in environment, therefore represent the object do not tracked.
In this example, method 1200 can be performed to determine that but object is expected and exists not detected by laser or unexpected object is detected the example of (that is, laser measurement is lost due to sunlight or changed).As mentioned above, for the reflects laser data received, can determine that object exists thus causes laser to be reflected.But under sunny condition, sunlight can cause reflection, thus cause determining the mistake of object.In order to determine when that laser data reflects or the sunlight of generation error Laser emission based on the real object be present in environment instead of based on causing, computing equipment can be configured to the laser data of laser data and known representative object and/or knownly to compare because sunlight reflects the laser data caused.Therefore, the pattern of the laser data of the laser data received and known representative object can be compared to identify may be laser data because other factors except object cause.
The pattern of the laser data of any number can be stored, and carry out more uncorrelated laser data with reference to described pattern, so that the laser data uncorrelated laser data being categorized as indicated object or the laser data caused due to weather condition.
As another example distinguished in the laser data caused by object and carrying out between the laser data caused by other weather condition, computing equipment can be configured to the movement of following the trail of the object represented by laser data.Therefore, all laser data all can be used as and represent mobile object to follow the trail of, and can think in sight with the object of strange mode movement or at short notice appearing and subsiding to as if the laser data that causes due to weather condition reflect the error object caused.
In this example, method 1200 can comprise and determines with the one or more object onrelevant in environment thus represent the laser data point of the object do not traced into.LIDAR can be configured to perform the first scanning to environment, and is associated with the one or more objects in environment by the laser data point in multiple laser data point.Then, LIDAR can be configured to perform the second scanning to environment, and determines the laser data point with one or more object matching based on the position of the object represented by laser data point.Then, lack based on the one or more objects in scanning with first and mate, the laser data point uncorrelated with the one or more objects in environment in the second scanning can be determined.In further example, second scanning can represent after the first scan or after multiple scanning to the scanning of environment, to make object tracing there occurs a period of time, so that compare the second scan-data.
At square frame 1208, method 1200 comprises as the vehicle moves, determines that the object do not tracked remains on the substantially identical relative position relative to vehicle.In one example, the coordinate position of the object do not tracked can be determined in time, and the distance of vehicle and the angle relative to vehicle can be determined.As the vehicle moves, the object rested on apart from vehicle same distance place will be considered to also moving.In addition, as the vehicle moves, the object remaining on equal angular relative to vehicle also will be considered to also moving.But when the angle of object is higher than 45 degree or larger, object can be considered to caused by sunlight reflection.In other example, the object being considered to movement identically with vehicle substantially can be confirmed as caused by sunlight, and it keeps substantial constant under fair weather condition.
At square frame 1210, it is sunny instruction that method 1200 comprises by the weather condition of the environment of computing equipment identification vehicle.Such as, LIDAR equipment use laser irradiates target, then analyzes reflection, and the sunlight that reflects of reflectance target can make LIDAR equipment saturated or produce invalid or reading not too accurately.For detecting and for the LIDAR equipment of positioning optical reflectance signature, the laser artifact caused by sunlight direct irradiation can being generated.Sunlight can be overlapping with optical maser wavelength, and LIDAR equipment can receive sunlight and be interpreted as sunlight to create the laser returned of artifact.Therefore, for any laser data (such as, laser data unmatched with the object tracked) received do not tracked, can determine that the weather condition of the environment of vehicle is sunny instruction.
In addition (or alternatively), at square frame 1206, method 1200 can comprise determines that the number of the uncorrelated laser data point with the one or more objects in environment exceedes predetermined threshold, is then sunny instruction by the weather condition of the environment of computing equipment identification vehicle.Such as, if there is the laser data point uncorrelated with object of minority, then this can be because many factors (such as, such as false reflects laser) cause; But for the laser data point of certain number of thresholds, higher probability can be, these laser data points cause due to the sunlight received.
In this example, method 1200 can be performed to determine that but object is expected and exists not detected by laser or unexpected extra objects is detected the example of (that is, laser measurement to be lost due to sunlight or changed) by laser.In this example, when one or more lasers of vehicle move forward, vehicle can collect the data point comprising scope and strength information for same position (point or region) from some directions and/or in the different time.Such as, each data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it.The strength rating be associated with high reverse--bias surface can higher than lower reflecting surface.Similarly, the strength rating be associated with the dark objects (black, Dark Blue, brown etc.) absorbing more light can lower than the lighter object (white, cream-coloured, silver color etc.) that can reflect more light.In this respect, sunlight can represent lighter environment, and therefore, can increase the strength rating of the object perceived.
In some instances, the strength rating of laser data point can be determined scale, and such as, from 0 to 250, wherein 0 is dark and 250 is bright.Therefore, specularity more by force, brighter surface can be associated with the strength rating closer to 250, and more weak, the darker surface of specularity can be associated with the strength rating closer to 0.Laser scanning data can be received and process to generate geographical position coordinates.These geographical position coordinates can comprise the GPS latitude and longitude coordinate (x, y) with altitude component (z), or can be associated with other system of axess.The result of this process is one group of data point.Each data point in this group data point can comprise indication laser receives the reflectivity of the object of light strength rating and location information from it: (x, y, z).
The mean intensity of the laser data point of environment can compare with threshold value, to identify the instruction of sunny condition further.Such as, as mentioned above, the average intensity value of the laser data having the environment of mist to have can slightly lower than the average intensity value of the laser data of sunny environment.If the laser data the checked point of certain percentum has the strength rating higher than certain threshold value, then can determine that weather condition is sunny.Such as, use the scale of as above 0 to 250, if there are at least 850 points (or at least 85% of these laser data points) in 1000 laser data points and this 1000 points to have intensity higher than threshold value 200 on road, then can think that environment is bright.As mentioned above, higher strength rating can indicate sunny high probability.Such as, also other threshold values and percentum can be used.Such as, the laser data representing sunny environment or dark situation can be stored and access for comparing.
In further example, method 1200 can comprise the position determining vehicle, determines the distribution of the anticipated value of the laser data of the environment of the position of vehicle, and the laser data that determined distribution and the environment for vehicle are collected is compared.Distribution for the anticipated value of the laser data of the position of sunny condition can such as be retrieved from data storage apparatus based on the position of vehicle or receive from server.Many different weather conditions can generate and store the map of the intensity of the laser data of opsition dependent.Based on the comparison between determined distribution and the laser data of collecting for the environment of vehicle higher than threshold value, can identify that weather condition is the second sunny instruction.Such as, if described difference is very high, then the laser data that this laser data and expection are seen is not mated.
In this example, using method 1200, vehicle can be configured to operate in autonomous mode, and the instruction of the environment of vehicle can be utilized to determine that the driving of vehicle determines.May expect to control vehicle in a different manner according to various weather condition or road conditions, therefore, when fine, vehicle can operate according to " sunny " driving technique (such as, allow more braked space, underspeed).Vehicle also possibly cannot operate under particular weather condition, or chaufeur can differently operate in case of variable weather conditions, therefore, in some instances, based on sunny instruction, vehicle can provide instruction to be transformed into manual mode with instruction request, such as by providing alarm to be started the operation to vehicle by forbidding autonomous mode to chaufeur.
Except make drive determine except, or as to make drive decision replacement, weather condition can also trigger the alarm being sent to chaufeur.Alarm can ask chaufeur to control vehicle, or also can provide the alarm about described condition to chaufeur simply.Alarm can be any other signal of audible signal, visual signal, sense of touch or sensory signal and/or the attention obtaining chaufeur.In this illustration, after alerting driver, vehicle can receive input from chaufeur, such as steering wheel rotation, applying braking, applying accelerator, pressing emergency cut-off etc.
Except making and driving and determine, or as to making the replacement of driving and determining, weather condition can also cause the setting of sensor of change vehicle.Such as, it can be that sunny instruction is modified based on the weather condition of the environment of vehicle that the exposure of camera is arranged, or arranging of LIDAR unit can be modified to reduce the exposure time.Also other actions be can take based on this identification, such as shield or sun shield disposed.
In other example, using method 1200, vehicle can be configured to operate in autonomous mode, and the function of method 1200 can contribute to the object determining to be erroneously identified in environment, and the weather condition that the object be erroneously identified itself can trigger the environment of vehicle is sunny instruction.
Figure 13 is the block diagram of the exemplary method for determining the object in the environment that identifies mistakenly according at least some embodiment described herein.Method 1300 shown in Figure 13 proposes the embodiment of method, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200 (or being used by the assembly of vehicle 100 or vehicle 200), and additionally can perform outside the method 1200 shown in Figure 12.Method 1300 can represent the module of program code, fragment or part, and it comprises and can perform one or more instructions (such as, machine readable code) for the specific logical function in implementation procedure or step by treater.
At square frame 1302, method 1300 comprises the laser data receiving and collects for the environment of vehicle, and it can be similar to the function of description in the method 1200 of Figure 12.
At square frame 1304, method 1300 comprises the laser data point that the representative determined in multiple laser data point by computing equipment remains on the object of the substantially identical given position relative to vehicle as the vehicle moves.Such as, laser data can be received in time, and computing equipment can receive the coordinate position of laser data, and compare coordinate position with determine the object that such as represented by laser data as the vehicle moves whether relative vehicle remain on the information of identical relative position.In the ordinary course of things, such as, this situation can represent at laser data point the automobile or other object that autonomous vehicle following, and each in automobile and autonomous vehicle keeps occurring when identical speed.But, in other example, this can laser data point represent solar facula or caused by the reflection based on sunlight in occur.
At square frame 1306, method 1300 comprise determine laser data point along with time remaining constant.Whether method 1300 can comprise determines with the one or more objects uncorrelated laser data point in environment along with time remaining is constant.For with object (such as the automobile in the autonomous vehicle front) laser data that the is associated point changed along with the time relative to the position of vehicle in environment, described laser data point can be considered to actv..
In some instances, for along with time remaining constant and remain on the laser data point of the position substantially the same relative to automobile when the car is moving, at square frame 1308, method 1300 can comprise the error object represented by the laser data point that computing equipment identification is given in environment.
In some instances, the laser data point representing the error object in environment can be because the overexposure error of LIDAR unit causes, such as, be that the reflected signal of more by force or more high strength and LIDAR unit are being found the returning the most by force of light beam that it launches itself and reflected for when processing compared with receiving the returning of the signal that sends with LIDAR unit of sun glare of vehicle reflection.Infrared light is a part for the normal spectrum of sunlight, and it is brighter, and the infrared light of scattering is more, and LIDAR unit can receive described infrared light and is interpreted as the object that in environment, physics exists.
Therefore, the laser data point remaining on the object at same position place relative to vehicle in representative as the vehicle moves along with time remaining constant, and this laser data point is with just in the uncorrelated example of tracked object, and laser data point can be identified as the bad laser data representing error object.
At square frame 1310, method 1300 comprise determine the error object represented in environment laser data point whether higher than threshold value.Based on representing the laser data point of error object in environment lower than threshold value, at square frame 1312, method 1300 comprises alternatively and again processes laser data.
But based on representing the laser data point of error object in environment higher than threshold value, at square frame 1314, it is the second sunny instruction that method 1300 comprises by the weather condition of the environment of computing equipment identification vehicle.Such as, when there is a lot of wrong laser data, this can be another instruction that the environment that causes receiving wrong laser data is sunny.
In further example, method 1300 can also comprise the position of the object representated by laser data determined as described in square frame 1304, and is in desired location based on object or in desired location, identifies that the environment of vehicle is sunny.Such as, when laser data representative relative to vehicle with the object of certain altitude or angle hanging or obstacle, such as miter angle and at vehicle up direction 5-20 foot place, and such object is when continuing to be passed in time representated by collected laser data, computing equipment can determine that laser data does not represent real object or obstacle.But laser data can be observe due to the falseness at the fixed position place relative to vehicle or return caused from the laser that sunlight receives.Therefore, based on the laser data of the object represented in unexpected region, this can be the additional factor will considered when environment-identification is sunny.
In further example, utilize autonomous vehicle or other vehicles, the time in one day and geographic position (such as, using GPS) can be determined, and use this information, the apparent position of the sun can be determined.According to the position of the sun relative to vehicle, the laser data of the object represented in presumptive area can be identified as is due to caused by the reflection of the sun or light.Such as, the laser data representing the object (such as at the object relative to vehicle miter angle place) of the direct apparent position towards the sun substantially can be confirmed as being error object.
In some instances, it is also conceivable to (except in laser data with the uncorrelated laser data point of tracked object except) additional data, to determine that the weather condition of the environment at vehicle place is sunny, or provide fine more high probability or confidence (such as, uncorrelated laser data point causes due to sunray or sun reflection, instead of causes due to tracing object).
Figure 14 determines the block diagram of the exemplary method of the instruction that weather condition is sunny further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.Method 1400 shown in Figure 14 proposes the embodiment of method, and such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200 (or being used by the assembly of vehicle 100 or vehicle 200).Method 1400 can represent the module of program code, fragment or part, and it comprises and can perform one or more instructions (such as, machine readable code) for the specific logical function in implementation procedure or step by treater.
As shown in square frame 1402, method 1400 comprise determine laser data whether with RADAR data consistent.Such as, autonomous vehicle can collect the RADAR data of the environment of vehicle, and the existence of the object of RADAR data instruction in vehicle environmental.For the reflection caused due to the sun, not return data incited somebody to action by radar, but RADAR data can identify the object under sunny condition.Laser data is also used to tracing object as mentioned above, and when the existence of the object indicated by RADAR data and the object indicated by LIDAR data exist inconsistent time, LIDAR data can be considered to off-square.But indicate any extra objects do not indicated by RADAR data can be solar facula or the instruction of other laser data returned caused by the sun reflects by LIDAR data.In such an example, in a certain threshold time period, described object detected when the object seen according to LIDAR data at certain distance, but when not seeing the object of coupling according to RADAR, the instruction that environment is sunny can be obtained.
In some instances, but by LIDAR data tracing to any object of not seeing in RADAR data can be classified as being because weather condition causes.As shown in method 1400, based on RADAR data and LIDAR data inconsistent, it is sunny instruction that method 1400 can comprise the weather condition of environment identifying vehicle, as shown in square frame 1404.
Method 1400 may be advanced to square frame 1406, and wherein method 1400 comprises and determines that whether laser data is consistent with camera data.Such as, autonomous vehicle can comprise camera, and can receive the view data that the environment for vehicle collects.Sunny environment, sunray, solar facula, the sun or other factors to may reside in image or visible in the picture, and image can adopt object detection technique to process, such as, by providing image to object detection server and receiving the response of the object in indicating image.
Figure 15 A is the example concept diagram of sunny instruction based on the weather condition of camera image environment-identification.In Figure 15 A, vehicle 1502 travels from the teeth outwards, and comprises the sensor of the data of collecting vehicle 1502 peripheral region.Sensor can comprise LIDAR unit, and it can receive the reflected beam from the region in environment, and comprises camera, and it can be configured to the image collecting environment.Another vehicle 1504 can travel in the front of vehicle 1502.Camera can the image of capture region 1506.
Figure 15 B is the example concept diagram of the image of being caught by the vehicle 1502 in Figure 15 A.The part that the image show the afterbody of vehicle is covered by sunray or covers.The color that this image can be processed the multiple pixels determined in view data is substantially brighter than other pixels, or determines that the color of multiple pixels of being covered by sunray is substantially brighter than threshold value.Based on the percentum of the basic pixel higher than predetermined color, the second instruction of fair weather condition can be obtained.In Figure 15 B, about 50-55% of image comprises sunray, and therefore, threshold value can be met.Also lower percentum can be used.
Therefore, referring back to Figure 14, according to camera data and LIDAR data consistent (such as, both determining sunny condition), the second instruction that environment is sunny can be obtained.
In other example, view data can be processed with the object in recognition image, and when the object indicated by camera image and the object indicated by LIDAR data are inconsistent, also can obtain sunny instruction.Such as, the image in Figure 15 B can comprise solar facula 1508, its be the camera of light owing to comprising scattering in the camera lens flare caused by, cause the visible artefacts in image or fuzzy.Such solar facula 1508 may by any LIDAR Data Detection or expression, therefore, camera data and laser data inconsistent.The inconsistent instruction that can be used as sunny environment like this.
Referring back to Figure 14, method 1400 may be advanced to square frame 1408, and wherein method 1400 comprises and determines that whether laser data is consistent with weather data.Such as, the Weather information for the position of vehicle can receive from server via network, and when Weather information instruction high temperature, low clouds or sunny condition, can obtain the second instruction that environment is sunny.Weather data can alternatively or additionally from on-vehicle vehicle sensor, such as rainfall sensor or " rain detector ", photodetector or other sensing systems receive, and when LIDAR data misdirection return and from weather sensor output instruction sunlight exist time, can obtain environment sunny second instruction.
In this example, method 1400 can be performed the further instruction that provides the environment of vehicle sunny.Except the method 1200 of Figure 12 and the method 1300 of Figure 13, can also manner of execution 1400 alternatively.Such as, uncorrelated laser spots (such as, generated by sunray) can be uncorrelated relative to the vehicle of laser detection or may be uncorrelated relative to the vehicle of detections of radar, and for environment-identification sunny according to being verified by cross reference other sensing data, Weather informations etc.Any combination of the function described in Figure 14 can be combined with the function in Figure 12 and/or Figure 13 with the confidence level of the raising providing environment sunny.
Further, except above-mentioned sensing data, the input from other sensors on vehicle also can be used to additionally determining weather condition.Such as, these sensors can comprise tyre pressure sensor, engine temperature sensing unit, brake heat sensor, brake plate state sensor, tire protector sensor, fuel sensor, oil level and mass sensor, air mass sensor (particulate in detected temperatures, humidity or air) etc.
In addition, the many sensors on vehicle provide the data be processed in real-time, and that is, sensor constantly can upgrade and export with the environment being reflected in certain hour continuously or on request or sense within the scope of certain hour.
Figure 16 comprises and identifies that the environment of vehicle is the example concept lateral plan of sunny instruction.In figure 16, vehicle 1602 travels from the teeth outwards, and another vehicle 1604 is at the traveling ahead of vehicle 1602.Vehicle 1602 can comprise LIDAR unit 1606, its be configured to receive for the front of vehicle 1602 region (such as, the laser data of collecting as shown by an arrow of fig. 16), and laser data can indicate vehicle 1604 and vehicle 1602 distance d
1.Environment can be sunny, and as shown in figure 16, and LIDAR unit 1606 can receive the reflection because the sun causes.Such reflection can represent the error object in environment, all objects 1608 as shown in figure 16.But, object 1608 may not be the object that the follow-up system of vehicle 1602 tracks, because other sensor (such as, RADAR) possibly cannot detect that As time goes on object 1608 or LIDAR unit 1608 can not may track object 1608 always.When the laser data received not match tracing arrive object time, uncorrelated laser data can be considered to be due to the sun reflection cause, cause the environment of vehicle to be sunny instruction.
The position that the position of the laser data of collecting can reflect due to the sun object caused with predefined or can representing of storing compares.Exemplarily, roughly above the object tracked indicated by RADAR data and (when comparing with the some cloud stored) represent the laser data point of the object of rounded shape substantially, can process further the object caused by reflecting due to the sun is verified by comparing with the some cloud of storage.In this respect, vehicle 1602 can detect the one or more clouds around vehicle 1604, and the position of data point in cloud and number can constantly change.Therefore, the cloud reflecting the data point caused due to the sun can not have clear and definite structure, and a part for solid object, the tail end of such as vehicle, is associated with the data point defining clear surface.Similar point cloud data also can find at the rear of other vehicles.Such discovery can indicative for environments sunny.
In further example, Figure 17 detects the block diagram of the exemplary method of weather condition according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.The embodiment of method 1700 put forward the methods shown in Figure 17, such as, it can be used by the vehicle 100 illustrated with reference to figure 1 and Fig. 2 respectively and describe and/or vehicle 200, or is used by the assembly of vehicle 100 or vehicle 200.Method 1700 can be to implement discretely with any means described herein or to combine the conventional method of enforcement.
At square frame 1702, method 1700 comprises the laser data receiving and collect for the environment of vehicle.Laser data comprises the multiple laser data points based on the object in environment, and described object is perceived as physics to be existed due to reflection or rear orientation light.The assembly of vehicle or vehicle, such as computing equipment or treater, can be configured to the data laser collected by receiving.
In square frame 1704, method 1700 comprises and being associated with the one or more objects in environment by the laser data point in multiple laser data point by computing equipment.As an example, follow-up system can be used to tracing object, and can determine the laser data point that is associated with the object tracked.
At square frame 1706, method 1700 comprise by multiple laser data point, with environment in the uncorrelated given laser data point of one or more objects be defined as representing the object do not tracked.In this example, for the laser data being considered to representative object, laser data can be associated with the object in environment, and remaining reception or collect laser data can be considered to uncorrelated with the object in environment.
In this example, method 1700 can be performed to determine that but object is expected and exists not detected by laser or unexpected object is detected the example of (that is, laser measurement is lost due to sunlight or changed).As mentioned above, for the reflects laser data received, can determine that object exists thus causes laser to be reflected.Such as, but under particular weather condition, under having mist condition, mist can cause reflection, thus cause determining the mistake of object.In order to determine when that laser data is based on the real object be present in environment instead of based on the weather correlated condition of reflection causing Laser emission, computing equipment can be configured to by the laser data of laser data and known representative object and/or known be that the laser data caused due to weather associated reflections compares.Therefore, the pattern of the laser data of the laser data received and known representative object can be compared to identify may be laser data because other factors except object cause.
The pattern of the laser data of any number can be stored, and carry out more uncorrelated laser data with reference to described pattern, so that the laser data uncorrelated laser data being categorized as indicated object or the laser data caused due to weather condition (such as mist).
As another example distinguished in the laser data caused by object and carrying out between the laser data caused by other condition (such as mist), computing equipment can be configured to the movement of following the trail of the object represented by laser data.Therefore, the object all laser data all can being used as representative movement is followed the trail of, and can think in sight with the object of strange mode movement or at short notice appearing and subsiding reflect to liking the laser data caused due to weather condition the error object caused.
In some instances, the laser data of collecting for the environment of vehicle can be collected the scanning of environment by performing in time when the vehicle is running.When vehicle moves by environment, object in environment can be followed the trail of based on being confirmed as due to the laser data received existing object.Then laser data unmatched with the object tracked can be identified.
In one example, laser data can to there is water in air or usually have the Cloudy conditions of mist relevant, and such water droplet or cloud be vehicle follow-up system follow the trail of less than project.Such as, for detecting the LIDAR equipment of also positioning optical reflectance signature, the water droplet in air or the cloud feature class in air are similar to object cloud, because light pulse is reflected from particulate.LIDAR equipment possibly cannot penetrating fog, and therefore, when there is mist, many laser spots can return due to mist.Therefore, for any laser data (such as, laser data unmatched with the object tracked) received do not tracked, the object do not tracked can be determined.
In this example, method 1700 can comprise and determines the uncorrelated laser data point with the one or more objects in environment.LIDAR can be configured to perform the first scanning to environment, and is associated with the one or more objects in environment by the laser data point in multiple laser data point.Then, LIDAR can be configured to perform the second scanning to environment, and determines the laser data point with one or more object matching based on the position of the object represented by laser data point.Then, lack based on the one or more objects in scanning with first and mate, the laser data point uncorrelated with the one or more objects in environment in the second scanning can be determined.In further example, second scanning can represent after the first scan or after multiple scanning to the scanning of environment, to make object tracing there occurs a period of time, so that compare the second scan-data.
At square frame 1708, method 1700 comprises to be determined, by the instruction of the weather condition of computing equipment environment-identification based on one or more object do not tracked.In some instances, can based on determining that there is multiple object do not tracked identifies.
In addition (or alternatively), at square frame 1706, method 1700 can comprise determines that the number of the uncorrelated laser data point with the one or more objects in environment exceedes predetermined threshold, then by the instruction of computing equipment identification weather condition.Such as, if there is the laser data point uncorrelated with object of minority, then this may cause due to many factors; But for the laser data point of certain number of thresholds, higher probability can be, these laser data points are that laser beam owing to launching is by water reflection, due to sunlight, cause due to mist or due to some other weather condition.
Method 1700 can also comprise determines concrete weather condition.Such as, in order to determine fair weather condition, method can comprise the object determining not track as the vehicle moves and remain on roughly the same relative position relative to vehicle.In one example, the coordinate position of the object do not tracked can be determined in time, and the distance of vehicle and the angle relative to vehicle can be determined.As the vehicle moves, the object rested on apart from vehicle same distance place will be considered to also moving.In addition, as the vehicle moves, the object of equal angular is kept will to be considered to also moving relative to vehicle.But when the angle of object is greater than 45 degree or larger, object can be considered to caused by sunlight reflection.In other example, the object being considered to movement identically with vehicle substantially can be confirmed as caused by sunlight, and it keeps substantial constant under fair weather condition.
In this example, using method 1700, vehicle can be configured to operate in autonomous mode, and the instruction of weather condition can be utilized to determine that the driving of vehicle determines.May expect to control vehicle in a different manner according to various weather condition or road conditions, therefore, when road wets, vehicle can operate according to " wet surface " driving technique (such as, allow more braked space, underspeed).Vehicle also possibly cannot operate under some weather condition, and therefore, in some instances, vehicle can provide instruction to be transformed into manual mode with instruction request, such as by providing alarm to be started the operation to vehicle by forbidding autonomous mode to chaufeur.
Except make drive determine except, or as to make drive decision replacement, road weather condition and/or pavement conditions can trigger the alarm being sent to chaufeur.Alarm can ask chaufeur to control vehicle, or also can provide the alarm about described condition to chaufeur simply.Alarm can be any other signal of audible signal, visual signal, sense of touch or sensory signal and/or the attention obtaining chaufeur.In this illustration, after alerting driver, vehicle can receive input from chaufeur, such as steering wheel rotation, applying braking, applying accelerator, pressing emergency cut-off etc.
In further example, can based on there is the sunny degree of degree that how many uncorrelated laser data point determines that road wets, the degree of greasy weather gas or sky.In the example that road is very wet, a large amount of uncorrelated laser data points can be there is.The pattern of vehicle operating can further based on the degree that road is wet, such as autonomous mode can be adopted for moist road, and road being considered to the example that wets very much, vehicle can be transformed into manual operation mode or provide the request being transformed into manual operation mode.
Except making and driving and determine, or as to making the replacement of driving and determining, weather condition can also cause the setting of sensor of change vehicle.Such as, it can be that sunny instruction is modified based on the weather condition of the environment of vehicle that the exposure of camera is arranged, or arranging of LIDAR unit can be modified to reduce the exposure time.Also other actions be can take based on this identification, such as shield or sun shield disposed.
In some instances, (except with the object tracked uncorrelated laser data point except) additional data can be considered the further instruction providing weather condition, or the more high probability of concrete weather condition or confidence (such as, due to uncorrelated laser data point that water causes) are provided.
Figure 18 determines the block diagram of the exemplary method of the instruction of weather condition further according to the on-vehicle vehicle sensor that utilizes of at least some embodiment described herein.
As shown in square frame 1802, method 1800 can comprise determine laser data whether with RADAR data consistent, and if inconsistent, then method 1800 is included in the instruction that square frame 1804 identifies weather condition.Such as, autonomous vehicle can collect the RADAR data of the environment of vehicle, and the existence of the object of RADAR data instruction in vehicle environmental.For the water kicked up or mist or sunlight, radar will not return data, but RADAR data will identify the automobile in autonomous vehicle front.Laser data is also used to tracing object as mentioned above, and when the existence of the object indicated by RADAR data and the object indicated by LIDAR data exist inconsistent time, LIDAR data can be considered to off-square.But indicate any extra objects do not indicated by RADAR data can be the instruction of some weather condition by LIDAR data, and in order to determine concrete weather condition, can the further process of manner of execution 400.
In some instances, but by LIDAR data tracing to any object of not seeing in RADAR data can be classified as being because weather condition causes.As shown in method 1800, based on RADAR data and LIDAR data inconsistent, the instruction of weather condition can be obtained.Method 1800 can also comprise determine erroneous matching number whether higher than threshold value.Such as, return for a small amount of mistake, such as reflects laser data that are false or mistake, can not take further action or again can process laser data (as shown in square frame 1812).But when the number that mistake returns is higher than threshold value, method can comprise identification weather condition.
In further example, the uncorrelated and round-shaped laser data point formed around the object indicated by RADAR of the object indicated with RADAR can be confirmed as because mist causes.In some instances, the concentration of mist also can be estimated.Such as, in environment with the uncorrelated object of laser data point (such as, mist) and (and may being associated with laser data point) environment of being indicated by RADAR data in given object between distance can be determined, and can based on the concentration of this distance estimations mist.Use two sensors (LIDAR and RADAR), can the existence of detected object within the specific limits, and detect the position of mist to object position between distance the visbility of mist or the measurement of concentration can be provided.Concentration can be expressed as the distance of visbility, is such as such as less than the visbility of 5 kilometers, and this distance can be detect the position of mist and object position between distance determined.In further example, the determination of the concentration of mist can based on the number of uncorrelated laser data point.In the example that heavy fog or thick fog exist, a large amount of uncorrelated laser data points can be there is.
Method 1800 may be advanced to square frame 1806, and wherein method 1800 comprises and determines that whether laser data is consistent with camera data.Such as, autonomous vehicle can comprise camera, and can receive the view data that the environment for vehicle collects.Large in the rain, water can be assembled on road, and can be visible in the picture.In addition, also can identify in the picture rainy.Image can adopt object detection technique to process, such as, by providing image to object detection server and receiving the response of the object in indicating image.Therefore, in some instances, view data can be processed to identify wet surface, and when the object indicated by camera and object indicated by LIDAR data are inconsistent, and also can obtain surface that vehicle travels thereon is wet instruction.
In further example, view data can be processed the amount of the view data of the reflection determining the object be included in the environment of vehicle.Such as, when rainy, other object on brake lamp and road provides the reflection in image.The reflection that many technology are come in detected image can be used, such as such as by the path of motion of analytical characteristic point with reflection is modeled as the region comprising two different layers moved relative to each other.Based on the amount of view data of the reflection of the one or more objects in the environment of instruction vehicle higher than threshold value, can obtain surface that vehicle travels thereon is the second wet instruction.For other weather conditions, such as mist, mist can be visible in the picture, therefore, object detection technique can be used to process image, such as, by image being supplied to object detection server and receiving the response of the object in indicating image.
Method 1800 may be advanced to square frame 1808, and wherein method 1800 comprises and determines that whether laser data is consistent with other sensing datas.Such as, autonomous vehicle can have rainfall sensor or " rain detector ", and when the existing of the wet surface of LIDAR data instruction and rainfall sensor instruction rainfall, it is wet for can obtaining surface that vehicle travels or weather is the second rainy instruction.The output possibly cannot depending merely on rainfall sensor determines whether surface is wet, because stopping a period of time inside face after raining can remain wet, or surperficially when not raining can gather due to puddle or other water but wet.
In further example, the data that processing is provided by various sensor can comprise process previous time point obtain and expect the environmental data that is continuously present in environment.Such as, detailed cartographic information (such as, identify the very detailed map of the shape of road and height, four corners, cross walk, speed restriction, traffic signal, building, label, real-time traffic information or other such object and information) can be provided or via database access, and the expection brightness of different sections of highway or the data of laser intensity data value of description road can be comprised.When laser data is not mated with expection brightness or laser intensity data value, the instruction on the surface of wetting can be determined.
Method 1800 may be advanced to square frame 1810, and wherein method 1800 comprises and determines that whether laser data is consistent with weather data.Such as, can receive for the Weather information of the position of vehicle via network from server, and when Weather information instruction is being rained or when raining recently, can obtain road based on Weather information is wet second to indicate.When Weather information instruction high temperature, low clouds or sunny condition, the second instruction that environment is sunny can be obtained.When Weather information instruction may have mist or have mist condition, the second instruction that mist exists can be obtained.In further example, additional weather data, such as temperature or dew point, can to determine or by determining with server communication from on-vehicle vehicle sensor, and the weather condition that can obtain the environment of vehicle based on temperature or dew point comprises the second instruction of mist.Such as, when the difference between temperature and dew point is approximately less than 2.5 degrees Celsius or 4 Degrees Fahrenheit, mist can be formed.Under mist usually occurs in the relative humidity close to 100%, but mist can be formed at a lower humidity, and mist can not be formed sometimes under the relative humidity of 100%.The relative humidity readings of 100% means that air can not hold extra moisture again; If extra moisture is added into, air will become oversaturation.Mist can be formed suddenly, and can dissipate rapidly equally, and this depends on which side of temperature at dew point.
Method 1800 may be advanced to square frame 1812, and wherein method 1800 comprises reprocessing laser data to attempt identified object and the object identified by follow-up system to compare.Such as, method 1800 can also comprise based on to the access by the determined information of other automobiles on road, confirms the determination of weather condition.
Should be appreciated that layout described herein only for the object of example.Thus, it will be understood to those of skill in the art that and can instead use other to arrange and other element (such as the grouping etc. of machine, interface, function, order and function), and some elements together can be omitted according to the result expected.In addition, many elements of description are functional entitys, and it can be embodied as discrete or distributed assembly by any suitable combination and position or be combined with other assembly, or other structural constituents being described to absolute construction can be combined.
Although disclosed various aspect and embodiment herein, other side and embodiment will be obvious to those skilled in the art.Various aspect disclosed herein and embodiment are for illustrative purposes, and are not intended to limit, real scope should by claims and such claim have the right the equivalent enjoyed four corner indicate.It is also understood that, term used herein is only the object in order to describe specific embodiment, and is not intended to limit.
Claims (20)
1. a method, comprising:
Receive the laser data of collecting for the environment of vehicle, wherein, laser data comprises multiple laser data point;
By computing equipment, the laser data point in multiple laser data point is associated with the one or more objects in environment;
By in multiple laser data point, with environment in the uncorrelated given laser data point of one or more objects be defined as representing the object do not tracked; And
Determined, by the instruction of the weather condition of computing equipment environment-identification based on one or more object do not tracked.
2. the method for claim 1, wherein receive the laser data that the environment for vehicle collects to comprise: receive by performing the laser data collected by one or more scannings of environment, and the method also comprises:
Based on the laser data received in one or more scannings of environment, when vehicle moves the one or more objects by following the trail of during environment in environment.
3. the method for claim 1, also comprises:
For the first scanning to environment, by computing equipment, the laser data point in multiple laser data point is associated with the one or more objects in environment;
For the second scanning to environment, the laser data point with one or more object matching is determined in the position based on the object representated by laser data point; And
Lack and the mating of the one or more objects in environment based in the first scanning, determine the uncorrelated given laser data point with one or more object.
4. the method for claim 1, wherein comprised by the instruction of the weather condition of computing equipment environment-identification:
Determine that the number of the uncorrelated given laser data point with the one or more objects in environment is higher than threshold number.
5. the method for claim 1, also comprises:
Receive from one or more additional sensor the additional data that the environment for vehicle collects, wherein, additional data indicates the existence of the one or more objects in the environment of vehicle;
Determine laser data point in multiple laser data point, that be associated with the one or more objects in the environment indicated by additional data; And
Based on laser data point with the one or more objects in the environment indicated by additional data without being associated, the instruction of the weather condition of environment-identification.
6. the method for claim 1, also comprises:
Receive the radar data collected for the environment of vehicle, wherein, radar data indicates the existence of the one or more objects in the environment of vehicle;
Determine laser data point in multiple laser data point, that be associated with the one or more objects in the environment indicated by radar data; And
Based on laser data point and the one or more object onrelevants in the environment indicated by radar data, the instruction of the weather condition of environment-identification.
7. the method for claim 1, also comprises:
Receive the view data of catching from the camera being coupled to vehicle; And
Based on the second instruction of the weather condition of view data environment-identification.
8. the method for claim 1, also comprises:
Receive for the Weather information of the position of vehicle by network from server; And
Based on the second instruction of the weather condition of Weather information environment-identification.
9. the method for claim 1, also comprises:
Receive for the Current Temperatures of the position of vehicle by network from server; And
Based on the second instruction of the weather condition of Current Temperatures environment-identification.
10. the method for claim 1, also comprises:
Rainfall data are received from the rainfall sensor being coupled to vehicle; And
Based on the second instruction of the weather condition of rainfall data identification environment.
11. the method for claim 1, wherein described vehicle be configured to autonomous mode operation, and the method also comprises the instruction of the weather condition based on environment, determines that the driving of vehicle determines.
12. the method for claim 1, also comprise:
Determine the speed of vehicle, and
Wherein, it is wet for comprising by the instruction of the weather condition of computing equipment environment-identification the surface that the speed based on vehicle travels higher than threshold value identification vehicle thereon.
13. the method for claim 1, also comprise:
The laser data point of the pattern caused due to mist with the uncorrelated given laser data point of the one or more objects in environment and the representative that stores is compared; And
Based on described comparison, comprised the instruction of mist by the weather condition of computing equipment environment-identification.
14. the method for claim 1, also comprise:
By in multiple laser data point, with environment in the uncorrelated given laser data point of one or more objects be defined as representing the object do not tracked in the given position relative to vehicle;
Determine that the object do not tracked when vehicle moves remains on the substantially identical relative position relative to vehicle; And
Be sunny instruction by the weather condition of computing equipment environment-identification.
15. the method for claim 1, also comprise:
Determine the geographic position of vehicle and the time in one day;
Based on the geographic position of vehicle and the time in one day, determine the apparent position of the sun relative to vehicle; And
Based on the apparent position of the sun relative to vehicle, by the second instruction of the weather condition of computing equipment environment-identification.
16. 1 kinds of non-transitory computer-readable medium wherein storing instruction, described instruction causes computing equipment n-back test when being run by computing equipment, and described function comprises:
Receive the laser data of collecting for the environment of vehicle, wherein, laser data comprises multiple laser data point;
Laser data point in multiple laser data point is associated with the one or more objects in environment;
By in multiple laser data point, with environment in the uncorrelated given laser data point of one or more objects be defined as representing the object do not tracked; And
Determined based on one or more object do not tracked, the instruction of the weather condition of environment-identification.
17. non-transitory computer-readable medium as claimed in claim 16, wherein, receive laser data that the environment for vehicle collects and comprise and receiving by performing the laser data collected by one or more scannings of environment, and described function also comprise:
Based on the laser data received in one or more scannings of environment, move the one or more objects by following the trail of during environment in environment at vehicle.
18. non-transitory computer-readable medium as claimed in claim 16, wherein, described function also comprises:
Receive from one or more additional sensor the additional data that the environment for vehicle collects, wherein, additional data indicates the existence of the one or more objects in the environment of vehicle;
Determine laser data point in multiple laser data point, that be associated with the one or more objects in the environment indicated by additional data; And
Based on laser data point with the one or more objects in the environment indicated by additional data without being associated, the instruction of the weather condition of environment-identification.
19. 1 kinds of systems, comprising:
At least one treater; And
Data storage apparatus, comprise and can be run with the instruction making system n-back test by least one treater described, described function comprises:
Receive the laser data of collecting for the environment of vehicle, wherein, laser data comprises multiple laser data point;
Laser data point in multiple laser data point is associated with the one or more objects in environment;
By in multiple laser data point, with environment in the uncorrelated given laser data point of one or more objects be defined as representing the object do not tracked; And
Determined based on one or more object do not tracked, the instruction of the weather condition of environment-identification.
20. systems as claimed in claim 19, wherein, described vehicle is along road driving, and described function also comprises:
Receive the radar data collected for the environment of vehicle, wherein, radar data indicates the existence of the one or more objects in the environment of vehicle;
Determine laser data point in multiple laser data point, that be associated with the one or more objects in the environment indicated by radar data; And
Based on laser data point with the one or more objects in the environment indicated by radar data without being associated, the instruction of the weather condition of environment-identification.
Applications Claiming Priority (9)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/860,664 | 2013-04-11 | ||
US13/860,664 US9207323B2 (en) | 2013-04-11 | 2013-04-11 | Methods and systems for detecting weather conditions including wet surfaces using vehicle onboard sensors |
US13/873,442 | 2013-04-30 | ||
US13/873,442 US8983705B2 (en) | 2013-04-30 | 2013-04-30 | Methods and systems for detecting weather conditions including fog using vehicle onboard sensors |
US13/888,883 US9025140B2 (en) | 2013-05-07 | 2013-05-07 | Methods and systems for detecting weather conditions including sunlight using vehicle onboard sensors |
US13/888,883 | 2013-05-07 | ||
US13/888,634 | 2013-05-07 | ||
US13/888,634 US9632210B2 (en) | 2013-05-07 | 2013-05-07 | Methods and systems for detecting weather conditions using vehicle onboard sensors |
PCT/US2014/033125 WO2014168851A1 (en) | 2013-04-11 | 2014-04-07 | Methods and systems for detecting weather conditions using vehicle onboard sensors |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105324287A true CN105324287A (en) | 2016-02-10 |
CN105324287B CN105324287B (en) | 2018-07-06 |
Family
ID=51689937
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480033640.0A Active CN105324287B (en) | 2013-04-11 | 2014-04-07 | Use the method and system of onboard sensor detection weather condition |
Country Status (6)
Country | Link |
---|---|
EP (2) | EP2983955B1 (en) |
JP (4) | JP6189523B2 (en) |
KR (3) | KR102142361B1 (en) |
CN (1) | CN105324287B (en) |
DK (1) | DK2983955T3 (en) |
WO (1) | WO2014168851A1 (en) |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106291736A (en) * | 2016-08-16 | 2017-01-04 | 张家港长安大学汽车工程研究院 | Pilotless automobile track dynamic disorder object detecting method |
CN108732589A (en) * | 2017-04-24 | 2018-11-02 | 百度（美国）有限责任公司 | The training data of Object identifying is used for using 3D LIDAR and positioning automatic collection |
CN109689463A (en) * | 2016-09-13 | 2019-04-26 | 松下知识产权经营株式会社 | Pavement state forecasting system, driving assist system, pavement state prediction technique and data distributing method |
CN109753058A (en) * | 2017-11-01 | 2019-05-14 | 丰田自动车株式会社 | Automatic driving vehicle |
CN109844562A (en) * | 2016-10-21 | 2019-06-04 | 伟摩有限责任公司 | The occupancy grid generated for autonomous vehicle perception and the radar planned |
CN109952491A (en) * | 2016-11-14 | 2019-06-28 | 伟摩有限责任公司 | The object smoothly generated from sensing data using cartographic information |
CN110058240A (en) * | 2018-01-18 | 2019-07-26 | 亚德诺半导体无限责任公司 | The dynamic control and actuating of radar and Vehicular system for weather detection |
CN110441783A (en) * | 2018-08-23 | 2019-11-12 | 爱贝欧汽车系统有限公司 | For anallatic method and apparatus |
CN110662984A (en) * | 2017-05-23 | 2020-01-07 | 罗伯特·博世有限公司 | Method and device for object detection and lidar system |
CN111160561A (en) * | 2018-11-08 | 2020-05-15 | 安波福技术有限公司 | Deep learning for object detection using struts |
CN111164458A (en) * | 2017-08-09 | 2020-05-15 | 法雷奥开关和传感器有限责任公司 | Determining a maximum range of a LIDAR sensor |
CN111175713A (en) * | 2018-10-23 | 2020-05-19 | 百度（美国）有限责任公司 | Method and system for radar simulation and object classification |
CN111291697A (en) * | 2020-02-19 | 2020-06-16 | 北京百度网讯科技有限公司 | Method and device for recognizing obstacle |
CN111800582A (en) * | 2020-07-31 | 2020-10-20 | 上海眼控科技股份有限公司 | Frontal surface fog detection method and device, computer equipment and readable storage medium |
CN111902737A (en) * | 2018-03-12 | 2020-11-06 | 三菱电机株式会社 | Fog determining device, fog determining method, and fog determining program |
CN112041702A (en) * | 2018-04-11 | 2020-12-04 | 欧若拉创新公司 | Control of autonomous vehicles based on environmental object classification determined using phase-coherent LIDAR data |
CN113039580A (en) * | 2018-11-05 | 2021-06-25 | 图森有限公司 | System and method for detecting trailer angle |
CN113826028A (en) * | 2019-03-31 | 2021-12-21 | 伟摩有限责任公司 | Radar field of view extension |
CN114312794A (en) * | 2022-01-12 | 2022-04-12 | 苏州挚途科技有限公司 | System and method for identifying severe weather environment of vehicle running |
Families Citing this family (54)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CA2744630C (en) | 2010-06-30 | 2018-10-09 | Camoplast Solideal Inc. | Wheel of a track assembly of a tracked vehicle |
US8985250B1 (en) | 2010-12-14 | 2015-03-24 | Camoplast Solideal Inc. | Track drive mode management system and methods |
GB2522544A (en) * | 2014-12-18 | 2015-07-29 | Daimler Ag | Method for operating a driving assistance system of a vehicle |
CA3100440A1 (en) | 2015-03-04 | 2016-09-09 | Camso Inc. | Track system for traction of a vehicle |
JP6575776B2 (en) * | 2015-05-22 | 2019-09-18 | パナソニックＩｐマネジメント株式会社 | Road information detection device and road information detection method |
EP3313717B1 (en) | 2015-06-29 | 2020-10-14 | Camso Inc. | Systems and methods for monitoring a track system for traction of a vehicle |
DE102015112103A1 (en) * | 2015-07-24 | 2017-01-26 | Preh Gmbh | Detection device for detecting fog for a motor vehicle |
DE102016115073A1 (en) * | 2016-08-15 | 2018-02-15 | Valeo Schalter Und Sensoren Gmbh | Method for operating a distance measuring device of a vehicle, distance measuring device and driver assistance system |
US10183677B2 (en) * | 2016-09-20 | 2019-01-22 | Ford Global Technologies, Llc | Ice and snow detection systems and methods |
JP6589895B2 (en) | 2017-01-19 | 2019-10-16 | トヨタ自動車株式会社 | Object recognition device and collision avoidance device |
JP6787157B2 (en) * | 2017-01-31 | 2020-11-18 | 株式会社デンソー | Vehicle control device |
JP6876269B2 (en) * | 2017-03-08 | 2021-05-26 | スズキ株式会社 | Road surface condition estimation device |
WO2018189913A1 (en) * | 2017-04-14 | 2018-10-18 | マクセル株式会社 | Information processing device and information processing method |
US10948922B2 (en) * | 2017-06-16 | 2021-03-16 | Sensors Unlimited, Inc. | Autonomous vehicle navigation |
CN109145692B (en) * | 2017-06-28 | 2022-01-07 | 奥迪股份公司 | Vehicle driving assistance system and method |
US10447973B2 (en) | 2017-08-08 | 2019-10-15 | Waymo Llc | Rotating LIDAR with co-aligned imager |
WO2019059083A1 (en) * | 2017-09-20 | 2019-03-28 | 株式会社小糸製作所 | Vehicle sensor system, vehicle provided with said vehicle sensor system, and vehicle |
CN109709530A (en) | 2017-10-26 | 2019-05-03 | 株式会社小糸制作所 | Sensor-based system and vehicle |
CA3085012A1 (en) | 2017-12-08 | 2018-12-07 | Camso Inc. | Systems and methods for monitoring off-road vehicles |
JP6949238B2 (en) * | 2018-02-26 | 2021-10-13 | フェデックス コーポレイト サービシズ，インコーポレイティド | Systems and methods for improving collision avoidance in logistics ground support devices using fusion of multi-sensor detection |
JP6746032B2 (en) | 2018-03-12 | 2020-08-26 | 三菱電機株式会社 | Fog identification device, fog identification method, and fog identification program |
US10676085B2 (en) | 2018-04-11 | 2020-06-09 | Aurora Innovation, Inc. | Training machine learning model based on training instances with: training instance input based on autonomous vehicle sensor data, and training instance output based on additional vehicle sensor data |
WO2019236588A1 (en) | 2018-06-04 | 2019-12-12 | The Research Foundation For The State University Of New York | System and method associated with expedient determination of location of one or more object(s) within a bounded perimeter of 3d space based on mapping and navigation to a precise poi destination using a smart laser pointer device |
KR102181862B1 (en) * | 2018-09-19 | 2020-11-24 | 한국전자기술연구원 | A lidar having a structure in which a light emitting axis and a light receiving axis coincide |
JP7210208B2 (en) * | 2018-09-28 | 2023-01-23 | 株式会社デンソー | Providing device |
DE102018126506A1 (en) * | 2018-10-24 | 2020-04-30 | Valeo Schalter Und Sensoren Gmbh | Rain detection with an environment sensor for point-by-point detection of an environment of a vehicle, in particular with a LiDAR-based environment sensor |
US11403492B2 (en) | 2018-11-02 | 2022-08-02 | Aurora Operations, Inc. | Generating labeled training instances for autonomous vehicles |
US11256263B2 (en) | 2018-11-02 | 2022-02-22 | Aurora Operations, Inc. | Generating targeted training instances for autonomous vehicles |
US11829143B2 (en) | 2018-11-02 | 2023-11-28 | Aurora Operations, Inc. | Labeling autonomous vehicle data |
US11209821B2 (en) * | 2018-11-02 | 2021-12-28 | Aurora Operations, Inc. | Labeling autonomous vehicle data |
US11086319B2 (en) | 2018-11-02 | 2021-08-10 | Aurora Operations, Inc. | Generating testing instances for autonomous vehicles |
US11163312B2 (en) | 2018-11-02 | 2021-11-02 | Aurora Operations, Inc. | Removable automotive LIDAR data collection POD |
DE102018127712A1 (en) * | 2018-11-07 | 2020-05-07 | Valeo Schalter Und Sensoren Gmbh | Method for recognizing at least one tire track for a motor vehicle by means of an optoelectronic sensor, optoelectronic sensor and motor vehicle |
CN111238494B (en) * | 2018-11-29 | 2022-07-19 | 财团法人工业技术研究院 | Carrier, carrier positioning system and carrier positioning method |
US11214275B2 (en) | 2019-01-31 | 2022-01-04 | Toyota Motor Engineering & Manufacturing North America, Inc. | Vehicles, systems, and methods for changing a vehicle driving mode |
US11630209B2 (en) * | 2019-07-09 | 2023-04-18 | Waymo Llc | Laser waveform embedding |
DE102019213089A1 (en) * | 2019-08-30 | 2021-03-04 | Robert Bosch Gmbh | Determination of a humidity parameter using the range of a sensor |
US11554757B2 (en) | 2019-09-03 | 2023-01-17 | Ford Global Technologies, Llc | Vehicle sensor assembly |
US11643072B2 (en) * | 2019-09-27 | 2023-05-09 | Zoox, Inc. | Planning accommodations for particulate matter |
US11640170B1 (en) | 2019-10-29 | 2023-05-02 | Zoox, Inc. | Identification of particulate matter in sensor data |
JP7294081B2 (en) * | 2019-11-18 | 2023-06-20 | 株式会社デンソー | In-vehicle measuring device unit |
FR3105143B1 (en) * | 2019-12-20 | 2021-12-17 | Valeo Vision | Method for detecting a local condition of the road on which a motor vehicle is traveling |
DE102020112488A1 (en) * | 2020-05-08 | 2021-11-11 | HELLA GmbH & Co. KGaA | Device for detecting a weather condition in a vehicle apron |
KR102205534B1 (en) * | 2020-09-17 | 2021-01-20 | 주식회사 에스프렉텀 | Weather information analysis method and system based on driving vehicle information |
US11656629B1 (en) | 2020-12-08 | 2023-05-23 | Waymo Llc | Detection of particulate matter in autonomous vehicle applications |
WO2022185085A1 (en) * | 2021-03-03 | 2022-09-09 | 日産自動車株式会社 | Object detection method and object detection device |
CN112849161B (en) * | 2021-03-28 | 2022-06-07 | 重庆长安汽车股份有限公司 | Meteorological condition prediction method and device for automatic driving vehicle, automobile and controller |
US20220390612A1 (en) * | 2021-06-07 | 2022-12-08 | Waymo Llc | Determination of atmospheric visibility in autonomous vehicle applications |
WO2023276321A1 (en) * | 2021-06-29 | 2023-01-05 | 株式会社Ihi | Road surface state determination system, road surface state determination device, vehicle driving control device, and road surface state determination method |
CN113968302B (en) * | 2021-09-26 | 2023-01-06 | 武汉小安科技有限公司 | Control method and device for electric bicycle and electric bicycle |
EP4198573A1 (en) * | 2021-12-14 | 2023-06-21 | Tusimple, Inc. | System and method for detecting rainfall for an autonomous vehicle |
KR102459218B1 (en) * | 2022-01-28 | 2022-10-26 | 이지디텍터 주식회사 | A method for providing three-dimensional visibility information and a navigation system and a mobility and a program using the same |
WO2023146023A1 (en) * | 2022-01-28 | 2023-08-03 | 이지디텍터 주식회사 | Method for providing 3d visibility information and method for generating visibility model therefor |
WO2024003964A1 (en) * | 2022-06-27 | 2024-01-04 | 日立Astemo株式会社 | Vehicle control device and vehicle control method |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1302784A2 (en) * | 2001-10-09 | 2003-04-16 | IBEO Automobile Sensor GmbH | Method for determining visibility |
US20090032712A1 (en) * | 2007-08-03 | 2009-02-05 | Valeo Vision | Method for detecting a spray of water at the rear of a vehicle |
US20100253539A1 (en) * | 2009-04-02 | 2010-10-07 | Gm Global Technology Operations, Inc. | Vehicle-to-vehicle communicator on full-windshield head-up display |
US20110060478A1 (en) * | 2009-09-09 | 2011-03-10 | Gm Global Technology Operations, Inc. | Vehicular terrain detection system and method |
US20120083982A1 (en) * | 2010-10-05 | 2012-04-05 | Zachary Thomas Bonefas | System and method for governing a speed of an autonomous vehicle |
CN104768822A (en) * | 2012-09-20 | 2015-07-08 | 谷歌公司 | Detecting road weather conditions |
Family Cites Families (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3273530B2 (en) * | 1994-10-31 | 2002-04-08 | オムロン株式会社 | Distance measuring device and distance measuring method capable of estimating weather conditions |
JP3472896B2 (en) * | 1994-12-28 | 2003-12-02 | オムロン株式会社 | Traffic information system |
JPH08184675A (en) * | 1994-12-28 | 1996-07-16 | Mitsubishi Motors Corp | Warning device of distance between two cars |
US6611610B1 (en) * | 1997-04-02 | 2003-08-26 | Gentex Corporation | Vehicle lamp control |
JP3932623B2 (en) * | 1997-10-09 | 2007-06-20 | 日産自動車株式会社 | Auto fog lamp device |
JP3913911B2 (en) * | 1998-10-19 | 2007-05-09 | 本田技研工業株式会社 | Vehicle obstacle detection device |
JP3838418B2 (en) * | 2001-02-27 | 2006-10-25 | オムロン株式会社 | Ranging device for vehicles |
GB0115455D0 (en) * | 2001-06-25 | 2001-08-15 | Univ Nottingham | Method of controlling ground vehicle HVAC system based on using weather and positioning satellite data |
JP3664110B2 (en) * | 2001-07-04 | 2005-06-22 | 日産自動車株式会社 | Object type determination device and object type determination method |
JP3994941B2 (en) * | 2003-07-22 | 2007-10-24 | オムロン株式会社 | Radar equipment for vehicles |
JP4074577B2 (en) * | 2003-11-07 | 2008-04-09 | ダイハツ工業株式会社 | Vehicle detection method and vehicle detection device |
JP2005180994A (en) * | 2003-12-17 | 2005-07-07 | Nissan Motor Co Ltd | Front obstacle detecting device for vehicle, and method |
US7272474B1 (en) * | 2004-03-31 | 2007-09-18 | Carnegie Mellon University | Method and system for estimating navigability of terrain |
JP2006300595A (en) * | 2005-04-18 | 2006-11-02 | Advics:Kk | Navigation system |
FR2884637B1 (en) * | 2005-04-19 | 2007-06-29 | Valeo Vision Sa | METHOD OF DETECTING NIGHT MIST AND SYSTEM FOR IMPLEMENTING SAID METHOD |
JP2007322231A (en) * | 2006-05-31 | 2007-12-13 | Fujifilm Corp | Road surface condition detector |
US7633383B2 (en) * | 2006-08-16 | 2009-12-15 | International Business Machines Corporation | Systems and arrangements for providing situational awareness to an operator of a vehicle |
JP5262057B2 (en) * | 2006-11-17 | 2013-08-14 | 株式会社豊田中央研究所 | Irradiation device |
FR2919727B1 (en) * | 2007-08-03 | 2010-07-30 | Valeo Vision | METHOD FOR DETECTION BY A VEHICLE OF A VISIBILITY DISPERSING PHENOMENON |
US20090259349A1 (en) * | 2008-04-11 | 2009-10-15 | Ease Diagnostics | Delivering commands to a vehicle |
JP2009300166A (en) * | 2008-06-11 | 2009-12-24 | Asmo Co Ltd | Raindrop detecting apparatus, raindrop detecting method and vehicle-use wiper device |
US8699755B2 (en) * | 2009-02-20 | 2014-04-15 | Navteq B.V. | Determining travel path features based on retroreflectivity |
JP5441462B2 (en) * | 2009-03-23 | 2014-03-12 | オムロンオートモーティブエレクトロニクス株式会社 | Vehicle imaging device |
JP5316471B2 (en) * | 2010-04-27 | 2013-10-16 | 株式会社デンソー | Object recognition apparatus and program |
EP2606472A2 (en) * | 2010-06-11 | 2013-06-26 | Estill, James A. | System and method for manipulating data having spatial coordinates |
EP2439716B1 (en) * | 2010-09-16 | 2013-11-13 | Ricoh Company, Ltd. | Object identification device, moving object controlling apparatus having object identification device and information presenting apparatus having object identification device |
JP5967463B2 (en) * | 2010-09-16 | 2016-08-10 | 株式会社リコー | Object identification device, and moving body control device and information providing device provided with the same |
US9358883B2 (en) * | 2011-05-20 | 2016-06-07 | GM Global Technology Operations LLC | Method for use with vehicle having selectable transfer case |
JP2013020288A (en) * | 2011-07-07 | 2013-01-31 | Mitsubishi Motors Corp | Estimation device of running-unstable road surface |
JP5404974B2 (en) * | 2011-08-24 | 2014-02-05 | 三菱電機株式会社 | In-vehicle information display device |
-
2014
- 2014-04-07 KR KR1020207011507A patent/KR102142361B1/en active IP Right Grant
- 2014-04-07 JP JP2016507580A patent/JP6189523B2/en not_active Expired - Fee Related
- 2014-04-07 DK DK14783297.6T patent/DK2983955T3/en active
- 2014-04-07 KR KR1020197031785A patent/KR20190125517A/en not_active IP Right Cessation
- 2014-04-07 WO PCT/US2014/033125 patent/WO2014168851A1/en active Application Filing
- 2014-04-07 CN CN201480033640.0A patent/CN105324287B/en active Active
- 2014-04-07 EP EP14783297.6A patent/EP2983955B1/en active Active
- 2014-04-07 KR KR1020157032355A patent/KR102040353B1/en active IP Right Grant
- 2014-04-07 EP EP18203552.7A patent/EP3456597B1/en active Active
-
2017
- 2017-08-02 JP JP2017149815A patent/JP6495388B2/en active Active
-
2019
- 2019-03-06 JP JP2019040947A patent/JP2019105648A/en active Pending
-
2020
- 2020-12-10 JP JP2020204937A patent/JP7072628B2/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1302784A2 (en) * | 2001-10-09 | 2003-04-16 | IBEO Automobile Sensor GmbH | Method for determining visibility |
US20090032712A1 (en) * | 2007-08-03 | 2009-02-05 | Valeo Vision | Method for detecting a spray of water at the rear of a vehicle |
US20100253539A1 (en) * | 2009-04-02 | 2010-10-07 | Gm Global Technology Operations, Inc. | Vehicle-to-vehicle communicator on full-windshield head-up display |
US20110060478A1 (en) * | 2009-09-09 | 2011-03-10 | Gm Global Technology Operations, Inc. | Vehicular terrain detection system and method |
US20120083982A1 (en) * | 2010-10-05 | 2012-04-05 | Zachary Thomas Bonefas | System and method for governing a speed of an autonomous vehicle |
CN104768822A (en) * | 2012-09-20 | 2015-07-08 | 谷歌公司 | Detecting road weather conditions |
Cited By (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106291736A (en) * | 2016-08-16 | 2017-01-04 | 张家港长安大学汽车工程研究院 | Pilotless automobile track dynamic disorder object detecting method |
CN109689463A (en) * | 2016-09-13 | 2019-04-26 | 松下知识产权经营株式会社 | Pavement state forecasting system, driving assist system, pavement state prediction technique and data distributing method |
CN109689463B (en) * | 2016-09-13 | 2022-03-15 | 松下知识产权经营株式会社 | Road surface state prediction system, driving support system, road surface state prediction method, and data distribution method |
CN109844562B (en) * | 2016-10-21 | 2023-08-01 | 伟摩有限责任公司 | Radar generated occupancy grid for autonomous vehicle awareness and planning |
CN109844562A (en) * | 2016-10-21 | 2019-06-04 | 伟摩有限责任公司 | The occupancy grid generated for autonomous vehicle perception and the radar planned |
CN109952491A (en) * | 2016-11-14 | 2019-06-28 | 伟摩有限责任公司 | The object smoothly generated from sensing data using cartographic information |
CN108732589A (en) * | 2017-04-24 | 2018-11-02 | 百度（美国）有限责任公司 | The training data of Object identifying is used for using 3D LIDAR and positioning automatic collection |
CN108732589B (en) * | 2017-04-24 | 2022-05-13 | 百度（美国）有限责任公司 | Automatic acquisition of training data for object recognition using 3D LIDAR and localization |
CN110662984A (en) * | 2017-05-23 | 2020-01-07 | 罗伯特·博世有限公司 | Method and device for object detection and lidar system |
CN111164458A (en) * | 2017-08-09 | 2020-05-15 | 法雷奥开关和传感器有限责任公司 | Determining a maximum range of a LIDAR sensor |
CN111164458B (en) * | 2017-08-09 | 2024-03-22 | 法雷奥开关和传感器有限责任公司 | Determining a maximum range of a LIDAR sensor |
CN109753058A (en) * | 2017-11-01 | 2019-05-14 | 丰田自动车株式会社 | Automatic driving vehicle |
US11194043B2 (en) | 2018-01-18 | 2021-12-07 | Analog Devices International Unlimited Company | Radar for weather detection and dynamic control and actuation of vehicle systems |
CN110058240B (en) * | 2018-01-18 | 2023-11-10 | 亚德诺半导体国际无限责任公司 | Radar for weather detection and dynamic control and actuation of vehicle systems |
CN110058240A (en) * | 2018-01-18 | 2019-07-26 | 亚德诺半导体无限责任公司 | The dynamic control and actuating of radar and Vehicular system for weather detection |
CN111902737A (en) * | 2018-03-12 | 2020-11-06 | 三菱电机株式会社 | Fog determining device, fog determining method, and fog determining program |
CN111902737B (en) * | 2018-03-12 | 2022-02-22 | 三菱电机株式会社 | Fog determining device, fog determining method and computer readable storage medium |
CN112041702A (en) * | 2018-04-11 | 2020-12-04 | 欧若拉创新公司 | Control of autonomous vehicles based on environmental object classification determined using phase-coherent LIDAR data |
US11933902B2 (en) | 2018-04-11 | 2024-03-19 | Aurora Operations, Inc. | Control of autonomous vehicle based on environmental object classification determined using phase coherent LIDAR data |
CN110441783B (en) * | 2018-08-23 | 2021-08-03 | 爱贝欧汽车系统有限公司 | Method and device for optical distance measurement |
CN110441783A (en) * | 2018-08-23 | 2019-11-12 | 爱贝欧汽车系统有限公司 | For anallatic method and apparatus |
CN111175713B (en) * | 2018-10-23 | 2023-08-04 | 百度（美国）有限责任公司 | Method and system for radar simulation and object classification |
CN111175713A (en) * | 2018-10-23 | 2020-05-19 | 百度（美国）有限责任公司 | Method and system for radar simulation and object classification |
CN113039580B (en) * | 2018-11-05 | 2023-12-15 | 图森有限公司 | On-board control system for tractors and trailers and method for determining the angle between a tractor and a trailer |
CN113039580A (en) * | 2018-11-05 | 2021-06-25 | 图森有限公司 | System and method for detecting trailer angle |
CN111160561B (en) * | 2018-11-08 | 2023-12-01 | 动态Ad有限责任公司 | Deep learning of object detection using struts |
CN111160561A (en) * | 2018-11-08 | 2020-05-15 | 安波福技术有限公司 | Deep learning for object detection using struts |
CN113826028A (en) * | 2019-03-31 | 2021-12-21 | 伟摩有限责任公司 | Radar field of view extension |
CN111291697A (en) * | 2020-02-19 | 2020-06-16 | 北京百度网讯科技有限公司 | Method and device for recognizing obstacle |
CN111291697B (en) * | 2020-02-19 | 2023-11-21 | 阿波罗智能技术(北京)有限公司 | Method and device for detecting obstacles |
CN111800582A (en) * | 2020-07-31 | 2020-10-20 | 上海眼控科技股份有限公司 | Frontal surface fog detection method and device, computer equipment and readable storage medium |
CN114312794A (en) * | 2022-01-12 | 2022-04-12 | 苏州挚途科技有限公司 | System and method for identifying severe weather environment of vehicle running |
Also Published As
Publication number | Publication date |
---|---|
JP2016522886A (en) | 2016-08-04 |
KR102040353B1 (en) | 2019-11-04 |
JP7072628B2 (en) | 2022-05-20 |
EP2983955A1 (en) | 2016-02-17 |
KR102142361B1 (en) | 2020-08-07 |
JP6189523B2 (en) | 2017-08-30 |
CN105324287B (en) | 2018-07-06 |
EP2983955B1 (en) | 2019-06-05 |
EP2983955A4 (en) | 2016-11-02 |
JP2017194487A (en) | 2017-10-26 |
JP2021047204A (en) | 2021-03-25 |
EP3456597B1 (en) | 2023-06-07 |
JP2019105648A (en) | 2019-06-27 |
JP6495388B2 (en) | 2019-04-03 |
EP3456597A2 (en) | 2019-03-20 |
KR20200044157A (en) | 2020-04-28 |
EP3456597A3 (en) | 2019-03-27 |
KR20150141190A (en) | 2015-12-17 |
WO2014168851A1 (en) | 2014-10-16 |
DK2983955T3 (en) | 2019-07-15 |
KR20190125517A (en) | 2019-11-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230069346A1 (en) | Methods and Systems for Detecting Weather Conditions using Vehicle Onboard Sensors | |
JP7072628B2 (en) | Methods and systems for detecting weather conditions using in-vehicle sensors | |
US9632210B2 (en) | Methods and systems for detecting weather conditions using vehicle onboard sensors | |
US11802831B1 (en) | Characterizing optically reflective features via hyper-spectral sensor | |
US9207323B2 (en) | Methods and systems for detecting weather conditions including wet surfaces using vehicle onboard sensors | |
US9360556B2 (en) | Methods and systems for detecting weather conditions including fog using vehicle onboard sensors | |
US9025140B2 (en) | Methods and systems for detecting weather conditions including sunlight using vehicle onboard sensors | |
US9121703B1 (en) | Methods and systems for controlling operation of a laser device | |
US9097800B1 (en) | Solid object detection system using laser and radar sensor fusion | |
US8818609B1 (en) | Using geometric features and history information to detect features such as car exhaust in point maps | |
US9043072B1 (en) | Methods and systems for correcting an estimated heading using a map | |
US9086481B1 (en) | Methods and systems for estimating vehicle speed | |
US9080866B1 (en) | Methods and systems for detection of reflective markers at long range | |
US9335766B1 (en) | Static obstacle detection | |
CN105358397A (en) | Predictive reasoning for controlling speed of a vehicle | |
JP2023507108A (en) | Adjusting the vehicle sensor field of view volume | |
CN108482378A (en) | The method and controller of movement for controlling vehicle | |
CN104768822A (en) | Detecting road weather conditions | |
IL294243A (en) | Identification of proxy calibration targets for a fleet of vehicles |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
TA01 | Transfer of patent application right |
Effective date of registration: 20171030Address after: California, USAApplicant after: Weimo Co.,Ltd.Address before: California, USAApplicant before: Giant HoldingsEffective date of registration: 20171030Address after: California, USAApplicant after: Giant HoldingsAddress before: California, USAApplicant before: Google Inc. |
|
TA01 | Transfer of patent application right | ||
GR01 | Patent grant | ||
GR01 | Patent grant |