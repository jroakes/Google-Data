US8019604B2 - Method and apparatus for uniterm discovery and voice-to-voice search on mobile device - Google Patents
Method and apparatus for uniterm discovery and voice-to-voice search on mobile device Download PDFInfo
- Publication number
- US8019604B2 US8019604B2 US11/962,866 US96286607A US8019604B2 US 8019604 B2 US8019604 B2 US 8019604B2 US 96286607 A US96286607 A US 96286607A US 8019604 B2 US8019604 B2 US 8019604B2
- Authority
- US
- United States
- Prior art keywords
- phoneme
- uniterms
- audio
- electronic device
- uniterm
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
- 238000000034 method Methods 0.000 title claims abstract description 55
- 238000004215 lattice model Methods 0.000 claims abstract description 40
- 230000006870 function Effects 0.000 claims description 46
- 238000003860 storage Methods 0.000 claims description 7
- 238000010295 mobile communication Methods 0.000 claims description 6
- 230000007246 mechanism Effects 0.000 claims description 3
- 238000004891 communication Methods 0.000 abstract description 44
- 230000008569 process Effects 0.000 description 21
- 238000013459 approach Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000012545 processing Methods 0.000 description 6
- 238000000605 extraction Methods 0.000 description 5
- 238000013179 statistical model Methods 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 4
- 230000001413 cellular effect Effects 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 230000015654 memory Effects 0.000 description 3
- 230000005236 sound signal Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 101100521334 Mus musculus Prom1 gene Proteins 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000002354 daily effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000003467 diminishing effect Effects 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 238000002474 experimental method Methods 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000035755 proliferation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 210000003813 thumb Anatomy 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/63—Querying
- G06F16/632—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/685—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using automatically derived transcript of audio data, e.g. lyrics
Definitions
- the present invention generally relates to communication devices and in particular to mechanisms and methodology for performing audio content search by voice query on communication devices.
- cellular phones and other communication devices typically provide a search function on the device support for performing searches within content that is stored/maintained on the device.
- search functions cab be performed using a text-based search technology.
- words In text based search technology, “words” (or character combinations) plays a critical role. These words may be manually inputted into the device using the devices input mechanism (keypad, touch screen, and the like); It is well-known that the it is a challenge task for user to enter text on mobile devices such as cell-phone. Therefore, it is desirable and more convenient that the words are provided as audio data that is spoken by the user and detected by the devices microphone. In view of the following sections, it is also necessary that voice be used as a query form where user can easily mimic the sound stored as content.
- the searching methodology is based on speech-to-text such as a dictation system, wherein speech is first converted into text using a dictionary of known spoken words/terms.
- One of the methods utilized relies on a use of phonemes derived from the audio data to perform searches and is referred to as a phoneme-based approach (as opposed to a manually-input text based approach).
- the process of discovering “words” from audio data input remains a challenging task on mobile communication devices. It is also a difficult task on a server-based computer system because the performance of the speech recognition system is dependent on the language coverage and word-coverage of the dictionaries and the language models.
- Another recent phoneme-based approach to deciphering audio data does not need actual word discovery. But, the approach makes uses of very limited contextual information, such as one phoneme or two phoneme segments in the phoneme lattice as feature vector, and involves sequentially processing the features of audio data. The approach thus needs to sequentially process the features of the audio data, and the limited locality information results in an expensive fine match.
- FIG. 1 is a block diagram of an example mobile communication device configured with hardware and software components for providing uniterm generation/discovery and voice-to-voice search functionality, in accordance with embodiments of the invention
- FIG. 2 is a sequence diagram illustrating use of hardware and software components to complete the sequence of operations during uniterm discovery/generation and voice-to-voice searching using the discovered uniterms, in accordance with one embodiment of the invention
- FIG. 3 is a block diagram illustrating an isolated view of the uniterm generation/discovery engine, according to one embodiment of the invention.
- FIG. 4 is a flow chart of the method by which the uniterms are generated (or discovered) from voice/audio input, according to one embodiment of the invention
- FIG. 5 is a block diagram illustration the functional components utilized to complete voice-to-voice searches, utilizing uniterms and a statistical latent lattice model generated from a speech query, in accordance with one embodiment of the invention.
- FIG. 6 is a flow chart of the method by which a search is completed using uniterms that are generated from voice/audio input, according to one embodiment of the invention.
- the illustrative embodiments provide a method, system and communication device for enabling uniterm discovery from audio content, and voice-to-voice searching of audio content stored on a device using discovered uniterms.
- Audio/voice input signal is received (or captured) by a microphone or other audio receiving device.
- the audio signal is stored as audio data and sent to a uniterm discovery and search (UDS) engine within the device.
- the audio data may be associated with other non-audio content that is also stored within the device.
- the UDS engine retrieves (or discovers) a number of uniterms from the audio signal and associates the uniterms with the audio data.
- the uniterms for the audio database are organized as a phoneme uniterm tree structure to ensure an efficient coarse search.
- the UDS engine When a voice search is initiated at the device, the UDS engine generates a statistical latent lattice model from the voice query and scores the uniterms tree from the audio database against the latent lattice model. Following a further refinement, the best group of uniterms are then determined and segments of the stored audio data and/or other content, such as the best phoneme paths, corresponding to the best group of uniterms are selected as the candidate list of inputs for the fine search. The fine search is then conducted based on the match between the best paths of the candidate list and the query lattice. The final results are produced from the fine search ranks of the candidate list.
- the use of specific component, device and/or parameter names are for example only and not meant to imply any limitations on the invention.
- the invention may thus be implemented with different nomenclature/terminology utilized to describe the components/devices/parameters herein, without limitation.
- Each term utilized herein is to be given its broadest interpretation given the context in which that terms is utilized.
- the term “uniterm” is defined as a sequence of symbols (or phoneme strings) derived from segments of audio data stored within an audio database. Within the latent statistical model, provided below, the uniterms are be represented as symbols (X 1 . . . Xn) that are then scored against the latent statistical model using a set of probabilities, as defined herein.
- FIG. 1 depicts a block diagram representation of an example device within which the features of the invention are practiced. Specifically, the device is illustrated having components that enable the device to operate as a mobile communication device, such as a cellular/mobile phone. Thus, for consistency throughout the description, the device is referred to as communication device 100 . It is however appreciated that the features of the invention described herein are fully applicable to other types of devices (including other communications devices, other than cellular phones, and other computing devices) and that the illustration of communication device 100 and description thereof as a mobile phone is provided solely for illustration.
- communication device may be a personal digital assistant (PDA), a BlackberryTM, an Ipod®, or other similar potable device, which is designed or enhanced with the functionality to store content associated with voice/audio data and perform a search of the content using voice-to-voice searching, as described herein.
- PDA personal digital assistant
- Ipod® an Ipod®
- the communication device may also be non-portable (e.g., a computer, a desktop phone, or vehicle-integrated car phone) with similar voice-to-voice search capabilities/functionality built in.
- communication device 100 comprises central controller 105 , which is connected to memory 110 and which controls the communications operations of communication device 100 . Included among these operations are the generation, transmission, reception, and decoding of speech (or audio), encoded light, and data signals.
- controller 105 comprises digital signal processor (DSP) 106 , which handles the receipt and transmission of analog and/or digital signals.
- DSP digital signal processor
- Controller 105 also comprises programmable microprocessor 107 , which controls the overall functions of communication device 100 . While shown as separate components, it is understood that the functionality provided by both processing components within controller 105 may be integrated into a single component.
- microprocessor 107 is a conventional multi-purpose microprocessor, such as an MCORE family processor, and DSP 106 is a 56600 Series DSP, each device being available from Motorola, Inc.
- Communication device 100 also comprises input devices, of which keypad 120 , and microphone (mic) 130 are illustrated connected to controller 105 .
- Microphone 130 represents any type of acoustic capture/receiving device that detects/captures audio (or acoustic) sounds/signals that may be converted into a digital/analog representation and manipulated within communication device 100 .
- communication device also supports receipt of voice/audio input via one or more externally connected/coupled devices, including Bluetooth® (BT) headset 131 (paired with internal BT adapter 133 ) and wired microphone 132 (inserted into plug-in jack 134 ).
- communication device 100 comprises output devices, including speaker 135 and display 140 .
- Communication device 100 includes a camera 145 , which enables communication device 100 to record still images and/or moving video.
- microphone 130 is provided for converting speech (voice or audio input) from the user into electrical signals (voice or audio data), while internal speaker 140 provides acoustic signals (output) to the user.
- voice coder/decoder vocoder
- controller 105 provides analog-to-digital and or digital-to-analog signal conversion.
- communication device 100 further includes transceiver 170 , which is connected to antenna 175 .
- Transceiver 170 in combination with antenna 175 , enable communication device 100 to transmit and receive wireless radio frequency (RF) signals from and to communication device 100 .
- Transceiver 170 includes an RF modulator/demodulator circuit (not shown) that generates and deciphers/converts the RF signals.
- RF radio frequency
- communication device 100 is a mobile phone, some of the received RF signals may be converted into speech/audio signals, which are outputted via speaker 140 .
- Communication device 100 may be a Global System for Mobile communications (GSM) phone and include a Subscriber Identity Module (SIM) card adapter 160 .
- SIM card adapter 160 enables a SIM card (not specifically shown) to be inserted and accessed by controller 105 .
- FIG. 1 may vary depending on implementation. Other internal hardware or peripheral devices may be used in addition to or in place of the hardware depicted in FIG. 1 . Thus, the depicted example is meant solely for illustration and is not meant to imply architectural limitations with respect to the present invention.
- UDS utility 115 (or interchangeably referred to as Voice-to-Voice Search (VVS) utility).
- VVS Voice-to-Voice Search
- UDS utility 115 When executed by microprocessor 107 , key functions provided by UDS utility 115 include, but are not limited to: (1) retrieving/discovering one or more uniterms from audio data and associating the discovered uniterms with content stored within the communication device 100 ; (2) maintaining an audio database ( 230 , FIG.
- a voice search is initiated (i.e., a voice query detected) at the device 100 , generating a statistical latent lattice model from the voice query and scoring the uniterms stored in the database against the latent lattice model utilizing a series of probability evaluations to produce a set of best “scoring” uniterms, corresponding to specific ones of the stored content; and (4) returning the content associated with the best uniterms as the result of the voice query.
- the returned content is identified by an audio label/tag from which the best scoring uniterm(s) were generated.
- aspects of the disclosed embodiments provide a process of automatically generating a “dictionary” representation for voice search (during a uniterm Discovery Process) and then utilizing this dictionary in voice search (during a Search Process).
- the invention involves extracting phoneme strings from segments of audio data in which the phoneme string is considered to be a good estimate of the actual phonetic content by virtue of the phoneme string's consistency within the phoneme lattice.
- These phoneme strings extracted from all of the utterances in an audio database, play the role of words in subsequently attempting to match a new utterance having the same lexical content.
- the invention seeks to identify which of these “words” (referred to herein as “uniterms”) also appear with consistency within the lattice representation (i.e., the statistical latent lattice model) of a new utterance.
- the identified uniterms allow the incoming utterance to be associated with the corresponding content in the audio database.
- One embodiment of the invention provides a sememeless term or vocabulary discovery strategy, where a sememe is a unit of transmitted or intended meaning (of a smallest unit of word).
- the invention recognizes that use of a sememeless term or discovery strategy is more practical since the audio segments may contain non-speech sound such as noise and music, or foreign terms, names of people, and places that are missing from the dictionary.
- the invention further recognizes that performing searches with vocabulary such as “in dictation” is very challenging on mobile devices with limited computational power.
- the voice-to-voice methodology requires very little computational power for large vocabulary conversational speech recognition (LVCSR).
- LVCSR large vocabulary conversational speech recognition
- One embodiment of the invention enhances the phoneme-based approach to performing voice searches.
- voice-to-voice searches are provided without requiring “word” discovery, by adding the use of contextual information, and thus eliminating the need for sequential processing of audio data.
- the functionality of the described embodiments also removes the explicit word boundaries in the audio database, when compared with voice to text search.
- Embodiments of the described invention make use of the sememeless term or vocabulary discovery strategy. As described in greater details below, during the discovery process, phoneme recognition is performed on the utterances and a phoneme lattice is generated.
- the top N branches (uniterms) with best scores from the uniterm tree are determined and kept, and then a fine search is performed on the lattice associated with the top N uniterms.
- the described embodiments thus provide a more practical and efficient solution to evaluate audio segments, which may contain none-intelligence speech or sounds, such as noise and music, and/or foreign terms, names of people, and places that are not represented within a standard language dictionary.
- UDS engine 200 comprises functional components (i.e., hardware and functional software/utility) within communication device 100 , which functional components complete specific portions of uniterm discovery and indexing (which are also illustrated by FIG. 3 ) and uniterm searching (which is also illustrated by FIG. 5 ).
- the searching side of UDS engine 200 includes the following functional components with the corresponding, described functionality:
- a voice query 201 is received (on the searching side of the UDS engine 200 ( FIG. 2 )) to search for particular content that is identified by a previously stored voice/audio input.
- the voice query 201 is received and analyzed by the speech recognizer 210 , which generates the voice query's phoneme lattice 212 .
- Voice query 201 is received/detected by a speech input device of communication device 100 ( FIG. 1 ), such as internal microphone 130 , Bluetooth 131 , and/or external microphone 132 ( FIG. 1 ).
- speech recognizer 210 may include or be associated with a vocodec, which converts the audio/voice signal into its representative audio/voice data.
- the indexing side of UDS engine further includes audio database 230 , which is utilized to store the audio content, segments of which are later retrieved following the voice-to-voice search initiated by the voice query 201 .
- audio database 230 is utilized to store the audio content, segments of which are later retrieved following the voice-to-voice search initiated by the voice query 201 .
- stored audio content from audio database 230 is sent through speech recognizer 210 , which generates the audio content phoneme lattice 211 .
- multiple phoneme lattices are generated, each corresponding to a segment of audio content within the audio database 230 .
- the audio content phoneme lattice 211 is then passed through uniterm extraction function 214 , which generates a plurality of uniterms corresponding to the different audio content (and segments thereof) within the audio database 230 .
- the uniterms generated by uniterm extraction function 214 are stored within bestpath and uniterm index database 218 , with the uniterms indexed according to some pre-established pattern to form a phonene uniterm tree that is utilized during the coarse search function.
- the uniterms for the audio database are organized as a phoneme uniterm tree structure to ensure an efficient coarse search.
- the best paths are determined from the phoneme lattice and also stored within the bestpath and uniterm index database 218 . During the voice-to-voice search uniterms and best paths are forwarded to the coarse search function 220 for scoring against the statistical latent lattice model 215 .
- speech recognizer 210 receives audio/voice input and generates a corresponding phoneme lattice 211 and 212 .
- UDS utility 115 FIG. 1
- feature extraction generates a plurality of uniterms, represented via uniterm index ( 218 ) stored within bestpath & uniterm index database 218 .
- the voice-to-voice search functionality has no explicit word boundaries in the audio database ( 230 ).
- a user of the communication device ( 100 ) is able to simply utter a sequence of sounds to extract content (e.g., pictures, video, documents, from the content stored within the communication device) and audibly (by voice tagging/association) highlight the content or portions thereof.
- the probabilistic estimates that can be used in the phoneme lattice statistical model are phoneme conditional probabilistic estimates, and N-gram counts can be extracted from the phoneme lattice.
- an N-gram conditional probability is utilized to determine a conditional probability of item X given previously seen item(s), i.e. p(item X
- an N-gram conditional probability is used to determine the probability of an item occurring based on N ⁇ 1 item strings before it.
- a bi-gram phoneme conditional probability can be expressed as p(X N
- a phoneme unigram “conditional” probabilistic estimate is simply the probabilistic estimate of X occurring in a given set of phonemes (i.e., the estimate is not really a conditional probability).
- yz) can be estimated from unigram and bi-gram conditional probabilities as p ( x
- y,z ) a*p ( x
- y )+ ⁇ * p ( x )+ ⁇ where ⁇ , ⁇ , ⁇ and ⁇ are given constants based on experiments and with the condition that ⁇ + ⁇ + ⁇ + ⁇ 1.
- the process also involves an evaluation of phoneme string scores.
- the following equation is provided to calculate the probabilistic estimate of a phoneme string p(x 1 x 2 . . . x M
- L ) p ( x 1
- L is the estimated probability that the indexing term having the phoneme string x 1 x 2 . . . x M occurs in the utterance from which lattice L was generated. Further, the probabilistic estimate is determined from the unigram [p(x 1
- L) associated with an indexing term for a particular utterance for which a lattice L has been generated can be determined more generally as: p ( x 1 x 2 . . . x M
- L ) p ( x 1
- L is the estimated probability that the indexing term having the phoneme string x 1 x 2 . . . x M occurred in the utterance from which lattice L was generated.
- the uniterm length can vary within 6-10 phonemes long or the uniterm length can be a fixed number such as 8. A long length can increase the tree size and decrease the search efficiency. A too short can decrease the search accuracy.
- N used for the N gram conditional probabilities typically has a value of 2 or 3, other values, such as 1 or 4 or even values greater than 4 could be used.
- a value of 1 for N may diminish the accuracy of the embodiments taught herein, while a value of 4 and higher (for N) may require ever increasing amounts of processing resources, with diminishing amounts of improvement, in some implementations.
- the value M which identifies how many phonemes are in an indexing term, may be in the range of 5 to 10.
- This probabilistic estimate which is a number in the range from 0 to 1, is used to assign a score of the indexing term. For example, the score may be identical to the probabilistic estimate or may be a linear function of the probabilistic estimate.
- FIGS. 3 and 4 respectively illustrate the functional components and method by which the uniterm discovery process is implemented, according to one embodiment.
- FIG. 3 illustrates the interconnected functions that perform uniterm discovery (indexing) of an example UDS engine ( 200 ). The functions execute to first produce a phoneme lattice 211 , from which best paths 318 and ultimately uniterms 319 are discovered.
- the functions of FIG. 3 and FIG. 2 overlap and, therefore, only the differences and/or additional functionality presented in FIG. 3 are now described. Additionally, the functions of FIG. 3 are referenced when describing the method process ( FIG. 4 ), which details the functional processes by which the uniterms are discovered (and indexed).
- the process of FIG. 4 begins at block 401 , and proceeds to block 403 at which stored audio/voice input is retrieved from audio database 230 .
- the audio/voice data may have been originally received at/detected by an audio input device of communication device ( 100 ), and the audio data may be stored along with other non-audio content. That is, the communication device may provide a special audio receive mode, which allows received audio to be associated with other types of content (as a name/identifying/descriptive tag).
- phoneme recognition 310 is performed (by speech recognizer 210 , FIG. 2 ) on the received audio/voice data, as shown at block 405 .
- a phoneme lattice 211 is generated.
- a latent lattice model 315 is produced from the generated phoneme lattice(s) 211 , at block 409 .
- the phoneme lattice 211 is evaluated and phoneme strings with certain lengths are extracted from the phoneme lattice(s) as best paths 318 , as provided at block 411 .
- the phoneme strings with a length that is at least equal to a pre-set minimum length are extracted from (or identified within) the phoneme lattice(s) as the one or more best paths 318 . These best paths 318 are then scored against the latent lattice model 315 (i.e., latent lattice model 315 is evaluated using the best paths 318 ).
- the top N best strings (referred to as uniterms) 319 are chosen as the “vocabularies” to represent the phoneme lattice 211 (i.e., represent the audio data segments).
- best paths 315 are extracted from the phoneme lattice 211 , and then the N best phoneme strings (uniterms) 319 are extracted from the latent lattice model 315 according to the best paths 318 .
- the process then ends at termination block 415 .
- the discovered uniterms may be stored in an indexed format to provide a phoneme uniterm tree that may be utilized for performing the coarse search function, described below.
- the search process is completed via two search functions: a coarse search function, followed by a fine search function.
- the coarse search function the UDS utility 115 scores the discovered uniterms (from the database) against the latent lattice model to find a set of possible candidates, which set may include more candidates than required to be outputted as a result of the search.
- the fine search function compares/scores the candidates resulting from the coarse search with the phoneme lattices of the voice query to yield final search results for the voice query.
- multiple different voices may record different content with similar words utilized to describe the different content.
- One functionality provided by the invention involves being able to match content related to specific uniterms, while being able to differentiate voices based on contextual information. As an example, given one query, there may be two to three content items spoken by different speakers hidden in the multiple number of segments of audio data.
- FIGS. 5 and 6 illustrate the functional components and method by which uniterm searching within the voice-to-voice search application is implemented, according to one embodiment.
- FIG. 5 illustrates interconnected functions that perform uniterm searching for a voice query within an example UDS engine ( 200 ). The functions execute to first produce a query phoneme lattice, from which uniterms are discovered and then matched.
- FIG. 3 the functions of FIG. 5 and FIG. 2 (previously described) overlap and, therefore, only the differences and/or additional functionality presented in FIG. 5 are now described.
- the functions of FIG. 5 are referenced when describing the method process ( FIG. 6 ), which details the functional processes by which the uniterms (of the stored audio data) are scored against the latent statistical lattice model generated from a voice query to perform voice-to-voice searching within the communication device ( 100 ).
- the method of FIG. 6 begins at block 601 and proceeds to block 603 , which illustrates receipt of a voice query 201 at the communication device 100 ( FIG. 1 ).
- phoneme recognition is performed (via speech recognizer 210 ) on the voice query 201 to produce phoneme lattice 212 of the voice query.
- the UDS utility 115 converts the voice query's phoneme lattice 212 into a latent statistical lattice model 215 , at block 607 .
- the UDS utility 115 retrieves a uniterm phoneme tree 518 , which is a prefix tree built from all the “uniterms” discovered from the audio database ( 230 ).
- the UDS utility 115 scores the phoneme tree 518 against the latent statistical lattice model 215 (i.e., performs a statistical probability of a match of the uniterms to the latent lattice model 215 ). Based on the resulting scores, the UDS utility 115 determines which branches of the uniterm phoneme tree are the top N branches (or uniterms) 522 , and the UDS utility 115 keeps these top N branches 522 , as provided at block 613 .
- the top N branches are those branches with the best scores, and the UDS utility evaluates all of the resulting scores to determine which branches of the uniterm tree are the top branches, which have one of a highest score relative to other branches or a score above a pre-set minimum score.
- the segments of the stored audio data and/or other content, such as the best phoneme paths, corresponding to the best group of uniterms are selected as the candidate list of inputs for the fine search.
- the final results produced from the fine search are selected from the ranks of this candidate list.
- UDS utility 115 performs a fine match/search using the voice query's phoneme lattice 212 , as shown at block 615 . In one embodiment (as illustrated by FIG.
- the UDS utility 115 utilizes the phoneme lattice ( 211 , FIG. 2 / 3 ) of the stored audio data (in database 230 ) as an input, along with the top N branches ( 729 ), to perform the fine search.
- the resulting top N audio segments 535 resulting from the fine search ( 525 ) are outputted (e.g., presented to the querying user), as shown at block 617 . Then, the process ends at block 619 .
- FIGS. 5 and 8 illustrate various methods by which the above processes of the illustrative embodiments are completed.
- the methods illustrated in FIGS. 5 and 8 have been described with reference to components shown in the other figures, it should be understood that this is merely for convenience and alternative components and/or configurations thereof can be employed when implementing the various methods. Key portions of the methods may be completed by UDS engine 200 ( FIG. 2 ) and corresponding UDS utility 115 ( FIG. 1 ) executing within communication device 100 ( FIG. 1 ) and controlling specific operations of/on communication device 100 , and the methods are thus described from the perspective of either/both UDS engine 200 and UDS utility 115 .
- one or more of the methods may be embodied in a computer readable medium containing computer readable code such that a series of steps are performed when the computer readable code is executed on a computing device.
- certain steps of the methods are combined, performed simultaneously or in a different order, or perhaps omitted, without deviating from the spirit and scope of the invention.
- the method steps are described and illustrated in a particular sequence, use of a specific sequence of steps is not meant to imply any limitations on the invention. Changes may be made with regards to the sequence of steps without departing from the spirit or scope of the present invention. Use of a particular sequence is therefore, not to be taken in a limiting sense, and the scope of the present invention is defined only by the appended claims.
- the processes in embodiments of the present invention may be implemented using any combination of software, firmware or hardware.
- the programming code (whether software or firmware) will typically be stored in one or more machine readable storage mediums such as fixed (hard) drives, diskettes, optical disks, magnetic tape, semiconductor memories such as ROMs, PROMs, etc., thereby making an article of manufacture in accordance with the invention.
- the article of manufacture containing the programming code is used by either executing the code directly from the storage device, by copying the code from the storage device into another storage device such as a hard disk, RAM, etc., or by transmitting the code for remote execution using transmission type media such as digital and analog communication links.
- the methods of the invention may be practiced by combining one or more machine-readable storage devices containing the code according to the present invention with appropriate processing hardware to execute the code contained therein.
- An apparatus for practicing the invention could be one or more processing devices and storage systems containing or having network access to program(s) coded in accordance with the invention.
- an illustrative embodiment of the present invention is described in the context of a fully functional computer (server) system with installed (or executed) software, those skilled in the art will appreciate that the software aspects of an illustrative embodiment of the present invention are capable of being distributed as a program product in a variety of forms, and that an illustrative embodiment of the present invention applies equally regardless of the particular type of media used to actually carry out the distribution.
- a non exclusive list of types of media includes recordable type (tangible) media such as floppy disks, thumb drives, hard disk drives, CD ROMs, DVDs, and transmission type media such as digital and analogue communication links.
- the software aspects of the invention are provided on a computer disk that is provided with the cell phone or other portable device, and the functionality of the UDS engine and/or UDS utility may be uploaded to the device using a computer with USB (Universal Serial Bus) connection or BT connection.
- the software may be downloaded from a service provider website or other online source.
- the software may be bought off-the shelf as a generic software offering (i.e., not proprietary and/or packaged with the device).
Abstract
Description
-
- (a)
speech recognizer 210, which receives audio/voice input (voice query) 201 and performs a recognition function to generate acorresponding phoneme lattice 212. The phoneme lattice is utilized to generate a statisticallatent lattice model 215, which is utilized to score uniterms that are stored within the audio database 230 (or within bestpath and uniterm index database 218); - (b) coarse search function 220 (which is a basic uniterm scoring subroutine that performs an initial scoring of all uniterms within the bestpath and uniterm index database 218), scores the uniterms (specifically, the phoneme uniterm tree) of the stored audio/voice data and retrieved from bestpath and
uniterm index database 218 against the statisticallatent lattice model 215. The scoring is performed via a process involving a series of probability analyses, described below.Coarse search function 220 generatescoarse search candidates 222 as the result of scoring the uniterms (or uniterm tree) against the statisticallatent lattice model 215; and - (c) fine search function 225 (which is a more specific uniterm scoring subroutine, which only scores the results of the coarse search function 220), receives the
coarse search candidates 222 from thecoarse search function 220 along with a copy of thephoneme lattice 212 fromspeech recognizer 210.Fine search function 225 performs a more refined analysis of the phoneme lattice compared with the coarse search candidates to generatefine search output 227.Fine search output 227 is the result produced (i.e., content retrieved from audio database) as the output of the voice-to-voice search initiated byvoice query 201, which search is performed using the set of stored uniterms (or phoneme uniterm tree) and the statisticallatent lattice model 215 generated from thevoice query 201.
- (a)
p(x|y,z)=a*p(x|y,z)+β*p(x|y)+γ*p(x)+ε
where α, β, γ and ε are given constants based on experiments and with the condition that α+β+γ+ε=1.
p(x 1 x 2 . . . x M |L)=p(x 1 |L)p(x 2 |x 1 ,L) . . . p(x M |x M−1 ,L),
where p(x1x2 . . . xM|L) is the estimated probability that the indexing term having the phoneme string x1x2 . . . xM occurs in the utterance from which lattice L was generated. Further, the probabilistic estimate is determined from the unigram [p(x1|L)] and bi-gram [p(xM|xM−1,L)] conditional probabilities of the phoneme lattice statistical model.
p(x 1 x 2 . . . x M |L)=p(x 1 |L)p(x 2 |x 1 ,L)p(x 3 |x 2 ,x 1 ,L) . . . p(x M |x M−1 , . . . x M+1−N ,L),
where p(x1x2 . . . xM|L) is the estimated probability that the indexing term having the phoneme string x1x2 . . . xM occurred in the utterance from which lattice L was generated. The probability/probabilistic estimate is determined from N gram (e.g., for tri-gram, N=3) conditional probabilities p(x1|L), p(x2|x1,L), . . . , p(xM|xM−1, . . . xM+1−N,L) of the phoneme lattice statistical model. The score of an uniterm can be calculated as:
S=log(p(x 1 x 2 . . . x M |L))/M+f(M),
where f(M) is a function which penalizes the short strings, for example f(M)=b*log(M) and b=0.02. The uniterm length can vary within 6-10 phonemes long or the uniterm length can be a fixed number such as 8. A long length can increase the tree size and decrease the search efficiency. A too short can decrease the search accuracy.
Claims (18)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/962,866 US8019604B2 (en) | 2007-12-21 | 2007-12-21 | Method and apparatus for uniterm discovery and voice-to-voice search on mobile device |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11/962,866 US8019604B2 (en) | 2007-12-21 | 2007-12-21 | Method and apparatus for uniterm discovery and voice-to-voice search on mobile device |
Publications (2)
Publication Number | Publication Date |
---|---|
US20090164218A1 US20090164218A1 (en) | 2009-06-25 |
US8019604B2 true US8019604B2 (en) | 2011-09-13 |
Family
ID=40789660
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/962,866 Expired - Fee Related US8019604B2 (en) | 2007-12-21 | 2007-12-21 | Method and apparatus for uniterm discovery and voice-to-voice search on mobile device |
Country Status (1)
Country | Link |
---|---|
US (1) | US8019604B2 (en) |
Cited By (100)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110246195A1 (en) * | 2010-03-30 | 2011-10-06 | Nvoq Incorporated | Hierarchical quick note to allow dictated code phrases to be transcribed to standard clauses |
US20140067402A1 (en) * | 2012-08-29 | 2014-03-06 | Lg Electronics Inc. | Displaying additional data about outputted media data by a display device for a speech search command |
US9305317B2 (en) | 2013-10-24 | 2016-04-05 | Tourmaline Labs, Inc. | Systems and methods for collecting and transmitting telematics data from a mobile device |
US20180218735A1 (en) * | 2008-12-11 | 2018-08-02 | Apple Inc. | Speech recognition involving a mobile device |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8015005B2 (en) * | 2008-02-15 | 2011-09-06 | Motorola Mobility, Inc. | Method and apparatus for voice searching for stored content using uniterm discovery |
US8880399B2 (en) * | 2010-09-27 | 2014-11-04 | Rosetta Stone, Ltd. | Utterance verification and pronunciation scoring by lattice transduction |
US9311914B2 (en) * | 2012-09-03 | 2016-04-12 | Nice-Systems Ltd | Method and apparatus for enhanced phonetic indexing and search |
US10847152B2 (en) * | 2017-03-28 | 2020-11-24 | Samsung Electronics Co., Ltd. | Method for operating speech recognition service, electronic device and system supporting the same |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050010412A1 (en) | 2003-07-07 | 2005-01-13 | Hagai Aronowitz | Phoneme lattice construction and its application to speech recognition and keyword spotting |
US20060264209A1 (en) | 2003-03-24 | 2006-11-23 | Cannon Kabushiki Kaisha | Storing and retrieving multimedia data and associated annotation data in mobile telephone system |
US7181398B2 (en) * | 2002-03-27 | 2007-02-20 | Hewlett-Packard Development Company, L.P. | Vocabulary independent speech recognition system and method using subword units |
US20070061420A1 (en) | 2005-08-02 | 2007-03-15 | Basner Charles M | Voice operated, matrix-connected, artificially intelligent address book system |
US20070106509A1 (en) | 2005-11-08 | 2007-05-10 | Microsoft Corporation | Indexing and searching speech with text meta-data |
US7257533B2 (en) * | 1999-03-05 | 2007-08-14 | Canon Kabushiki Kaisha | Database searching and retrieval using phoneme and word lattice |
US20070198511A1 (en) | 2006-02-23 | 2007-08-23 | Samsung Electronics Co., Ltd. | Method, medium, and system retrieving a media file based on extracted partial keyword |
US7337116B2 (en) * | 2000-11-07 | 2008-02-26 | Canon Kabushiki Kaisha | Speech processing system |
US20090210226A1 (en) | 2008-02-15 | 2009-08-20 | Changxue Ma | Method and Apparatus for Voice Searching for Stored Content Using Uniterm Discovery |
US7849070B2 (en) | 2005-08-03 | 2010-12-07 | Yahoo! Inc. | System and method for dynamically ranking items of audio content |
-
2007
- 2007-12-21 US US11/962,866 patent/US8019604B2/en not_active Expired - Fee Related
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7257533B2 (en) * | 1999-03-05 | 2007-08-14 | Canon Kabushiki Kaisha | Database searching and retrieval using phoneme and word lattice |
US7337116B2 (en) * | 2000-11-07 | 2008-02-26 | Canon Kabushiki Kaisha | Speech processing system |
US7181398B2 (en) * | 2002-03-27 | 2007-02-20 | Hewlett-Packard Development Company, L.P. | Vocabulary independent speech recognition system and method using subword units |
US20060264209A1 (en) | 2003-03-24 | 2006-11-23 | Cannon Kabushiki Kaisha | Storing and retrieving multimedia data and associated annotation data in mobile telephone system |
US20050010412A1 (en) | 2003-07-07 | 2005-01-13 | Hagai Aronowitz | Phoneme lattice construction and its application to speech recognition and keyword spotting |
US7725319B2 (en) * | 2003-07-07 | 2010-05-25 | Dialogic Corporation | Phoneme lattice construction and its application to speech recognition and keyword spotting |
US20070061420A1 (en) | 2005-08-02 | 2007-03-15 | Basner Charles M | Voice operated, matrix-connected, artificially intelligent address book system |
US7849070B2 (en) | 2005-08-03 | 2010-12-07 | Yahoo! Inc. | System and method for dynamically ranking items of audio content |
US20070106509A1 (en) | 2005-11-08 | 2007-05-10 | Microsoft Corporation | Indexing and searching speech with text meta-data |
US20070198511A1 (en) | 2006-02-23 | 2007-08-23 | Samsung Electronics Co., Ltd. | Method, medium, and system retrieving a media file based on extracted partial keyword |
US20090210226A1 (en) | 2008-02-15 | 2009-08-20 | Changxue Ma | Method and Apparatus for Voice Searching for Stored Content Using Uniterm Discovery |
Non-Patent Citations (2)
Title |
---|
Feipeng Li and Changxue MA, "Langauge Independent Voice Indexing and Search," Prepared and submitted for Proceedings of the 30th Annual International ACM SIGIR 2007 Conference on Research and Development in Information Retrieval, not accepted for the conference. |
Sung Yun Jung, "PCT International Search Report and Written Opinion," WIPO, ISA/KR, Korean Intellectual Property Office, Daejeon, Republic of Korea, Jul. 28, 2009. Search Report for Related Application. |
Cited By (141)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US20180218735A1 (en) * | 2008-12-11 | 2018-08-02 | Apple Inc. | Speech recognition involving a mobile device |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US20110246195A1 (en) * | 2010-03-30 | 2011-10-06 | Nvoq Incorporated | Hierarchical quick note to allow dictated code phrases to be transcribed to standard clauses |
US8831940B2 (en) * | 2010-03-30 | 2014-09-09 | Nvoq Incorporated | Hierarchical quick note to allow dictated code phrases to be transcribed to standard clauses |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US9547716B2 (en) * | 2012-08-29 | 2017-01-17 | Lg Electronics Inc. | Displaying additional data about outputted media data by a display device for a speech search command |
US20140067402A1 (en) * | 2012-08-29 | 2014-03-06 | Lg Electronics Inc. | Displaying additional data about outputted media data by a display device for a speech search command |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US9305317B2 (en) | 2013-10-24 | 2016-04-05 | Tourmaline Labs, Inc. | Systems and methods for collecting and transmitting telematics data from a mobile device |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
Also Published As
Publication number | Publication date |
---|---|
US20090164218A1 (en) | 2009-06-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8019604B2 (en) | Method and apparatus for uniterm discovery and voice-to-voice search on mobile device | |
EP2252995B1 (en) | Method and apparatus for voice searching for stored content using uniterm discovery | |
US9905228B2 (en) | System and method of performing automatic speech recognition using local private data | |
US20200258506A1 (en) | Domain and intent name feature identification and processing | |
CN107590135B (en) | Automatic translation method, device and system | |
CN106233374B (en) | Keyword model generation for detecting user-defined keywords | |
US20080130699A1 (en) | Content selection using speech recognition | |
US10917758B1 (en) | Voice-based messaging | |
CN111710333B (en) | Method and system for generating speech transcription | |
US8209171B2 (en) | Methods and apparatus relating to searching of spoken audio data | |
EP1936606B1 (en) | Multi-stage speech recognition | |
US8972260B2 (en) | Speech recognition using multiple language models | |
WO2017076222A1 (en) | Speech recognition method and apparatus | |
US8731926B2 (en) | Spoken term detection apparatus, method, program, and storage medium | |
US11862174B2 (en) | Voice command processing for locked devices | |
CN110097870B (en) | Voice processing method, device, equipment and storage medium | |
CN110797027A (en) | Multi-recognizer speech recognition | |
WO2007005098A2 (en) | Method and apparatus for generating and updating a voice tag | |
US10866948B2 (en) | Address book management apparatus using speech recognition, vehicle, system and method thereof | |
GB2451938A (en) | Methods and apparatus for searching of spoken audio data | |
TWI731921B (en) | Speech recognition method and device | |
US11328713B1 (en) | On-device contextual understanding | |
GB2465384A (en) | A speech recognition based method and system for retrieving data | |
KR20070069821A (en) | Wireless telecommunication terminal and method for searching voice memo using speaker-independent speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: MOTOROLA, INC.,ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MA, CHANGXUE;REEL/FRAME:020412/0121Effective date: 20071214Owner name: MOTOROLA, INC., ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MA, CHANGXUE;REEL/FRAME:020412/0121Effective date: 20071214 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY, INC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA, INC;REEL/FRAME:025673/0558Effective date: 20100731 |
|
ZAAA | Notice of allowance and fees due |
Free format text: ORIGINAL CODE: NOA |
|
ZAAB | Notice of allowance mailed |
Free format text: ORIGINAL CODE: MN/=. |
|
ZAAA | Notice of allowance and fees due |
Free format text: ORIGINAL CODE: NOA |
|
ZAAB | Notice of allowance mailed |
Free format text: ORIGINAL CODE: MN/=. |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: CHANGE OF NAME;ASSIGNOR:MOTOROLA MOBILITY, INC.;REEL/FRAME:029216/0282Effective date: 20120622 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034421/0001Effective date: 20141028 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20230913 |