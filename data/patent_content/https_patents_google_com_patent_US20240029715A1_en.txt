US20240029715A1 - Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data - Google Patents
Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data Download PDFInfo
- Publication number
- US20240029715A1 US20240029715A1 US18/355,508 US202318355508A US2024029715A1 US 20240029715 A1 US20240029715 A1 US 20240029715A1 US 202318355508 A US202318355508 A US 202318355508A US 2024029715 A1 US2024029715 A1 US 2024029715A1
- Authority
- US
- United States
- Prior art keywords
- speech
- training
- encoder
- textual
- alignment
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 221
- 238000000034 method Methods 0.000 claims abstract description 84
- 230000015654 memory Effects 0.000 claims description 32
- 238000012545 processing Methods 0.000 claims description 17
- 230000003750 conditioning effect Effects 0.000 claims description 12
- 238000013515 script Methods 0.000 claims description 11
- 230000000306 recurrent effect Effects 0.000 claims description 6
- 238000013528 artificial neural network Methods 0.000 claims description 5
- 238000004891 communication Methods 0.000 claims description 4
- 230000008569 process Effects 0.000 description 53
- 238000013518 transcription Methods 0.000 description 31
- 230000035897 transcription Effects 0.000 description 31
- 238000009826 distribution Methods 0.000 description 29
- 239000013598 vector Substances 0.000 description 17
- 230000006870 function Effects 0.000 description 9
- 238000004590 computer program Methods 0.000 description 8
- 230000004913 activation Effects 0.000 description 7
- 238000001994 activation Methods 0.000 description 7
- 201000007201 aphasia Diseases 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 230000000873 masking effect Effects 0.000 description 5
- 230000001143 conditioned effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000000295 complement effect Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- ATJFFYVFTNAWJD-UHFFFAOYSA-N Tin Chemical compound [Sn] ATJFFYVFTNAWJD-UHFFFAOYSA-N 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000001737 promoting effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
Definitions
- This disclosure relates to using aligned text and speech representations to train automatic speech recognition models without transcribed speech data.
- ASR Automatic speech recognition
- WER word error rate
- latency e.g., delay between the user speaking and the transcription
- one challenge in developing deep learning-based ASR models is a substation amount of transcribed speech is required during training.
- unspoken text data is used to train the ASR models to supplement a small set of transcribed speech training data.
- this challenge is further complicated when training ASR models with low-resource languages that include zero available transcribed speech training data.
- One aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for using aligned text and speech representations to train automatic speech recognition models without transcribed speech data.
- the operations include receiving training data that includes unspoken textual utterances in a target language. Each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech.
- the operations also include generating a corresponding alignment output for each unspoken textual utterance of the received training data using an alignment model trained on transcribed speech utterances in one or more training languages each different than the target language.
- the operations also include generating a corresponding encoded textual representation for each alignment output using a text encoder.
- the operations also include training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
- training the speech recognition model includes training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning.
- the speech recognition model includes an audio encoder and a decoder.
- the decoder may include a recurrent neural network transducer (RNN-T) architecture.
- the audio encoder includes the text encoder, a speech encoder, and a shared encoder.
- the encoder may include a plurality of multi-headed self-attention layers.
- the audio encoder may include a Conformer encoder.
- the operations further include conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language.
- Conditioning the at least one of the audio encoder or the decoder includes conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers.
- the alignment model may be trained on transcribed speech utterances in one or more training languages different than the target language.
- training the speech recognition model includes: generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space for each alignment output; for each transcribed speech utterance in the one or more training languages, determining an encoded audio representation of the transcribed speech utterance using a speech encoder and generating a second encoded shared representation of the transcribed speech utterance in the shared latent representation space.
- training the speech recognition model includes training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages.
- Each unspoken textual utterance may include a sequence of words, word-pieces, graphemes, and/or phonemes.
- the operations further include converting a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages using a pronunciation model.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations.
- the operations include receiving training data that includes unspoken textual utterances in a target language. Each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech.
- the operations also include generating a corresponding alignment output for each unspoken textual utterance of the received training data using an alignment model trained on transcribed speech utterances in one or more training languages each different than the target language.
- the operations also include generating a corresponding encoded textual representation for each alignment output using a text encoder.
- the operations also include training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
- training the speech recognition model includes training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning.
- the speech recognition model includes an audio encoder and a decoder.
- the decoder may include a recurrent neural network transducer (RNN-T) architecture.
- the audio encoder includes the text encoder, a speech encoder, and a shared encoder.
- the encoder may include a plurality of multi-headed self-attention layers.
- the audio encoder may include a Conformer encoder.
- the operations further include conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language.
- Conditioning the at least one of the audio encoder or the decoder includes conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers.
- the alignment model may be trained on transcribed speech utterances in one or more training languages different than the target language.
- training the speech recognition model includes: generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space for each alignment output; for each transcribed speech utterance in the one or more training languages, determining an encoded audio representation of the transcribed speech utterance using a speech encoder and generating a second encoded shared representation of the transcribed speech utterance in the shared latent representation space.
- training the speech recognition model includes training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages.
- Each unspoken textual utterance may include a sequence of words, word-pieces, graphemes, and/or phonemes.
- the operations further include converting a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages using a pronunciation model.
- FIG. 1 is a schematic view of an example speech recognition system.
- FIG. 2 is a schematic view of an example speech recognition model.
- FIGS. 3 A- 3 C are schematic views of an example training process for training an audio encoder of the speech recognition model of FIG. 2 .
- FIG. 4 is a schematic view of an alignment model used during the example training process for training the audio encoder of the speech recognition model in FIGS. 3 A- 3 C .
- FIG. 5 is a schematic view of an example training process for training a duration predictor of the alignment model.
- FIG. 6 is a schematic view of an example training process for training an upsampler of the alignment model.
- FIG. 7 is a flowchart of an example arrangement of operations for a method of using aligned text and speech representations to train speech recognition models without transcribed speech data.
- FIG. 8 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- Training state-of-the-art automated speech recognition (ASR) models typically requires a substantial amount of labeled training data including speech utterances each paired with a corresponding transcription (i.e., ground-truth label). Obtaining the substantial amount of labeled training data can be very costly, especially for low-resource languages.
- recent approaches for training ASR models include self-supervised training using large amounts of unlabeled training data (i.e., speech not paired with any corresponding transcriptions and/or unspoken text not paired with any corresponding speech) to complement a relatively small amount of labeled training data.
- these approaches still require some small amount of labeled training data in a particular language the ASR model is being trained to recognize.
- implementations herein are directed towards methods and systems of using aligned text and speech representation to train ASR models without transcribed speech training data.
- training the ASR model includes receiving training data that includes unspoken textual utterances in a target language where each unspoken textual utterance is not paired with any corresponding utterance of non-synthetic (or synthetic) speech.
- An alignment model generates a corresponding alignment output (i.e., aligned text representation) for each unspoken textual utterance of the received training data.
- the alignment model is trained on transcribed (i.e., labeled) speech utterances in one or more training languages each of which is different than the target language the ASR model is trained to recognize.
- a text encoder generates a corresponding encoded textual representation for each alignment output.
- the ASR model is trained on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the ASR model to learn how to recognize speech in the target language when no labeled training data in the target language is available. That is, the ASR model is trained without using any transcribed speech utterances in the target language for supervised learning.
- FIG. 1 illustrates an ASR system 100 implementing an ASR model 200 that resides on a user device 102 of a user 104 and/or on a remote computing device 201 (e.g., one or more servers of a distributed system executing in a cloud-computing environment) in communication with the user device 102 .
- a remote computing device 201 e.g., one or more servers of a distributed system executing in a cloud-computing environment
- the user device 102 is depicted as a mobile computing device (e.g., a smart phone), the user device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped with data processing hardware and memory hardware 113 .
- a tablet device e.g., a smart phone
- the user device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped with data processing hardware and memory hardware 113 .
- IoT Internet-of-Things
- the user device 102 includes an audio subsystem 108 configured to receive an utterance 106 spoken by the user 104 (e.g., the user device 102 may include one or more microphones for recording the spoken utterance 106 ) and convert the utterance 106 into a corresponding digital format associated with input acoustic frames 110 capable of being processed by the ASR system 100 .
- the user speaks a respective utterance 106 in a natural language of English for the phrase “What is the weather in New York City?” and the audio subsystem 108 converts the utterance 106 into corresponding acoustic frames 110 for input to the ASR system 100 .
- the ASR model 200 receives, as input, the acoustic frames 110 corresponding to the utterance 106 , and generates/predicts, as output, a corresponding transcription 120 (e.g., recognition result/hypothesis) of the utterance 106 .
- the user device 102 and/or the remote computing device 201 also executes a user interface generator 107 configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 102 .
- the transcription 120 output from the ASR system 100 is processed, e.g., by a natural language understanding (NLU) module executing on the user device 102 or the remote computing device 201 , to execute a user command.
- NLU natural language understanding
- a text-to-speech system may convert the transcription into synthesized speech for audible output by another device.
- the original utterance 106 may correspond to a message the user 104 is sending to a friend in which the transcription 120 is converted to synthesized speech for audible output to the friend to listen to the message conveyed in the original utterance 106 .
- the ASR model 200 may operate in a streaming fashion, a non-streaming fashion, or some combination thereof.
- the ASR model 200 operates in the streaming fashion by, while receiving the sequence of acoustic frames 110 , encoding the sequence of acoustic frames 110 and then decoding the encoded sequence of acoustic frames 110 into an initial transcription (e.g., speech recognition result/hypothesis) 120 .
- the initial transcription 120 may correspond to words, word pieces, and/or individual characters generated by the ASR model 200 as soon as they are spoken.
- the ASR model 200 operates in the non-streaming fashion by receiving and processing additional right-context to improve upon the initial transcription 120 thereby generating a final transcription 120 . That is, the ASR model 200 processes additional input audio data or encoded acoustic frames (e.g., right-context) to improve the transcription 120 output by the ASR model 200 , but at increased latency.
- an example ASR model 200 may include a Recurrent Neural Network-Transducer (RNN-T) model architecture which adheres to latency constraints with interactive applications.
- RNN-T Recurrent Neural Network-Transducer
- the use of the RNN-T model architecture is exemplary only, as the ASR model 200 may include other architectures such as transformer-transducer and conformer-transducer model architectures among others.
- the RNN-T model 200 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., no communication with a remote server is required).
- the RNN-T model 200 includes an encoder network 210 , a prediction network 220 , and a joint network 230 .
- the encoder network 210 which is roughly analogous to an acoustic model (AM) in a traditional ASR system, includes a stack of self-attention layers (e.g., Conformer or Transformer layers) or a recurrent network of stacked Long Short-Term Memory (LSTM) layers.
- the encoder network e.g., audio encoder
- the encoder network 210 includes a dual encoder framework that has a text encoder 202 and a speech encoder 204 ( FIGS. 3 B and 3 C ).
- the prediction network 220 is also an LSTM network, which, like a language model (LM), processes the sequence of non-blank symbols output by a final Softmax layer 240 so far, y 0 , . . . , y ui ⁇ 1 , into a dense representation Putt.
- the prediction network 220 and the joint network 230 may be referred to as a decoder that includes an RNN-T architecture.
- the representations produced by the encoder and prediction/decoder networks 210 , 220 are combined by the joint network 230 .
- the prediction network 220 may be replaced by an embedding look-up table to improve latency by outputting looked-up sparse embeddings in lieu of processing dense representations.
- the joint network then predicts P(y i
- the joint network 230 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses.
- the “possible speech recognition hypotheses” correspond to a set of output labels each representing a symbol/character in a specified natural language.
- the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space.
- the joint network 230 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels.
- This set of values can be a vector and can indicate a probability distribution over the set of output labels.
- the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited.
- the set of output labels can include wordpieces, phonemes, and/or entire words, in addition to or instead of graphemes.
- the output distribution of the joint network 230 can include a posterior probability value for each of the different output labels.
- the output y i of the joint network 230 can include 100 different probability values, one for each output label.
- the probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer 240 ) for determining the transcription 120 .
- the Softmax layer 240 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by the RNN-T model 200 at the corresponding output step. In this manner, the RNN-T model 200 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. The RNN-T model 200 does assume an output symbol is independent of future acoustic frames 110 , which allows the RNN-T model to be employed in the streaming fashion, the non-streaming fashion, or some combination thereof.
- the audio encoder 210 of the RNN-T model includes a plurality of multi-head (e.g., 8 heads) self-attention layers.
- the plurality of multi-head self-attention layers may include Conformer layers (e.g., Conformer-encoder), transformer layers, performer layers, convolution layers (including lightweight convolution layers), or any other type of multi-head self-attention layers.
- the plurality of multi-head self-attention layers may include any number of layers, for instance 16 layers.
- the audio encoder 210 may operate in the streaming fashion (e.g., the audio encoder 210 outputs initial higher-order feature representations as soon as they are generated), in the non-streaming fashion (e.g., the audio encoder 210 outputs subsequent higher-order feature representations by processing additional right-context to improve initial higher-order feature representations), or in a combination of both the streaming and non-streaming fashion.
- FIGS. 3 A- 3 C illustrate an example training process 300 for training the ASR model 200 ( FIG. 2 ).
- the training process 300 described herein describes training the audio encoder 210 of the ASR model 200 , however, it is understood that the training process 300 may also include pre-training and/or fine-tune training of the audio encoder 210 .
- implementations described herein contemplate the training process 300 training the audio encoder 210 of the ASR model 200 without training the decoder (e.g., prediction network 220 and joint network 230 ) of the ASR model 200 .
- the training process 300 may additionally or alternatively train other components of the ASR model 200 (e.g., prediction network 220 and/or joint network 230 ) jointly with the audio encoder 210 .
- the training process 300 trains the audio encoder 210 using available training data that includes a set of unspoken textual utterances (X text ) 320 , a set of transcribed non-synthetic speech utterances (X sup ) 304 , and/or un-transcribed non-synthetic speech utterances (X unsup ) 306 .
- each unspoken textual utterance 320 in the set of unspoken textual utterances 320 includes text-only data (i.e., unpaired data) in a target language such that each unspoken textual utterance 320 is not paired with any corresponding spoken audio representation (i.e., speech) of the utterance.
- the target language is any language the training process 300 uses to train the audio encoder 210 to recognize where zero transcribed (i.e., labeled) training data is used during training.
- the unspoken textual utterance 320 may include any sequence text chunks including words, word-pieces, phonemes, and/or graphemes.
- the available training data may also include the un-transcribed non-synthetic speech utterances 306 (also referred to as simply “un-transcribed speech utterance 306 ”) each including audio-only data (i.e., unpaired data) in the target language such that the un-transcribed speech utterance 306 is not paired with any corresponding transcription.
- the training data includes the un-transcribed speech utterances 306 in addition to the unspoken textual utterances 306
- the un-transcribed speech utterances 306 represent different training utterances than the unspoken textual utterances 320 such that the training process 300 cannot simply pair the unspoken textual utterances 320 with the un-transcribed speech utterances 306 to generate labeled training data.
- each transcribed non-synthetic speech utterance 304 (also referred to as simply “transcribed speech utterance 304 ”) includes a corresponding transcription 302 paired with a corresponding non-synthetic speech representation of the corresponding transcribed speech utterance 304 in one or more training languages.
- Each of the one or more training languages is different from the target language.
- the one or more training languages may include 52 languages with transcribed speech utterances and the target language may include another 50 languages (e.g., each different than the 52 training languages) with text-only training data.
- the transcribed speech utterances 304 are used to train an alignment model 400 to generate alignment outputs 402 for the transcribed speech utterances 304 in the one or more training languages.
- the trained alignment model 400 is configured to receive, as input, the unspoken textual utterances 320 in the target language (e.g., different from each of the one or more training languages used to train the alignment model 400 ) and generate, as output, alignment outputs 402 in the target language used to train the audio encoder 210 .
- training the alignment model 400 using the transcribed speech utterances 304 in the one or more training languages enables the alignment model 400 to generate alignment outputs 402 in the target language even though the alignment model 400 was never trained with any training data in the target language.
- the training process 300 includes a contrastive self-supervised loss part 300 a ( FIG. 3 A ), a supervised loss part 300 b ( FIG. 3 B ), and a consistency regularization part 300 c ( FIG. 3 C ).
- the training process 300 trains the audio encoder 210 on a total loss (L tts4pretrain2 ) based on: contrastive losses (L w2v ) 316 derived using the contrastive self-supervised loss part 300 a from the unspoken training text utterances (X text ) 320 (e.g., in the target language), a corpus of transcribed non-synthetic speech utterances (X sup ) 304 (e.g., in the one or more training languages), and un-transcribed non-synthetic speech utterances (X unsup ) 306 (e.g., in the target language); supervised losses (L aux ) 342 , 344 derived using the supervised loss part 300 b from the unspoken
- the contrastive self-supervised loss part 300 a of the training process 300 may employ an alignment model 400 that is configured to generate, at each of a plurality of output steps, alignment outputs (i.e., textual representations) 402 for each of a plurality of unspoken training text utterances 320 .
- the unspoken textual utterances 320 includes unspoken text that is text-only data in the target language, i.e., unpaired data, such that each unspoken textual utterance (X text ) 320 is not paired with any synthesized or non-synthesized speech. Accordingly, the alignment model 400 generates a corresponding alignment output 402 for each of the unspoken textual utterances 320 .
- the alignment model 400 includes an embedding extractor 410 , duration predictor 420 , and an upsampler 430 .
- the embedding extractor 410 receives the unspoken textual utterance 320 that includes a sequence of text chunks including words, word-pieces, phonemes, and/or graphemes and extracts a corresponding initial textual representation (e t ) 412 .
- the initial textual representation 412 includes embedding lexical information from the unspoken textual utterance 320 .
- the embedding extractor 410 may receive a transcription 302 corresponding to a transcribed non-synthetic speech utterance 304 ( FIG. 3 C ).
- the duration predictor 420 receives the initial textual representation 412 from the embedding extractor 410 and predicts a corresponding text chunk duration (i.e., word, word-piece, phoneme, and/or grapheme duration) 422 .
- the text chunk duration 422 indicates a duration the corresponding text chunk would be spoken if a human (or text-to-speech system) spoke the unspoken textual utterance 320 .
- the unspoken textual utterance 320 may include a sequence of phonemes and the duration predictor 420 predicts a phoneme duration 422 for each phoneme in the sequence of phonemes.
- the duration predictor 420 predicts the phoneme duration 422 by predicting a probability of non-zero duration for each phoneme and predicting a probability of continuous phoneme duration for each phoneme.
- the sequence of phonemes includes regular phonemes, silences between word boundaries, and punctuation marks, only the regular phonemes are associated with non-zero duration while the silences and punctuation marks are generally associated with the continuous phoneme duration.
- the duration predictor 420 may use a sigmoid activation following a first one of two independent activations to predict the probability of non-zero duration and use a soft plus activation following a second one of the two independent projections to predict the continuous text chunk duration 422 for each text chunk.
- the duration predictor 420 determines, for each text chunk, whether the probability of non-zero duration is less than a threshold value, and when the probability of non-zero duration is less than the threshold value, a multiplier may zero-out the continuous text chunk duration 422 predicted by the softplus activation for the corresponding text chunk. Otherwise, when the probability of non-zero duration is not less than the threshold value, the predicted text chunk duration 422 may be set equal to the continuous phoneme duration predicted by the softplus activation.
- the upsampler 430 receives, for each unspoken textual utterance 320 , the corresponding initial textual representation 412 and the predicted text chunk duration 422 , and generates an alignment output (ê t ) 402 having a number of frames by upsampling the initial textual representation 412 using the corresponding predicted text chunk duration 422 .
- the alignment output 402 represents an aligned speech-text representation.
- the alignment model 400 sends the alignment output 402 to a text encoder 202 of the audio encoder 210 ( FIGS. 3 B and 3 C ).
- the alignment model 400 sends the alignment output 402 to a shared encoder 250 (e.g., bypassing the text encoder 202 ) of the audio encoder 210 ( FIGS. 3 B and 3 C ).
- the alignment output 402 serves as the encoded textual representation 312 such that the shared encoder 250 may receive the alignment output 402 directly from the alignment model 400 .
- paired training data is available and the upsampler 430 generates the alignment output 402 as follows:
- the upsampler 430 includes resampler and refiner layers that align the initial textual embedding 412 to align with a corresponding encoded audio representation 314 ( FIGS. 3 B and 3 C ) directly.
- the upsampler 430 generates the alignment output 402 as follows:
- the number of frames of the alignment output 402 indicates a predicted speech duration of the unspoken textual utterance 320 .
- the number of frames of the alignment output 402 maps (i.e., aligns) the sequence of text chunks of the unspoken textual utterance 320 to speech frames.
- the upsampler 430 includes resampler and refiner layers that replicate the initial textual embedding 412 to match the predicted text chunk duration 422 (i.e., speech duration).
- the alignment output 402 includes a textual representation of the unspoken textual utterance 320 having a timing component that aligns with how a human would speak the unspoken textual utterance 320 .
- the embedding extractor 410 receives the language identifier 321 that uniquely identifies the language of the one or more training languages and/or the target language to condition the alignment model 400 .
- training the alignment model 400 using training data in the one or more training languages, and then, generating alignment outputs 402 in the target language (e.g., different than each of the training languages) leads to low quality alignment outputs 402 because a script (e.g., Brahimic) of the target language has no overlap with scripts of the one or more training languages.
- the alignments model 400 includes a pronunciation model that converts a script of the unspoken textual utterances 320 in the target language into a representation (e.g., phonetic representation) shared across multiple languages.
- the alignment model 400 may transliterate the script of the unspoken textual utterances 320 into a different script. That is, the different script may align with the scripts of the transcribed speech utterances 304 .
- a text-to-speech (TTS) system generates an audible output to give the unspoken textual utterance 320 the timing component of human speech such that a training process may use the audible output from the TTS system (i.e., synthetic speech) to train the audio encoder 210 .
- the alignment model 400 advantageously generates the alignment output 402 thereby mapping the sequence of text chunks to speech frames directly.
- the training process 300 does not require any TTS system to generate synthetic speech from the unspoken textual utterances 320 to train the audio encoder 210 . That is, neither the training process 300 nor the alignment model 400 converts the unspoken textual utterance 320 into synthetic speech, but rather generates alignment outputs 402 (i.e., text alignments).
- FIG. 5 illustrates an example training process 500 for training the alignment model 400 using transcribed non-synthetic speech utterances 304 in the one or more training languages. That is, each of the one or more training languages is a different language than the target language audio encoder 210 ( FIG. 3 ) is trained to recognize. Moreover, each transcribed non-synthetic speech utterance 304 has a corresponding transcription 302 .
- the speech encoder 204 receives, as input, each transcribed non-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 of FIG.
- features/vectors e.g., mel-frequency spectrograms such as the acoustic frames 110 of FIG.
- the alignment model 400 receives the transcription 302 corresponding to the same non-synthetic speech utterance 304 and generates an alignment output 402 according to Equation 1.
- the text encoder 202 receives, as input, the alignment output 402 and generates, as output, for each of the plurality of output steps, an encoded textual representation (ê t ) 312 that corresponds to the transcription 302 at the corresponding output step.
- a modality loss module 550 receives the encoded textual representation 312 and the encoded audio representation 314 and generates a modality loss 552 based on comparing the encoded textual representation 3412 and the encoded audio representation as follows:
- Equation 3 adds the mean squared error (MSE) of the encoded textual representation (ê t ) 312 and the encoded audio representation (e s ) 314 to RNN-T model alignments between predicted text targets and the encoded audio representations (e s ) 314 to determine the modality loss ( MM ) 552 .
- the encoded audio representations 314 serve as a ground-truth label to train the alignment model 400 to generate alignment outputs 402 that align to the corresponding non-synthetic speech utterances 304 .
- the training process 700 may use the modality loss 552 to update parameters of the alignment model 400 .
- the training process 700 may update parameters of the duration predictor 420 and/or the upsampler 430 ( FIG. 4 ).
- FIG. 6 illustrates an example training process 600 for training the alignment model 400 using paired training data (i.e., transcribed speech utterances 304 in the one or more training languages) and unpaired training data (i.e., unspoken textual utterances 320 in the target language). That is, the training process 600 uses transcribed non-synthetic speech utterances 304 that have corresponding transcriptions 302 (i.e., paired training data) and unspoken textual utterances 320 (i.e., unpaired training data) to learn speech-aligned alignment outputs 402 .
- paired training data i.e., transcribed speech utterances 304 in the one or more training languages
- unpaired training data i.e., unspoken textual utterances 320 in the target language
- the speech encoder 204 receives, as input, each transcribed non-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 of FIG. 1 ) and generates, as output, for each of the plurality of output steps, an encoded audio representation (e s ) 314 that corresponds to the transcribed non-synthetic speech utterance 304 at the corresponding output step.
- the alignment model 400 receives the transcription 302 corresponding to the same non-synthetic speech utterance 304 and generates an alignment output according to Equation 1.
- the alignment model 400 may receive the unspoken textual utterance 320 and generate an alignment output 402 according to Equation 2.
- the text encoder 202 receives, as input, the alignment output 402 and generates, as output, for each of the plurality of output steps, an encoded textual representation (e t ) 314 .
- the audio encoder 210 may include a shared encoder 250 that receives, as input, the encoded textual representations 312 , and generates, as output, a first encoded shared representation 322 .
- the shared encoder 250 may also receive, as input, the encoded audio representations 314 and generate, as output, a second encoded shared representation 324 .
- An auxiliary decoder 390 receives, as input, the first and second encoded shared representations 322 , 324 and generates, as output, corresponding first and second probability distributions 392 , 294 over possible speech recognition hypotheses.
- An alignment masked loss module 650 receives the first probability distribution 392 corresponding to the encoded textual representation 312 and the second probability distribution 394 corresponding to the encoded audio representation 314 and generates an alignment loss 652 as follows:
- A-MLM RNNT ( t
- the alignment loss 652 from Equation 4 may be applied over the masked, sampled encoded textual representations 312 in a frequency and time domain. Notably, the alignment loss 652 may be used as a training objective for both paired training data and unpaired training data.
- the training process 600 may use the alignment loss 652 to update parameters of the alignment model 400 .
- the training process 800 may update parameters of the duration predictor 420 and/or the upsampler 430 ( FIG. 4 ).
- the audio encoder 210 includes a speech encoder 204 and a text encoder 202 , described in more detail with reference to FIGS. 3 B and 3 C .
- the audio encoder 210 (alternatively the speech encoder 204 or the text encoder 202 ( FIGS. 3 B and 3 C )) includes a Conformer encoder including a stack of Conformer blocks each of which includes a stack of multi-headed self-attention, depth wise convolution, and feed-forward layers.
- the audio encoder 210 may include another type of encoder having a stack of multi-head self-attention layers/blocks, such as a transformer or performer encoder.
- the Conformer encoder 210 can naturally be split into a feature encoder, including a convolution subsampling block 212 , and a context network, including a linear layer 214 and a stack of Conformer blocks 216 .
- the convolution subsampling block 212 has two two-dimensional-convolution layers, both with strides (2, 2), resulting in a 4 ⁇ reduction in the feature sequence length.
- the convolution subsampling block 212 receives, as input, a sequence of input features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 of FIG.
- the convolution subsampling block 212 may receive, as input, each alignment output 402 generated by the alignment model 400 from the unspoken textual utterances 320 and generate, as output, for each of the plurality of output steps, an encoded textual feature 213 that corresponds to a respective one of the alignment outputs 402 .
- encoded audio and textual features 211 , 213 (i.e., interchangeably referred to as “encoded features 211 , 213 ”) output from the convolution subsampling block 212 may be fed to a masking module 218 where some of the encoded features 211 , 213 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding masked encoded audio features 211 , 211 m and masked encoded textual features 213 , 213 m .
- the masking module 218 masks the randomly chosen encoded features 211 , 213 for masking by randomly sampling without replacement a certain proportion p of all time steps to be start indices and then masks the subsequent M consecutive time steps from every sample index, whereby some spans may overlap.
- the linear layer 214 and the Conformer blocks 216 of the context network receive the masked encoded features 211 m , 213 m (or encoded features 211 , 213 not chosen by the masking module 218 ) and outputs corresponding contrastive context vectors (i.e., encoded representation) 215 from masked encoded features 211 m , 213 m .
- a quantizer 217 receives the encoded features 211 , 213 as input, and generates quantized vectors (i.e., target context vectors) 219 as output. Thereafter, a contrastive loss module 315 derives a contrastive loss (L w2v ) 316 between the contrastive context vectors 215 at the masked positions and the target context vectors 219 as follows.
- L w2v contrastive loss
- c t is contrastive context vector 215 centered over a masked output step (i.e., time step) t and q t represents a target context vector 219 at the output step tin a set of K+1 candidate target context vectors 219 which includes q t and K distractors. Distractors may be uniformly sampled from other masked output steps of the same utterance.
- the contrastive loss 316 is optimized between the contrastive context vectors 215 at the masked positions and the target context vectors 219 .
- the contrastive loss 316 (L w2v ) is optimized for both real/human (non-synthetic) and unspoken textual utterances 320 represented by alignment outputs 402 , with additional auxiliary losses derived from the transcribed non-synthetic speech utterances 304 and the alignment outputs 402 as described in greater detail below with reference to FIG. 3 B .
- the contrastive self-supervised loss part 300 a of the training process 300 trains the audio encoder 210 using the contrastive loss 316 derived from the corresponding encoded features 211 , 213 associated with each alignment output 402 , each transcribed non-synthetic speech utterance 304 , and each un-transcribed non-synthetic speech utterance 306 provided as input to the audio encoder 210 .
- Training the audio encoder 210 may include updating parameters of the audio encoder 210 based on the contrastive losses 316 .
- the supervised loss part 300 b of the training process 300 is configured to inject lexical information into the audio encoder 210 during training based on supervised loss terms 342 , 344 derived from the transcribed non-synthetic speech utterances and the alignment outputs 402 corresponding to unspoken textual utterances 320 output by the alignment model 400 .
- the supervised loss part 300 b leverages one or more auxiliary decoders 390 for generating the supervised loss terms 342 , 344 .
- the auxiliary decoders 390 may include Connectionist Temporal Classification (CTC) decoders, Listen Attend Spell (LAS) decoders, or RNN-T decoders.
- auxiliary decoders 390 may include at least one of a phoneme decoder configured to decode a sequence of phonemes or a wordpiece decoder configured to decode a sequence of word pieces.
- the auxiliary decoders 390 could also include a grapheme decoder configured to decode a sequence of graphemes.
- the text encoder 202 of the audio encoder is configured to receive alignment outputs 402 (i.e., text embeddings) from the alignment model and the speech encoder is configured to receive transcribed non-synthetic speech utterances 304 . That is, the text encoder 202 of the audio encoder generates encoded textual representations 3123 for alignment outputs 402 (e.g., corresponding to an unspoken textual utterance 320 ) and the speech encoder 204 of the audio encoder 210 generates encoded audio representations 314 for speech inputs (i.e., transcribed non-synthetic speech utterances 304 ).
- the encoded textual representations 312 and the encoded audio representations 314 may not both be compatible with the auxiliary decoders 390 .
- the audio encoder 210 may also include a shared encoder 250 that receives the encoded textual representations 312 as input, and generates a first encoded shared representation 322 (e text ) as output.
- the shared encoder 250 receives the encoded audio representations 314 as input, and generates a second encoded shared representation (e sup ) 324 as output. Accordingly, the shared encoder 250 generates the first and second encoded shared representations 322 , 324 into a shared latent representation space compatible with the auxiliary decoder 390 .
- the shared encoder 250 receives, as input, each encoded textual representation 312 that corresponds to the alignment output 402 generated from the unspoken textual utterance 320 and generates, as output, for each of the plurality of output steps, the first encoded shared representation (e text ) 322 that corresponds to the alignment output 402 at the corresponding output step.
- the auxiliary decoder 390 including the phoneme decoder, wordpiece decoder, or the byte decoder receives, as input, each first encoded shared representation 322 output from the shared encoder 250 and generates, as output, a first probability distribution 392 over possible speech recognition hypotheses for the corresponding alignment output 402 at the corresponding time step.
- the first probability distribution 392 over possible speech recognition hypotheses includes one of possible phoneme labels, possible word piece labels, or possible grapheme labels.
- a supervised loss module 340 may determine an alignment output loss term 342 based on the first probability distribution 392 over possible speech recognition hypotheses for the alignment output 402 corresponding to the unspoken textual utterance 320 .
- the corresponding unspoken textual utterance 320 in which the alignment output 402 is generated from also serves as a ground-truth transcription.
- the supervised loss part 300 b may train the audio encoder 210 on the alignment output loss term 342 by updating parameters of the audio encoder 210 based on the alignment output loss term 342 .
- the shared encoder 250 receives, as input, each transcribed encoded audio representation 314 that corresponds to the non-synthetic speech utterance 304 and generates, as output, for each of the plurality of output steps, a second encoded shared representation (e sup ) 324 that corresponds to the transcribed non-synthetic speech utterance 304 at the corresponding output step.
- a second encoded shared representation (e sup ) 324 that corresponds to the transcribed non-synthetic speech utterance 304 at the corresponding output step.
- the auxiliary decoder 390 including the phoneme decoder, the wordpiece decoder, or the byte decoder receives, as input, each second encoded shared representation 324 output from the shared encoder 250 and generates, as output, a second probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribed non-synthetic speech utterance 304 at the corresponding output step.
- the second probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of possible phoneme labels, the possible word piece labels, or the possible grapheme labels.
- the supervised loss module 340 may determine a non-synthetic speech loss term 344 based on the second probability distribution 394 over possible non-synthetic speech recognition hypotheses and the corresponding transcription 302 paired with the transcribed non-synthetic speech utterance 304 .
- the corresponding transcription 302 serves as a ground-truth transcription and may include a sequence of target phonemes, target word pieces, and/or target graphemes.
- the supervised loss part 300 b may train the audio encoder 210 on the non-synthetic speech loss term 344 by updating parameters of the audio encoder 210 based on the non-synthetic speech loss term 344 .
- the supervised loss part 300 b of the training process 300 uses another auxiliary decoder 390 to generate a third probability distribution 393 over possible speech recognition hypotheses based on the first encoded shared representation (e text ) 322 for the alignment output 402 at the corresponding output step, whereby the supervised loss module 340 may determine another alignment output loss term 342 based on the third probability distribution 393 and the unspoken textual utterance 320 corresponding to the alignment output 402 .
- the other auxiliary decoder 390 includes the other one of the phoneme decoder, word piece decoder, or the grapheme decoder and the third probability distribution 393 over possible speech recognition hypotheses includes the other one of the possible phoneme labels, the possible word piece labels, or the possible grapheme labels.
- the other auxiliary decoder 390 also generates a fourth probability distribution 395 over possible non-synthetic speech recognition hypotheses for the corresponding second encoded shared representation 324 at the corresponding output step, whereby the supervised loss module 340 may determine another non-synthetic speech loss term 344 based on the fourth probability distribution 395 and the corresponding transcription 302 that is paired with the transcribed non-synthetic speech representation 304 .
- the fourth probability distribution 395 over possible non-synthetic speech recognition hypotheses includes the other one of the possible phoneme labels, the possible word piece labels, or the possible grapheme labels.
- the supervised loss part 300 b of the training process 300 may similarly the audio encoder 210 on the other alignment output loss term 342 and the other non-synthetic speech loss term 344 .
- the un-transcribed non-synthetic speech utterances 306 and the unspoken textual utterances 320 each correspond to “unpaired” training data whereby the contrastive loss (L w2v ) 316 derived from the unspoken textual utterances (X text ) 320 may be combined with the supervised loss aux associated with the alignment output loss term 342 to obtain an unspoken textual loss function, text , as follows.
- the contrastive loss (L w2v ) 316 derived from the un-transcribed non-synthetic speech utterances (X unsuo ) 306 may be used to express an unsupervised speech loss function, unsup_speech , as follows.
- the alignment outputs 402 and the un-transcribed non-synthetic utterances 306 may be separated or mixed within each batch.
- the loss mask ⁇ is applied when combining the loss functions text and of Equations 5 and 6 to obtain an unpaired data loss function, unpaired , as follows:
- the transcribed non-synthetic speech utterances 304 corresponds to “paired” and “supervised” training data whereby the derived contrastive loss L w2v and the derived supervised loss aux associated with the non-synthetic speech loss term 344 may be combined to obtain a paired data loss function, paired , as follows:
- the ASR model recognizes audio from the target language during inference with graphemes from the training languages.
- the supervised part 300 b of the training process 300 employs residual adaptor layers 330 that condition at least one of the audio encoder 210 or the decoder (e.g., prediction network 220 and joint network 230 ( FIG. 2 )) on a language identifier 321 that uniquely identifies the target language.
- Each residual adaptor layer 330 includes a small feed forward network including self-attention layers (e.g., 2 self-attention layers) with a bottleneck dimension.
- the residual identifier output 332 is fed to the shared encoder 250 such that the audio encoder 210 is conditioned on the language identifier 321 and does not recognize audio from the target language during inference with graphemes from the training languages.
- the consistency regularization part (i.e., modality matching part) 300 c of the training process 300 is configured to promote the audio encoder 210 to learn consistent predictions between non-synthetic speech (e.g., real/human speech) and alignment outputs 402 corresponding to unspoken textual utterances 320 by generating a consistent loss term ( cons ( ⁇ )) 352 between training utterance pairs 301 that each include a corresponding one of the transcribed non-synthetic speech utterances (X sup ) 304 and a paired alignment output 404 of the same utterance as the corresponding transcribed non-synthetic speech utterance 304 .
- non-synthetic speech e.g., real/human speech
- alignment outputs 402 corresponding to unspoken textual utterances 320
- a consistent loss term ( cons ( ⁇ )) 352 between training utterance pairs 301 that each include a corresponding one of the transcribed non-synthetic speech
- the non-synthetic speech utterance 304 and the paired alignment output 404 of each training utterance pair 301 is associated with a same ground-truth transcription.
- the consistent loss term 352 between the transcribed non-synthetic speech utterance 304 and paired alignment output 404 of the same training utterance provides an unsupervised training aspect by encouraging the audio encoder 210 to behave consistently regardless of whether the training utterance belongs to non-synthetic speech (i.e., speech training data) or the alignment output (i.e., text training data) and independent of supervised loss terms between the ground-truth transcription 302 and each of: non-synthetic speech recognition hypotheses output by the auxiliary decoder 390 ; and speech recognition hypotheses output by the auxiliary decoder 390 .
- the alignment model 400 may generate each paired alignment output 404 using the corresponding transcription 302 that is paired with transcribed non-synthetic speech utterance 304 .
- the non-synthetic speech representation 304 is associated with paired alignment output 404 generated by the alignment model 400 mapping the unspoken textual utterance 320 into speech frames.
- the text encoder 202 receives, as input, each paired alignment output 404 and generates, as output, for each of the plurality of output steps, an encoded textual representation 313 that corresponds to the paired alignment output 404 at the corresponding output step.
- the shared encoder 250 receives, as input, the encoded textual representation 313 and generates, as output, a first encoded shared representation (e* sup ) 323 .
- the auxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded shared representation 323 output from the shared encoder 250 and generates, as output, a first probability distribution 311 over possible speech recognition hypotheses for the corresponding paired alignment output 404 at the corresponding output step.
- the first probability distribution 311 over possible speech recognition hypotheses includes one of possible phoneme labels or possible word piece labels.
- the speech encoder 204 receives, as input, each transcribed non-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 of FIG. 1 ) and generates, as output, for each of a plurality of output steps, a encoded audio representation 314 that corresponds to the transcribed non-synthetic speech utterance 304 at the corresponding output step.
- the shared encoder 250 receives, as input, the encoded audio representation 314 and generates, as output, a second encoded shared representation (e sup ) 324 .
- the auxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded shared representation 324 output from the shared encoder 250 and generates, as output, a second probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribed non-synthetic speech utterance 304 at the corresponding time step.
- the second probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of the possible phoneme labels or the possible word piece labels.
- the consistency regularization part 300 c of the training process 300 further determines, at each of the plurality of time steps for each training utterance pair 301 , the consistent loss term ( cons ( ⁇ )) 352 for the corresponding training utterance pair 301 based on the first probability distribution 311 over possible speech recognition hypotheses and the second probability distribution 394 over possible non-synthetic speech recognition hypotheses.
- the training process 300 may employ a consistency loss term module 350 configured to receive, at each time step, the corresponding non-synthetic speech and speech recognition results 311 , 394 output by the auxiliary decoder 390 , and determine the consistency loss term 352 for the corresponding training utterance pair 301 at the time step.
- the consistency regularization part 300 c of the training process 300 determines the consistent loss term 352 based on a Kullback-Leibler divergence (D KL ) between the first probability distribution 311 over possible speech recognition hypotheses and the second probability distribution 394 over possible non-synthetic speech recognition hypotheses.
- D KL Kullback-Leibler divergence
- the consistent loss term 352 based on D KL may be expressed by the following equation:
- the consistent loss term 352 determined for the training utterance pair 301 at each time step provides an “unsupervised” loss term that is independent of the accuracy of the auxiliary decoder 390 (e.g., independent of the supervised loss terms 342 , 344 of FIG. 3 B ), and thus, may be employed to update parameters of the audio encoder 210 for promoting consistency between non-synthetic speech representations and alignment outputs of the same utterances.
- the consistent loss term 352 may correspond to an average loss term obtained for the batch.
- the consistent loss term 352 permits the audio encoder 210 to learn to behave the same, e.g., make consistent encoded representation predictions on both non-synthetic speech (e.g., real/human speech) and alignment outputs of a same training utterance, regardless of whether the training utterance belongs to non-synthetic speech or alignment outputs.
- non-synthetic speech e.g., real/human speech
- the training process 300 may combine the unpaired data loss function ( unpaired ), the paired data loss function ( paired ), and the consistent loss term ( cons ) to obtain an overall loss term, tts4pretrain2 , that may be expressed as follows.
- the training process 300 for training the audio encoder 210 applies encoder consistency regularization.
- encoder consistency regularization does not require hypothesized labels and therefore has the advantage being allowed to be applied to all the training data 304 , 306 , 320 .
- Encoder consistency regularization may be applied via Hierarchical Contrastive consistency Regularization (HCCR) techniques where encoder activations e, e* from original/non-augmented and augmented speech are projected through an auxiliary network to generate z and z*. Thereafter, positive and negative pairs are constructive and a contrastive loss l t,z,z * is calculated as follows:
- a Convolutional Neural Network (CNN) projection network may calculate projections over increasing length segments of encoder activations e (30, 50, 120 ms) to yield 3 views (V) and draw negative examples from the same utterance for short segments, and from other utterances in the batches with 120 ms segments. Accordingly, an HCCR loss may be calculated over the transcribed non-synthetic speech utterances 304 (paired speech), the un-transcribed non-synthetic speech utterances 306 (unpaired speech), and the alignment outputs 402 generated from the unspoken textual utterances 320 as follows:
- the HCCR loss calculated by Equation 13 may be added to Equation 11 with a coefficient of 1 e ⁇ 3 as part of the overall loss term, tts4pretrain2 , for use in pre-training the audio encoder 210 .
- Implementations described above describe the training process 300 training the training the audio encoder 210 for a target language, however, it is understood that the training process 300 may also be employed to train the audio encoder for multiple target languages each different from the one or more training languages. As such, the audio encoder 210 for a multilingual ASR model 200 . In some instances, the training process 300 may be employed to train end-to-end ASR models with decoder structures (i.e., non-pre-training) or fine-tune an ASR model to perform downstream tasks such as speech translation or natural language understanding. Moreover, implementations described above describe the training process using each part 300 a - c of the training process 300 .
- any combination of the training parts 300 a - c may be used to train the audio encoder 210 using any combination of unspoken textual utterances 320 , transcribed non-synthetic speech utterances 304 , and/or untranscribed non-synthetic speech utterances 306 independently.
- the transcribed non-synthetic speech utterances 304 in the one or more training languages may initially be used to train the alignment model 400 ( FIGS. 5 and 6 ). Thereafter, using the alignment model 400 trained using labeled training data in the one or more training languages, the training process 300 may train the audio encoder using the alignment output loss term 342 derived from the unspoken textual utterances 320 in the target language(s) during the supervised loss part 300 b .
- the training process 300 may leverage high-resource languages (e.g., where an abundance of labeled training data already exists) to initially train the alignment model 400 and use the trained alignment model 400 to train the audio encoder 210 on a low-resource (e.g., little or zero labeled training data exists) target language.
- high-resource languages e.g., where an abundance of labeled training data already exists
- the alignment model 400 learns to generate alignment outputs in target languages even though the alignment model 400 was never trained on any data (or zero labeled training data) in the target language.
- the consistency regularization part 300 c of the training process 300 employs the residual adaptor layers 330 that condition at least one of the audio encoder 210 or the decoder (e.g., prediction network 220 and joint network 230 ( FIG. 2 )) on the language identifier 321 that uniquely identifies the target language.
- Each residual adaptor layer 330 includes a small feed forward network including self-attention layers (e.g., 2 self-attention layers) with a bottleneck dimension.
- the residual identifier output 332 is fed to the shared encoder 250 such that the audio encoder 210 is conditioned on the language identifier 321 and does not recognize audio from the target language during inference with graphemes from the training languages.
- FIG. 7 is a flowchart of an example arrangement of operations for a computer-implemented method 700 of using aligned text and speech representations to train automatic speech recognition models without transcribed speech data.
- the method 700 may execute on data processing hardware 810 ( FIG. 8 ) using instructions stored on memory hardware 820 ( FIG. 8 ) may reside on the user device 102 and/or remote computer/server 201 of FIG. 1 corresponding to a computing device 800 ( FIG. 8 ).
- the method 700 includes receiving training data including unspoken textual utterances 320 in a target language. Each unspoken textual utterance 320 not paired with any corresponding spoken utterance of non-synthetic speech (or synthetic speech).
- the method 700 includes generating a corresponding alignment output 402 for each unspoken textual utterance 320 of the received training data using an alignment model 400 .
- the alignment model 400 is trained on transcribed speech utterances 304 in one or more training languages each different than the target language. That is, the alignment model 400 is trained on the training languages and generates the alignment outputs 402 for the unspoken textual utterances 320 in the target language that were unseen by the alignment model 400 during training.
- the method 700 includes generating, using a text encoder 202 , a corresponding encoded textual representation 312 for each alignment output 402 .
- the method 700 includes training a speech recognition model 200 on the encoded textual representation 312 generated for the alignment outputs 402 corresponding to the unspoken textual utterances 320 in the target language to teach the speech recognition model 200 to learn how to recognize speech in the target language.
- the only transcribed (i.e., paired) training data used to train the speech recognition model 200 to learn how to recognize speech in the target language is the transcribed speech utterances 304 in the one or more training languages, each of which is different than the target language, used to train the alignment model 400 .
- FIG. 8 is a schematic view of an example computing device 800 that may be used to implement the systems and methods described in this document.
- the computing device 800 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 800 includes a processor 810 , memory 820 , a storage device 830 , a high-speed interface/controller 840 connecting to the memory 820 and high-speed expansion ports 850 , and a low speed interface/controller 860 connecting to a low speed bus 870 and a storage device 830 .
- Each of the components 810 , 820 , 830 , 840 , 850 , and 860 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 810 can process instructions for execution within the computing device 800 , including instructions stored in the memory 820 or on the storage device 830 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 880 coupled to high speed interface 840 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 800 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 820 stores information non-transitorily within the computing device 800 .
- the memory 820 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 820 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 800 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 830 is capable of providing mass storage for the computing device 800 .
- the storage device 830 is a computer-readable medium.
- the storage device 830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 820 , the storage device 830 , or memory on processor 810 .
- the high speed controller 840 manages bandwidth-intensive operations for the computing device 800 , while the low speed controller 860 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 840 is coupled to the memory 820 , the display 880 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 850 , which may accept various expansion cards (not shown).
- the low-speed controller 860 is coupled to the storage device 830 and a low-speed expansion port 890 .
- the low-speed expansion port 890 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 800 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 800 a or multiple times in a group of such servers 800 a , as a laptop computer 800 b , or as part of a rack server system 800 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method includes receiving training data that includes unspoken textual utterances in a target language. Each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech. The method also includes generating a corresponding alignment output for each unspoken textual utterance using an alignment model trained on transcribed speech utterance in one or more training languages each different than the target language. The method also includes generating a corresponding encoded textual representation for each alignment output using a text encoder and training a speech recognition model on the encoded textual representations generated for the alignment outputs. Training the speech recognition model teaches the speech recognition model to learn how to recognize speech in the target language.
Description
- This U.S. Patent application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/369,213, filed on Jul. 22, 2022. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to using aligned text and speech representations to train automatic speech recognition models without transcribed speech data.
- Automatic speech recognition (ASR), the process of taking an audio input and transcribing it into text, has greatly been an important technology that is used in mobile devices and other devices. In general, automatic speech recognition attempts to provide accurate transcriptions of what a person has said by taking an audio input (e.g., speech utterance) and transcribing the audio input into text. Modern ASR models continue to improve in both accuracy (e.g. a low word error rate (WER)) and latency (e.g., delay between the user speaking and the transcription) based on the ongoing development of deep neural networks. However, one challenge in developing deep learning-based ASR models is a substation amount of transcribed speech is required during training. In some instances, unspoken text data is used to train the ASR models to supplement a small set of transcribed speech training data. Yet, this challenge is further complicated when training ASR models with low-resource languages that include zero available transcribed speech training data.
- One aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for using aligned text and speech representations to train automatic speech recognition models without transcribed speech data. The operations include receiving training data that includes unspoken textual utterances in a target language. Each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech. The operations also include generating a corresponding alignment output for each unspoken textual utterance of the received training data using an alignment model trained on transcribed speech utterances in one or more training languages each different than the target language. The operations also include generating a corresponding encoded textual representation for each alignment output using a text encoder. The operations also include training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, training the speech recognition model includes training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning. In some examples, the speech recognition model includes an audio encoder and a decoder. The decoder may include a recurrent neural network transducer (RNN-T) architecture. In these examples, the audio encoder includes the text encoder, a speech encoder, and a shared encoder. Here, the encoder may include a plurality of multi-headed self-attention layers. The audio encoder may include a Conformer encoder. In these examples, the operations further include conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language. Conditioning the at least one of the audio encoder or the decoder includes conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers. The alignment model may be trained on transcribed speech utterances in one or more training languages different than the target language.
- In some implementations training the speech recognition model includes: generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space for each alignment output; for each transcribed speech utterance in the one or more training languages, determining an encoded audio representation of the transcribed speech utterance using a speech encoder and generating a second encoded shared representation of the transcribed speech utterance in the shared latent representation space. Here, training the speech recognition model includes training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages. Each unspoken textual utterance may include a sequence of words, word-pieces, graphemes, and/or phonemes. In some examples, the operations further include converting a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages using a pronunciation model.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations. The operations include receiving training data that includes unspoken textual utterances in a target language. Each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech. The operations also include generating a corresponding alignment output for each unspoken textual utterance of the received training data using an alignment model trained on transcribed speech utterances in one or more training languages each different than the target language. The operations also include generating a corresponding encoded textual representation for each alignment output using a text encoder. The operations also include training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, training the speech recognition model includes training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning. In some examples, the speech recognition model includes an audio encoder and a decoder. The decoder may include a recurrent neural network transducer (RNN-T) architecture. In these examples, the audio encoder includes the text encoder, a speech encoder, and a shared encoder. Here, the encoder may include a plurality of multi-headed self-attention layers. The audio encoder may include a Conformer encoder. In these examples, the operations further include conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language. Conditioning the at least one of the audio encoder or the decoder includes conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers. The alignment model may be trained on transcribed speech utterances in one or more training languages different than the target language.
- In some implementations training the speech recognition model includes: generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space for each alignment output; for each transcribed speech utterance in the one or more training languages, determining an encoded audio representation of the transcribed speech utterance using a speech encoder and generating a second encoded shared representation of the transcribed speech utterance in the shared latent representation space. Here, training the speech recognition model includes training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages. Each unspoken textual utterance may include a sequence of words, word-pieces, graphemes, and/or phonemes. In some examples, the operations further include converting a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages using a pronunciation model.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech recognition system. -
FIG. 2 is a schematic view of an example speech recognition model. -
FIGS. 3A-3C are schematic views of an example training process for training an audio encoder of the speech recognition model ofFIG. 2 . -
FIG. 4 is a schematic view of an alignment model used during the example training process for training the audio encoder of the speech recognition model inFIGS. 3A-3C . -
FIG. 5 is a schematic view of an example training process for training a duration predictor of the alignment model. -
FIG. 6 is a schematic view of an example training process for training an upsampler of the alignment model. -
FIG. 7 is a flowchart of an example arrangement of operations for a method of using aligned text and speech representations to train speech recognition models without transcribed speech data. -
FIG. 8 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- Training state-of-the-art automated speech recognition (ASR) models typically requires a substantial amount of labeled training data including speech utterances each paired with a corresponding transcription (i.e., ground-truth label). Obtaining the substantial amount of labeled training data can be very costly, especially for low-resource languages. Thus, recent approaches for training ASR models include self-supervised training using large amounts of unlabeled training data (i.e., speech not paired with any corresponding transcriptions and/or unspoken text not paired with any corresponding speech) to complement a relatively small amount of labeled training data. However, these approaches still require some small amount of labeled training data in a particular language the ASR model is being trained to recognize. Yet, in some instances, there is zero available labeled training data for certain low-resource languages. As such, in these instances, there is not any labeled training data to complement the large amounts of unlabeled training data.
- Accordingly, implementations herein are directed towards methods and systems of using aligned text and speech representation to train ASR models without transcribed speech training data. In particular, training the ASR model includes receiving training data that includes unspoken textual utterances in a target language where each unspoken textual utterance is not paired with any corresponding utterance of non-synthetic (or synthetic) speech. An alignment model generates a corresponding alignment output (i.e., aligned text representation) for each unspoken textual utterance of the received training data. Notably, the alignment model is trained on transcribed (i.e., labeled) speech utterances in one or more training languages each of which is different than the target language the ASR model is trained to recognize. Moreover, a text encoder generates a corresponding encoded textual representation for each alignment output. Thereafter, the ASR model is trained on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the ASR model to learn how to recognize speech in the target language when no labeled training data in the target language is available. That is, the ASR model is trained without using any transcribed speech utterances in the target language for supervised learning.
-
FIG. 1 illustrates an ASR system 100 implementing anASR model 200 that resides on auser device 102 of auser 104 and/or on a remote computing device 201 (e.g., one or more servers of a distributed system executing in a cloud-computing environment) in communication with theuser device 102. Although theuser device 102 is depicted as a mobile computing device (e.g., a smart phone), theuser device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped with data processing hardware andmemory hardware 113. - The
user device 102 includes anaudio subsystem 108 configured to receive anutterance 106 spoken by the user 104 (e.g., theuser device 102 may include one or more microphones for recording the spoken utterance 106) and convert theutterance 106 into a corresponding digital format associated with input acoustic frames 110 capable of being processed by the ASR system 100. In the example shown, the user speaks arespective utterance 106 in a natural language of English for the phrase “What is the weather in New York City?” and theaudio subsystem 108 converts theutterance 106 into corresponding acoustic frames 110 for input to the ASR system 100. Thereafter, theASR model 200 receives, as input, the acoustic frames 110 corresponding to theutterance 106, and generates/predicts, as output, a corresponding transcription 120 (e.g., recognition result/hypothesis) of theutterance 106. In the example shown, theuser device 102 and/or theremote computing device 201 also executes a user interface generator 107 configured to present a representation of thetranscription 120 of theutterance 106 to theuser 104 of theuser device 102. In some configurations, thetranscription 120 output from the ASR system 100 is processed, e.g., by a natural language understanding (NLU) module executing on theuser device 102 or theremote computing device 201, to execute a user command. Additionally or alternatively, a text-to-speech system (e.g., executing on any combination of theuser device 102 or the remote computing device 201) may convert the transcription into synthesized speech for audible output by another device. For instance, theoriginal utterance 106 may correspond to a message theuser 104 is sending to a friend in which thetranscription 120 is converted to synthesized speech for audible output to the friend to listen to the message conveyed in theoriginal utterance 106. - The
ASR model 200 may operate in a streaming fashion, a non-streaming fashion, or some combination thereof. TheASR model 200 operates in the streaming fashion by, while receiving the sequence of acoustic frames 110, encoding the sequence of acoustic frames 110 and then decoding the encoded sequence of acoustic frames 110 into an initial transcription (e.g., speech recognition result/hypothesis) 120. Thus, theinitial transcription 120 may correspond to words, word pieces, and/or individual characters generated by theASR model 200 as soon as they are spoken. On the other hand, theASR model 200 operates in the non-streaming fashion by receiving and processing additional right-context to improve upon theinitial transcription 120 thereby generating afinal transcription 120. That is, theASR model 200 processes additional input audio data or encoded acoustic frames (e.g., right-context) to improve thetranscription 120 output by theASR model 200, but at increased latency. - Referring to
FIG. 2 , anexample ASR model 200 may include a Recurrent Neural Network-Transducer (RNN-T) model architecture which adheres to latency constraints with interactive applications. The use of the RNN-T model architecture is exemplary only, as theASR model 200 may include other architectures such as transformer-transducer and conformer-transducer model architectures among others. The RNN-T model 200 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., no communication with a remote server is required). The RNN-T model 200 includes anencoder network 210, a prediction network 220, and ajoint network 230. Theencoder network 210, which is roughly analogous to an acoustic model (AM) in a traditional ASR system, includes a stack of self-attention layers (e.g., Conformer or Transformer layers) or a recurrent network of stacked Long Short-Term Memory (LSTM) layers. For instance, the encoder network (e.g., audio encoder) 210 reads a sequence of d-dimensional feature vectors (e.g., acoustic frames 110 (FIG. 1 )) x=(x1, x2, . . . , xT), where xt∈encoder network 210 includes a dual encoder framework that has atext encoder 202 and a speech encoder 204 (FIGS. 3B and 3C ). - Similarly, the prediction network 220 is also an LSTM network, which, like a language model (LM), processes the sequence of non-blank symbols output by a
final Softmax layer 240 so far, y0, . . . , yui−1, into a dense representation Putt. Together, the prediction network 220 and thejoint network 230 may be referred to as a decoder that includes an RNN-T architecture. Finally, with the RNN-T model architecture, the representations produced by the encoder and prediction/decoder networks 210, 220 are combined by thejoint network 230. The prediction network 220 may be replaced by an embedding look-up table to improve latency by outputting looked-up sparse embeddings in lieu of processing dense representations. The joint network then predicts P(yi|xti , y0, . . . , yui−1 ), which is a distribution over the next output symbol. Stated differently, thejoint network 230 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses. Here, the “possible speech recognition hypotheses” correspond to a set of output labels each representing a symbol/character in a specified natural language. For example, when the natural language is English, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space. Accordingly, thejoint network 230 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels. This set of values can be a vector and can indicate a probability distribution over the set of output labels. In some cases, the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited. For example, the set of output labels can include wordpieces, phonemes, and/or entire words, in addition to or instead of graphemes. The output distribution of thejoint network 230 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output yi of thejoint network 230 can include 100 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer 240) for determining thetranscription 120. - The
Softmax layer 240 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by the RNN-T model 200 at the corresponding output step. In this manner, the RNN-T model 200 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. The RNN-T model 200 does assume an output symbol is independent of future acoustic frames 110, which allows the RNN-T model to be employed in the streaming fashion, the non-streaming fashion, or some combination thereof. - In some examples, the
audio encoder 210 of the RNN-T model includes a plurality of multi-head (e.g., 8 heads) self-attention layers. For example, the plurality of multi-head self-attention layers may include Conformer layers (e.g., Conformer-encoder), transformer layers, performer layers, convolution layers (including lightweight convolution layers), or any other type of multi-head self-attention layers. The plurality of multi-head self-attention layers may include any number of layers, for instance 16 layers. Moreover, theaudio encoder 210 may operate in the streaming fashion (e.g., theaudio encoder 210 outputs initial higher-order feature representations as soon as they are generated), in the non-streaming fashion (e.g., theaudio encoder 210 outputs subsequent higher-order feature representations by processing additional right-context to improve initial higher-order feature representations), or in a combination of both the streaming and non-streaming fashion. -
FIGS. 3A-3C illustrate an example training process 300 for training the ASR model 200 (FIG. 2 ). The training process 300 described herein describes training theaudio encoder 210 of theASR model 200, however, it is understood that the training process 300 may also include pre-training and/or fine-tune training of theaudio encoder 210. Moreover, implementations described herein contemplate the training process 300 training theaudio encoder 210 of theASR model 200 without training the decoder (e.g., prediction network 220 and joint network 230) of theASR model 200. Yet, it is understood that the training process 300 may additionally or alternatively train other components of the ASR model 200 (e.g., prediction network 220 and/or joint network 230) jointly with theaudio encoder 210. - The training process 300 trains the
audio encoder 210 using available training data that includes a set of unspoken textual utterances (Xtext) 320, a set of transcribed non-synthetic speech utterances (Xsup) 304, and/or un-transcribed non-synthetic speech utterances (Xunsup) 306. Notably, each unspokentextual utterance 320 in the set of unspokentextual utterances 320 includes text-only data (i.e., unpaired data) in a target language such that each unspokentextual utterance 320 is not paired with any corresponding spoken audio representation (i.e., speech) of the utterance. Here, the target language is any language the training process 300 uses to train theaudio encoder 210 to recognize where zero transcribed (i.e., labeled) training data is used during training. The unspokentextual utterance 320 may include any sequence text chunks including words, word-pieces, phonemes, and/or graphemes. Optionally, the available training data may also include the un-transcribed non-synthetic speech utterances 306 (also referred to as simply “un-transcribed speech utterance 306”) each including audio-only data (i.e., unpaired data) in the target language such that theun-transcribed speech utterance 306 is not paired with any corresponding transcription. Notably, when the training data includes theun-transcribed speech utterances 306 in addition to the unspokentextual utterances 306, theun-transcribed speech utterances 306 represent different training utterances than the unspokentextual utterances 320 such that the training process 300 cannot simply pair the unspokentextual utterances 320 with theun-transcribed speech utterances 306 to generate labeled training data. - On the other hand, each transcribed non-synthetic speech utterance 304 (also referred to as simply “transcribed
speech utterance 304”) includes acorresponding transcription 302 paired with a corresponding non-synthetic speech representation of the corresponding transcribedspeech utterance 304 in one or more training languages. Each of the one or more training languages is different from the target language. For example, the one or more training languages may include 52 languages with transcribed speech utterances and the target language may include another 50 languages (e.g., each different than the 52 training languages) with text-only training data. As will become apparent, the transcribedspeech utterances 304 are used to train analignment model 400 to generatealignment outputs 402 for the transcribedspeech utterances 304 in the one or more training languages. Thereafter, the trainedalignment model 400 is configured to receive, as input, the unspokentextual utterances 320 in the target language (e.g., different from each of the one or more training languages used to train the alignment model 400) and generate, as output,alignment outputs 402 in the target language used to train theaudio encoder 210. Thus, training thealignment model 400 using the transcribedspeech utterances 304 in the one or more training languages enables thealignment model 400 to generatealignment outputs 402 in the target language even though thealignment model 400 was never trained with any training data in the target language. - For simplicity, the training process 300 includes a contrastive self-supervised loss part 300 a (
FIG. 3A ), a supervised loss part 300 b (FIG. 3B ), and a consistency regularization part 300 c (FIG. 3C ). The training process 300 trains theaudio encoder 210 on a total loss (Ltts4pretrain2) based on: contrastive losses (Lw2v) 316 derived using the contrastive self-supervised loss part 300 a from the unspoken training text utterances (Xtext) 320 (e.g., in the target language), a corpus of transcribed non-synthetic speech utterances (Xsup) 304 (e.g., in the one or more training languages), and un-transcribed non-synthetic speech utterances (Xunsup) 306 (e.g., in the target language); supervised losses (Laux) 342, 344 derived using the supervised loss part 300 b from the unspoken training text utterances (Xtext) 320 and the transcribed non-synthetic speech utterances (Xsup) 304; and consistency losses ( - Referring to
FIG. 3A , the contrastive self-supervised loss part 300 a of the training process 300 may employ analignment model 400 that is configured to generate, at each of a plurality of output steps, alignment outputs (i.e., textual representations) 402 for each of a plurality of unspokentraining text utterances 320. The unspokentextual utterances 320 includes unspoken text that is text-only data in the target language, i.e., unpaired data, such that each unspoken textual utterance (Xtext) 320 is not paired with any synthesized or non-synthesized speech. Accordingly, thealignment model 400 generates acorresponding alignment output 402 for each of the unspokentextual utterances 320. - Referring now to
FIG. 4 , in some examples, thealignment model 400 includes an embeddingextractor 410,duration predictor 420, and anupsampler 430. The embeddingextractor 410 receives the unspokentextual utterance 320 that includes a sequence of text chunks including words, word-pieces, phonemes, and/or graphemes and extracts a corresponding initial textual representation (et) 412. The initialtextual representation 412 includes embedding lexical information from the unspokentextual utterance 320. Additionally or alternatively, the embeddingextractor 410 may receive atranscription 302 corresponding to a transcribed non-synthetic speech utterance 304 (FIG. 3C ). Theduration predictor 420 receives the initialtextual representation 412 from the embeddingextractor 410 and predicts a corresponding text chunk duration (i.e., word, word-piece, phoneme, and/or grapheme duration) 422. Thetext chunk duration 422 indicates a duration the corresponding text chunk would be spoken if a human (or text-to-speech system) spoke the unspokentextual utterance 320. For example, the unspokentextual utterance 320 may include a sequence of phonemes and theduration predictor 420 predicts aphoneme duration 422 for each phoneme in the sequence of phonemes. In this example, theduration predictor 420 predicts thephoneme duration 422 by predicting a probability of non-zero duration for each phoneme and predicting a probability of continuous phoneme duration for each phoneme. As the sequence of phonemes includes regular phonemes, silences between word boundaries, and punctuation marks, only the regular phonemes are associated with non-zero duration while the silences and punctuation marks are generally associated with the continuous phoneme duration. Accordingly, theduration predictor 420 may use a sigmoid activation following a first one of two independent activations to predict the probability of non-zero duration and use a soft plus activation following a second one of the two independent projections to predict the continuoustext chunk duration 422 for each text chunk. Theduration predictor 420 determines, for each text chunk, whether the probability of non-zero duration is less than a threshold value, and when the probability of non-zero duration is less than the threshold value, a multiplier may zero-out the continuoustext chunk duration 422 predicted by the softplus activation for the corresponding text chunk. Otherwise, when the probability of non-zero duration is not less than the threshold value, the predictedtext chunk duration 422 may be set equal to the continuous phoneme duration predicted by the softplus activation. - The
upsampler 430 receives, for each unspokentextual utterance 320, the corresponding initialtextual representation 412 and the predictedtext chunk duration 422, and generates an alignment output (êt) 402 having a number of frames by upsampling the initialtextual representation 412 using the corresponding predictedtext chunk duration 422. Here, thealignment output 402 represents an aligned speech-text representation. In some examples, thealignment model 400 sends thealignment output 402 to atext encoder 202 of the audio encoder 210 (FIGS. 3B and 3C ). In other examples (not shown), thealignment model 400 sends thealignment output 402 to a shared encoder 250 (e.g., bypassing the text encoder 202) of the audio encoder 210 (FIGS. 3B and 3C ). In these other examples, thealignment output 402 serves as the encodedtextual representation 312 such that the sharedencoder 250 may receive thealignment output 402 directly from thealignment model 400. In yet other examples, paired training data is available and theupsampler 430 generates thealignment output 402 as follows: -
ê t=θRefiner(Resample(e t,AlignRNN-T(e s ,t))) (1) - Here, the
upsampler 430 includes resampler and refiner layers that align the initial textual embedding 412 to align with a corresponding encoded audio representation 314 (FIGS. 3B and 3C ) directly. However, when paired training data is not available, theupsampler 430 generates thealignment output 402 as follows: -
ê t=θRefiner(Resample(e t,θduration(e t))) (2) - In particular, the number of frames of the
alignment output 402 indicates a predicted speech duration of the unspokentextual utterance 320. Stated differently, the number of frames of thealignment output 402 maps (i.e., aligns) the sequence of text chunks of the unspokentextual utterance 320 to speech frames. Here, theupsampler 430 includes resampler and refiner layers that replicate the initial textual embedding 412 to match the predicted text chunk duration 422 (i.e., speech duration). As such, thealignment output 402 includes a textual representation of the unspokentextual utterance 320 having a timing component that aligns with how a human would speak the unspokentextual utterance 320. In some examples, the embeddingextractor 410 receives thelanguage identifier 321 that uniquely identifies the language of the one or more training languages and/or the target language to condition thealignment model 400. - In some implementations, training the
alignment model 400 using training data in the one or more training languages, and then, generatingalignment outputs 402 in the target language (e.g., different than each of the training languages) leads to lowquality alignment outputs 402 because a script (e.g., Brahimic) of the target language has no overlap with scripts of the one or more training languages. To that end, in some examples, thealignments model 400 includes a pronunciation model that converts a script of the unspokentextual utterances 320 in the target language into a representation (e.g., phonetic representation) shared across multiple languages. In other examples, thealignment model 400 may transliterate the script of the unspokentextual utterances 320 into a different script. That is, the different script may align with the scripts of the transcribedspeech utterances 304. - Notably, in most instances, a text-to-speech (TTS) system generates an audible output to give the unspoken
textual utterance 320 the timing component of human speech such that a training process may use the audible output from the TTS system (i.e., synthetic speech) to train theaudio encoder 210. However, thealignment model 400 advantageously generates thealignment output 402 thereby mapping the sequence of text chunks to speech frames directly. As such, the training process 300 does not require any TTS system to generate synthetic speech from the unspokentextual utterances 320 to train theaudio encoder 210. That is, neither the training process 300 nor thealignment model 400 converts the unspokentextual utterance 320 into synthetic speech, but rather generates alignment outputs 402 (i.e., text alignments). -
FIG. 5 illustrates anexample training process 500 for training thealignment model 400 using transcribednon-synthetic speech utterances 304 in the one or more training languages. That is, each of the one or more training languages is a different language than the target language audio encoder 210 (FIG. 3 ) is trained to recognize. Moreover, each transcribednon-synthetic speech utterance 304 has acorresponding transcription 302. In the example shown, thespeech encoder 204 receives, as input, each transcribednon-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 ofFIG. 1 ) and generates, as output, for each of a plurality of output steps, an encoded audio representation (es) 314 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding output step. In parallel, thealignment model 400 receives thetranscription 302 corresponding to the samenon-synthetic speech utterance 304 and generates analignment output 402 according toEquation 1. Thetext encoder 202 receives, as input, thealignment output 402 and generates, as output, for each of the plurality of output steps, an encoded textual representation (êt) 312 that corresponds to thetranscription 302 at the corresponding output step. - A modality loss module 550 receives the encoded textual representation 312 and the encoded audio representation 314 and generates a modality loss 552 based on comparing the encoded textual representation 3412 and the encoded audio representation as follows:
- Equation 3 adds the mean squared error (MSE) of the encoded textual representation (êt) 312 and the encoded audio representation (es) 314 to RNN-T model alignments between predicted text targets and the encoded audio representations (es) 314 to determine the modality loss (
audio representations 314 serve as a ground-truth label to train thealignment model 400 to generatealignment outputs 402 that align to the correspondingnon-synthetic speech utterances 304. Thetraining process 700 may use themodality loss 552 to update parameters of thealignment model 400. For example, thetraining process 700 may update parameters of theduration predictor 420 and/or the upsampler 430 (FIG. 4 ). -
FIG. 6 illustrates anexample training process 600 for training thealignment model 400 using paired training data (i.e., transcribedspeech utterances 304 in the one or more training languages) and unpaired training data (i.e., unspokentextual utterances 320 in the target language). That is, thetraining process 600 uses transcribednon-synthetic speech utterances 304 that have corresponding transcriptions 302 (i.e., paired training data) and unspoken textual utterances 320 (i.e., unpaired training data) to learn speech-aligned alignment outputs 402. In the example shown, thespeech encoder 204 receives, as input, each transcribednon-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 ofFIG. 1 ) and generates, as output, for each of the plurality of output steps, an encoded audio representation (es) 314 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding output step. In parallel, thealignment model 400 receives thetranscription 302 corresponding to the samenon-synthetic speech utterance 304 and generates an alignment output according toEquation 1. Additionally or alternatively, thealignment model 400 may receive the unspokentextual utterance 320 and generate analignment output 402 according to Equation 2. Thetext encoder 202 receives, as input, thealignment output 402 and generates, as output, for each of the plurality of output steps, an encoded textual representation (et) 314. - The
audio encoder 210 may include a sharedencoder 250 that receives, as input, the encodedtextual representations 312, and generates, as output, a first encoded sharedrepresentation 322. The sharedencoder 250 may also receive, as input, the encodedaudio representations 314 and generate, as output, a second encoded sharedrepresentation 324. Anauxiliary decoder 390 receives, as input, the first and second encoded sharedrepresentations second probability distributions 392, 294 over possible speech recognition hypotheses. - An alignment masked loss module 650 receives the first probability distribution 392 corresponding to the encoded textual representation 312 and the second probability distribution 394 corresponding to the encoded audio representation 314 and generates an alignment loss 652 as follows:
- The
alignment loss 652 from Equation 4 may be applied over the masked, sampled encodedtextual representations 312 in a frequency and time domain. Notably, thealignment loss 652 may be used as a training objective for both paired training data and unpaired training data. Thetraining process 600 may use thealignment loss 652 to update parameters of thealignment model 400. For example, thetraining process 800 may update parameters of theduration predictor 420 and/or the upsampler 430 (FIG. 4 ). - Referring back to
FIG. 3A , in some implementations, theaudio encoder 210 includes aspeech encoder 204 and atext encoder 202, described in more detail with reference toFIGS. 3B and 3C . In the example shown, the audio encoder 210 (alternatively thespeech encoder 204 or the text encoder 202 (FIGS. 3B and 3C )) includes a Conformer encoder including a stack of Conformer blocks each of which includes a stack of multi-headed self-attention, depth wise convolution, and feed-forward layers. Alternatively, theaudio encoder 210 may include another type of encoder having a stack of multi-head self-attention layers/blocks, such as a transformer or performer encoder. The Conformer encoder 210 can naturally be split into a feature encoder, including aconvolution subsampling block 212, and a context network, including alinear layer 214 and a stack of Conformer blocks 216. In some implementations, theconvolution subsampling block 212 has two two-dimensional-convolution layers, both with strides (2, 2), resulting in a 4× reduction in the feature sequence length. Theconvolution subsampling block 212 receives, as input, a sequence of input features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 ofFIG. 1 ) associated with each transcribednon-synthetic speech utterance 304 and each un-transcribednon-synthetic speech utterance 306, and generates, as output, for each of a plurality of output steps, an encoded audio feature 211 that corresponds to a respective one of the transcribednon-synthetic speech utterances 304 or a respective one of the un-transcribednon-synthetic speech utterances 306. Theconvolution subsampling block 212 may receive, as input, eachalignment output 402 generated by thealignment model 400 from the unspokentextual utterances 320 and generate, as output, for each of the plurality of output steps, an encoded textual feature 213 that corresponds to a respective one of the alignment outputs 402. - The encoded audio and textual features 211, 213 (i.e., interchangeably referred to as “encoded features 211, 213”) output from the
convolution subsampling block 212 may be fed to amasking module 218 where some of the encoded features 211, 213 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding masked encoded audio features 211, 211 m and masked encodedtextual features 213, 213 m. In some examples, themasking module 218 masks the randomly chosen encoded features 211, 213 for masking by randomly sampling without replacement a certain proportion p of all time steps to be start indices and then masks the subsequent M consecutive time steps from every sample index, whereby some spans may overlap. After masking is applied, thelinear layer 214 and the Conformer blocks 216 of the context network receive the masked encodedfeatures features quantizer 217 receives the encoded features 211, 213 as input, and generates quantized vectors (i.e., target context vectors) 219 as output. Thereafter, acontrastive loss module 315 derives a contrastive loss (Lw2v) 316 between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219 as follows. -
- where ct is
contrastive context vector 215 centered over a masked output step (i.e., time step) t and qt represents atarget context vector 219 at the output step tin a set of K+1 candidatetarget context vectors 219 which includes qt and K distractors. Distractors may be uniformly sampled from other masked output steps of the same utterance. - The
contrastive loss 316 is optimized between thecontrastive context vectors 215 at the masked positions and thetarget context vectors 219. After theaudio encoder 210 converges on the un-transcribednon-synthetic speech utterances 306, the training procedure is repeated on both the alignment outputs 402 corresponding to the unspokentextual utterance 320 and the transcribednon-synthetic speech utterances 304. Thus, the contrastive loss 316 (Lw2v) is optimized for both real/human (non-synthetic) and unspokentextual utterances 320 represented byalignment outputs 402, with additional auxiliary losses derived from the transcribednon-synthetic speech utterances 304 and the alignment outputs 402 as described in greater detail below with reference toFIG. 3B . Accordingly, the contrastive self-supervised loss part 300 a of the training process 300 trains theaudio encoder 210 using thecontrastive loss 316 derived from the corresponding encoded features 211, 213 associated with eachalignment output 402, each transcribednon-synthetic speech utterance 304, and each un-transcribednon-synthetic speech utterance 306 provided as input to theaudio encoder 210. Training theaudio encoder 210 may include updating parameters of theaudio encoder 210 based on thecontrastive losses 316. - Referring to
FIG. 3B , the supervised loss part 300 b of the training process 300 is configured to inject lexical information into theaudio encoder 210 during training based onsupervised loss terms textual utterances 320 output by thealignment model 400. Notably, the supervised loss part 300 b leverages one or moreauxiliary decoders 390 for generating thesupervised loss terms auxiliary decoders 390 may include Connectionist Temporal Classification (CTC) decoders, Listen Attend Spell (LAS) decoders, or RNN-T decoders. Theseauxiliary decoders 390 may include at least one of a phoneme decoder configured to decode a sequence of phonemes or a wordpiece decoder configured to decode a sequence of word pieces. Theauxiliary decoders 390 could also include a grapheme decoder configured to decode a sequence of graphemes. - During the supervised loss part 300 b, the
text encoder 202 of the audio encoder is configured to receive alignment outputs 402 (i.e., text embeddings) from the alignment model and the speech encoder is configured to receive transcribednon-synthetic speech utterances 304. That is, thetext encoder 202 of the audio encoder generates encoded textual representations 3123 for alignment outputs 402 (e.g., corresponding to an unspoken textual utterance 320) and thespeech encoder 204 of theaudio encoder 210 generates encodedaudio representations 314 for speech inputs (i.e., transcribed non-synthetic speech utterances 304). Here, the encodedtextual representations 312 and the encodedaudio representations 314 may not both be compatible with theauxiliary decoders 390. Thus, theaudio encoder 210 may also include a sharedencoder 250 that receives the encodedtextual representations 312 as input, and generates a first encoded shared representation 322 (etext) as output. Moreover, the sharedencoder 250 receives the encodedaudio representations 314 as input, and generates a second encoded shared representation (esup) 324 as output. Accordingly, the sharedencoder 250 generates the first and second encoded sharedrepresentations auxiliary decoder 390. - In particular, the shared
encoder 250 receives, as input, each encodedtextual representation 312 that corresponds to thealignment output 402 generated from the unspokentextual utterance 320 and generates, as output, for each of the plurality of output steps, the first encoded shared representation (etext) 322 that corresponds to thealignment output 402 at the corresponding output step. Theauxiliary decoder 390 including the phoneme decoder, wordpiece decoder, or the byte decoder receives, as input, each first encoded sharedrepresentation 322 output from the sharedencoder 250 and generates, as output, afirst probability distribution 392 over possible speech recognition hypotheses for thecorresponding alignment output 402 at the corresponding time step. In some examples, thefirst probability distribution 392 over possible speech recognition hypotheses includes one of possible phoneme labels, possible word piece labels, or possible grapheme labels. Thereafter, asupervised loss module 340 may determine an alignmentoutput loss term 342 based on thefirst probability distribution 392 over possible speech recognition hypotheses for thealignment output 402 corresponding to the unspokentextual utterance 320. Here, the corresponding unspokentextual utterance 320 in which thealignment output 402 is generated from, also serves as a ground-truth transcription. The supervised loss part 300 b may train theaudio encoder 210 on the alignmentoutput loss term 342 by updating parameters of theaudio encoder 210 based on the alignmentoutput loss term 342. - Similarly, during the supervised loss part 300 b, the shared
encoder 250 receives, as input, each transcribed encodedaudio representation 314 that corresponds to thenon-synthetic speech utterance 304 and generates, as output, for each of the plurality of output steps, a second encoded shared representation (esup) 324 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding output step. Theauxiliary decoder 390 including the phoneme decoder, the wordpiece decoder, or the byte decoder receives, as input, each second encoded sharedrepresentation 324 output from the sharedencoder 250 and generates, as output, asecond probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribednon-synthetic speech utterance 304 at the corresponding output step. In some examples, thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of possible phoneme labels, the possible word piece labels, or the possible grapheme labels. Thereafter, thesupervised loss module 340 may determine a non-syntheticspeech loss term 344 based on thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses and thecorresponding transcription 302 paired with the transcribednon-synthetic speech utterance 304. Here, the correspondingtranscription 302 serves as a ground-truth transcription and may include a sequence of target phonemes, target word pieces, and/or target graphemes. The supervised loss part 300 b may train theaudio encoder 210 on the non-syntheticspeech loss term 344 by updating parameters of theaudio encoder 210 based on the non-syntheticspeech loss term 344. - In some implementations, the supervised loss part 300 b of the training process 300 uses another
auxiliary decoder 390 to generate a third probability distribution 393 over possible speech recognition hypotheses based on the first encoded shared representation (etext) 322 for thealignment output 402 at the corresponding output step, whereby thesupervised loss module 340 may determine another alignmentoutput loss term 342 based on the third probability distribution 393 and the unspokentextual utterance 320 corresponding to thealignment output 402. Here, the otherauxiliary decoder 390 includes the other one of the phoneme decoder, word piece decoder, or the grapheme decoder and the third probability distribution 393 over possible speech recognition hypotheses includes the other one of the possible phoneme labels, the possible word piece labels, or the possible grapheme labels. In these implementations, the otherauxiliary decoder 390 also generates a fourth probability distribution 395 over possible non-synthetic speech recognition hypotheses for the corresponding second encoded sharedrepresentation 324 at the corresponding output step, whereby thesupervised loss module 340 may determine another non-syntheticspeech loss term 344 based on the fourth probability distribution 395 and thecorresponding transcription 302 that is paired with the transcribednon-synthetic speech representation 304. Here, the fourth probability distribution 395 over possible non-synthetic speech recognition hypotheses includes the other one of the possible phoneme labels, the possible word piece labels, or the possible grapheme labels. The supervised loss part 300 b of the training process 300 may similarly theaudio encoder 210 on the other alignmentoutput loss term 342 and the other non-syntheticspeech loss term 344. - The un-transcribed
non-synthetic speech utterances 306 and the unspokentextual utterances 320 each correspond to “unpaired” training data whereby the contrastive loss (Lw2v) 316 derived from the unspoken textual utterances (Xtext) 320 may be combined with the supervised loss -
- During training of the
audio encoder 210, the alignment outputs 402 and the un-transcribednon-synthetic utterances 306 may be separated or mixed within each batch. In order to force theaudio encoder 210 to learn representations that are effective for bothalignment outputs 402 corresponding to unspokentextual utterances 320 and non-synthetic (human/real) speech, the loss mask σ is applied when combining the loss functions - The transcribed
non-synthetic speech utterances 304 corresponds to “paired” and “supervised” training data whereby the derived contrastive loss Lw2v and the derived supervised loss - In some scenarios, after training the
audio encoder 210, the ASR model recognizes audio from the target language during inference with graphemes from the training languages. As such, in some implementations, the supervised part 300 b of the training process 300 employs residual adaptor layers 330 that condition at least one of theaudio encoder 210 or the decoder (e.g., prediction network 220 and joint network 230 (FIG. 2 )) on alanguage identifier 321 that uniquely identifies the target language. Eachresidual adaptor layer 330 includes a small feed forward network including self-attention layers (e.g., 2 self-attention layers) with a bottleneck dimension. Thus, theresidual identifier output 332 is fed to the sharedencoder 250 such that theaudio encoder 210 is conditioned on thelanguage identifier 321 and does not recognize audio from the target language during inference with graphemes from the training languages. - Referring to
FIG. 3C , the consistency regularization part (i.e., modality matching part) 300 c of the training process 300 is configured to promote theaudio encoder 210 to learn consistent predictions between non-synthetic speech (e.g., real/human speech) andalignment outputs 402 corresponding to unspokentextual utterances 320 by generating a consistent loss term (non-synthetic speech utterance 304. As such, thenon-synthetic speech utterance 304 and the paired alignment output 404 of each training utterance pair 301 is associated with a same ground-truth transcription. In short, theconsistent loss term 352 between the transcribednon-synthetic speech utterance 304 and paired alignment output 404 of the same training utterance provides an unsupervised training aspect by encouraging theaudio encoder 210 to behave consistently regardless of whether the training utterance belongs to non-synthetic speech (i.e., speech training data) or the alignment output (i.e., text training data) and independent of supervised loss terms between the ground-truth transcription 302 and each of: non-synthetic speech recognition hypotheses output by theauxiliary decoder 390; and speech recognition hypotheses output by theauxiliary decoder 390. - Similar to the alignment outputs 402 generated from the unspoken
textual utterances 320 inFIG. 3B , thealignment model 400 may generate each paired alignment output 404 using the correspondingtranscription 302 that is paired with transcribednon-synthetic speech utterance 304. Here, thenon-synthetic speech representation 304 is associated with paired alignment output 404 generated by thealignment model 400 mapping the unspokentextual utterance 320 into speech frames. - During the consistency regularization part 300 c, the
text encoder 202 receives, as input, each paired alignment output 404 and generates, as output, for each of the plurality of output steps, an encodedtextual representation 313 that corresponds to the paired alignment output 404 at the corresponding output step. The sharedencoder 250 receives, as input, the encodedtextual representation 313 and generates, as output, a first encoded shared representation (e*sup) 323. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each first encoded sharedrepresentation 323 output from the sharedencoder 250 and generates, as output, afirst probability distribution 311 over possible speech recognition hypotheses for the corresponding paired alignment output 404 at the corresponding output step. In some examples, thefirst probability distribution 311 over possible speech recognition hypotheses includes one of possible phoneme labels or possible word piece labels. - Similarly, the
speech encoder 204 receives, as input, each transcribednon-synthetic speech utterance 304 as a sequence of features/vectors (e.g., mel-frequency spectrograms such as the acoustic frames 110 ofFIG. 1 ) and generates, as output, for each of a plurality of output steps, a encodedaudio representation 314 that corresponds to the transcribednon-synthetic speech utterance 304 at the corresponding output step. The sharedencoder 250 receives, as input, the encodedaudio representation 314 and generates, as output, a second encoded shared representation (esup) 324. Theauxiliary decoder 390 including the phoneme decoder or the wordpiece decoder receives, as input, each second encoded sharedrepresentation 324 output from the sharedencoder 250 and generates, as output, asecond probability distribution 394 over possible non-synthetic speech recognition hypotheses for the corresponding transcribednon-synthetic speech utterance 304 at the corresponding time step. In some examples, thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses includes the one of the possible phoneme labels or the possible word piece labels. - With continued reference to
FIG. 3C , the consistency regularization part 300 c of the training process 300 further determines, at each of the plurality of time steps for each training utterance pair 301, the consistent loss term (first probability distribution 311 over possible speech recognition hypotheses and thesecond probability distribution 394 over possible non-synthetic speech recognition hypotheses. For instance, the training process 300 may employ a consistencyloss term module 350 configured to receive, at each time step, the corresponding non-synthetic speech and speech recognition results 311, 394 output by theauxiliary decoder 390, and determine theconsistency loss term 352 for the corresponding training utterance pair 301 at the time step. - In some examples, the consistency regularization part 300 c of the training process 300 determines the consistent loss term 352 based on a Kullback-Leibler divergence (DKL) between the first probability distribution 311 over possible speech recognition hypotheses and the second probability distribution 394 over possible non-synthetic speech recognition hypotheses. The consistent loss term 352 based on DKL may be expressed by the following equation:
- Here, the
consistent loss term 352 determined for the training utterance pair 301 at each time step provides an “unsupervised” loss term that is independent of the accuracy of the auxiliary decoder 390 (e.g., independent of thesupervised loss terms FIG. 3B ), and thus, may be employed to update parameters of theaudio encoder 210 for promoting consistency between non-synthetic speech representations and alignment outputs of the same utterances. In batch training, theconsistent loss term 352 may correspond to an average loss term obtained for the batch. In other words, theconsistent loss term 352 permits theaudio encoder 210 to learn to behave the same, e.g., make consistent encoded representation predictions on both non-synthetic speech (e.g., real/human speech) and alignment outputs of a same training utterance, regardless of whether the training utterance belongs to non-synthetic speech or alignment outputs. -
-
- where λ1 may be equal to 1.0 and λ2 is equal to 0.1. The training process 300 may pre-train the audio encoder 210 using the overall loss term,
audio encoder 210 to effectively teach theaudio encoder 210 to learn shared representations between speech and text in the target language even though no labeled training data in the target language is available. After training theaudio encoder 210, the training process 300 may fine-tune the pre-trained audio encoder on transcribed speech utterances that may include supervised training samples of both alignment outputs corresponding to unspokentextual utterance 320 and non-synthetic (e.g., human speech).
- where λ1 may be equal to 1.0 and λ2 is equal to 0.1. The training process 300 may pre-train the audio encoder 210 using the overall loss term,
- In some implementations, the training process 300 for training the
audio encoder 210 applies encoder consistency regularization. Unlike decoder consistency regularization applied to auxiliary decoder(s) during the consistency regularization part 300 c that requires hypothesized labels (e.g.,transcripts 302 and unspoken textual utterances 320), encoder consistency regularization does not require hypothesized labels and therefore has the advantage being allowed to be applied to all thetraining data -
- Specific to HCCR, a Convolutional Neural Network (CNN) projection network may calculate projections over increasing length segments of encoder activations e (30, 50, 120 ms) to yield 3 views (V) and draw negative examples from the same utterance for short segments, and from other utterances in the batches with 120 ms segments. Accordingly, an HCCR loss may be calculated over the transcribed non-synthetic speech utterances 304 (paired speech), the un-transcribed non-synthetic speech utterances 306 (unpaired speech), and the alignment outputs 402 generated from the unspoken
textual utterances 320 as follows: -
-
- Implementations described above describe the training process 300 training the training the
audio encoder 210 for a target language, however, it is understood that the training process 300 may also be employed to train the audio encoder for multiple target languages each different from the one or more training languages. As such, theaudio encoder 210 for amultilingual ASR model 200. In some instances, the training process 300 may be employed to train end-to-end ASR models with decoder structures (i.e., non-pre-training) or fine-tune an ASR model to perform downstream tasks such as speech translation or natural language understanding. Moreover, implementations described above describe the training process using each part 300 a-c of the training process 300. Yet, it is understood any combination of the training parts 300 a-c may be used to train theaudio encoder 210 using any combination of unspokentextual utterances 320, transcribednon-synthetic speech utterances 304, and/or untranscribednon-synthetic speech utterances 306 independently. - For instance, the transcribed
non-synthetic speech utterances 304 in the one or more training languages may initially be used to train the alignment model 400 (FIGS. 5 and 6 ). Thereafter, using thealignment model 400 trained using labeled training data in the one or more training languages, the training process 300 may train the audio encoder using the alignmentoutput loss term 342 derived from the unspokentextual utterances 320 in the target language(s) during the supervised loss part 300 b. Advantageously, the training process 300 may leverage high-resource languages (e.g., where an abundance of labeled training data already exists) to initially train thealignment model 400 and use the trainedalignment model 400 to train theaudio encoder 210 on a low-resource (e.g., little or zero labeled training data exists) target language. Simply put, by training the alignment model using the large amount of labeled training data with one or more high-resource languages thealignment model 400 learns to generate alignment outputs in target languages even though thealignment model 400 was never trained on any data (or zero labeled training data) in the target language. - In some scenarios, after training the
audio encoder 210, the ASR model recognizes audio from the target language during inference with graphemes from the training languages. As such, in some implementations, the consistency regularization part 300 c of the training process 300 employs the residual adaptor layers 330 that condition at least one of theaudio encoder 210 or the decoder (e.g., prediction network 220 and joint network 230 (FIG. 2 )) on thelanguage identifier 321 that uniquely identifies the target language. Eachresidual adaptor layer 330 includes a small feed forward network including self-attention layers (e.g., 2 self-attention layers) with a bottleneck dimension. Thus, theresidual identifier output 332 is fed to the sharedencoder 250 such that theaudio encoder 210 is conditioned on thelanguage identifier 321 and does not recognize audio from the target language during inference with graphemes from the training languages. -
FIG. 7 is a flowchart of an example arrangement of operations for a computer-implementedmethod 700 of using aligned text and speech representations to train automatic speech recognition models without transcribed speech data. Themethod 700 may execute on data processing hardware 810 (FIG. 8 ) using instructions stored on memory hardware 820 (FIG. 8 ) may reside on theuser device 102 and/or remote computer/server 201 ofFIG. 1 corresponding to a computing device 800 (FIG. 8 ). - At
operation 702, themethod 700 includes receiving training data including unspokentextual utterances 320 in a target language. Each unspokentextual utterance 320 not paired with any corresponding spoken utterance of non-synthetic speech (or synthetic speech). Atoperation 704, themethod 700 includes generating acorresponding alignment output 402 for each unspokentextual utterance 320 of the received training data using analignment model 400. Thealignment model 400 is trained on transcribedspeech utterances 304 in one or more training languages each different than the target language. That is, thealignment model 400 is trained on the training languages and generates the alignment outputs 402 for the unspokentextual utterances 320 in the target language that were unseen by thealignment model 400 during training. Atoperation 706, themethod 700 includes generating, using atext encoder 202, a corresponding encodedtextual representation 312 for eachalignment output 402. Atoperation 708, themethod 700 includes training aspeech recognition model 200 on the encodedtextual representation 312 generated for the alignment outputs 402 corresponding to the unspokentextual utterances 320 in the target language to teach thespeech recognition model 200 to learn how to recognize speech in the target language. Notably, the only transcribed (i.e., paired) training data used to train thespeech recognition model 200 to learn how to recognize speech in the target language is the transcribedspeech utterances 304 in the one or more training languages, each of which is different than the target language, used to train thealignment model 400. -
FIG. 8 is a schematic view of anexample computing device 800 that may be used to implement the systems and methods described in this document. Thecomputing device 800 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 800 includes aprocessor 810,memory 820, astorage device 830, a high-speed interface/controller 840 connecting to thememory 820 and high-speed expansion ports 850, and a low speed interface/controller 860 connecting to a low speed bus 870 and astorage device 830. Each of thecomponents processor 810 can process instructions for execution within thecomputing device 800, including instructions stored in thememory 820 or on thestorage device 830 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 880 coupled tohigh speed interface 840. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 800 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 820 stores information non-transitorily within thecomputing device 800. Thememory 820 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 820 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 800. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 830 is capable of providing mass storage for thecomputing device 800. In some implementations, thestorage device 830 is a computer-readable medium. In various different implementations, thestorage device 830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 820, thestorage device 830, or memory onprocessor 810. - The
high speed controller 840 manages bandwidth-intensive operations for thecomputing device 800, while thelow speed controller 860 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 840 is coupled to thememory 820, the display 880 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 850, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 860 is coupled to thestorage device 830 and a low-speed expansion port 890. The low-speed expansion port 890, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 800 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 800 a or multiple times in a group ofsuch servers 800 a, as alaptop computer 800 b, or as part of arack server system 800 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (24)
1. A computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations comprising:
receiving training data comprising unspoken textual utterances in a target language, each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech;
generating, using an alignment model, a corresponding alignment output for each unspoken textual utterance of the received training data, the alignment model trained on transcribed speech utterances in one or more training languages each different than the target language;
generating, using a text encoder, a corresponding encoded textual representation for each alignment output; and
training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
2. The computer-implemented method of claim 1 , wherein training the speech recognition model comprises training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning.
3. The computer-implemented method of claim 1 , wherein the speech recognition model comprises an audio encoder and a decoder.
4. The computer-implemented method of claim 3 , wherein the decoder comprises a recurrent neural network transducer (RNN-T) architecture.
5. The computer-implemented method of claim 3 , wherein the audio encoder comprises:
the text encoder;
a speech encoder; and
a shared encoder.
6. The computer-implemented method of claim 3 , wherein the audio encoder comprises a plurality of multi-headed self-attention layers.
7. The computer-implemented method of claim 3 , wherein the audio encoder comprises a Conformer encoder.
8. The computer-implemented method of claim 3 , wherein the operations further comprise conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language.
9. The computer-implemented method of claim 8 , wherein conditioning the at least one of the audio encoder or the decoder comprises conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers.
10. The computer-implemented method of claim 1 , wherein training the speech recognition model comprises:
for each alignment output, generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space;
for each transcribed speech utterance in the one or more training languages:
determining, using a speech encoder, an encoded audio representation of the transcribed speech utterance; and
generating, using the shared encoder, a second encoded shared representation of the transcribed speech utterance in the shared latent representation space,
wherein training the speech recognition model comprises training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages.
11. The computer-implemented method of claim 1 , wherein each unspoken textual utterance comprises a sequence of words, word-pieces, graphemes, and/or phonemes.
12. The computer-implemented method of claim 1 , wherein the operations further comprise converting, using a pronunciation model, a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages.
13. A system comprising:
data processing hardware; and
memory hardware in communication with the data processing hardware, the memory hardware storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising:
receiving training data comprising unspoken textual utterances in a target language, each unspoken textual utterance not paired with any corresponding spoken utterance of non-synthetic speech;
generating, using an alignment model, a corresponding alignment output for each unspoken textual utterance of the received training data, the alignment model trained on transcribed speech utterances in one or more training languages each different than the target language;
generating, using a text encoder, a corresponding encoded textual representation for each alignment output; and
training a speech recognition model on the encoded textual representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language to teach the speech recognition model to learn how to recognize speech in the target language.
14. The system of claim 13 , wherein training the speech recognition model comprises training the speech recognition model without using any transcribed speech utterances in the target language for supervised learning.
15. The system of claim 13 , wherein the speech recognition model comprises an audio encoder and a decoder.
16. The system of claim 15 , wherein the decoder comprises a recurrent neural network transducer (RNN-T) architecture.
17. The system of claim 15 , wherein the audio encoder comprises:
the text encoder;
a speech encoder; and
a shared encoder.
18. The system of claim 15 , wherein the audio encoder comprises a plurality of multi-headed self-attention layers.
19. The system of claim 15 , wherein the audio encoder comprises a Conformer encoder.
20. The system of claim 15 , wherein the operations further comprise conditioning at least one of the audio encoder or the decoder on a language identifier uniquely identifying the target language.
21. The system of claim 20 , wherein conditioning the at least one of the audio encoder or the decoder comprises conditioning the at least one of the audio encoder or the decoder on the language identifier using residual adaptor layers.
22. The system of claim 13 , wherein training the speech recognition model comprises:
for each alignment output, generating, using a shared encoder, a first encoded shared representation of the alignment output in a shared latent representation space;
for each transcribed speech utterance in the one or more training languages:
determining, using a speech encoder, an encoded audio representation of the transcribed speech utterance; and
generating, using the shared encoder, a second encoded shared representation of the transcribed speech utterance in the shared latent representation space,
wherein training the speech recognition model comprises training the speech recognition model on the first encoded shared representations generated for the alignment outputs corresponding to the unspoken textual utterances in the target language and the second encoded shared representation generated for the transcribed speech utterances in the one or more training languages.
23. The system of claim 13 , wherein each unspoken textual utterance comprises a sequence of words, word-pieces, graphemes, and/or phonemes.
24. The system of claim 13 wherein the operations further comprise converting, using a pronunciation model, a script of the unspoken textual utterances in the target language into a phonetic representation shared across multiple languages.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/355,508 US20240029715A1 (en) | 2022-07-22 | 2023-07-20 | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263369213P | 2022-07-22 | 2022-07-22 | |
US18/355,508 US20240029715A1 (en) | 2022-07-22 | 2023-07-20 | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data |
Publications (1)
Publication Number | Publication Date |
---|---|
US20240029715A1 true US20240029715A1 (en) | 2024-01-25 |
Family
ID=87570831
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/355,508 Pending US20240029715A1 (en) | 2022-07-22 | 2023-07-20 | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data |
Country Status (2)
Country | Link |
---|---|
US (1) | US20240029715A1 (en) |
WO (1) | WO2024020154A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11222620B2 (en) * | 2020-05-07 | 2022-01-11 | Google Llc | Speech recognition using unspoken text and speech synthesis |
WO2023183680A1 (en) * | 2022-03-20 | 2023-09-28 | Google Llc | Alignment prediction to inject text into automatic speech recognition training |
-
2023
- 2023-07-20 WO PCT/US2023/028267 patent/WO2024020154A1/en unknown
- 2023-07-20 US US18/355,508 patent/US20240029715A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2024020154A1 (en) | 2024-01-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929060B2 (en) | Consistency prediction on streaming sequence models | |
US11610586B2 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
JP2023545988A (en) | Transformer transducer: One model that combines streaming and non-streaming speech recognition | |
US11961515B2 (en) | Contrastive Siamese network for semi-supervised speech recognition | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
US20230186901A1 (en) | Attention-Based Joint Acoustic and Text On-Device End-to-End Model | |
US20220310065A1 (en) | Supervised and Unsupervised Training with Contrastive Loss Over Sequences | |
US20220310080A1 (en) | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation | |
US11715458B2 (en) | Efficient streaming non-recurrent on-device end-to-end model | |
US20230317059A1 (en) | Alignment Prediction to Inject Text into Automatic Speech Recognition Training | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
US20240029715A1 (en) | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data | |
US20240153484A1 (en) | Massive multilingual speech-text joint semi-supervised learning for text-to-speech | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training | |
US20230013587A1 (en) | Advancing the Use of Text and Speech in ASR Pretraining With Consistency and Contrastive Losses | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
US20240153498A1 (en) | Contextual Biasing With Text Injection | |
US20230298565A1 (en) | Using Non-Parallel Voice Conversion for Speech Conversion Models | |
US20230107695A1 (en) | Fusion of Acoustic and Text Representations in RNN-T | |
US20230103722A1 (en) | Guided Data Selection for Masked Speech Modeling | |
US20240028829A1 (en) | Joint Speech and Text Streaming Model for ASR | |
US20230107248A1 (en) | Deliberation of Streaming RNN-Transducer by Non-Autoregressive Decoding | |
US20240013777A1 (en) | Unsupervised Data Selection via Discrete Speech Representation for Automatic Speech Recognition | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20240029720A1 (en) | Context-aware Neural Confidence Estimation for Rare Word Speech Recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ROSENBERG, ANDREW;CHEN, ZHEHUAI;BAPNA, ANKUR;AND OTHERS;REEL/FRAME:064335/0903Effective date: 20220722 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |