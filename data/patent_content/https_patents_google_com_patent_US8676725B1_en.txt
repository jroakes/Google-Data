US8676725B1 - Method and system for entropy-based semantic hashing - Google Patents
Method and system for entropy-based semantic hashing Download PDFInfo
- Publication number
- US8676725B1 US8676725B1 US12/794,380 US79438010A US8676725B1 US 8676725 B1 US8676725 B1 US 8676725B1 US 79438010 A US79438010 A US 79438010A US 8676725 B1 US8676725 B1 US 8676725B1
- Authority
- US
- United States
- Prior art keywords
- hash function
- bit hash
- affinity matrix
- bit
- hypothesis
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Definitions
- Embodiments of the present invention relate to identifying semantic nearest neighbors in a feature space.
- LSH Locality Sensitive Hashing
- Parameter sensitive hashing is one such extension. It chooses a set of weak binary classifiers to generate bits for hash keys. The classifiers are selected according to the criteria that nearby objects in a dataset are more likely to have a same class label than more distant objects. A major drawback of this type of approach is the requirement of evaluation on object pairs, which has size quadratic to the number of objects. Hence, its scalability to larger scale datasets is limited.
- RBM Restricted Boltzmann machines
- Spectral hashing takes a completely different approach to generate hash code. Spectral hashing first rotates feature space to statistically orthogonal axes using principal component analysis (PCA). Then, a special basis function is applied to carve each axis independently to generate hash bits. As a result, bits in a hash code are independent, which leads to a compact representation with short code length. Experiments show that spectral hashing outperforms RBM. However, spectral hashing is developed on the assumption that objects are spread in a Euclidean space with a particular distribution—either uniform or Gaussian. This is seldom true in a real world data set.
- Embodiments relate to methods and systems for identifying semantic nearest neighbors for an object in a feature space.
- a method embodiment includes generating an affinity matrix for objects in a given feature space, wherein the affinity matrix identifies the semantic similarity between each pair of objects in the feature space, training a multi-bit hash function using a greedy algorithm that increases the Hamming distance between dissimilar objects in the feature space while minimizing the Hamming distance between similar objects, and identifying semantic nearest neighbors for an object in a second feature space using the multi-bit hash function.
- a system embodiment includes a hash generator configured to generate the affinity matrix and configured to train the multi-bit hash function, and a similarity determiner configured to identify semantic nearest neighbors for an object in a second feature space using the multi-bit hash function.
- FIG. 1 illustrates a system for identifying semantic nearest neighbors for an object in a feature space, according to an embodiment.
- FIG. 2 is a flowchart of a method of identifying semantic nearest neighbors for an object in a feature space, according to an embodiment.
- FIG. 3 is a flowchart of a method for training a hash function, according to an embodiment.
- FIG. 4 is a flowchart of a method for training a hash function, according to another embodiment.
- FIG. 5 is a flowchart of a method for training a hash function, according to yet another embodiment.
- FIGS. 6A-6D are a set of plots comparing experimental results between a method according to an embodiment of the present invention and state-of-the-art methods of image retrieval.
- FIG. 7 illustrates exemplary pairs of affinity matrix blocks and hypothesis lists, according to an embodiment.
- FIG. 8 illustrates an example computer useful for implementing components of the embodiments.
- Embodiments relate to methods and systems for identifying semantic nearest neighbors for an object in a feature space.
- the affinity matrix of a training data set is used to train a hash function such that the Hamming distances correlate to the similarities specified in the affinity matrix.
- the hash function may be a collection of bit functions, and training the hash function is a greedy process that incrementally selects bit functions to expand the hash code.
- the hash function selects an initial bit function by minimizing a graph cut under a normalization constraint.
- computing the graph cut may take linear time.
- the normalization constraint has square complexity in computation, but embodiments also provide an approximate linear time solution.
- Embodiments provide an approximate linear time solution by minimizing the graph cut while maximizing conditional entropy on each pair of bit functions. Such approximation can improve the learning time of the algorithm.
- the generated hash codes have small mutual information and, therefore, are compact.
- a trained hash function maps objects to binary vectors such that neighboring objects (i.e., objects with similar semantics) have small Hamming distances between their representative binary vectors, while irrelevant objects have large Hamming distances between their binary vectors. Therefore, these binary vectors can be used for fast semantic nearest-neighbor retrieval.
- training the hash function takes time linear to the data size. This makes the hash function feasible to tasks with an evolving dataset, in which periodically updating or re-training the hash function is required.
- FIG. 1 is a diagram of system 100 for identifying semantic nearest neighbors for an object in a feature space, according to an embodiment. While the following is described in terms of data that includes images, the invention is not limited to this embodiment. Embodiments of the invention can be used in conjunction with any other form of data such as video, audio and textual data. For example, embodiments of the invention can be used in any system having generally the structure of FIG. 1 , or that would benefit from the operation, methods and functions as described herein.
- feature as used herein may be any form of feature or feature descriptor used to represent images, text, video, audio and/or their characteristics.
- System 100 includes hash generator 120 , similarity determiner 140 and repository 104 .
- Repository 104 includes a plurality of data sets. Such data sets can include, but are not limited to, digital images, text, video and other forms of data. Such data can be multi-dimensional or even single dimensional data. Data sets in repository 104 may also include image statistics (histograms of color or texture).
- hash generator 120 generates hashing functions using the data sets present in repository 104 .
- similarity determiner 140 determines similarity between data sets present in repository 104 using the hashing functions generated by hash generator 120 . The operation of hash generator 120 is described further below.
- hash generator 120 may be implemented on, among other things, any device that can retrieve, download or view digital image (or other data) from any form of a network 102 .
- a device can include, but is not limited to, a personal computer, mobile device such as a mobile phone, workstation, embedded system, game console, television, set-top box, or any other computing device that can support image viewing or image retrieval.
- Such a device includes, but is not limited to, a device having a processor and memory for executing and storing instructions.
- Such a device may include software, firmware, and hardware or some combination thereof.
- the software may include one or more applications and an operating system.
- the hardware can include, but is not limited to, a processor, memory and user interface display.
- An optional input device such as a mouse or other pointing device, may be used.
- embodiments of the present invention may be implemented in a lookup platform of a face recognition system or other image or video retrieval system.
- FIG. 2 illustrates an exemplary overall operation of the system described in FIG. 1 .
- FIG. 2 is a flowchart of a method 200 for identifying semantic nearest neighbors for an object in a feature space, according to an embodiment of the present invention.
- an affinity matrix is generated for objects in a given feature space, where the affinity matrix identifies the semantic similarity between each pair of objects in the feature space.
- hash generator 120 may generate an affinity matrix using objects or data stored in repository 104 .
- the affinity can be set to a value ‘1’ for all pairs of objects with matching labels, and can be set to a value of ‘0’ otherwise.
- the objects are images of faces, and the labels are the name of the person depicted
- two images of ‘John Doe’ would have affinity 1
- an image of ‘John’ and an image of ‘Jane’ would have affinity 0.
- the Euclidean distance between the objects in repository 104 can be computed. This computed value can then be negated in order to convert the Euclidean distance into an affinity.
- a multi-bit hash function is trained using a greedy algorithm that increases the Hamming distance between dissimilar objects in the feature space while minimizing the Hamming distance between similar objects.
- hash generator 120 can train a multi-bit hash function using a greedy algorithm and the affinity matrix generated in step 202 .
- the multi-bit hash function is used to identify semantic nearest neighbors for an object in a second feature space.
- similarity determiner 140 can use the multi-bit hash function to identify semantic nearest neighbors for an object in a second feature space.
- a similarity preserving hash function is trained by hash generator 120 using a given training set ⁇ x i ⁇ I and affinity matrix S of the training set. This hash function maps objects from the feature space to a Hamming space such that objects with high similarity measures will have small Hamming distances.
- d k (i,j) is the distance based on b k .
- H T (i,j) is the Hamming distance between two hash codes generated by B T . Therefore,
- the hash learning problem is formulated by hash generator 120 as a distribution learning process.
- hash generator 120 defines another distribution W (T) using Hamming distance H T .
- hash function B T can be learned by minimizing the Kullback-Leibler divergence, i.e., K L(S ⁇ W (T) ).
- the Kullback-Leibler divergence is known to those skilled in the art and is a non-symmetric measure of the difference between two probability distributions.
- ⁇ is set to be 1 in the following derivations.
- Directly optimizing (1) is a challenging task, especially when B T has a large hypothesis space.
- a greedy approach is adopted by hash generator 120 to accomplish, for example, step 204 of method 200 .
- hash generator 120 accomplishes a greedy approach by factorizing (1) into a recursive equation.
- a sub-optimal algorithm is obtained that incrementally learns the hash function one bit at a time.
- J T (from (1)) can be rewritten as:
- L T represents the ‘improvement’ from adding binary function b T to the hash function B T-1 . If L T is negative, adding b T is favorable because it further reduces the cross entropy defined in (1).
- the hash function can be learned by hash generator 120 by incrementally selecting new binary functions to expand the hash code.
- the learning algorithm is formalized as follows:
- FIG. 3 is a flowchart of a method for training a hash function according to an embodiment.
- a generated affinity matrix is normalized.
- the affinity matrix generated in step 202 of flowchart 200 can be normalized by hash generator 120 .
- a single-bit hash function is determined that minimizes the graph cut of the normalized affinity matrix while maximizing the sum of the pairwise Hamming distances.
- hash generator 120 determines a single bit hash function using the affinity matrix normalized in step 302 .
- an additional single-bit hash function is determined that minimizes the graph cut of the normalized affinity matrix while maximizing the entropy between the additional single-bit hash function and all previously-determined single-bit hash functions, based on an evaluation of Hamming distance spread by the sum of all previously-determined single-bit hash functions.
- step 308 the additional single-bit hash function and all previously-determined single-bit hash functions are combined into a multi-bit hash function.
- hash generator 120 may combine the additional single-bit hash function and all previously-determined single-bit hash functions into a multi-bit hash function.
- method 300 returns to step 306 and continues to perform steps 306 and 308 until a specified number of bits are included in the multi-bit hash function.
- cut S (b t+1 ) ⁇ i,j;b t+1 (i) ⁇ b t+1 (j)
- S ij is the total loss of assigning similar objects to different binary code in b t+1 . This term is minimized when similar objects are assigned the same binary code.
- cut S (b t+1 ) can be trivially minimized by hash generator 120 by assigning all objects the same label, collapsing all Hamming distances to zero.
- the greedy algorithm performed by hash generator 120 incrementally adds bits to the hash function to increase the Hamming distance between dissimilar objects while keeping the Hamming distance between similar objects small.
- a method in accordance with such a greedy algorithm is further detailed in FIG. 4 .
- the greedy algorithm described above may be simple, but exactly computing (4) has an inherited quadratic cost. It requires computing H T (i,j) for every possible (i,j), and constantly updating those values as t increases. This makes the hash learning algorithm intractable for large datasets.
- two fast linear time approximate algorithms can instead be used by hash generator 120 to accomplish, for example, step 106 of method 100 , neither of which compute H T .
- the first approximate algorithm uses the property
- hash generator may efficiently compute (4).
- sum(H 2 ) can be computed using only N, N 1 , N 11 , and N 10 :
- L 2 can be computed by hash generator 120 without explicitly computing H 2 (i,j). In addition, because it only takes linear time to get the counts of N 1 , N 11 , and N 10 , this method is a linear time algorithm.
- Equation (5) can also be written as:
- cut( H 1 ,b 2 )/sum( H t ) is the Hamming distance spread that b 2 has induced to H 1 . Increasing the spread reduces L 2 .
- selecting b t+1 depends on cut( H t ,b t+1 )/sum( H t ) which has quadratic computational complexity.
- hash generator 120 uses an approximate algorithm that avoids this computation. The algorithm is based on the result for two-bit hash code, and measures b t+1 against every bit in H t separately.
- sum(H t ) is rewritten as sum(B t ). This is valid because Hamming distance H t is determined by hash function B t .
- hash generator 120 decomposes B t into a set of 1-bit hash functions and measures the improvement b t+1 induces on each of these hash functions.
- the selection of b t+1 is according to the following criterion:
- min b k ⁇ B t cut( d k ,b t+1 )/sum( ⁇ b k ⁇ ) is a heuristic approximation to cut( H t ,b t+1 )/sum( H t )
- b t+1 induces a certain amount of Hamming-distance spread on any of the binary functions in B T
- b t+1 is expected to induce good quality spread on Hamming distance H t , which is the sum of all of these one-bit functions.
- a method in accordance with such a SPEC-spread algorithm is further detailed in FIG. 4 .
- FIG. 4 is a flowchart of a method for training a hash function according to an embodiment.
- a generated affinity matrix is normalized.
- the affinity matrix generated in step 202 of flowchart 200 can be normalized by hash generator 120 .
- a single-bit hash function is determined that minimizes the graph cut of the normalized affinity matrix while maximizing the sum of the pairwise Hamming distances.
- hash generator 120 determines a single bit hash function using the affinity matrix normalized in step 402 .
- an additional single-bit hash function is determined function that minimizes the graph cut of the normalized affinity matrix while maximizing the entropy between the additional single-bit hash function and each previously-determined single-bit hash function, based on an evaluation of the Hamming distance spread by each individual previously-determined single-bit hash function.
- step 408 the additional single-bit hash function and all previously-determined single-bit hash functions are combined into a multi-bit hash function.
- hash generator 120 may combine the additional single-bit hash function and all previously-determined single-bit hash functions into a multi-bit hash function.
- method 400 returns to step 406 and continues to perform steps 406 and 408 until a specified number of bits are included in the multi-bit hash function.
- b k ) has a strong correlation with the Hamming distance spread cut( d k ,b l )/sum( d k ).
- b k ) will also be the maximal solution to ( d k ,b l )/sum( ⁇ b k ⁇ ).
- b k ) can be computed as:
- b k ) is the lower bound on the conditional entropies between b t+1 and each of the binary functions in B t .
- Minimizing the negative of this bound in (8) presents a constraint to maximizing this minimal conditional entropy. This can be further explained using mutual information.
- binary function b t+1 should have small cut S (b t+1 ), large bit entropy H(b t+1 ) and small mutual information with each of the binary functions in B T , which is measured by the upper bound max b k ⁇ B t I(b t+1 ,b k ). With such minimal mutual information constraints, the hash function learned by hash generator 120 can produce a compact code.
- a method in accordance with such a SPEC-entropy algorithm is further detailed in FIG. 5 .
- a generated affinity matrix is normalized.
- the affinity matrix generated in step 202 of flowchart 200 can be normalized by hash generator 120 .
- a single-bit hash function is determined that minimizes the graph cut of the normalized affinity matrix while maximizing the entropy of the bit.
- hash generator 120 determines a single bit hash function using the affinity matrix normalized in step 502 .
- an additional single-bit hash function is determined that minimizes the graph cut of the normalized affinity matrix while maximizing the conditional entropy between the additional single-bit hash function and each individual, previously-determined single-bit hash function.
- step 508 the additional single-bit hash function and all previously-determined single-bit hash functions are combined into a multi-bit hash function.
- hash generator 120 may combine the additional single-bit hash function and all previously-determined single-bit hash functions into a multi-bit hash function.
- method 500 returns to step 506 and continues to perform steps 506 and 508 until a specified number of bits are included in the multi-bit hash function.
- a decision stump performs binary classification by thresholding on a feature value. It can be computed fast, which is ideal for applications involving nearest neighbor search. Decision stumps may have a bounded hypothesis space. For a dataset with N objects and M feature dimensions, the number of hypotheses is
- MN. Using this property together with the special structure of the two hashing algorithms defined in (7) and (9), the learning time can be further reduced by hash generator 120 using dynamic programming.
- H be the hypothesis space of decision stumps and h ⁇ H be one such stump. Because S is fixed, for each h, cut S (h) can be pre-computed by hash generator 120 . The value can then be determined (using for e.g., a look-up table), rather than recomputed, during the learning process. Repeatedly evaluating max b k ⁇ B t sum( ⁇ b k ,h ⁇ ) in (7) is particularly expensive, and this cost grows as t increases. However, by using the property
- min b k ⁇ ⁇ B l + 1 ⁇ ⁇ sum ⁇ ( ⁇ b k , h ⁇ ) max ⁇ ( min b k ⁇ B l ⁇ sum ⁇ ( ⁇ b k , h ⁇ ) , sum ⁇ ( ⁇ b t + 1 , h ⁇ ) ) for each stump h, the value of max b k ⁇ B t sum( ⁇ b k ,h ⁇ ) can be stored and updated by hash generator 120 using the recurrence above each time a new binary function is added to the hash function. This reduces the per-bit learning time from O(tMN) to O(MN).
- a similar method can be applied to compute min
- semantic nearest neighbors for objects in a feature space can be identified in step 206 of method 200 .
- the performance of exemplary hash learning algorithms as discussed above has been evaluated on two tasks: retrieving semantically similar images from the LabelMe image database provided by the Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, and performing nearest-neighbor recognition of celebrity face images.
- the first experimental dataset includes approximately 13,500 image thumbnails from the LabelMe dataset. Each image is represented using a 512-dimensional Gist feature vector. Ground truth similarity is obtained by calculating the L2 distance between these Gist vectors, and thresholding the values. The dataset was divided into a training set containing 80% of the samples, and a test set containing the remainder. After training, hash codes were computed for all samples. For each test sample, the nearest neighbors (based on Hamming distance between codes) were found from amongst the training samples, and performance was evaluated by measuring the precision and recall.
- Performance is compared to two baseline algorithms.
- the first is the state of the art Spectral Hashing (Y. Weiss et al., Spectral Hashing, Advances in Neural Information Processing Systems 21, MIT Press, Cambridge, Mass., 2008, which is incorporated herein by reference in its entirety.)
- the second is a simple yet effective technique, which will be referred to herein as PCA hashing (see B. Wang et al., Efficient Duplicate image Detection Algorithm for Web Images and Large-Scale Database, Technical Report, Microsoft Research, 2005; and X.-J. Wang et al., Annosearch: Image Auto-annotation by Search, IEEE Conference on Computer Vision and Pattern Recognition , vol. 2, pp.
- PCA hashing computes a k-bit hash code by projecting each sample to the k principal components of the training set, then binarizing the coefficients, by setting each to 1 if it exceeds the average value seen for the training set, and 0 otherwise.
- the inventors also tried applying the algorithms after first transforming the input Gist values using PCA.
- FIGS. 6A though 6 D are a comparisons of hashing algorithms on the LabelMe image retrieval task.
- 64-bit hash codes were trained, measuring the performance when retrieving all samples within a fixed Hamming distance radius to the test images.
- the plots in FIGS. 6A-6D are as follows: FIG. 6A-Precision vs. Hamming radius; FIG. 6 B—Recall vs. Hamming radius; FIG. 6 C—Precision vs. Recall; FIG. 6 D—a plot of precision within the top 15 nearest neighbors averaged over test images, as the code length in bits increases from 2 to 100 bits.
- the Precision vs. Recall plot FIG.
- the dataset included 3387 celebrities, each with between 5 and 500 faces.
- the celebrities were split into two sets: a training set of 1684 celebrities, and a held-out set of 1703, with no intersection between the two.
- Each of these sets were further subdivided into a gallery, containing 70% of the faces, and a test set, containing the remaining 30%.
- the top performing algorithm from the LabelMe experiment, SPEC-Entropy was trained on the gallery portion of the training celebrities, and hash codes were computed for the remaining faces. Ground truth similarity information was determined by matching the name used to label each face.
- Test faces were recognized by returning the label of the nearest gallery sample, based on the Hamming distances between hash codes, and recognition accuracy was averaged across all testing samples. Although the baseline Neven Vision face recognition system was able to score 88.77% accuracy on the test set, the Neven Vision system requires a larger number of bits in a feature vector to obtain such accuracy.
- the results of the model according to an embodiment of the invention are as follows: 500 bits—85.72% accuracy; 750 bits—87.04% accuracy; 1000 bits—87.6% accuracy; 1250 bits—87.98% accuracy; 1500 bits—88.15% accuracy.
- the training set is re-arranged by hash generator 120 .
- hash generator 120 uses a ‘MapReduce’ framework to re-arrange data in the training set.
- MapReduce developed by Google Inc.
- MapReduce framework includes a master node and a plurality of worker nodes.
- the master node receives an input problem, divides the input problem into smaller sub-problems, and distributes the sub-problems to worker nodes.
- a worker node may repeat the process of dividing the sub-problems into smaller problems, leading to a multi-level tree structure. In this way, worker nodes process sub-problems, and pass the results of the sub-problems back to their parent nodes or master node. During a ‘Reduce’ stage the master node then takes the answers to all the sub-problems and combines them to generate an answer to the original input problem received by the master node.
- a ‘General Broadcast Reduce’ framework is an iterative MapReduce framework.
- the master node after completing all of the steps described above and obtaining a result of the original input problem, the master node broadcasts a message to all worker nodes (e.g., the answer to the problem), then starts again at the beginning, sending tasks to the workers and collecting their output.
- the master node may start again at the beginning to verify if the answer to the original input problem was correct.
- a ‘join MapReduce’ framework is a way of applying the MapReduce framework to two different sources of data.
- each task that a worker node processes includes of a pair of records (e.g., one record from data source 1 and the other record from data source 2 ).
- hash generator 120 indexes the input data (e.g., training data) records by sample ID values.
- each record contains a list of feature index-value pairs with respect to a training sample.
- the output records may be keyed by feature indices, and each output record contains a list of sample index-feature value pairs with respect to the given feature index.
- such output data is referred to as a feature value list(s).
- hash generator 120 With the rearranged data, hash generator 120 generates a number of hypotheses from each feature value list using, for example, the MapReduce framework.
- a hypothesis is a binary classifier with a given classification threshold.
- samples with feature values (of a given feature index) larger than the threshold may be classified as ‘1’ and ‘0’ if otherwise.
- the output generated by hash generator 120 is a set of hypothesis lists.
- Each hypothesis list includes all hypotheses of a given feature index.
- hash generator 120 computes the cut value of each hypothesis using, for example, the MapReduce framework.
- each hypothesis needs to traverse the affinity matrix of the training set. For a large scale data set, this affinity matrix may be too large for memory and it may be time consuming to use look-up based methods to traverse the affinity matrix.
- hash generator 120 may divide the affinity matrix into a number of blocks (e.g, a, b, c . . . etc.).
- hash generator 120 also divides the hypotheses into groups of hypothesis lists (e.g., A, B, C, etc.).
- hash generator 120 may include two different mappers to map the number of blocks (e.g, a, b, c . . . etc.) and the hypothesis lists (A, B, C, . . . etc.) separately and in parallel.
- hash generator 120 processes pairs of blocks and lists, such as (A,a), as shown in FIG. 7 .
- a processing operation may be accomplished by worker nodes within a MapReduce framework.
- hash generator 120 aggregates the results of the processing to obtain a cut value of each hypothesis in hypothesis list A.
- hash generator 120 may obtain a cut value of each hypothesis in hypothesis lists B and C.
- such an aggregation of results may be accomplished by running another instance of the MapReduce framework.
- hash generator 120 may initialize a general broadcast reduce framework to learn the SPEC hashing function.
- a general broadcast reduce is an iterative process between master nodes and worker nodes in the MapReduce framework.
- a master node selects a hypothesis and adds it to a hash function.
- each worker node works on a group of hypothesis lists.
- the master node sends information of the most recently selected hash bit to all worker nodes.
- Each worker node uses the information to update spread values of all its hypotheses.
- a worker node may then select the best (or optimal) hypothesis it has and return it to the master node.
- the master node may select the best hypothesis and the iteration continues. In an embodiment, such an iterative learning process continues till the specified number of bits are included in the hash function generated by hash generator 120 .
- system and components of embodiments described herein are implemented using one or more computers, such as example computer 802 shown in FIG. 8 .
- hash generator 120 or similarity determiner 140 can be implemented using computer(s) 802 .
- Computer 802 can be any commercially available and well known computer capable of performing the functions described herein, such as computers available from International Business Machines, Apple, Oracle, HP, Dell, Cray, etc.
- Computer 802 includes one or more processors (also called central processing units, or CPUs), such as a processor 806 .
- processors also called central processing units, or CPUs
- Processor 806 is connected to a communication infrastructure 804 .
- Computer 802 also includes a main or primary memory 808 , such as random access memory (RAM).
- Primary memory 808 has stored therein control logic 868 A (computer software), and data.
- Computer 802 also includes one or more secondary storage devices 810 .
- Secondary storage devices 810 include, for example, a hard disk drive 812 and/or a removable storage device or drive 814 , as well as other types of storage devices, such as memory cards and memory sticks.
- Removable storage drive 814 represents a floppy disk drive, a magnetic tape drive, a compact disk drive, an optical storage device, tape backup, etc.
- Removable storage drive 814 interacts with a removable storage unit 816 .
- Removable storage unit 816 includes a computer useable or readable storage medium 864 A having stored therein computer software 868 B (control logic) and/or data.
- Removable storage unit 816 represents a floppy disk, magnetic tape, compact disk, DVD, optical storage disk, or any other computer data storage device.
- Removable storage drive 814 reads from and/or writes to removable storage unit 816 in a well known manner.
- Computer 802 also includes input/output/display devices 866 , such as monitors, keyboards, pointing devices, Bluetooth devices, etc.
- input/output/display devices 866 such as monitors, keyboards, pointing devices, Bluetooth devices, etc.
- Computer 802 further includes a communication or network interface 818 .
- Network interface 818 enables computer 802 to communicate with remote devices.
- network interface 818 allows computer 802 to communicate over communication networks or mediums 864 B (representing a form of a computer useable or readable medium), such as LANs, WANs, the Internet, etc.
- Network interface 818 may interface with remote sites or networks via wired or wireless connections.
- Control logic 868 C may be transmitted to and from computer 802 via communication medium 864 B.
- Any tangible apparatus or article of manufacture comprising a computer useable or readable medium having control logic (software) stored therein is referred to herein as a computer program product or program storage device.
- Embodiments can work with software, hardware, and/or operating system implementations other than those described herein. Any software, hardware, and operating system implementations suitable for performing the functions described herein can be used. Embodiments are applicable to both a client and to a server or a combination of both.
Abstract
Description
sum(H k)=Σij e −H
and
cuts(b l)=Σi,j S ij d l(i,j)=Σi,j;b
This choice of naming will be explained further in the following section.
Let
cut(H k ,b l)=Σi,j;b
Putting (2) and (3) together provides:
- 1. Starting with t=0, initialize an empty hash function B0.
- 2. Find binary function bt+1 that minimizes:
- 3. Set Bt+1={Bt, bt+1} and increment t by one. Repeat step 2 until either the desired code length is reached or no candidate for bt+1 has negative Lt+1. In an embodiment, steps (1)-(3), may be performed by
hash generator 120.
cutS(bt+1)=Σi,j;b
is the total loss of assigning similar objects to different binary code in bt+1. This term is minimized when similar objects are assigned the same binary code. In an embodiment, taken in isolation, cutS (bt+1) can be trivially minimized by
cut(H t ,b t+1)=i,j;b
bt+1 should assign different codes to as many pairs (i,j), especially those with small Hamming distance HT(i,j). This countervailing force is referred to as the Hamming-distance spread, and is measured by
cut(H t ,b t+1)/sum(H t)≦1
The larger its value is, the better the spread.
and measures bt+1 against each dk (i.e., pairwise Hamming distance) separately. In contrast, the second approximate algorithm allows
Case with 2-Bit Hash Code
cut(H 1 ,b 2)/sum(H t)
is the Hamming distance spread that b2 has induced to H1. Increasing the spread reduces L2.
SPEC-Spread Algorithm
cut(H t ,b t+1)/sum(H t)
which has quadratic computational complexity. In an embodiment,
sum({b k ,b 1})=Σi,j e −(d
is denoted as the sum of the 2-bit hash function {bk, b1}. Given current hash function Bt={b1, . . . , bt},
Applying (6) to (7),
cut(dk,bt+1)/sum({bk}) is the measurement of Hamming-distance spread that bt+1 induces on each of these 1-bit hash functions. By applying minb
minb
is a heuristic approximation to
cut(H t ,b t+1)/sum(H t)
Knowing that bt+1 induces a certain amount of Hamming-distance spread on any of the binary functions in BT, bt+1 is expected to induce good quality spread on Hamming distance Ht, which is the sum of all of these one-bit functions.
cut(d k ,b l)/sum(d k).
(d k ,b l)/sum({b k}).
Based on this observation, another heuristic-based approximate algorithm that uses minimal conditional entropy to approximate the log term in (8) is:
According to this equation, binary function bt+1 should have small cutS (bt+1), large bit entropy H(bt+1) and small mutual information with each of the binary functions in BT, which is measured by the upper bound maxb
maxb
in (7) is particularly expensive, and this cost grows as t increases. However, by using the property
for each stump h, the value of
maxb
can be stored and updated by
in (9).
Claims (14)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/794,380 US8676725B1 (en) | 2009-06-05 | 2010-06-04 | Method and system for entropy-based semantic hashing |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18462909P | 2009-06-05 | 2009-06-05 | |
US12/794,380 US8676725B1 (en) | 2009-06-05 | 2010-06-04 | Method and system for entropy-based semantic hashing |
Publications (1)
Publication Number | Publication Date |
---|---|
US8676725B1 true US8676725B1 (en) | 2014-03-18 |
Family
ID=50240455
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/794,380 Active 2032-04-14 US8676725B1 (en) | 2009-06-05 | 2010-06-04 | Method and system for entropy-based semantic hashing |
Country Status (1)
Country | Link |
---|---|
US (1) | US8676725B1 (en) |
Cited By (31)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140086492A1 (en) * | 2011-05-27 | 2014-03-27 | Osaka Prefecture University Public Corporation | Storage method and storage device for database for approximate nearest neighbor search |
US20140195499A1 (en) * | 2013-01-10 | 2014-07-10 | International Business Machines Corporation | Real-time classification of data into data compression domains |
US20140304236A1 (en) * | 2013-04-05 | 2014-10-09 | Canon Kabushiki Kaisha | Hash value generation apparatus, system, determination method, program, and storage medium |
US20150058622A1 (en) * | 2013-08-20 | 2015-02-26 | Hewlett-Packard Development Company, L.P. | Data stream traffic control |
US9053122B2 (en) | 2013-01-10 | 2015-06-09 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US20150199224A1 (en) * | 2014-01-10 | 2015-07-16 | Instep Software, Llc | Method and Apparatus for Detection of Anomalies in Integrated Parameter Systems |
CN105095162A (en) * | 2014-05-19 | 2015-11-25 | 腾讯科技（深圳）有限公司 | Text similarity determining method and device, electronic equipment and system |
US9564918B2 (en) | 2013-01-10 | 2017-02-07 | International Business Machines Corporation | Real-time reduction of CPU overhead for data compression |
US9672358B1 (en) * | 2015-11-04 | 2017-06-06 | Invincea, Inc. | Methods and apparatus for detecting malware samples with similar image sets |
CN106951865A (en) * | 2017-03-21 | 2017-07-14 | 东莞理工学院 | A kind of secret protection biometric discrimination method based on Hamming distances |
EP3127343A4 (en) * | 2014-04-07 | 2017-10-18 | The Nielsen Company (US), LLC | Methods and apparatus to identify media using hash keys |
CN109389147A (en) * | 2018-08-28 | 2019-02-26 | 昆明理工大学 | A kind of similar determination method of image based on improvement PHash algorithm |
US20190273903A1 (en) * | 2015-07-31 | 2019-09-05 | Versitech Limited | A multi-overlay variable support and order kernel-based representation for image deformation and view synthesis |
US10659329B1 (en) * | 2017-04-28 | 2020-05-19 | EMC IP Holding Company LLC | Container distance measurement and clustering |
US10885098B2 (en) | 2015-09-15 | 2021-01-05 | Canon Kabushiki Kaisha | Method, system and apparatus for generating hash codes |
US20210224475A1 (en) * | 2018-11-07 | 2021-07-22 | Mitsubishi Electric Corporation | Information processing device, information processing method, and storage medium storing information processing program |
US11093474B2 (en) * | 2018-11-15 | 2021-08-17 | Bank Of America Corporation | Computer architecture for emulating multi-dimensional string correlithm object dynamic time warping in a correlithm object processing system |
US11424005B2 (en) | 2019-07-01 | 2022-08-23 | Micron Technology, Inc. | Apparatuses and methods for adjusting victim data |
US20220293166A1 (en) * | 2021-03-15 | 2022-09-15 | Micron Technology, Inc. | Apparatuses and methods for sketch circuits for refresh binning |
US11462291B2 (en) | 2020-11-23 | 2022-10-04 | Micron Technology, Inc. | Apparatuses and methods for tracking word line accesses |
US11482275B2 (en) | 2021-01-20 | 2022-10-25 | Micron Technology, Inc. | Apparatuses and methods for dynamically allocated aggressor detection |
US11521669B2 (en) | 2019-03-19 | 2022-12-06 | Micron Technology, Inc. | Semiconductor device having cam that stores address signals |
US11568918B2 (en) | 2019-08-22 | 2023-01-31 | Micron Technology, Inc. | Apparatuses, systems, and methods for analog accumulator for determining row access rate and target row address used for refresh operation |
US11600326B2 (en) | 2019-05-14 | 2023-03-07 | Micron Technology, Inc. | Apparatuses, systems, and methods for a content addressable memory cell and associated comparison operation |
US11664063B2 (en) | 2021-08-12 | 2023-05-30 | Micron Technology, Inc. | Apparatuses and methods for countering memory attacks |
US11688451B2 (en) | 2021-11-29 | 2023-06-27 | Micron Technology, Inc. | Apparatuses, systems, and methods for main sketch and slim sketch circuit for row address tracking |
US20230205889A1 (en) * | 2021-12-24 | 2023-06-29 | Vast Data Ltd. | Securing a storage system |
US11694738B2 (en) | 2018-06-19 | 2023-07-04 | Micron Technology, Inc. | Apparatuses and methods for multiple row hammer refresh address sequences |
US11699476B2 (en) | 2019-07-01 | 2023-07-11 | Micron Technology, Inc. | Apparatuses and methods for monitoring word line accesses |
US11854618B2 (en) | 2019-06-11 | 2023-12-26 | Micron Technology, Inc. | Apparatuses, systems, and methods for determining extremum numerical values |
US11984148B2 (en) | 2021-09-09 | 2024-05-14 | Micron Technology, Inc. | Apparatuses and methods for tracking victim rows |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7949186B2 (en) * | 2006-03-15 | 2011-05-24 | Massachusetts Institute Of Technology | Pyramid match kernel and related techniques |
US8213689B2 (en) | 2008-07-14 | 2012-07-03 | Google Inc. | Method and system for automated annotation of persons in video content |
-
2010
- 2010-06-04 US US12/794,380 patent/US8676725B1/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7949186B2 (en) * | 2006-03-15 | 2011-05-24 | Massachusetts Institute Of Technology | Pyramid match kernel and related techniques |
US8213689B2 (en) | 2008-07-14 | 2012-07-03 | Google Inc. | Method and system for automated annotation of persons in video content |
Non-Patent Citations (25)
Title |
---|
A. Andoni, M. Datar, N. Immorlica, P. Indyk, and V. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. Nearest Neighbor Methods for Learning and Vision, Neural Processing Information Series, MIT Press, 2005. * |
Bentley, Jon, "K-d Trees for Semidynamic Point Sets," In SCG '90: Proceedings of the Sixth Annual Symposium on Computational Geometry, Published in 1990; pp. 187-197. |
C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Y. Ng, and K. Olukotun. Map-reduce for machine learning on multicore. In NIPS '07: Proceedings of Twenty-First Annual Conference on Neural Information Processing Systems. Neural Information Processing Systems Foundation, 2007. * |
Datar et al., "Locality-Sensitive Hashing Scheme Based on p-Stable Distributions," In SCG '04: Proceedings of the Twentieth Annual Symposium on Computational Geometry, Jun. 9-11, 2004; pp. 253-262. |
Jain et al., "Fast Image Search for Learned Metrics," In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2008; pp. 1-8. |
Ke et al., "Computer Vision for Music Identification: Video Demonstration," IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005, vol. 2, Jun. 20-25, 2005; 1 page. |
Lin et al., "SPEC Hashing: Similarity Preserving algorithm for Entropy-based Coding," 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 13-19, 2010, pp. 848-854. |
M. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the 34th Annual ACM Symposium on Theory of Computing, pp. 380-388, 2002. * |
Malik, Hassan Haider; Kender, John Ronald. WO/2008/154029. * |
Meila et al., "Learning Segmentation by Random Walks," In Advances in Neural Information Processing Systems, Published in 2001; pp. 873-879. |
Phillips et at., "FRVT 2006 and ICE 2006 Large-Scale Results," Technical Report NISTIR 7408, National Institute of Standards and Technology, Published Mar. 2007; pp. 1-55. |
R. R. Salakhutdinov and G. E. Hinton. Semantic hashing. In SIGIR workshop on Information Retrieval and applications of Graphical Models, 2007. * |
Raginsky et al., "Locality-Sensitive Binary Codes from Shift-Invariant Kernels," In Advances in Neural Information Processing Systems 22, Published in 2009; pp. 1-9. |
Rahimi et al., "Random Features for Large-Scale Kernel Machines," In Advances in Neural Information Processing Systems 22, Published in 2009; pp. 1-8. |
Scheidat T, Vielhauer C: Biometric hashing for handwriting: entropy based feature selection and semantic fusion. Proc of SPIE 2008. * |
Shakhnarovich et al., "Fast Pose Estimation with Parameter-Sensitive Hashing," Ninth IEEE International Conference on Computer Vision, vol. 2, Published Oct. 13-16, 2003; pp. 1-9. |
Torralba et al., "Small Codes and Large Image Databases for Recognition," In Computer Vision and Pattern Recognition, Published in 2008; pp. 1-8. |
Wang et al., "AnnoSearch: Image Auto-Annotation by Search," Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), Published in 2006; pp. 1-8. |
Wang et al., "Large-Scale Duplicate Detection for Web Image Search," ICME 2006, Published in 2006; pp. 353-356. |
Wang, Bin, et al. "Large-scale duplicate detection for web image search." Multimedia and Expo, 2006 IEEE International Conference on. IEEE, 2006. * |
Xiang-song Hou; Cao Yuan-da; Zhi-tao Guan; , "A Semantic Search Model based on Locality-sensitive Hashing in mobile P2P," Advanced Communication Technology, 2008. ICACT 2008. 10th International Conference on , vol. 3, no., pp. 1635-1640, Feb. 17-20, 2008. * |
Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS, Dec. 2008. * |
Yagnik et al., "Learning People Annotation from the Web via Consistency Learning," In Proceedings of the International Workshop on Multimedia Information Retrieval, Published Sep. 28-29, 2007; pp. 285-290. |
Yang et al., "Unifying Discriminative Visual Codebook Generation with Classifier Training for Object Category Recognition," In Computer Vision and Pattern Recognition, Published in 2008; pp. 1-8. |
Zhao et al., "Large Scale Learning and Recognition of Faces in Web Videos," In Proceedings of: Automatic Face & Gesture Recognition, FG '08 8th IEEE, Published in 2008; pp. 1-7. |
Cited By (45)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140086492A1 (en) * | 2011-05-27 | 2014-03-27 | Osaka Prefecture University Public Corporation | Storage method and storage device for database for approximate nearest neighbor search |
US9792350B2 (en) * | 2013-01-10 | 2017-10-17 | International Business Machines Corporation | Real-time classification of data into data compression domains |
US9239842B2 (en) | 2013-01-10 | 2016-01-19 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US9564918B2 (en) | 2013-01-10 | 2017-02-07 | International Business Machines Corporation | Real-time reduction of CPU overhead for data compression |
US10387376B2 (en) | 2013-01-10 | 2019-08-20 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US9588980B2 (en) | 2013-01-10 | 2017-03-07 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US9053122B2 (en) | 2013-01-10 | 2015-06-09 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US9053121B2 (en) | 2013-01-10 | 2015-06-09 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US20140195499A1 (en) * | 2013-01-10 | 2014-07-10 | International Business Machines Corporation | Real-time classification of data into data compression domains |
US20140304236A1 (en) * | 2013-04-05 | 2014-10-09 | Canon Kabushiki Kaisha | Hash value generation apparatus, system, determination method, program, and storage medium |
US9485222B2 (en) * | 2013-08-20 | 2016-11-01 | Hewlett-Packard Development Company, L.P. | Data stream traffic control |
US20150058622A1 (en) * | 2013-08-20 | 2015-02-26 | Hewlett-Packard Development Company, L.P. | Data stream traffic control |
US9379951B2 (en) * | 2014-01-10 | 2016-06-28 | Instep Software, Llc | Method and apparatus for detection of anomalies in integrated parameter systems |
US20150199224A1 (en) * | 2014-01-10 | 2015-07-16 | Instep Software, Llc | Method and Apparatus for Detection of Anomalies in Integrated Parameter Systems |
EP3127343A4 (en) * | 2014-04-07 | 2017-10-18 | The Nielsen Company (US), LLC | Methods and apparatus to identify media using hash keys |
CN105095162A (en) * | 2014-05-19 | 2015-11-25 | 腾讯科技（深圳）有限公司 | Text similarity determining method and device, electronic equipment and system |
US10742954B2 (en) * | 2015-07-31 | 2020-08-11 | Versitech Limited | Multi-overlay variable support and order kernel-based representation for image deformation and view synthesis |
US20190273903A1 (en) * | 2015-07-31 | 2019-09-05 | Versitech Limited | A multi-overlay variable support and order kernel-based representation for image deformation and view synthesis |
US10885098B2 (en) | 2015-09-15 | 2021-01-05 | Canon Kabushiki Kaisha | Method, system and apparatus for generating hash codes |
US9852297B1 (en) * | 2015-11-04 | 2017-12-26 | Invincea, Inc. | Methods and apparatus for detecting malware samples with similar image sets |
US9672358B1 (en) * | 2015-11-04 | 2017-06-06 | Invincea, Inc. | Methods and apparatus for detecting malware samples with similar image sets |
US10592667B1 (en) * | 2015-11-04 | 2020-03-17 | Invincea, Inc. | Methods and apparatus for detecting malware samples with similar image sets |
CN106951865B (en) * | 2017-03-21 | 2020-04-07 | 东莞理工学院 | Privacy protection biological identification method based on Hamming distance |
CN106951865A (en) * | 2017-03-21 | 2017-07-14 | 东莞理工学院 | A kind of secret protection biometric discrimination method based on Hamming distances |
US10659329B1 (en) * | 2017-04-28 | 2020-05-19 | EMC IP Holding Company LLC | Container distance measurement and clustering |
US11694738B2 (en) | 2018-06-19 | 2023-07-04 | Micron Technology, Inc. | Apparatuses and methods for multiple row hammer refresh address sequences |
CN109389147A (en) * | 2018-08-28 | 2019-02-26 | 昆明理工大学 | A kind of similar determination method of image based on improvement PHash algorithm |
CN109389147B (en) * | 2018-08-28 | 2022-02-08 | 昆明理工大学 | Image similarity judgment method based on improved PHash algorithm |
US20210224475A1 (en) * | 2018-11-07 | 2021-07-22 | Mitsubishi Electric Corporation | Information processing device, information processing method, and storage medium storing information processing program |
US11836449B2 (en) * | 2018-11-07 | 2023-12-05 | Mitsubishi Electric Corporation | Information processing device and information processing method for judging the semantic relationship between words and sentences |
US11093474B2 (en) * | 2018-11-15 | 2021-08-17 | Bank Of America Corporation | Computer architecture for emulating multi-dimensional string correlithm object dynamic time warping in a correlithm object processing system |
US11521669B2 (en) | 2019-03-19 | 2022-12-06 | Micron Technology, Inc. | Semiconductor device having cam that stores address signals |
US11600326B2 (en) | 2019-05-14 | 2023-03-07 | Micron Technology, Inc. | Apparatuses, systems, and methods for a content addressable memory cell and associated comparison operation |
US11854618B2 (en) | 2019-06-11 | 2023-12-26 | Micron Technology, Inc. | Apparatuses, systems, and methods for determining extremum numerical values |
US11699476B2 (en) | 2019-07-01 | 2023-07-11 | Micron Technology, Inc. | Apparatuses and methods for monitoring word line accesses |
US11424005B2 (en) | 2019-07-01 | 2022-08-23 | Micron Technology, Inc. | Apparatuses and methods for adjusting victim data |
US11568918B2 (en) | 2019-08-22 | 2023-01-31 | Micron Technology, Inc. | Apparatuses, systems, and methods for analog accumulator for determining row access rate and target row address used for refresh operation |
US11462291B2 (en) | 2020-11-23 | 2022-10-04 | Micron Technology, Inc. | Apparatuses and methods for tracking word line accesses |
US11482275B2 (en) | 2021-01-20 | 2022-10-25 | Micron Technology, Inc. | Apparatuses and methods for dynamically allocated aggressor detection |
US11600314B2 (en) * | 2021-03-15 | 2023-03-07 | Micron Technology, Inc. | Apparatuses and methods for sketch circuits for refresh binning |
US20220293166A1 (en) * | 2021-03-15 | 2022-09-15 | Micron Technology, Inc. | Apparatuses and methods for sketch circuits for refresh binning |
US11664063B2 (en) | 2021-08-12 | 2023-05-30 | Micron Technology, Inc. | Apparatuses and methods for countering memory attacks |
US11984148B2 (en) | 2021-09-09 | 2024-05-14 | Micron Technology, Inc. | Apparatuses and methods for tracking victim rows |
US11688451B2 (en) | 2021-11-29 | 2023-06-27 | Micron Technology, Inc. | Apparatuses, systems, and methods for main sketch and slim sketch circuit for row address tracking |
US20230205889A1 (en) * | 2021-12-24 | 2023-06-29 | Vast Data Ltd. | Securing a storage system |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8676725B1 (en) | Method and system for entropy-based semantic hashing | |
US10007679B2 (en) | Enhanced max margin learning on multimodal data mining in a multimedia database | |
Lin et al. | Spec hashing: Similarity preserving algorithm for entropy-based coding | |
EP3983948A1 (en) | Optimised machine learning | |
US10387430B2 (en) | Geometry-directed active question selection for question answering systems | |
Wu et al. | Semi-supervised nonlinear hashing using bootstrap sequential projection learning | |
US6941303B2 (en) | System and method for organizing, compressing and structuring data for data mining readiness | |
US20160307113A1 (en) | Large-scale batch active learning using locality sensitive hashing | |
CN113177132B (en) | Image retrieval method based on depth cross-modal hash of joint semantic matrix | |
CN109271486B (en) | Similarity-preserving cross-modal Hash retrieval method | |
Yan et al. | Active learning from multiple knowledge sources | |
CN111080551B (en) | Multi-label image complement method based on depth convolution feature and semantic neighbor | |
Lin et al. | Optimizing ranking measures for compact binary code learning | |
CN115293919B (en) | Social network distribution outward generalization-oriented graph neural network prediction method and system | |
CN111382283A (en) | Resource category label labeling method and device, computer equipment and storage medium | |
Fu et al. | Multiple instance learning for group record linkage | |
Yilmaz et al. | RELIEF-MM: effective modality weighting for multimedia information retrieval | |
US10198695B2 (en) | Manifold-aware ranking kernel for information retrieval | |
Wang et al. | Revealing the fog-of-war: A visualization-directed, uncertainty-aware approach for exploring high-dimensional data | |
He et al. | A hybrid feature selection method based on genetic algorithm and information gain | |
Dhoot et al. | Efficient Dimensionality Reduction for Big Data Using Clustering Technique | |
Wang et al. | A multi-label least-squares hashing for scalable image search | |
CN115146103A (en) | Image retrieval method, image retrieval apparatus, computer device, storage medium, and program product | |
Chávez et al. | Decomposability of disat for index dynamization | |
US20210174228A1 (en) | Methods for processing a plurality of candidate annotations of a given instance of an image, and for learning parameters of a computational model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LIN, RUEI-SUNG;ROSS, DAVID;YAGNIK, JAY;REEL/FRAME:030977/0949Effective date: 20100604 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |