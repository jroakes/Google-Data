JP2023525066A - Depth estimation based on subject bottom edge position - Google Patents
Depth estimation based on subject bottom edge position Download PDFInfo
- Publication number
- JP2023525066A JP2023525066A JP2022567599A JP2022567599A JP2023525066A JP 2023525066 A JP2023525066 A JP 2023525066A JP 2022567599 A JP2022567599 A JP 2022567599A JP 2022567599 A JP2022567599 A JP 2022567599A JP 2023525066 A JP2023525066 A JP 2023525066A
- Authority
- JP
- Japan
- Prior art keywords
- camera
- subject
- bottom edge
- image data
- distance
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000013507 mapping Methods 0.000 claims abstract description 105
- 238000000034 method Methods 0.000 claims abstract description 47
- 230000008859 change Effects 0.000 claims description 10
- 230000009471 action Effects 0.000 claims description 8
- 238000010801 machine learning Methods 0.000 claims description 7
- 230000000007 visual effect Effects 0.000 claims description 4
- 238000001514 detection method Methods 0.000 claims description 3
- 238000003708 edge detection Methods 0.000 claims description 2
- 230000000875 corresponding effect Effects 0.000 description 58
- 230000003287 optical effect Effects 0.000 description 28
- 238000004891 communication Methods 0.000 description 18
- 230000006870 function Effects 0.000 description 15
- 238000005259 measurement Methods 0.000 description 13
- 238000012549 training Methods 0.000 description 11
- 238000013528 artificial neural network Methods 0.000 description 9
- 238000012545 processing Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 238000013500 data storage Methods 0.000 description 5
- 230000007246 mechanism Effects 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 238000001228 spectrum Methods 0.000 description 5
- 238000013459 approach Methods 0.000 description 4
- 230000001771 impaired effect Effects 0.000 description 4
- 238000003825 pressing Methods 0.000 description 4
- 241001465754 Metazoa Species 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- 238000012546 transfer Methods 0.000 description 3
- 206010011878 Deafness Diseases 0.000 description 2
- 230000003213 activating effect Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 230000005670 electromagnetic radiation Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000005484 gravity Effects 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 230000007774 longterm Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000001960 triggered effect Effects 0.000 description 2
- 241000699670 Mus sp. Species 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 238000007667 floating Methods 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 238000002329 infrared spectrum Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000001429 visible spectrum Methods 0.000 description 1
- 210000000707 wrist Anatomy 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/536—Depth or shape recovery from perspective effects, e.g. by using vanishing points
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/521—Depth or shape recovery from laser ranging, e.g. using interferometry; from the projection of structured light
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/764—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using classification, e.g. of video objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10048—Infrared image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20092—Interactive image processing based on input by user
- G06T2207/20104—Interactive definition of region of interest [ROI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
Abstract
方法は、カメラから、環境における被写体を表わす画像データを受信することと、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定することとを含み得る。方法はまた、垂直位置と画像データの高さとの間の被写体下端比率を決定することと、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定することとを含む。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定し得る。方法は加えて、カメラと被写体との間の物理的距離の推定値の指標を生成することを含み得る。The method may include receiving image data representing an object in the environment from a camera and determining a vertical position of a bottom edge of the object within the image data based on the image data. The method also includes determining a subject bottom ratio between the vertical position and the height of the image data, and estimating the physical distance between the camera and the subject via a distance projection model and based on the subject bottom ratio. determining a value. The distance projection model defines, for each respective candidate subject bottom ratio of a plurality of candidate subject bottom ratios, a mapping between (i) the respective candidate subject bottom ratio and (ii) the corresponding physical distance in the environment. can. The method may additionally include generating an indication of the physical distance estimate between the camera and the subject.
Description
関連出願との相互参照
本願は、２０２０年６月３日に出願され、「被写体下端位置に基づいた深度推定」（Depth Estimation Based on Object Bottom Position）と題された、米国仮特許出願第６３／０３３，９６４号の優先権を主張する。当該仮出願全体は、あたかもこの記載で完全に述べられたかのように、本明細書に引用により援用される。
CROSS-REFERENCE TO RELATED APPLICATIONS This application is filed June 3, 2020 and is entitled "Depth Estimation Based on Object Bottom Position." 033,964 is claimed. The entire provisional application is hereby incorporated by reference as if fully set forth in this description.
背景
立体カメラは、被写体に関連付けられた距離または深度を決定するために使用され得る。具体的には、立体カメラは、被写体の二つ以上の画像を同時に取り込み得る。立体カメラの画像センサ間の既知の距離と、二つ以上の同時に取り込まれた画像における被写体の表現間の相違とに基づいて、距離または深度が決定され得る。同様に、カメラは、被写体距離または深度を決定するために、構造化光パターンプロジェクタと組み合わされて使用され得る。具体的には、パターンが異なる深度の被写体上に投影される際にパターンが変形され、分散され、または他の態様で変更される程度に基づいて、距離または深度が決定され得る。各アプローチでは、深度または距離測定は、いくつかのコンピューティングデバイス上で利用できないかもしれない撮像ハードウェアを伴う。
Background Stereoscopic cameras can be used to determine the distance or depth associated with an object. Specifically, a stereoscopic camera may simultaneously capture two or more images of a subject. Based on the known distance between the stereo camera's image sensors and the difference between representations of the object in two or more simultaneously captured images, the distance or depth can be determined. Similarly, a camera can be used in combination with a structured light pattern projector to determine subject distance or depth. Specifically, the distance or depth may be determined based on the degree to which the pattern is distorted, dispersed, or otherwise altered as it is projected onto objects at different depths. In each approach, depth or range measurements involve imaging hardware that may not be available on some computing devices.
要約
カメラによって生成された画像データが、カメラと画像データ内で表わされた被写体との間の距離を決定するために使用され得る。画像データ内の被写体の下端の垂直位置を識別し、垂直位置を画像データの全高で除算して被写体下端比率を得ることによって、被写体までの推定距離が決定され得る。距離投影モデルが、被写体下端比率を、カメラと被写体との間の物理的距離の対応する推定値にマッピングし得る。距離投影モデルは、カメラが環境内の特定の高さに位置付けられ、被写体の下端が環境の地面と接しているという仮定の下で動作し得る。また、ゼロでないピッチ角で配向されたカメラによって画像データが生成される場合、オフセット計算器が、ゼロでないカメラピッチを補償するために被写体下端比率に加算されるべきオフセットを決定し得る。
Summary Image data produced by a camera can be used to determine the distance between the camera and an object represented in the image data. By identifying the vertical position of the bottom edge of the subject in the image data and dividing the vertical position by the total height of the image data to obtain the subject bottom edge ratio, an estimated distance to the subject can be determined. A distance projection model may map the subject bottom ratio to a corresponding estimate of the physical distance between the camera and the subject. A range projection model may operate under the assumption that the camera is positioned at a certain height in the environment and that the bottom edge of the subject is in contact with the ground of the environment. Also, if the image data is generated by a camera oriented at a non-zero pitch angle, an offset calculator may determine an offset to be added to the subject bottom edge ratio to compensate for the non-zero camera pitch.
第１の例示的な実施形態では、コンピュータが実現する方法が提供され、方法は、カメラから、環境における被写体を表わす画像データを受信することを含む。方法はまた、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定することと、垂直位置と画像データの高さとの間の被写体下端比率を決定することとを含む。方法は加えて、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定することを含む。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定する。方法はさらに、カメラと被写体との間の物理的距離の推定値の指標を生成することを含む。 In a first exemplary embodiment, a computer-implemented method is provided, the method including receiving image data representing an object in an environment from a camera. The method also includes determining a vertical position of the bottom edge of the subject in the image data based on the image data, and determining a subject bottom edge ratio between the vertical position and the height of the image data. The method additionally includes determining an estimate of the physical distance between the camera and the subject via a range projection model and based on the subject toe ratio. The distance projection model defines, for each respective candidate subject bottom ratio of a plurality of candidate subject bottom ratios, a mapping between (i) the respective candidate subject bottom ratio and (ii) the corresponding physical distance in the environment. do. The method further includes generating an indication of the physical distance estimate between the camera and the subject.
第２の例示的な実施形態では、コンピューティングシステムが提供され、コンピューティングシステムは、カメラと、プロセッサと、プロセッサによって実行されるとプロセッサに動作を行なわせる命令が格納された、非一時的コンピュータ読取可能記憶媒体とを含む。動作は、カメラから、環境における被写体を表わす画像データを受信することを含む。動作はまた、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定することと、垂直位置と画像データの高さとの間の被写体下端比率を決定することとを含む。動作は加えて、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定することを含む。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定する。動作はさらに、カメラと被写体との間の物理的距離の推定値の指標を生成することを含む。 In a second exemplary embodiment, a computing system is provided, the computing system comprising a camera, a processor, and a non-transitory computer stored with instructions that, when executed by the processor, cause the processor to perform operations. and a readable storage medium. The action includes receiving image data representing an object in the environment from the camera. The operations also include, based on the image data, determining a vertical position of the bottom edge of the subject within the image data and determining a subject bottom edge ratio between the vertical position and the height of the image data. The operations additionally include determining an estimate of the physical distance between the camera and the subject via a range projection model and based on the subject toe ratio. The distance projection model defines, for each respective candidate subject bottom ratio of a plurality of candidate subject bottom ratios, a mapping between (i) the respective candidate subject bottom ratio and (ii) the corresponding physical distance in the environment. do. The operations further include generating an indication of the physical distance estimate between the camera and the subject.
第３の例示的な実施形態では、コンピューティングシステムによって実行されるとコンピューティングシステムに動作を行なわせる命令が格納された、非一時的コンピュータ読取可能記憶媒体が提供される。動作は、カメラから、環境における被写体を表わす画像データを受信することを含む。動作はまた、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定することと、垂直位置と画像データの高さとの間の被写体下端比率を決定することとを含む。動作は加えて、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定することを含む。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定する。動作はさらに、カメラと被写体との間の物理的距離の推定値の指標を生成することを含む。 In a third exemplary embodiment, a non-transitory computer-readable storage medium is provided that stores instructions that, when executed by a computing system, cause the computing system to perform operations. The action includes receiving image data representing an object in the environment from the camera. The operations also include, based on the image data, determining a vertical position of the bottom edge of the subject within the image data and determining a subject bottom edge ratio between the vertical position and the height of the image data. The operations additionally include determining an estimate of the physical distance between the camera and the subject via a range projection model and based on the subject toe ratio. The distance projection model defines, for each respective candidate subject bottom ratio of a plurality of candidate subject bottom ratios, a mapping between (i) the respective candidate subject bottom ratio and (ii) the corresponding physical distance in the environment. do. The operations further include generating an indication of the physical distance estimate between the camera and the subject.
第４の例示的な実施形態では、システムが提供され、システムは、カメラから、環境における被写体を表わす画像データを受信するための手段を含む。システムはまた、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定するための手段と、垂直位置と画像データの高さとの間の被写体下端比率を決定するための手段とを含む。システムは加えて、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定するための手段を含む。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定する。システムはさらに、カメラと被写体との間の物理的距離の推定値の指標を生成するための手段を含む。 In a fourth exemplary embodiment, a system is provided, the system including means for receiving image data representing an object in an environment from a camera. The system also includes, based on the image data, means for determining a vertical position of the bottom edge of the subject within the image data, and means for determining a subject bottom edge ratio between the vertical position and the height of the image data. include. The system additionally includes means for determining an estimate of the physical distance between the camera and the subject via the range projection model and based on the subject toe ratio. The distance projection model defines, for each respective candidate subject bottom ratio of a plurality of candidate subject bottom ratios, a mapping between (i) the respective candidate subject bottom ratio and (ii) the corresponding physical distance in the environment. do. The system further includes means for generating an indication of the physical distance estimate between the camera and the subject.
これらのならびに他の実施形態、局面、利点、および代替例は、添付図面を適宜参照しながら以下の詳細な説明を読むことによって当業者には明らかになるであろう。また、本概要ならびにここに提供される他の説明および図面は、実施形態を単なる例として示すよう意図されており、したがって、多数の変形が可能である。たとえば、構造要素およびプロセスステップは、特許請求される実施形態の範囲内にとどまりつつ、並べ替えられ、組み合わされ、分散され、排除され、または他の態様で変更され得る。 These and other embodiments, aspects, advantages, and alternatives will become apparent to those of ordinary skill in the art upon reading the following detailed description, with appropriate reference to the accompanying drawings. Also, this summary and the other descriptions and drawings provided herein are intended to show embodiments by way of example only, and thus many variations are possible. For example, structural elements and process steps may be rearranged, combined, distributed, eliminated, or otherwise altered while remaining within the scope of the claimed embodiments.
詳細な説明
例示的な方法、デバイス、およびシステムが本明細書で説明される。「例」および「例の」という文言は、本明細書では「例、事例、または例示として機能すること」を意味するために使用されることが理解されるべきである。本明細書で「例」、「例の」、および／または「例示的」として説明されるどの実施形態もしくは特徴も、他の実施形態もしくは特徴よりも好ましいかまたは有利であると述べられない限り、必ずしもそのように解釈されるべきではない。このため、本明細書に提示される主題の範囲から逸脱することなく、他の実施形態を利用することができ、他の変更を行なうことができる。
DETAILED DESCRIPTION Exemplary methods, devices, and systems are described herein. It should be understood that the words "example" and "exemplary" are used herein to mean "serving as an example, instance, or illustration." Any embodiment or feature described herein as "example,""example," and/or "exemplary" is not stated to be preferred or advantageous over other embodiments or features. , should not necessarily be interpreted as such. As such, other embodiments may be utilized and other changes may be made without departing from the scope of the subject matter presented herein.
したがって、本明細書に記載される例示的な実施形態は、限定的であるよう意図されていない。本明細書で概して説明され図面に示される本開示の局面は、幅広く多様な異なる構成で配置され、置き換えられ、組み合わされ、分離され、および設計され得ることが容易に理解されるであろう。 Accordingly, the example embodiments described herein are not intended to be limiting. It will be readily appreciated that the aspects of the disclosure generally described herein and illustrated in the drawings can be arranged, interchanged, combined, separated and designed in a wide variety of different configurations.
また、文脈が別様に示唆しない限り、図面の各々に示される特徴は、互いに組み合わされて使用され得る。このため、図面は概して、各実施形態について図示された特徴がすべて必要であるとは限らないことを理解した上で、一つ以上の全体的な実施形態の構成要素局面として見られるべきである。 Also, the features shown in each of the drawings may be used in combination with each other unless context dictates otherwise. As such, the drawings should generally be viewed as component aspects of one or more overall embodiments, with the understanding that not all illustrated features are required for each embodiment. .
加えて、本明細書または特許請求の範囲における要素、ブロック、またはステップのいかなる列挙も、明確にする目的のためのものである。このため、そのような列挙は、これらの要素、ブロック、またはステップが、特定の配置を固守するかまたは特定の順序で実行されることを要求または暗示すると解釈されるべきではない。特に断りのない限り、図は縮尺通りに描かれていない。 Additionally, any recitation of elements, blocks, or steps in the specification or claims is for the purpose of clarity. Therefore, such listing should not be construed as requiring or implying that these elements, blocks or steps adhere to a particular arrangement, or that they be performed in a particular order. Figures are not drawn to scale unless otherwise noted.
概観
モバイルおよび／またはウェアラブルコンピューティングデバイスを含むコンピューティングデバイスは、環境の画像を取り込むために使用可能なカメラを含み得る。たとえば、カメラは、コンピューティングデバイスがユーザによって保持され、着用され、および／または使用されるときに、ユーザの前の環境に面し得る。したがって、本明細書で提供されるのは、カメラと、カメラによって生成された画像データにおいて表わされた被写体との間の距離を決定するために使用され得るシステムおよび動作である。これらのシステムおよび動作は、たとえば、カメラを含むコンピューティングデバイス上で実現されてもよく、このため、これらのコンピューティングデバイスが環境内の被写体までの距離を測定することを可能にする。
Overview Computing devices, including mobile and/or wearable computing devices, may include cameras that can be used to capture images of the environment. For example, the camera may face the environment in front of the user when the computing device is held, worn, and/or used by the user. Accordingly, provided herein are systems and operations that can be used to determine the distance between a camera and an object represented in image data generated by the camera. These systems and operations may be implemented on computing devices that include, for example, cameras, thus enabling these computing devices to measure distances to objects in the environment.
カメラと被写体との間の距離は、カメラによって生成された画像データにおいて表わされたような被写体の下端の位置を、対応する物理的距離にマッピングすることによって、決定され得る。このアプローチは、対象となる移動被写体および／または静止被写体双方の深度をリアルタイムで計算するための計算上安価なやり方を提供し得る。計算複雑性が比較的低いため、これらのシステムおよび動作は、携帯電話および／またはウェアラブルデバイス（たとえばスマートウォッチ、車載カメラなど）を含む、より低価格のデバイスおよび／または電力制限デバイス上で実現され得る。また、いくつの事例では、システムおよび動作は、一度に一つの視点からの環境を表わす単眼カメラによって生成された画像データに関して使用され得る。このため、たとえば、システムおよび動作は、立体カメラによって生成された立体画像データ、および／または、パターン光プロジェクタによって投影された構造化光パターンを含む画像データに基づく深度測定の代わりとして使用され得る。しかしながら、他の事例では、本明細書に開示されるシステムおよび動作は、立体画像および／または構造化光投影などの距離／深度測定のための他のアプローチと組み合わされて使用され得る。 The distance between the camera and the subject can be determined by mapping the position of the subject's bottom edge as represented in the image data generated by the camera to the corresponding physical distance. This approach may provide a computationally inexpensive way to calculate the depth of both moving and/or stationary objects of interest in real time. Due to their relatively low computational complexity, these systems and operations can be implemented on lower cost and/or power limited devices, including mobile phones and/or wearable devices (e.g., smartwatches, car cameras, etc.). obtain. Also, in some instances, the system and operations may be used with image data generated by a monocular camera representing the environment from one viewpoint at a time. Thus, for example, the system and operation may be used as a substitute for depth measurements based on stereoscopic image data produced by a stereoscopic camera and/or image data comprising structured light patterns projected by a pattern light projector. However, in other cases, the systems and operations disclosed herein may be used in combination with other approaches for distance/depth measurement such as stereoscopic imaging and/or structured light projection.
深度測定のプロセスは、カメラが環境内の地面から既知の高さに配置されていると仮定し得る。たとえば、カメラ、および／または、カメラを収容するコンピューティングデバイスは、胸、腕、手首、または胴回りといったユーザの身体の特定の位置でユーザに結合され、よって、長時間ほぼ同じ高さにとどまり得る。深度測定プロセスは、環境内のさまざまな被写体までの距離を決定するために、カメラによって取り込まれた画像に関して実行され得る。コンピューティングデバイスは、距離の視覚表現、可聴表現、触覚表現、および／または他の表現を生成し得る。このため、たとえば、コンピューティングデバイスは、視覚障害のあるユーザが、コンピューティングデバイスによって実行される深度測定プロセスの支援を受けて環境を通り抜けるために使用され得る。 The depth measurement process may assume that the camera is positioned at a known height from the ground within the environment. For example, a camera and/or a computing device housing a camera may be coupled to the user at a particular location on the user's body, such as the chest, arm, wrist, or waist, and thus remain at approximately the same height for an extended period of time. . A depth measurement process may be performed on the images captured by the camera to determine the distances to various objects in the environment. A computing device may generate visual, audible, tactile, and/or other representations of distance. Thus, for example, a computing device may be used by a visually impaired user to navigate an environment with the assistance of a depth measurement process performed by the computing device.
具体的には、既知の高さにあるカメラを用いて、環境の地面上の特定の距離に位置する各点が、カメラの画像センサ上の対応する位置に関連付けられ、そこで画像を生成することが予想され得る。特定の距離と画像センサ上のその対応する位置との関係は、距離投影モデルの一部を形成し得る、経験的に決定された（たとえば学習および／または訓練された）マッピングによって表わされ得る。このマッピングはカメラが既知の高さにあると仮定するため、カメラの高さが変更される際の距離測定を可能にするために、各々異なる高さに関連付けられた複数の異なるマッピングが、距離投影モデルの一部として提供され得る。 Specifically, with a camera at a known height, each point located at a particular distance on the ground of the environment is associated with a corresponding location on the camera's image sensor to generate an image there. can be expected. The relationship between a particular distance and its corresponding position on the image sensor may be represented by an empirically determined (e.g., learned and/or trained) mapping, which may form part of a range projection model. . Since this mapping assumes that the camera is at a known height, multiple different mappings, each associated with a different height, are used to allow distance measurements as the camera height is changed. It can be provided as part of the projection model.
カメラと、カメラによって取り込まれた画像において表わされた被写体との間の距離を決定するために、コンピューティングデバイスは、被写体の下端の垂直位置を識別するように構成され得る。被写体の下端が使用され得るのは、それが環境の地面上のある点と接していることが予想され、また、距離投影モデルのマッピングが画像位置を地面上の点の距離に関連付けるためである。被写体の下端の垂直位置を画像の高さで除算することによって被写体下端比率が得られ、それは、被写体の下端の垂直位置を画像高さの小数として表わす（すなわち、被写体下端比率は０～１であり得る）。垂直位置を、たとえば画素の絶対数としてではなく、比率として符号化することにより、画像がダウンサンプリングまたはアップサンプリングされる際に同じマッピングが使用され得る。すなわち、被写体下端比率を使用することは、マッピングが所与の画像アスペクト比について解像度不変であることを可能にする。 To determine the distance between the camera and the subject represented in the image captured by the camera, the computing device may be configured to identify the vertical position of the bottom edge of the subject. The bottom edge of the subject can be used because it is expected to be tangent to some point on the ground of the environment, and the mapping of the distance projection model relates the image position to the distance of the point on the ground. . Dividing the vertical position of the bottom edge of the subject by the image height yields the subject bottom ratio, which expresses the vertical position of the bottom edge of the subject as a fraction of the image height (i.e., the subject bottom ratio ranges from 0 to 1). could be). The same mapping can be used when the image is downsampled or upsampled, for example, by encoding the vertical position as a ratio rather than as an absolute number of pixels. That is, using the subject bottom edge ratio allows the mapping to be resolution invariant for a given image aspect ratio.
被写体下端比率に基づいて、距離投影モデルは、被写体までの物理的距離の推定値を決定し得る。具体的には、距離投影モデルは、カメラが位置付けられた高さの指標、カメラの配向（たとえば横長か縦長か）、画像のアスペクト比、および／または、一つ以上の追加のカメラパラメータに基づいて、使用されるべき特定のマッピングを選択し得る。すなわち、距離投影モデルによって提供される各マッピングは、対応する異なる一組のカメラパラメータに関連付けられ得る。このため、各被写体下端比率は、どのマッピングが使用されるかに依存して、異なる距離にマッピングされ得る。実際のカメラパラメータが、マッピングによって仮定されたカメラパラメータと整合する場合、距離推定値は正確である可能性が高いが、これら二つの組のカメラパラメータが異なる場合、距離推定値は誤っているかもしれない。推定距離の誤差は、二つの組のカメラパラメータの対応するパラメータ間の差に比例し得る。 Based on the subject toe ratio, a distance projection model can determine an estimate of the physical distance to the subject. Specifically, the distance projection model is based on an index of the height at which the camera is positioned, camera orientation (e.g., landscape or portrait), image aspect ratio, and/or one or more additional camera parameters. can select the particular mapping to be used. That is, each mapping provided by the range projection model can be associated with a different corresponding set of camera parameters. Thus, each subject bottom edge ratio can be mapped to different distances depending on which mapping is used. If the actual camera parameters match those assumed by the mapping, the range estimate is likely to be accurate, but if these two sets of camera parameters differ, the range estimate may be wrong. unknown. The estimated distance error may be proportional to the difference between the corresponding parameters of the two sets of camera parameters.
選択されたマッピングは、被写体下端比率を対応する物理的距離にマッピングすることによって被写体までの距離の推定値を決定するために使用され得る。とりわけ、いくつかの事例では、画像は構造化光の投影なしで単眼カメラによって生成され得るため、高さ、画像配向、アスペクト比などのカメラパラメータの指定は、他の場合であれば一対の立体画像または構造化光パターンによって提供されたであろう深度手がかりの代わりとして機能し得る。 The selected mapping can be used to determine an estimate of the distance to the object by mapping the object bottom ratio to the corresponding physical distance. Notably, in some cases, images can be produced by a monocular camera without structured light projection, so specification of camera parameters such as height, image orientation, and aspect ratio can be used in other cases as a pair of stereoscopic It can serve as a substitute for depth cues that might have been provided by an image or structured light pattern.
また、カメラが上方に傾斜されるにつれて、被写体の下端の位置は、画像において下降するように見え、また、カメラが下方に傾斜されるにつれて、被写体の下端の位置は、画像において上昇するように見え得る。カメラピッチによる被写体下端のこの見かけの変位は、オフセット計算器によって補償され得る。具体的には、オフセット計算器は、カメラピッチ角の正接とカメラの経験的に決定された焦点距離との積に基づいて、被写体下端比率で表わされる推定オフセットを決定し得る。推定オフセットは次に、被写体下端比率に加算され、この合計が入力として距離投影モデルに提供され得る。推定オフセットを加算することは、被写体下端の垂直位置を、カメラがゼロのピッチ角にあった場合に下端が存在したであろう位置まで戻す効果を有し得る。 Also, as the camera is tilted upward, the position of the bottom edge of the subject appears to descend in the image, and as the camera is tilted downward, the position of the bottom edge of the subject appears to rise in the image. can be seen This apparent displacement of the subject bottom edge due to camera pitch can be compensated for by an offset calculator. Specifically, the offset calculator may determine an estimated offset, expressed in subject bottom ratio, based on the product of the tangent of the camera pitch angle and the empirically determined focal length of the camera. The estimated offset is then added to the subject bottom edge ratio and this sum can be provided as an input to the range projection model. Adding the estimated offset can have the effect of returning the vertical position of the bottom edge of the subject to where it would have been if the camera was at a zero pitch angle.
例示的なコンピューティングデバイス
図１は、コンピューティングシステム１００の例示的なフォームファクタを示す。コンピューティングシステム１００は、たとえば、携帯電話、タブレットコンピュータ、またはウェアラブルコンピューティングデバイスであり得る。しかしながら、他の実施形態が可能である。コンピューティングシステム１００は、本体１０２と、ディスプレイ１０６と、ボタン１０８および１１０といったさまざまな要素を含み得る。コンピューティングシステム１００はさらに、前面カメラ１０４と、背面カメラ１１２と、前面赤外線カメラ１１４と、赤外線パターンプロジェクタ１１６とを含み得る。
Exemplary Computing Device FIG. 1 shows an exemplary form factor of
前面カメラ１０４は、本体１０２の、典型的には動作中にユーザに面する側（たとえば、ディスプレイ１０６と同じ側）に位置付けられ得る。背面カメラ１１２は、本体１０２の、前面カメラ１０４とは反対側に位置付けられ得る。カメラを前面および背面カメラと呼ぶことは任意であり、コンピューティングシステム１００は、本体１０２のさまざまな側に位置付けられた複数のカメラを含み得る。前面カメラ１０４および背面カメラ１１２は各々、可視光スペクトルで画像を取り込むように構成され得る。
ディスプレイ１０６は、陰極線管（cathode ray tube：ＣＲＴ）ディスプレイ、発光ダイオード（light emitting diode：ＬＥＤ）ディスプレイ、液晶（liquid crystal：ＬＣＤ）ディスプレイ、プラズマディスプレイ、有機発光ダイオード（organic light emitting diode：ＯＬＥＤ）ディスプレイ、または当該技術分野で公知の任意の他のタイプのディスプレイを表わし得る。いくつかの実施形態では、ディスプレイ１０６は、前面カメラ１０４、背面カメラ１１２、および／または赤外線カメラ１１４によって取り込み中の現在の画像、ならびに／もしくは、これらのカメラのうちの一つ以上によって取り込まれ得るかまたは最近取り込まれた画像のデジタル表現を表示し得る。このため、ディスプレイ１０６は、カメラのためのビューファインダとして機能し得る。ディスプレイ１０６はまた、コンピューティングシステム１００の任意の局面の設定および／または構成を調節することが可能であり得るタッチスクリーン機能もサポートし得る。
The
前面カメラ１０４は、画像センサと、レンズなどの関連光学素子とを含み得る。前面カメラ１０４は、ズーム能力を提供してもよく、または固定された焦点距離を有し得る。他の実施形態では、交換可能なレンズが前面カメラ１０４とともに使用され得る。前面カメラ１０４は、可変の機械的アパーチャと、機械的および／または電子シャッタとを有し得る。前面カメラ１０４はまた、静止画像、ビデオ画像、またはそれら双方を取り込むように構成され得る。さらに、前面カメラ１０４は、単眼カメラ、立体カメラ、または多眼カメラを表わし得る。背面カメラ１１２および／または赤外線カメラ１１４は、同様にまたは異なるように構成され得る。加えて、前面カメラ１０４、背面カメラ１１２、または赤外線カメラ１１４のうちの一つ以上は、一つ以上のカメラのアレイであり得る。
前面カメラ１０４および背面カメラ１１２のいずれかまたは双方は、対象被写体を照明するように可視光スペクトルで光照射野を提供する照明コンポーネントを含むかまたはそれに関連付けられ得る。たとえば、照明コンポーネントは、対象被写体のフラッシュまたは連続照明を提供し得る。照明コンポーネントはまた、構造化光、偏光、および特定のスペクトル成分を有する光のうちの一つ以上を含む光照射野を提供するように構成され得る。被写体から３次元（３Ｄ）モデルを回復するために使用される、公知である他のタイプの光照射野は、本明細書の実施形態の文脈内で可能である。
Either or both of
赤外線パターンプロジェクタ１１６は、赤外線構造化光パターンを対象被写体上に投影するように構成され得る。一例では、赤外線プロジェクタ１１６は、ドットパターンおよび／またはフラッドパターンを投影するように構成され得る。このため、赤外線プロジェクタ１１６は、対象被写体の異なる物理的特徴に対応する複数の深度値を決定するために、赤外線カメラ１１４と組み合わされて使用され得る。
すなわち、赤外線プロジェクタ１１６は、既知のおよび／または予め定められたドットパターンを対象被写体上に投影し、赤外線カメラ１１４は、投影されたドットパターンを含む対象被写体の赤外線画像を取り込み得る。コンピューティングシステム１００は次に、取り込まれた赤外線画像における領域と、投影されたドットパターンの特定の部分との対応を決定し得る。赤外線プロジェクタ１１６の位置と、赤外線カメラ１１４の位置と、取り込まれた赤外線画像内の投影されたドットパターンの特定の部分に対応する領域の位置とを与えられて、コンピューティングシステム１００は次に三角測量を使用して対象被写体の表面までの深度を推定し得る。投影されたドットパターンの異なる部分に対応する異なる領域についてこれを繰り返すことにより、コンピューティングシステム１００は、対象被写体のさまざまな物理的特徴または部分の深度を推定し得る。このようにして、コンピューティングシステム１００は、対象被写体の３次元（３Ｄ）モデルを生成するために使用され得る。
That is,
コンピューティングシステム１００はまた、カメラ１０４、１１２、および／または１１４が取り込むことができる（たとえば可視光および／または赤外光に関する）シーンの周囲輝度を連続的にまたは時々測定し得る周囲光センサを含み得る。いくつかの実現化例では、周囲光センサは、ディスプレイ１０６の表示輝度を調節するために使用され得る。加えて、周囲光センサは、カメラ１０４、１１２、または１１４のうちの一つ以上の露出長を測定するために、またはその測定に役立つために使用され得る。
コンピューティングシステム１００は、ディスプレイ１０６と、前面カメラ１０４、背面カメラ１１２、および／または前面赤外線カメラ１１４とを使用して、対象被写体の画像を取り込むように構成され得る。取り込まれた画像は、複数の静止画像であるか、またはビデオストリームであり得る。画像取り込みは、ボタン１０８を起動すること、ディスプレイ１０６上のソフトキーを押すこと、または何らかの他のメカニズムによってトリガされ得る。実現化例に依存して、画像取り込みは、たとえばボタン１０８を押すか、対象被写体の適切な照明条件に応じてか、デジタルカメラデバイス１００を予め定められた距離だけ動かすか、または予め定められた取り込みスケジュールに従って、特定の時間間隔で自動的に行なわれ得る。
上述のように、コンピューティングシステム１００の機能は、無線コンピューティングデバイス、携帯電話、タブレットコンピュータ、ラップトップコンピュータなどといったコンピューティングデバイスに統合され得る。例示の目的のために、図２は、カメラコンポーネント２２４を含み得る例示的なコンピューティングデバイス２００の構成要素のうちのいくつかを示す簡略ブロック図である。
As described above, the functionality of
何ら限定されることなく、例として、コンピューティングデバイス２００は、少なくとも何らかの画像取り込みおよび／または画像処理能力を備えた、セルラー携帯電話（たとえば、スマートフォン）、スチルカメラ、ビデオカメラ、コンピュータ（デスクトップコンピュータ、ノートブックコンピュータ、タブレットコンピュータ、またはハンドヘルドコンピュータなど）、携帯情報端末（personal digital assistant：ＰＤＡ）、ホームオートメーションコンポーネント、デジタルビデオレコーダ（digital video recorder：ＤＶＲ）、デジタルテレビ、リモートコントロール、ウェアラブルコンピューティングデバイス、ゲームコンソール、ロボットデバイス、または、何らかの他のタイプのデバイスであり得る。コンピューティングデバイス２００は、物理的画像処理システム、画像感知および処理アプリケーションがソフトウェアで動作する特定の物理的ハードウェアプラットフォーム、もしくは、画像取り込みおよび／または処理機能を実行するように構成されたハードウェアとソフトウェアとの他の組み合わせを表わし得るということが理解されるべきである。
By way of example and without limitation,
図２に示されるように、コンピューティングデバイス２００は、通信インターフェイス２０２と、ユーザインターフェイス２０４と、プロセッサ２０６と、データストレージ２０８と、カメラコンポーネント２２４とを含み得る。これらはすべて、システムバス、ネットワーク、または他の接続メカニズム２１０によって通信可能にともにリンクされ得る。
As shown in FIG. 2,
通信インターフェイス２０２は、コンピューティングデバイス２００が、アナログまたはデジタル変調を使用して、他のデバイス、アクセスネットワーク、および／または転送ネットワークと通信することを可能にし得る。このため、通信インターフェイス２０２は、基本電話サービス（plain old telephone service：ＰＯＴＳ）通信および／またはインターネットプロトコル（Internet protocol：ＩＰ）通信または他のパケット化通信といった、回線交換および／またはパケット交換通信を容易にし得る。たとえば、通信インターフェイス２０２は、無線アクセスネットワークまたはアクセスポイントとの無線通信のために構成されたチップセットおよびアンテナを含み得る。また、通信インターフェイス２０２は、イーサネット（登録商標）、ユニバーサルシリアルバス（Universal Serial Bus：ＵＳＢ）、または高精細度マルチメディアインターフェイス（High-Definition Multimedia Interface：ＨＤＭＩ（登録商標））ポートといった有線インターフェイスの形態をとるかまたはそれを含み得る。通信インターフェイス２０２はまた、Wi-Fi、BLUETOOTH（登録商標）、全地球測位システム（global positioning system：ＧＰＳ）、または広域無線インターフェイス（たとえば、WiMAXまたは３ＧＰＰ（登録商標）ロングタームエボリューション（Long-Term Evolution：ＬＴＥ））といった無線インターフェイスの形態をとるかまたはそれを含み得る。しかしながら、他の形態の物理層インターフェイスおよび他のタイプの標準または専用通信プロトコルが、通信インターフェイス２０２を介して使用され得る。さらに、通信インターフェイス２０２は、複数の物理的通信インターフェイス（たとえば、Wi-Fiインターフェイス、BLUETOOTH（登録商標）インターフェイス、および広域無線インターフェイス）を含み得る。
ユーザインターフェイス２０４は、コンピューティングデバイス２００が人間のユーザまたは人間以外のユーザと対話すること、たとえば、ユーザから入力を受信すること、およびユーザに出力を提供することを可能にするように機能し得る。このため、ユーザインターフェイス２０４は、キーパッド、キーボード、タッチ感知パネル、コンピュータマウス、トラックボール、ジョイスティック、マイクロホンといった入力コンポーネントを含み得る。ユーザインターフェイス２０４はまた、たとえばタッチ感知パネルと組み合わされ得る表示画面といった一つ以上の出力コンポーネントを含み得る。表示画面は、ＣＲＴ、ＬＣＤ、および／またはＬＥＤ技術、もしくは、現在公知であるかまたは後に開発される他の技術に基づき得る。ユーザインターフェイス２０４はまた、スピーカ、スピーカジャック、音声出力ポート、音声出力デバイス、イヤホン、および／または他の同様のデバイスを介して、可聴出力を生成するように構成され得る。ユーザインターフェイス２０４はまた、マイクロホンおよび／または他の同様のデバイスを介して、可聴発話、ノイズ、および／または信号を受信し、および／または取り込むように構成され得る。
いくつかの例では、ユーザインターフェイス２０４は、（たとえば可視スペクトルおよび赤外スペクトルの双方で）コンピューティングデバイス２００によってサポートされるスチルカメラおよび／またはビデオカメラ機能のためのビューファインダとして機能するディスプレイを含み得る。加えて、ユーザインターフェイス２０４は、カメラ機能の構成および合焦と画像の取り込みとを容易にする、一つ以上のボタン、スイッチ、ノブ、および／またはダイヤルを含み得る。これらのボタン、スイッチ、ノブ、および／またはダイヤルのうちのいくつかまたはすべてがタッチ感知パネルを介して実現されることが可能であり得る。
In some examples,
プロセッサ２０６は、一つ以上の汎用プロセッサ（たとえばマイクロプロセッサ）、および／または、一つ以上の専用プロセッサ（たとえばデジタル信号プロセッサ（digital signal processor：ＤＳＰ）、グラフィックス処理ユニット（graphics processing unit：ＧＰＵ）、浮動小数点ユニット（floating point unit：ＦＰＵ）、ネットワークプロセッサ、または特定用途向け集積回路（application-specific integrated circuit：ＡＳＩＣ））を含み得る。いくつかの事例では、専用プロセッサは、他の可能性の中でもとりわけ、画像処理、画像整列、および画像結合が可能であり得る。データストレージ２０８は、磁気、光学、フラッシュ、もしくは有機ストレージなどの一つ以上の揮発性および／または不揮発性ストレージコンポーネントを含んでいてもよく、プロセッサ２０６と全体的にまたは部分的に統合されてもよい。データストレージ２０８は、リムーバブルおよび／または非リムーバブルコンポーネントを含み得る。
プロセッサ２０６は、本明細書で説明されるさまざまな機能を実行するために、データストレージ２０８に格納されたプログラム命令２１８（たとえば、コンパイルされているかもしくはコンパイルされていないプログラム論理および／または機械コード）を実行することが可能であり得る。したがって、データストレージ２０８は、コンピューティングデバイス２００によって実行されると、コンピューティングデバイス２００に、本明細書および／または添付図面に開示される方法、プロセス、あるいは動作のいずれかを実行させるプログラム命令を格納した非一時的コンピュータ読取可能媒体を含み得る。プロセッサ２０６によるプログラム命令２１８の実行は、プロセッサ２０６がデータ２１２を使用することをもたらし得る。
例として、プログラム命令２１８は、コンピューティングデバイス２００にインストールされた、オペレーティングシステム２２２（たとえば、オペレーティングシステムカーネル、デバイスドライバ、および／または他のモジュール）と、一つ以上のアプリケーションプログラム２２０（たとえば、カメラ機能、アドレス帳、電子メール、ウェブ閲覧、ソーシャルネットワーキング、音声からテキストへの変換機能、テキスト翻訳機能、および／またはゲームアプリケーション）とを含み得る。同様に、データ２１２は、オペレーティングシステムデータ２１６と、アプリケーションデータ２１４とを含み得る。オペレーティングシステムデータ２１６は主にオペレーティングシステム２２２にアクセス可能であり、アプリケーションデータ２１４は主にアプリケーションプログラム２２０のうちの一つ以上にアクセス可能であり得る。アプリケーションデータ２１４は、コンピューティングデバイス２００のユーザに見えるかまたはユーザから隠されているファイルシステムに配置され得る。
By way of example,
アプリケーションプログラム２２０は、一つ以上のアプリケーションプログラミングインターフェイス（application programming interface：ＡＰＩ）を通してオペレーティングシステム２２２と通信し得る。これらのＡＰＩは、たとえば、アプリケーションプログラム２２０がアプリケーションデータ２１４を読み出すことおよび／または書き込むこと、通信インターフェイス２０２を介して情報を送信または受信すること、ユーザインターフェイス２０４上で情報を受信および／または表示することなどを容易にし得る。
Application programs 220 may communicate with
いくつかの用語では、アプリケーションプログラム２２０は、略して「アプリ」と呼ばれ得る。加えて、アプリケーションプログラム２２０は、一つ以上のオンラインアプリケーションストアまたはアプリケーションマーケットを通してコンピューティングデバイス２００にダウンロード可能であり得る。しかしながら、アプリケーションプログラムはまた、コンピューティングデバイス２００上のウェブブラウザを介してまたは物理的インターフェイス（たとえば、ＵＳＢポート）を通してなどといった他のやり方で、コンピューティングデバイス２００上にインストールされ得る。
In some terminology, application programs 220 may be referred to as "apps" for short. Additionally, application programs 220 may be downloadable to
カメラコンポーネント２２４は、アパーチャ、シャッタ、記録面（たとえば、写真フィルムおよび／または画像センサ）、レンズ、シャッタボタン、赤外線プロジェクタ、および／または可視光プロジェクタを含み得るが、それらに限定されない。カメラコンポーネント２２４は、可視光スペクトル（たとえば、３８０～７００ナノメートルの波長を有する電磁放射）で画像を取り込むように構成されたコンポーネントと、赤外光スペクトル（たとえば、７０１ナノメートル～１ミリメートルの波長を有する電磁放射）で画像を取り込むように構成されたコンポーネントとを含み得る。カメラコンポーネント２２４は、プロセッサ２０６によって実行されるソフトウェアによって少なくとも部分的に制御され得る。
例示的な深度決定システム
図３は、環境内のカメラと一つ以上の被写体との間の物理的距離の推定値を決定するために使用され得る例示的なシステムを示す。具体的には、システム３４０は、被写体下端検出器３０８と、被写体下端比率計算器３１０と、オフセット計算器３１２と、距離投影モデル３１４とを含み得る。これらは各々、本明細書で説明されるそれぞれの動作を実行するように構成されたハードウェアコンポーネントおよび／またはソフトウェアコンポーネントの組み合わせを表わし得る。システム３４０は、画像データ３００と、カメラのパラメータを示すメタデータとを、入力として受信するように構成され得る。メタデータは、画像データ３００の取り込み時の、カメラピッチ３０６といったカメラの姿勢に関する情報を含み得る。画像データ３００は、被写体３０２～３０４といった一つ以上の被写体をそこに表わし得る。被写体３０２～３０４は、他の可能性の中でもとりわけ、人間、動物、車両、ロボットデバイス、郵便ポスト、柱（たとえば灯柱、交通信号柱など）、および／またはベンチといった、環境のさまざまな移動および／または静止特徴を含み得る。
Exemplary Depth Determination System FIG. 3 illustrates an exemplary system that may be used to determine an estimate of physical distance between a camera and one or more objects in an environment. Specifically,
被写体下端検出器３０８は、画像データ３００内の被写体の下端の垂直位置を検出するように構成され得る。いくつかの実現化例では、垂直位置は画素で表わされ得る。たとえば、被写体下端検出器３０８は、被写体３０２の下端が画像データ３００の下端から２５０画素だけ上方に位置付けられていると決定し得る。被写体下端検出器３０８は、（ｉ）画像データ３００内の被写体３０２を検出し、（ｉｉ）画像データ３００内の被写体３０２の検出に基づいて画像データ３００内の被写体３０２の下端を検出し、（ｉｉｉ）被写体３０２の下端が環境の地面上に位置付けられていると決定するように構成された、一つ以上のアルゴリズムを実現し得る。被写体下端検出器３０８は、画像データ３００によって表わされる被写体３０４および／または他の被写体に関して、相応の動作を行ない得る。
Subject
被写体３０２が人間である場合、被写体下端検出器３０８の一つ以上のアルゴリズムは、人間を検出し、人間の足および／または靴（すなわち、画像データ３００内の人間の下端）を検出し、足および／または靴が地面と接していると決定するように構成され得る。同様に、被写体３０４が車両である場合、被写体下端検出器３０８の一つ以上のアルゴリズムは、車両を検出し、車両のホイールおよび／またはタイヤを検出し、ホイールおよび／またはタイヤが地面と接していると決定するように構成され得る。一つ以上のアルゴリズムは、さまざまな画像処理アルゴリズム、コンピュータビジョンアルゴリズム、および／または機械学習アルゴリズムを含み得る。
If the subject 302 is a human, one or more algorithms of the
被写体下端検出器３０８は、被写体の下端の垂直位置を被写体下端比率計算器３１０に提供するように構成され得る。被写体下端比率計算器３１０は、被写体（たとえば被写体３０２）の下端の垂直位置と画像データ３００の高さとの比率を計算するように構成され得る。具体的には、被写体下端比率計算器は、関数ｂ＝ｖ／ｈを実現し得る。式中、ｂは被写体下端比率であり、ｖは被写体の下端の垂直位置であり、ｈは画像データ３００の高さである。その目的のために、被写体下端比率計算器３１０は、画像データ３００に関連付けられたメタデータに基づいて、画像データ３００の配向を決定し得る。具体的には、被写体下端比率計算器３１０は、画像データ３００が横長配向（すなわち、画像データ３００の長辺が水平に配向された状態）で得られたか、または縦長配向（すなわち、画像データ３００の長辺が垂直に配向された状態）で得られたかを決定し得る。被写体下端比率計算器はこのため、画像データ３００の配向に基づいて、ｈの値を設定し得る。たとえば、３８４０画素×２１６０画素の解像度を有する画像データ３００については、画像データ３００の高さｈは、画像データ３００が縦長画像であるという決定に基づいて３４８０画素に設定され、または、画像データ３００が横長画像であるという決定に基づいて２１６０画素に設定され得る。
Subject
距離投影モデル３１４は、画像データ３００に表わされた被写体（たとえば被写体３０２）と画像データ３００を生成したカメラとの間の推定物理的距離３３６を決定するように構成され得る。具体的には、距離投影モデル３１４は、被写体下端比率計算器によって計算された被写体下端比率と、カメラピッチ３０６を勘案するようにオフセット計算器３１２によって計算された被写体下端比率までの推定オフセットとの合計に基づいて、推定物理的距離３３６を決定し得る。
Distance projection model 314 may be configured to determine an estimated
オフセット計算器３１２は、カメラピッチ３０６に基づいて、ゼロでないカメラピッチ角を勘案するように被写体下端比率をシフト／調節するべき量またはオフセットを決定するように構成され得る。具体的には、距離投影モデル３１４は、カメラの光軸が環境における地面と略平行に配向された間に画像データ３００が取り込まれたという仮定の下に実現され得る。カメラが正のピッチ角へ上方に傾斜されるにつれて、被写体下端比率計算器３１０によって計算される被写体下端比率は、ピッチ角がゼロであった場合の被写体下端比率と比べて減少する。同様に、カメラが負のピッチ角へ下方に傾斜されるにつれて、被写体下端比率計算器３１０によって計算される被写体下端比率は、ピッチ角がゼロであった場合の被写体下端比率と比べて増加する。このため、オフセット計算器３１２がなければ、推定物理的距離３３６は、図４Ｅおよび図４Ｆに示され、これらの図に関して説明されるように、正のピッチ角では過小推定値になるかもしれず、負のピッチ角では過大推定値になるかもしれない。
Offset
オフセット計算器３１２はこのため、距離投影モデル３１４がカメラピッチ３０６を補正することによって正確な距離推定値を生成することを可能にし得る。補正プロセスは、図５に示され、その図に関してより詳細に論じられる。被写体下端比率計算器３１０によって決定された被写体下端比率と、オフセット計算器３１２によって計算された推定オフセットとが加算され、この合計は、距離投影モデル３１４に入力として提供され得る。
Offset
距離投影モデル３１４は複数のマッピング３１６～３２６を含み、マッピング３１６～３２６のうちの一つ以上を介して推定物理的距離３３６を決定し得る。マッピング３１６～３２６の各々は、複数の被写体下端比率を、複数の対応する物理的被写体距離と関連付け得る。たとえば、マッピング３１６は、被写体下端比率３１８～３２２を、対応する物理的被写体距離３２０～３２４と関連付け得る。同様に、マッピング３２６は、被写体下端比率３２８～３３２を、対応する物理的被写体距離３３０～３３４と関連付け得る。マッピング３１６～３２６に関連付けられた被写体下端比率（たとえば、被写体下端比率３１８～３２２および２３８～３３２）は、候補被写体下端比率と呼ばれ得る。なぜなら、それらは各々、推定物理的距離３３６を決定するために使用される可能性があり得るためである。
Range projection model 314 may include multiple mappings 316-326 and determine estimated
マッピング３１６～３２６の各々は、たとえば、他の可能性の中でもとりわけ、画像データ３００の配向（すなわち、横長または縦長）、カメラが画像データ３００を取り込む際に配置される環境での高さ、（たとえば、カメラの画像センサの寸法とカメラのレンズの光学特性とによって規定されるような）画像データ３００を取り込むために使用されるカメラの視野、および／または、画像データ３００のアスペクト比を含み得る、対応する一組のカメラパラメータに関連付けられ得る。このため、画像データ３００に関連付けられたメタデータの一部として示され得る、画像データ３００に関連付けられたカメラパラメータの値に基づいて、マッピング３１６～３２６のうちの一つが選択され、推定物理的距離３３６を決定するために使用され得る。したがって、被写体下端比率３１８～３２２は、被写体下端比率３２８～３３２と似ているか、重複するか、または同一であり得るが、被写体下端比率３１８～３２２は、被写体下端比率３２８～３３２とは異なる一組の物理的被写体距離にマッピングし得る。すなわち、物理的被写体距離３２０～３２４は物理的被写体距離３３０～３３４とは異なり得るが、これら二つの組は重複してもよい。
Each of the mappings 316-326 may, for example, specify, among other possibilities, the orientation of the image data 300 (ie, landscape or portrait), the height in the environment in which the camera captures the image data 300, the height in the environment in which the camera captures the image data 300 ( For example, it may include the field of view of the camera used to capture the image data 300 and/or the aspect ratio of the image data 300 (as defined by the dimensions of the camera's image sensor and the optical properties of the camera's lens). , can be associated with a corresponding set of camera parameters. Thus, one of the mappings 316-326 is selected based on camera parameter values associated with the image data 300, which may be indicated as part of the metadata associated with the image data 300, to provide an estimated physical It can be used to determine
深度決定のための例示的なモデル
図４Ａは、カメラの例示的な幾何学的モデルを示す。幾何学的モデルは、距離投影モデル３１４のマッピング３１６～３２６を生成するための基盤として使用され得る。具体的には、図４Ａは、地面４０６を含む環境に配置された画像センサ４００とアパーチャ４０２とを示す。画像センサ４００とアパーチャ４０２とは、図４Ａで地面４０６と略平行に延びる光軸４０４を規定する。画像センサ４００（すなわち、その垂直中心）は、地面４０６から高さＨだけ上方に配置され、アパーチャ４０２は、画像センサ４００に対して（焦点）距離ｆに位置付けられる。
Exemplary Model for Depth Determination FIG. 4A shows an exemplary geometric model of a camera. A geometric model can be used as a basis for generating mappings 316 - 326 of the distance projection model 314 . Specifically, FIG. 4A shows
複数の線が、環境における地面４０６上のそれぞれの点から、アパーチャ４０２を通って、画像センサ４００上の対応する点へ投影される。具体的には、複数の線は、１メートルの線と、５メートルの線と、１０メートルの線と、２０メートルの線と、３０メートルの線と、無限遠基準線とを含む。５メートルの線（すなわち、Ｄ＝５メートル）はたとえば、画像センサ４００の中心に対する垂直位置ｄに対応してそこで画像を作成し、光軸４０４とともに角度θを形成する。１メートルの線は、観察可能および／または測定可能であり得る、アパーチャ４０２と被写体との間の最小距離に対応する。なぜなら、この線は、画像センサ４００の一番上の部分に投影するためである。
Multiple lines are projected from respective points on the
無限遠基準線は、環境における観察可能な最大距離、地平線までの距離、しきい値距離値を上回る距離、および／または無限遠距離に対応し得る。たとえば、無限遠基準線が地面４０６の上方で生じ、したがって、地面４０６に沿った測定可能距離に関連付けられていない場合、無限遠基準線は無限遠距離に関連付けられ得る。図４Ａでは、無限遠基準線は、光軸４０４とほぼ一致するように示されている。無限遠基準線と光軸４０４とを視覚的に区別するために、無限遠基準線の右側部分は光軸４０４を若干下回るように描かれ、無限遠基準線の左側は光軸４０４を若干上回るように描かれている。このため、図４Ａに示される構成では、無限遠基準線は、画像センサ４００のほぼ中心に対応し、そこで画像を作成する。（たとえば、カメラが空中輸送手段上に搭載される場合に）画像センサ４００の高さＨが図４Ａに示される高さから増加されるにつれて、無限遠基準線によって作成される画像は、画像センサ４００に沿って上方に移動し得る。同様に、（たとえば、カメラが床清掃ロボット上に搭載される場合に）画像センサ４００の高さＨが図４Ａに示される高さから減少されるにつれて、無限遠基準線によって作成される画像は、画像センサ４００に沿って下方に移動し得る。このため、高さＨが変化するにつれて、無限遠基準線が光軸４０４から外れる場合がある。１メートルの線、５メートルの線、１０メートルの線、２０メートルの線、および／または３０メートルの線に対応する画像の画像センサ４００上のそれぞれの位置は、画像センサ４００の高さＨの変化に同様に応じ得る。
The infinity baseline may correspond to the maximum observable distance in the environment, the distance to the horizon, the distance above a threshold distance value, and/or the infinity distance. For example, if the infinity reference line occurs above the
図４Ａの例示的な幾何学的モデルは、深度決定のために使用される画像データを生成するために使用され得るカメラのいくつかの構成要素、たとえばレンズを省略している。このため、この例示的な幾何学的モデルは、一部のカメラの正確な表現ではないかもしれず、また、被写体距離を計算するために幾何学的モデルを明示的に使用することは、誤った距離推定値をもたらすかもしれない。にもかかわらず、幾何学的モデルは、画像センサ４００上の垂直位置と、環境内の地面４０６に沿った対応する物理的距離との間には、非線形の関係（たとえば、tan（θ）＝ｄ／ｆ＝Ｈ／Ｄ、またはｄ＝Ｈｆ／Ｄ）があるということを示す。このため、図４Ａの幾何学的モデルのいかなる間違いも補正し、画像センサ４００上の位置を地面４０６に沿った対応する物理的距離に正確にマッピングするように、非線形数値モデル（たとえば、距離投影モデル３１４）が、訓練データに基づいて経験的に決定され得る。 The exemplary geometric model of FIG. 4A omits some components of the camera, such as lenses, that may be used to generate the image data used for depth determination. As such, this exemplary geometric model may not be an accurate representation of some cameras, and explicit use of the geometric model to calculate subject distance is erroneous. may yield a distance estimate. Nonetheless, the geometric model suggests that there is a non-linear relationship (e.g., tan(θ)= d/f=H/D, or d=Hf/D). For this reason, a nonlinear numerical model (e.g., a distance projection) is used to correct any errors in the geometric model of FIG. Model 314) can be empirically determined based on training data.
また、図４Ａの幾何学的モデルは、カメラの高さＨ（たとえば、画像センサ４００および／またはアパーチャ４０２の高さ）、距離ｆ、角度θ、（画像センサ４００のサイズ、画像センサ４００に集光するために使用されるレンズ、および／または、レンズによって生成されるズームレベルによって規定された）カメラの視野、画像データがそこから生成される画像センサ４００の部分（たとえば、画像データのアスペクト比）、ならびに／もしくは、画像センサ４００の配向（たとえば、横長か縦長か）を含む、いくつかのカメラパラメータの変更が、地面４０６に沿った物理的距離と画像センサ４００上の位置との関係（たとえばマッピング）を変更し得るということを示す。したがって、これらのカメラパラメータは、正確な距離推定値を生成するために、非線形数値モデルによって勘案され得る。
Also, the geometric model of FIG. The field of view of the camera (defined by the lens used to illuminate and/or the zoom level produced by the lens), the portion of the
具体的には、マッピング３１６～３２６の各々は、特定の一組のカメラパラメータに対応する場合があり、また、その特定の一組のカメラパラメータを有するカメラにとっては正確であるものの、異なる一組のカメラパラメータを有する異なるカメラが使用される場合には不正確かもしれない距離推定値を生成する場合がある。このため、画像データ３００を生成するために使用されるカメラに関連付けられた一組の実際のカメラパラメータに基づいて、マッピング３１６～３２６のうちの一つが選択され得る。具体的には、一組の実際のカメラパラメータに最もよく整合するカメラパラメータに関連付けられたマッピングが選択され得る。 Specifically, each of the mappings 316-326 may correspond to a particular set of camera parameters and, although accurate for a camera with that particular set of camera parameters, may be a different set. may produce range estimates that may be inaccurate if different cameras with camera parameters are used. As such, one of mappings 316 - 326 may be selected based on a set of actual camera parameters associated with the camera used to generate image data 300 . Specifically, the mapping associated with the camera parameters that best match the set of actual camera parameters may be selected.
たとえば、マッピング３１６～３２６のうちの第１のマッピングが、第１のカメラを備える第１のモバイルデバイスに対応する第１の一組のカメラパラメータに関連付けられ、一方、マッピング３１６～３２６のうちの第２のマッピングが、第１のカメラとは異なる第２のカメラを備える第２のモバイルデバイスに対応する第２の一組のカメラパラメータに関連付けられ得る。このため、第１のマッピングは、第１のモバイルデバイスによって生成された画像データに表わされた被写体までの距離を測定するために使用され、一方、第２のマッピングは、第２のモバイルデバイスによって生成された画像データに表わされた被写体までの距離を測定するために使用され得る。複数の異なるモバイルデバイスが各々、似ているかまたはほぼ同じ一組のカメラパラメータを有するカメラを使用する場合、一つのマッピングが複数の異なるモバイルデバイスによって使用され得る。加えて、各カメラは複数の異なる高さＨに位置付けられ得るため、各カメラは、異なる高さに各々対応する複数のマッピングに関連付けられ得る。 For example, a first one of mappings 316-326 is associated with a first set of camera parameters corresponding to a first mobile device with a first camera, while one of mappings 316-326 is A second mapping may be associated with a second set of camera parameters corresponding to a second mobile device with a second camera different from the first camera. Thus, the first mapping is used to measure the distance to the subject represented in the image data generated by the first mobile device, while the second mapping is used to measure the distance to the object represented in the image data generated by the second mobile device. can be used to measure the distance to an object represented in the image data generated by . A single mapping can be used by multiple different mobile devices if each uses a camera with a similar or nearly the same set of camera parameters. Additionally, since each camera can be positioned at multiple different heights H, each camera can be associated with multiple mappings, each corresponding to a different height.
図４Ｂは、被写体下端比率と物理的距離との間の例示的なマッピングのグラフィック表現を示す。マッピングは、画像センサ４００上の垂直位置を画素で表わすのではなく、垂直位置を対応する被写体下端比率で表わし得る。これは、マッピングが画像データ解像度に関して不変のままであることを可能にする。このため、そのようなマッピングは、画像データがダウンサンプリングまたはアップサンプリングされる際に被写体距離を決定するために使用され得る。なぜなら、（画像がトリミングされない、および／または、アスペクト比が同じままであると仮定すると）被写体に関連付けられた被写体下端比率は変化しないためである。
FIG. 4B shows a graphical representation of an exemplary mapping between subject bottom edge ratio and physical distance. Rather than representing vertical positions on
具体的には、ユーザインターフェイス（user interface：ＵＩ）４１０は、０．０、０．２５、０．３５、０．２４、０．４７、および０．５を含むそれぞれの被写体下端比率に対応する複数の水平線を示す。とりわけ、被写体下端比率０．５に関連付けられた水平線は、ＵＩ４１０のほぼ中央に位置付けられて、ＵＩ４１０をほぼ等しい上半分と下半分とに分割する。ＵＩ４１２は、１メートル、５メートル、１０メートル、２０メートル、３０メートル、および無限遠を含む対応する物理的距離でラベル付けされた、ＵＩ４１０と同じ複数の線を示す。すなわち、被写体下端比率０．０、０．２５、０．３５、０．２４、０．４７、および０．５は、物理的距離１メートル、５メートル、１０メートル、２０メートル、３０メートル、および図４Ａの無限遠基準線に関連付けられた距離（たとえば無限遠）に、それぞれ対応する。被写体下端比率は、距離投影モデル３１４のマッピング３１６～３２６のうちの一つを表わし得る関数Ｆ（ｂ）によって、対応する距離にマッピングされ得る。
Specifically, the user interface (UI) 410 corresponds to each subject bottom edge ratio including 0.0, 0.25, 0.35, 0.24, 0.47, and 0.5. Show multiple horizontal lines. Notably, the horizontal line associated with a subject-to-bottom ratio of 0.5 is positioned approximately in the center of the
ＵＩ４１０および４１２はまた、人間を表現し得る被写体４１４を含む画像データを表示する。境界ボックス４１６が被写体４１４を囲む。境界ボックス４１６は、被写体下端検出器３０８によって実現される第１のアルゴリズムの出力を表わしてもよく、また、被写体下端検出器３０８によって実現される第２のアルゴリズムのための検索領域を規定するために使用されてもよい。たとえば、境界ボックス４１６は、人間の表現を含むことが第１のアルゴリズムによって決定された関心領域を規定し得る。境界ボックス４１６は、人間の下端を識別しようとして人間の足および／または靴を識別するように構成された第２のアルゴリズムに入力として提供され得る。このため、境界ボックス４１６は、被写体下端を探す際に第２のアルゴリズムによって考慮される検索空間を減少させ得る。加えて、境界ボックス４１６が被写体ラベルまたは分類（たとえば、人間、車両、動物など）に関連付けられる場合、ラベルは、そのラベルに関連付けられた被写体の下端を探し出すための適切なアルゴリズムを選択するために使用され得る。たとえば、境界ボックス４１６が自動車の表現を含むとして分類される場合、人間の足および／または靴を探すアルゴリズムではなく、自動車のホイールおよび／またはタイヤを探すアルゴリズムが、被写体下端を検索するために選択され得る。
ＵＩ４１０および４１２はさらに、被写体４１４の下端に対応する線を示す。ＵＩ４１０では、この線は、（被写体４１４の下端が、ＵＩ４１０の下端から１／３上がった位置の少し下に位置付けられていることを示す）被写体下端比率０．３１でラベル付けされ、一方、ＵＩ４１２では、それは、距離６メートルでラベル付けされている。被写体下端比率０．３１と、場合によってはそれに対応する線とは、被写体下端比率計算器３１０および／またはオフセット計算器３１２の出力を表わし得る。被写体下端比率０．３１は、関数Ｆ（ｂ）を介して、対応する物理的距離６メートルにマッピングされ得る。
Ｆ（ｂ）は、経験的な訓練データに基づいて決定され得る。たとえば、カメラに対して複数の物理的距離が測定され、環境内で視覚的にマークされ得る。カメラは、これらの視覚的にマークされた距離を表わす訓練画像データを取り込むために使用され得る。訓練画像データを取り込む間、カメラは、環境内の予め定められた高さに配置され得る。このため、この訓練画像データに基づいて訓練された関数またはマッピングは、（ｉ）同じカメラ、もしくは、似ているかまたはほぼ同じ一組のカメラパラメータを有する別のカメラを使用して、（ｉｉ）似ているかまたはほぼ同じ予め定められた高さに位置付けられて、距離を測定するために有効であり得る。追加の関数またはマッピングは、異なる一組のカメラパラメータを有するカメラおよび／または異なる高さに位置付けられた同じカメラを使用して得られた訓練データに基づいて、同様の手順を使用して決定され得る。 F(b) can be determined based on empirical training data. For example, multiple physical distances can be measured relative to the camera and visually marked in the environment. A camera can be used to capture training image data representing these visually marked distances. While capturing training image data, the camera may be placed at a predetermined height within the environment. Thus, a function or mapping trained based on this training image data can be: (i) using the same camera, or another camera with a similar or nearly the same set of camera parameters, and (ii) Positioned at a similar or approximately the same predetermined height may be effective for measuring distance. Additional functions or mappings are determined using similar procedures based on training data obtained using cameras with different sets of camera parameters and/or the same cameras positioned at different heights. obtain.
他の例では、関数Ｆ（ｂ）は、人工知能（artificial intelligence：ＡＩ）および／または機械学習（machine learning：ＭＬ）モデルとして実現され得る。たとえば、人工ニューラルネットワーク（artificial neural network：ＡＮＮ）が、被写体下端比率と物理的距離との間のマッピングを実現するために使用され得る。いくつかの実現化例では、カメラパラメータの各組は、対応するＡＮＮに関連付けられ得る。すなわち、マッピング３１６～３２６の各々は、対応する一組のカメラパラメータを有するカメラによって取り込まれた画像データを使用して訓練された別個のＡＮＮを表わし得る。他の実現化例では、単一のＡＮＮが、マッピング３１６～３２６の各々を同時に実現し得る。その目的のために、このＡＮＮは、ＡＮＮが入力された被写体下端比率を対応する物理的距離にどのようにマッピングするかを調節し得る、カメラパラメータの少なくとも部分集合を、入力として受信するように構成され得る。このため、ＡＮＮは、各候補被写体下端比率を複数の物理的距離にマッピングするように構成されてもよく、特定の被写体下端比率についての特定の物理的距離が、カメラパラメータの値に基づいてＡＮＮによって選択されてもよい。 In other examples, function F(b) may be implemented as an artificial intelligence (AI) and/or machine learning (ML) model. For example, an artificial neural network (ANN) can be used to achieve the mapping between subject bottom ratio and physical distance. In some implementations, each set of camera parameters may be associated with a corresponding ANN. That is, each of mappings 316-326 may represent a separate ANN trained using image data captured by a camera having a corresponding set of camera parameters. In other implementations, a single ANN may implement each of the mappings 316-326 simultaneously. To that end, the ANN receives as input at least a subset of camera parameters that may adjust how the ANN maps the input subject bottom edge ratio to the corresponding physical distance. can be configured. Thus, the ANN may be configured to map each candidate subject bottom ratio to multiple physical distances, where a particular physical distance for a particular subject bottom ratio is determined by the ANN based on the value of the camera parameter. may be selected by
とりわけ、距離投影モデル３１４は、構造化光に依存することなく単眼カメラを使用して取り込まれた一つの可視光スペクトル画像に基づいて、被写体に関連付けられた距離を決定するように構成され得る。すなわち、距離は、立体画像データを使用することなく、または、環境上に予め定められたパターンを投影することなく、決定され得る。代わりに、被写体までの距離を正確に決定するために、距離投影モデル３１４および／またはオフセット計算器３１２は、カメラの他の局面の中でもとりわけ、環境に対するカメラの姿勢および／またはカメラの光学特性を規定するカメラパラメータに基づいて被写体距離を推定し得る。カメラの姿勢が変更される場合、および／または、異なるカメラが使用される場合、距離投影モデル３１４および／またはオフセット計算器３１２がたとえば適切なマッピングを使用することによってこの差を補償するように、カメラパラメータは更新され得る。しかしながら、場合によっては、システム３４０は、立体画像データおよび／または構造化光投影に依存する他の深度決定方法と組み合わされて使用され得る。
Among other things, the distance projection model 314 can be configured to determine the distance associated with the subject based on a single visible light spectrum image captured using a monocular camera without relying on structured light. That is, the distance can be determined without using stereoscopic image data or projecting a predetermined pattern onto the environment. Instead, to accurately determine the distance to the subject, the distance projection model 314 and/or the offset
例示的なモデル誤差および誤差補正
図４Ｃ、図４Ｄ、図４Ｅ、および図４Ｆは、実際のカメラパラメータが、距離投影モデル３１４によって仮定または使用されるカメラパラメータから外れる場合に生じ得る誤差を示す。具体的には、図４Ｃの上部は、画像センサ４００が高さＨから高さＨ’へ上方に移動され、その結果、光軸４０４が、線４１８によって示されるように、比例する量だけ上方に動くことを示す。この上方移動がなければ、画像センサ４００に最も近い被写体４１４の下端部分は、線４２２によって示されるように、画像センサ４００上のその中心から距離ｄだけ上方で画像を作成したであろう。しかしながら、上方移動の結果、画像は代わりに、線４２０によって示されるように、画像センサ４００の中心から距離ｄ’（ｄよりも大きい）だけ上方で作成される。
Exemplary Model Errors and Error Correction FIGS. 4C, 4D, 4E, and 4F illustrate errors that can occur when the actual camera parameters deviate from those assumed or used by the range projection model 314. FIG. Specifically, the upper portion of FIG. 4C shows that
アパーチャ４０２と被写体４１４との間の距離Ｄを計算するために使用されるマッピングが高さＨ’ではなく高さＨに対応する場合、マッピングは、被写体４１４が、距離Ｄではなく、図４Ｃの下部に示されるような距離Ｄ’に位置付けられていると誤って決定するかもしれない。具体的には、図４Ｃの下部は、線４１８が光軸４０４と一致し、線４２０が画像センサ４００上の同じ点から地面上の距離Ｄではなく距離Ｄ’へ延びるように、画像センサ４００が下方に戻るように移動されたことを示す。距離Ｄ’は距離Ｄよりも短く、その結果、距離推定値は過小推定値になる。この誤差は、ＨではなくＨ’のカメラ高さに対応するマッピングを使用することによって減少され、最小化され、または回避され得る。
If the mapping used to calculate the distance D between
同様に、図４Ｄの上部は、画像センサ４００が高さＨから高さＨ”へ下方に移動され、その結果、光軸４０４が、線４２４によって示されるように、比例する量だけ下方に動くことを示す。この下方移動がなければ、画像センサ４００に最も近い被写体４１４の下端部分は、線４２２によって示されるように、画像センサ４００上のその中心から距離ｄだけ上方で画像を作成したであろう。しかしながら、下方移動の結果、画像は代わりに、線４２６によって示されるように、画像センサ４００の中心から距離ｄ”（ｄよりも小さい）だけ上方で作成される。
Similarly, the top of FIG. 4D shows that
アパーチャ４０２と被写体４１４との間の距離Ｄを計算するために使用されるマッピングが高さＨ”ではなく高さＨに対応する場合、マッピングは、被写体４１４が、距離Ｄではなく、図４Ｄの下部に示されるような距離Ｄ”に位置付けられていると誤って決定するかもしれない。具体的には、図４Ｄの下部は、線４２４が光軸４０４と一致し、線４２６が画像センサ４００上の同じ点から地面上の距離Ｄではなく距離Ｄ”へ延びるように、画像センサ４００が上方に戻るように移動されたことを示す。距離Ｄ”は距離Ｄよりも長く、その結果、距離推定値は過大推定値になる。この誤差は、ＨではなくＨ”のカメラ高さに対応するマッピングを使用することによって減少され、最小化され、または回避され得る。
If the mapping used to calculate the distance D between the
いくつかの実現化例では、システム３４０はユーザインターフェイスを提供するように構成され、それを介して、画像センサ４００を含むカメラの高さが指定され得る。高さのこの指定に基づいて、対応するマッピングが、推定物理的距離３３６を決定する際に使用されるようにマッピング３１６～３２６から選択され得る。このため、画像センサ４００が指定された高さまたはその近くで保持されている間、図３のシステム３４０は、物理的距離の正確な推定値を生成し得る。しかしながら、画像センサ４００が指定された高さから外れた場合、物理的距離の推定値は誤っているかもしれず、誤差の大きさは、画像センサ４００を含むカメラの指定された高さと実際の高さとの差に比例し得る。
In some implementations,
他の実現化例では、カメラは、カメラの高さ、ひいては画像センサ４００の高さを測定するように構成されたデバイスを備え得る。たとえば、カメラは、発光体によって放出され、地面から反射され、光検出器によって検出された光の飛行時間に基づいた高さの測定を可能にするように構成された発光体および光検出器を含み得る。飛行時間測定時にカメラ、発光体、および／または光検出器の配向を検出することによって、測定された距離が実際には高さであることを確認するために、慣性計測装置（inertial measurement unit：ＩＭＵ）が使用され得る。具体的には、飛行時間測定値は、ＩＭＵによって検出された重力ベクトルと平行な方向に光が放出される場合の高さを示し得る。このため、高さの測定値に基づいて、対応するマッピングが、マッピング３１６～３２６から選択され得る。カメラの高さの変更が検出される場合、更新されるマッピングが、当該マッピングによって仮定される高さをカメラの実際の高さと一致させたまま保つように選択されてもよく、それにより、正確な距離測定を可能にする。
In other implementations, the camera may comprise a device configured to measure the height of the camera and thus the height of the
図４Ｅの上部は、画像センサ４００がゼロのピッチ角から正のピッチ角φへ上方に傾斜され、その結果、光軸４０４が、線４２８によって示されるように、上方に傾けられることを示す。アパーチャ４０２の高さＨ（ひいてはカメラの有効高さ）は、上方傾斜によって変更されないかもしれない。この上方傾斜がなければ、画像センサ４００に最も近い被写体４１４の下端部分は、図４Ｃおよび図４Ｄに示されるように、画像センサ４００上のその中心から距離ｄだけ上方で画像を作成したであろう。しかしながら、上方傾斜の結果、画像は代わりに、線４３０によって示されるように、画像センサ４００の中心から距離ｓ’（ｄよりも大きい）だけ上方で作成される。
The top of FIG. 4E shows that
画像センサ４００上の被写体４１４の下端の位置へのピッチ角φの効果が補正されない場合、距離投影モデル３１４は、被写体４１４が、距離Ｄではなく、図４Ｅの下部に示されるような距離Ｓ’に位置付けられていると誤って決定するかもしれない。具体的には、図４Ｅの下部は、線４２８が光軸４０４と一致し、線４３０が画像センサ４００上の同じ点から地面上の距離Ｄではなく距離Ｓ’へ延びるように、画像センサ４００が下方に戻るように傾斜されたことを示す。距離Ｓ’は距離Ｄよりも短く、その結果、距離推定値は過小推定値になる。この誤差は、被写体４１４について決定された被写体下端比率に推定オフセットを加算することによって減少され、最小化され、または回避され、それにより、被写体下端比率を、ピッチ角φがゼロである場合の被写体下端比率にシフトし得る。
If the effect of pitch angle φ on the position of the lower edge of
また、図４Ｆの上部は、画像センサ４００がゼロのピッチ角から負のピッチ角αへ下方に傾斜され、その結果、光軸４０４が、線４３２によって示されるように、下方に傾けられることを示す。アパーチャ４０２の高さＨは、下方傾斜によって変更されないかもしれない。この下方傾斜がなければ、画像センサ４００に最も近い被写体４１４の下端部分は、図４Ｃおよび図４Ｄに示されるように、画像センサ４００上のその中心から距離ｄだけ上方で画像を作成したであろう。しかしながら、下方傾斜の結果、画像は代わりに、線４３４によって示されるように、画像センサ４００の中心から距離ｓ”（ｄよりも小さい）だけ上方で作成される。
The top of FIG. 4F also shows that the
画像センサ４００上の被写体４１４の下端の位置へのピッチ角αの効果が補正されない場合、距離投影モデル３１４は、被写体４１４が、距離Ｄではなく、図４Ｆの下部に示されるような距離Ｓ”に位置付けられていると誤って決定するかもしれない。具体的には、図４Ｆの下部は、線４３２が光軸４０４と一致し、線４３４が画像センサ４００上の同じ点から地面上の距離Ｄではなく距離Ｓ”へ延びるように、画像センサ４００が上方に戻るように傾斜されたことを示す。距離Ｓ”は距離Ｄよりも長く、その結果、距離推定値は過大推定値になる。この誤差は、被写体４１４について決定された被写体下端比率に推定オフセットを加算することによって減少され、最小化され、または回避され、それにより、被写体下端比率を、ピッチ角αがゼロである場合の被写体下端比率にシフトし得る。
If the effect of the pitch angle α on the position of the lower edge of the
例示的なピッチ角補償
図５は、カメラのゼロでないピッチ角を補償するための例示的なアプローチを示す。具体的には、図５は、光軸５０４Ａが環境における地面と平行に延びる（たとえば、環境の重力ベクトルと垂直に延びる）ようにゼロのピッチ角を有する配向５００Ａおよび５０２Ａにそれぞれ位置付けられた、カメラの画像センサ５００およびアパーチャ５０２を示す。無限遠基準線５２０が画像センサ５００を通って投影されて示されており、画像センサ５００が配向５００Ａから配向５００Ｂへ下方に傾斜され、および／または、配向５００Ａから配向５００Ｃへ上方に傾斜される際の、画像センサ５００に対する無限遠基準線５２０の位置の見かけの変化を示している。加えて、図５では、画像センサ５００が配向５００Ａにある場合、無限遠基準線５２０は光軸５０４Ａと一致し、ひいては、被写体下端比率０．５に対応するように示される。しかしながら、画像センサ５００が配向５００Ａにとどまった状態で画像センサ５００の高さが変更されると、無限遠基準線５２０は光軸５０４Ａから外れ、異なる被写体下端比率（たとえば、高さに依存して０．０～１．０）に対応し得る。
Exemplary Pitch Angle Compensation FIG. 5 shows an exemplary approach for compensating for non-zero pitch angles of a camera. Specifically, FIG. 5 is positioned at
画像センサ５００およびアパーチャ５０２が配向５００Ａおよび５０２Ａから配向５００Ｃおよび５０２Ｃへそれぞれ上方に傾斜されると、無限遠基準線５２０は画像センサ５００に対して上方に動く。この上方傾斜中、アパーチャ５０２は基準トレース５０６内で回転し、画像センサ５００は、カメラの焦点距離ｆに等しい半径を有する焦点トレース５０８に沿って動く。このため、上方傾斜は、地面に対するカメラの高さＨを一定に保ちつつ、ピッチ角の正の変化を表わす。正のピッチ角は仰角と考えられ、一方、負のピッチ角は俯角と考えられ得る。
When
画像センサ５００が配向５００Ｃにある状態では、無限遠基準線５２０は、画像センサ５００の一番上の部分と一致する。このため、配向５００Ｃでは、無限遠基準線５２０は、対応する画像データ５１２内で画面比率ΔＬmax elevationだけ下方に移動される。無限遠基準線５２０が画像データ５１２内で上方ではなく下方に移動されるのは、画像センサ５００上に形成される画像が上下逆さまになっており（すなわち、反転されており）、このため、画像データが表示される際に被写体を正しい上下関係で見せるために、画像センサ５００の出力が反転されるためである。画像センサ５００が配向５００Ａにある際に、無限遠基準線５２０が画像センサ５００の中央と一致する場合、ΔＬmax elevationは０．５に等しくなり得る。しかしながら、ΔＬmax elevationは、画像センサ５００が環境において配置される高さに依存して、他の値を有していてもよい。
With
同様に、画像センサ５００およびアパーチャ５０２が配向５００Ａおよび５０２Ａから配向５００Ｂおよび５０２Ｂへそれぞれ下方に傾斜されると、無限遠基準線５２０は画像センサ５００に対して下方に動く。この下方傾斜中、アパーチャ５０２は基準トレース５０６内で回転し、画像センサ５００は焦点トレース５０８に沿って動く。このため、下方傾斜は、地面に対するカメラの高さＨを一定に保ちつつ、ピッチ角の負の変化を表わす。画像センサ５００が配向５００Ｂにある状態では、無限遠基準線５２０は、画像センサ５００の一番下の部分と一致する。このため、配向５００Ｂでは、無限遠基準線５２０は、対応する画像データ５１０内で画面比率ΔＬmax depressionだけ上方に移動される。無限遠基準線５２０が画像データ５１０内で下方ではなく上方に移動されるのは、画像センサ５００上に形成される画像の反転に起因する。
Similarly, when
画像センサ５００が配向５００Ａにある際に、無限遠基準線５２０が画像センサ５００の中央と一致する場合、ΔＬmax depressionは０．５に等しくなり得る。しかしながら、ΔＬmax depressionは、画像センサ５００の高さに依存して、他の値を有していてもよい。画像センサ５００が配置される高さにかかわらず、ΔＬmax elevationとΔＬmax depressionとの合計は１．０に等しくなり得る。
ΔL max depression can be equal to 0.5 if the
幾何学的モデル５１４は、画像センサ５００の配向５００Ｂおよび５００Ｃを示しており、カメラのピッチ角の変化を補償するために使用され得る数学的関係を決定するために使用され得る。具体的には、幾何学的モデル５１４は、配向５００Ｂが、負のピッチ角αmax depressionと、光軸５０４Ａの配向５０４Ｂへの回転と、ΔＬmax depressionによって無限遠基準線５２０に関連付けられた被写体下端比率のオフセットとに対応することを示す。このため、tan（αmax depression）＝ΔＬmax depression／ｆ、およびｆ＝ΔＬmax depression／tan（αmax depression）である。したがって、αmax depressionとφmax elevationとの間のピッチ角θ分のカメラ（すなわち、画像センサ５００およびアパーチャ５０２）の回転については、被写体下端比率のオフセットΔｂは、Δｂ＝ｆtan（θ）、言い換えればΔｂ＝（ΔＬmax depression／tan（αmax））tan（θ）によってモデル化される。この式は、画像データ３００に関連付けられたカメラピッチ３０６を補償する推定オフセットを決定するために、オフセット計算器３１２によって使用または実現され得る。推定オフセットΔｂは（たとえば画素数ではなく）被写体下端比率に換算して計算されるため、推定オフセットΔｂは、被写体下端比率計算器３１０によって計算された被写体下端比率に直接加算され得る。
とりわけ、対称的であるカメラについては、αmax depressionとφmax elevationとは同じ大きさを有し得るが、カメラピッチの異なる方向を示し得る。したがって、対応する数学的関係は、上で行なわれたようなαmax depressionではなく、φmax elevationに基づいて決定され得る。具体的には、配向５００Ｃは、正のピッチ角φmax elevationと、光軸５０４Ａの配向５０４Ｃへの回転と、ΔＬmax elevationによって無限遠基準線５２０に関連付けられた被写体下端比率のオフセットとに対応する。このため、tan（φmax elevation）＝ΔＬmax elevation／ｆ、およびｆ＝ΔＬmax elevation／tan（φmax elevation）である。したがって、αmax depressionとφmax elevationとの間のピッチ角θ分の画像センサ５００の回転については、被写体下端比率のオフセットΔｂは、Δｂ＝ｆtan（θ）、言い換えればΔｂ＝（ΔＬmax elevation／tan（φmax elevation））tan（θ）によってモデル化される。推定オフセットΔｂは、正のピッチ角については正であり（結果として、被写体下端比率は、オフセットと合計されると増加される）、負のピッチ角については負であり得る（結果として、被写体下端比率は、オフセットと合計されると減少される）。
Notably, for cameras that are symmetrical, α max depression and φ max elevation may have the same magnitude, but point to different directions of camera pitch. Therefore, the corresponding mathematical relationship can be determined based on φ max elevation rather than α max depression as done above. Specifically,
αmax depressionおよびφmax elevationの値は、カメラのための較正手順を介して経験的に決定される。較正手順中、カメラは、無限遠基準線５２０が画像センサ５００の下端または上端へ動かされるまでそれぞれ下方または上方に傾斜され、画像５１０または５１２に示されるオフセットをそれぞれもたらし得る。すなわち、較正は、画像センサ５００およびアパーチャ５０２を配向５００Ｂおよび５０２Ｂにそれぞれ配置し、αmax depressionおよびΔＬmax depressionの値を測定することによって、または、配向５００Ｃおよび５０２Ｃにそれぞれ配置し、これらの配向におけるφmax elevationおよびΔＬmax elevationの値を測定することによって、行なわれ得る。αmax depressionおよびφmax elevationの決定された値は、似ているかまたはほぼ同じレンズ、似ているかまたはほぼ同じセンササイズ（すなわち、長さおよび幅）、似ているかまたはほぼ同じ焦点距離ｆ、ならびに／もしくは、生成された画像データの似ているかまたはほぼ同じアスペクト比を含む光学部品の似ているかまたはほぼ同じ構成を有するカメラにとって有効であり得る。これらのカメラパラメータのうちの一つ以上が異なる場合、αmax depressionおよびφmax elevationの値は経験的に再決定され得る。
The values of α max depression and φ max elevation are determined empirically through a calibration procedure for the camera. During the calibration procedure, the camera may be tilted down or up, respectively, until the
例示的な使用事例
図６は、本明細書に開示される深度決定モデル、システム、デバイス、および手法についての例示的な使用事例を示す。具体的には、図６は、ほぼ胸の高さでコンピューティングデバイス６０２を着用しているユーザ６００を示す。コンピューティングデバイス６０２はコンピューティングシステム１００および／またはコンピューティングデバイス２００に対応し、カメラとシステム３４０の実現化例とを含み得る。コンピューティングデバイス６０２は、ランヤード、コード、ストラップ、または他の結合メカニズムを介してユーザ６００の首のまわりに掛けられ得る。それに代えて、コンピューティングデバイス６０２は、ユーザ６００の身体に、異なる位置で、および／または異なる連結メカニズムを通して接続され得る。このため、ユーザ６００が環境を通って歩く間、コンピューティングデバイス６０２およびそのカメラは、環境の地面より上方のほぼ固定された高さに位置付けられ得る（それは、ユーザ６００の動きに起因するいくつかの高さ変動を可能にする）。したがって、距離投影モデル３１４は、マッピング３１６～３２６から、このほぼ固定された高さに対応するマッピングを選択してもよく、このため、環境内で検出された被写体までの距離を決定するために使用され得る。
Exemplary Use Cases FIG. 6 illustrates exemplary use cases for the depth determination models, systems, devices, and techniques disclosed herein. Specifically, FIG. 6 shows a
具体的には、コンピューティングデバイス６０２上のカメラは、視野６０４によって示されるような、被写体６０６を含む環境を表わす画像データを取り込み得る。画像データ３００に対応し得るこの画像データに基づいて、図３のシステム３４０は、被写体６０６と、コンピューティングデバイス６０２、そのカメラ、および／またはユーザ６００との間の推定物理的距離３３６を決定するために使用され得る。推定物理的距離３３６に基づいて、コンピューティングデバイス６０２は、物理的距離３３６の表現を生成するように構成され得る。この表現は、他の可能性の中でもとりわけ、視覚表現、可聴表現、および／または触覚表現であり得る。このため、本明細書で説明される深度決定手法は、たとえば、視覚障害のある人々に、環境におけるさまざまな被写体までの距離を通知することによって、そのようなユーザが環境を横切ることを支援するために使用され得る。
Specifically, a camera on
たとえば、コンピューティングデバイス６０２は、そのディスプレイ上に、推定物理的距離３３６の指標を、被写体６０６の表示された表現の近傍に表示し、それにより、被写体６０６がコンピューティングデバイス６０２から水平に推定物理的距離３３６だけ隔てられていることを示し得る。別の例では、コンピューティングデバイス６０２は、一つ以上のスピーカを介して、推定物理的距離３３６を表現する発話を生成し得る。いくつかの事例では、この発話はまた、被写体（たとえば、人間、車両、動物、静止物体など）の分類、および／または、コンピューティングデバイス６０２の画面の垂直中心線に対する被写体６０６の水平方向を示し得る。このため、発話はたとえば「１時方向の２メートル先に箱があります」であってもよく、ここで、１時方向は、垂直中心線に対して３０度の水平方向を示すためにクロックポジションを使用する。さらに別の例では、推定物理的距離３３６の触覚表現がコンピューティングデバイス６０２の振動を介して生成されてもよく、ここで、振動のパターンは、ユーザ６００に対する被写体６０６の距離および配向に関する情報を符号化する。
For example,
さらに、いくつかの実現化例では、コンピューティングデバイス６０２は、ユーザ６００が、視野６０４の一部（すなわち、コンピューティングデバイス６０２のディスプレイの一部）をアクティブとして指定し、視野６０４の別の一部を非アクティブとして指定することを可能にし得る。この指定に基づいて、コンピューティングデバイス６０２は、視野６０４のアクティブな部分内に少なくとも部分的に含まれている被写体については距離推定値を生成し、アクティブな部分内に少なくとも部分的に含まれていない被写体（すなわち、視野６０４の非アクティブな部分内に全体が入っている被写体）についてはそのような距離推定値を生成することを省略するように構成され得る。たとえば、視覚障害のある人は、コンピューティングデバイス６０２を使用して、予想される歩行経路沿いにユーザの前に位置する被写体までの距離を測定したいかもしれないが、歩行経路の隣りに位置する被写体までの距離には興味がないかもしれない。このため、そのようなユーザは、ディスプレイ高さに等しい高さとディスプレイ幅よりも小さい幅とを有するコンピューティングデバイス６０２のディスプレイの長方形部分をアクティブとして指定し、このため、コンピューティングデバイス６０２に、ディスプレイの縁近くの画像データに表わされた被写体を無視させるかもしれない。
Further, in some implementations,
加えて、いくつかの実現化例では、コンピューティングデバイス６０２は、ユーザ６００が、距離を測定するべき被写体のクラスまたはタイプを指定することを可能にし得る。この指定に基づいて、コンピューティングデバイス６０２は、指定されたクラスまたはタイプのうちの一つに分類された被写体については距離推定値を生成し、指定されたクラスまたはタイプに該当しない被写体についてはそのような距離推定値を生成することを省略するように構成され得る。たとえば、視覚障害のある人は、コンピューティングデバイス６０２を使用して、他の人間、車両、および動物といった動く被写体までの距離を測定したいかもしれないが、ベンチ、灯柱、および／または郵便ポストといった動かない被写体までの距離には興味がないかもしれない。
Additionally, in some implementations,
追加の例示的な動作
図７は、被写体とカメラとの間の距離の推定値を決定することに関する動作のフローチャートを示す。これらの動作は、コンピューティングシステム１００、コンピューティングデバイス２００、システム３４０、および／またはコンピューティングデバイス６０２、および／またはさまざまな他のタイプのデバイスまたはデバイスサブシステムのうちの一つ以上によって実行され得る。図７の実施形態は、そこに示される特徴のうちの任意の一つ以上の除去によって簡略化されてもよい。さらに、これらの実施形態は、前述の図のいずれかの、もしくは本明細書で他の態様で説明される、特徴、局面、および／または実現化例と組み合わされてもよい。
Additional Exemplary Operations FIG. 7 shows a flowchart of operations involved in determining an estimate of the distance between the subject and the camera. These operations may be performed by one or more of
ブロック７００は、カメラから、環境における被写体を表わす画像データを受信することを伴い得る。
ブロック７０２は、画像データに基づいて、画像データ内の被写体の下端の垂直位置を決定することを伴い得る。
ブロック７０４は、垂直位置と画像データの高さとの間の被写体下端比率を決定することを伴い得る。
ブロック７０６は、距離投影モデルを介しておよび被写体下端比率に基づいて、カメラと被写体との間の物理的距離の推定値を決定することを伴い得る。距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）それぞれの候補被写体下端比率と（ｉｉ）環境における対応する物理的距離との間のマッピングを規定し得る。
ブロック７０８は、カメラと被写体との間の物理的距離の推定値の指標を生成することを伴い得る。
いくつかの実施形態では、マッピングは、カメラが環境内の予め定められた高さに配置されているという仮定に基づき得る。 In some embodiments, mapping may be based on the assumption that the camera is positioned at a predetermined height within the environment.
いくつかの実施形態では、画像データを取り込んでいる間にカメラの物理的高さが予め定められた高さを上回る場合、カメラと被写体との間の物理的距離の前記推定値は過小推定値であり得る。画像データを取り込んでいる間にカメラの物理的高さが予め定められた高さを下回る場合、カメラと被写体との間の物理的距離の推定値は過大推定値であり得る。 In some embodiments, said estimate of the physical distance between the camera and the subject is an underestimate if the physical height of the camera exceeds a predetermined height while capturing image data. can be The estimate of the physical distance between the camera and the subject may be an overestimate if the physical height of the camera is below the predetermined height while capturing image data.
いくつかの実施形態では、カメラに関連付けられたユーザインターフェイスを介して、予め定められた高さの指定が受信され得る。予め定められた高さの指定に基づいて、距離投影モデルは、カメラが予め定められた高さの指定に従って位置付けられていると仮定するようにマッピングを修正することによって構成され得る。 In some embodiments, a predetermined height designation may be received via a user interface associated with the camera. Based on the predetermined height specification, a range projection model can be constructed by modifying the mapping to assume that the camera is positioned according to the predetermined height specification.
いくつかの実施形態では、距離投影モデルを構成することは、予め定められた高さの指定に基づいて、複数の候補マッピングからマッピングを選択することを含み得る。複数の候補マッピングのうちの各それぞれのマッピングは、予め定められた高さの対応する指定に関連付けられ得る。 In some embodiments, constructing the distance projection model may include selecting a mapping from a plurality of candidate mappings based on a predetermined height specification. Each respective mapping of the plurality of candidate mappings may be associated with a corresponding designation of a predetermined height.
いくつかの実施形態では、距離投影モデルは機械学習モデルを含み得る。距離投影モデルを構成することは、予め定められた高さの指定に基づいて機械学習モデルの少なくとも一つの入力パラメータを調節することを含み得る。 In some embodiments, the distance projection model may include a machine learning model. Constructing the distance projection model may include adjusting at least one input parameter of the machine learning model based on the predetermined height specification.
いくつかの実施形態では、マッピングはカメラの幾何学的モデルに基づき得る。幾何学的モデルは、（ｉ）焦点距離を有し、環境内の予め定められた高さに配置されたカメラと、（ｉｉ）環境の地面とほぼ平行に配向されたカメラの光軸と、（ｉｉｉ）カメラの画像センサ上のそれぞれの点から環境の地面上の対応する点へ投影された複数の線のうちの各それぞれの線とを含み得る。各それぞれの候補被写体下端比率は、幾何学的モデルに基づいて、環境における対応する物理的距離に関連付けられ得る。 In some embodiments, mapping may be based on a geometric model of the camera. The geometric model includes (i) a camera having a focal length and positioned at a predetermined height within the environment, and (ii) an optical axis of the camera oriented approximately parallel to the ground of the environment; (iii) each respective line of a plurality of lines projected from respective points on the image sensor of the camera to corresponding points on the ground of the environment; Each respective candidate subject bottom edge ratio may be associated with a corresponding physical distance in the environment based on the geometric model.
いくつかの実施形態では、マッピングは、（ｉ）カメラの縦長配向に対応する第１のマッピングと、（ｉｉ）カメラの横長配向に対応する第２のマッピングとを含み得る。第１のマッピングに関連付けられた各それぞれの候補被写体下端比率は、縦長画像データ内の対応する垂直位置と縦長画像データの高さとの間にあり得る。第２のマッピングに関連付けられた各それぞれの候補被写体下端比率は、横長画像データ内の対応する垂直位置と横長画像データの高さとの間にあり得る。画像データの高さは、画像データを取り込んでいる間のカメラの配向に基づいて決定され得る。画像データを取り込んでいる間のカメラの配向に基づいて、第１のマッピングまたは第２のマッピングが、カメラと被写体との間の物理的距離の推定値を決定する際に使用されるために選択され得る。 In some embodiments, the mappings may include (i) a first mapping corresponding to a portrait orientation of the camera and (ii) a second mapping corresponding to a landscape orientation of the camera. Each respective candidate subject bottom edge ratio associated with the first mapping may be between a corresponding vertical position within the portrait image data and the height of the portrait image data. Each respective candidate subject bottom edge ratio associated with the second mapping may be between the corresponding vertical position in the landscape image data and the height of the landscape image data. The height of the image data can be determined based on the orientation of the camera while capturing the image data. Based on the orientation of the camera while capturing image data, either the first mapping or the second mapping is selected for use in determining an estimate of the physical distance between the camera and the subject. can be
いくつかの実施形態では、カメラに関連付けられた一つ以上のセンサから、カメラのピッチ角を示すセンサデータが得られ得る。カメラのピッチ角を示すセンサデータに基づいて、被写体下端比率の推定オフセットが決定され得る。被写体下端比率の推定オフセットは、ゼロのピッチ角に対するカメラのピッチ角によって引き起こされた垂直位置の変化を勘案し得る。被写体下端比率と推定オフセットとの合計が決定され得る。距離投影モデルは、合計に基づいて、カメラと被写体との間の物理的距離の推定値を決定するように構成され得る。 In some embodiments, sensor data indicative of the pitch angle of the camera may be obtained from one or more sensors associated with the camera. Based on the sensor data indicating the pitch angle of the camera, an estimated offset of the subject bottom edge ratio can be determined. The estimated offset of the subject bottom ratio may take into account the change in vertical position caused by the pitch angle of the camera relative to the zero pitch angle. A sum of the subject bottom edge ratio and the estimated offset may be determined. A range projection model can be configured to determine an estimate of the physical distance between the camera and the subject based on the summation.
いくつかの実施形態では、被写体下端比率の推定オフセットを決定することは、カメラの推定焦点距離とカメラのピッチ角の正接との積を求めることを含み得る。カメラの上方傾斜に関連付けられた正のピッチ角は、合計が被写体下端比率よりも高くなるように正の値を有する推定オフセットをもたらし得る。カメラの下方傾斜に関連付けられた負のピッチ角は、合計が被写体下端比率よりも低くなるように負の値を有する推定オフセットをもたらし得る。 In some embodiments, determining the estimated offset of the subject bottom edge ratio may include multiplying the estimated focal length of the camera by the tangent of the pitch angle of the camera. A positive pitch angle associated with camera uptilt may result in an estimated offset with a positive value such that the sum is higher than the subject bottom ratio. A negative pitch angle associated with a camera downtilt may result in an estimated offset that has a negative value such that the sum is less than the subject bottom ratio.
いくつかの実施形態では、推定焦点距離は、（ｉ）無限遠基準線をカメラの画像センサ上の初期位置から画像センサの上端へ第１の画面比率だけオフセットする最大ピッチ角の決定、または（ｉｉ）無限遠基準線を画像センサ上の初期位置から画像センサの下端へ第２の画面比率だけオフセットする最小ピッチ角の決定、のうちの少なくとも一つに基づき得る。第１の画面比率と第２の画面比率との合計は１に等しい。 In some embodiments, the estimated focal length is determined by (i) determining the maximum pitch angle that offsets the infinity reference line from the camera's initial position on the image sensor to the top of the image sensor by a first screen ratio, or ( ii) determining a minimum pitch angle that offsets the infinity reference line from an initial position on the image sensor to the bottom of the image sensor by a second screen ratio. The sum of the first screen ratio and the second screen ratio is equal to one.
いくつかの実施形態では、複数の候補被写体下端比率は、（ｉ）最小の測定可能な物理的距離に対応する、ゼロである第１の候補被写体下端比率から、（ｉｉ）最大の測定可能な物理的距離に対応する第２の候補被写体下端比率に及び得る。 In some embodiments, the plurality of candidate subject bottom edge ratios are (i) from a first candidate subject bottom edge ratio of zero, corresponding to the smallest measurable physical distance, to (ii) the largest measurable A second candidate subject bottom edge ratio corresponding to the physical distance may span.
いくつかの実施形態では、画像データ内の被写体の下端の垂直位置を決定することは、一つ以上の被写体検出アルゴリズムを介して、画像データ内の被写体の位置に対応する画像データ内の関心領域を決定することを含み得る。一つ以上の被写体下端検出アルゴリズムを介しておよび関心領域に基づいて、被写体の下端が識別され得る。被写体の下端を識別することに基づいて、被写体の下端が環境の地面と接していると決定され得る。被写体の下端が環境の地面と接していると決定することに基づいて、画像データ内の被写体の下端の垂直位置が決定され得る。 In some embodiments, determining the vertical position of the bottom edge of an object within the image data is performed by determining, via one or more object detection algorithms, a region of interest within the image data corresponding to the position of the object within the image data. may include determining the Through one or more subject edge detection algorithms and based on the region of interest, the subject's bottom edge may be identified. Based on identifying the bottom edge of the subject, it can be determined that the bottom edge of the subject is in contact with the ground of the environment. Based on determining that the bottom edge of the object is in contact with the ground of the environment, the vertical position of the bottom edge of the object in the image data can be determined.
いくつかの実施形態では、カメラから、環境における追加の被写体を表わす追加の画像データが受信され得る。追加の画像データに基づいて、追加の被写体の下端が画像データ内で見えないと決定され得る。追加の被写体の下端が画像データ内で見えないと決定することに基づいて、カメラと追加の被写体との間の物理的距離の追加の推定値が、最小の測定可能な物理的距離を下回る予め定められた値であると決定され得る。カメラと追加の被写体との間の物理的距離の追加の推定値の追加の指標が生成され得る。 In some embodiments, additional image data representing additional objects in the environment may be received from the camera. Based on the additional image data, it can be determined that the bottom edge of the additional subject is not visible in the image data. An additional estimate of the physical distance between the camera and the additional subject based on determining that the bottom edge of the additional subject is not visible in the image data is preliminarily below the minimum measurable physical distance. It can be determined to be a defined value. Additional measures of additional estimates of physical distances between the camera and additional subjects may be generated.
いくつかの実施形態では、カメラと被写体との間の物理的距離の推定値の指標を生成することは、（ｉ）物理的距離の推定値の視覚表現をディスプレイ上に表示すること、（ｉｉ）物理的距離の推定値を表現する可聴発話を生成すること、または（ｉｉｉ）物理的距離の推定値の触覚表現を生成すること、のうちの一つ以上を含み得る。 In some embodiments, generating an indication of the physical distance estimate between the camera and the subject includes (i) displaying a visual representation of the physical distance estimate on a display; (ii) a) generating audible speech representing the physical distance estimate; or (iii) generating a tactile representation of the physical distance estimate.
いくつかの実施形態では、カメラの視野のアクティブな部分の指定が受信され得る。被写体の少なくとも一部がカメラの視野のアクティブな部分内に含まれていると決定され得る。被写体の少なくとも一部がカメラの視野のアクティブな部分内に含まれていると決定することに基づいて、カメラと被写体との間の物理的距離の推定値の指標が生成され得る。カメラと対応する被写体との間の物理的距離のそれぞれの推定値の指標の生成は、カメラの視野のアクティブな部分の外側の被写体については省略され得る。 In some embodiments, a designation of the active portion of the camera's field of view may be received. It may be determined that at least a portion of the subject is included within the active portion of the field of view of the camera. An indication of the physical distance estimate between the camera and the subject may be generated based on determining that at least a portion of the subject is included within the active portion of the field of view of the camera. Generating an indication of each estimate of the physical distance between the camera and the corresponding object may be omitted for objects outside the active portion of the camera's field of view.
いくつかの実施形態では、複数の被写体クラスからの一つ以上の被写体クラスの選択が受信され得る。被写体が一つ以上の被写体クラスのうちの第１の被写体クラスに属すると決定され得る。被写体が第１の被写体クラスに属すると決定することに基づいて、カメラと被写体との間の物理的距離の推定値の指標が生成され得る。カメラと対応する被写体との間の物理的距離のそれぞれの推定値の指標の生成は、第１の被写体クラスに属さない被写体については省略され得る。 In some embodiments, a selection of one or more object classes from multiple object classes may be received. It may be determined that the object belongs to a first object class of the one or more object classes. Based on determining that the object belongs to the first object class, an indication of the physical distance estimate between the camera and the object may be generated. Generating an indication of each estimate of the physical distance between the camera and the corresponding object may be omitted for objects that do not belong to the first object class.
結び
本開示は、さまざまな局面の例示として意図される、本願に記載される特定の実施形態に関して限定されるべきではない。当業者には明らかであるように、その範囲から逸脱することなく、多くの修正および変形を行なうことができる。本明細書に記載されるものに加えて、本開示の範囲内の機能的に等価な方法および装置が、前述の説明から当業者には明らかであろう。そのような修正および変形は、特許請求の範囲内に入るよう意図される。
CONCLUSION The present disclosure should not be limited with respect to the particular embodiments described herein, which are intended as illustrations of various aspects. Many modifications and variations can be made without departing from its scope, as will be apparent to those skilled in the art. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those described herein, will be apparent to those skilled in the art from the foregoing descriptions. Such modifications and variations are intended to fall within the scope of the claims.
上記の詳細な説明は、添付図面を参照して、開示されるシステム、デバイス、および方法のさまざまな特徴および動作を説明する。図面では、文脈が別様に指示しない限り、同様の記号は典型的には同様の構成要素を識別する。本明細書および図面に記載された例示的な実施形態は、限定的であるよう意図されていない。本明細書に提示される主題の範囲から逸脱することなく、他の実施形態を利用することができ、他の変更を行なうことができる。本明細書で概して説明され図面に示される本開示の局面は、幅広く多様な異なる構成で配置され、置き換えられ、組み合わされ、分離され、および設計され得ることが容易に理解されるであろう。 The foregoing detailed description describes various features and operations of the disclosed systems, devices, and methods with reference to the accompanying drawings. In the drawings, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the specification and drawings are not intended to be limiting. Other embodiments may be utilized and other changes may be made without departing from the scope of the subject matter presented herein. It will be readily appreciated that the aspects of the disclosure generally described herein and illustrated in the drawings can be arranged, interchanged, combined, separated and designed in a wide variety of different configurations.
図面におけるメッセージフロー図、シナリオ、およびフローチャートのうちのいずれかまたはすべてに関して、ならびに本明細書で論じられるように、各ステップ、ブロック、および／または通信は、例示的な実施形態に従った情報の処理および／または情報の伝送を表わし得る。代替的な実施形態は、これらの例示的な実施形態の範囲内に含まれる。これらの代替的な実施形態では、たとえば、ステップ、ブロック、伝送、通信、要求、応答、および／またはメッセージとして記載される動作は、関与する機能性に依存して、ほぼ同時に実行されるかまたは逆の順序で実行されることを含む、示されるかまたは論じられる順序とは異なる順序で実行され得る。また、より多いかまたはより少ないブロックおよび／または動作を、本明細書で論じられるメッセージフロー図、シナリオ、およびフローチャートのいずれかとともに使用することができ、これらのメッセージフロー図、シナリオ、およびフローチャートは、部分的または全体的に、互いに組み合わされ得る。 With respect to any or all of the message flow diagrams, scenarios, and flowcharts in the drawings, and as discussed herein, each step, block, and/or communication represents information in accordance with the illustrative embodiments. It can represent processing and/or transmission of information. Alternate embodiments are included within the scope of these exemplary embodiments. In these alternative embodiments, operations described as steps, blocks, transmissions, communications, requests, responses, and/or messages, for example, may be performed substantially concurrently or may occur depending on the functionality involved. It may be performed out of order from that shown or discussed, including in reverse order. Also, more or fewer blocks and/or actions may be used with any of the message flow diagrams, scenarios, and flowcharts discussed herein, wherein these message flow diagrams, scenarios, and flowcharts are , may be partially or wholly combined with each other.
情報の処理を表わすステップまたはブロックは、本明細書で説明される方法または手法の特定の論理機能を実行するように構成され得る回路に対応してもよい。それに代えて、またはそれに加えて、情報の処理を表わすブロックは、モジュール、セグメント、または（関連データを含む）プログラムコードの一部分に対応してもよい。プログラムコードは、本方法または手法において特定の論理演算またはアクションを実現するためにプロセッサによって実行可能な一つ以上の命令を含み得る。プログラムコードおよび／または関連データは、ランダムアクセスメモリ（random access memory：ＲＡＭ）、ディスクドライブ、ソリッドステートドライブ、または別の記憶媒体を含む記憶装置といった、任意のタイプのコンピュータ読取可能媒体に格納され得る。 Steps or blocks representing processing of information may correspond to circuitry that may be configured to perform certain logical functions of the methods or techniques described herein. Alternatively or additionally, blocks representing processing of information may correspond to modules, segments, or portions of program code (including associated data). Program code may comprise one or more instructions executable by a processor to perform certain logical operations or actions in the present methods or techniques. The program code and/or associated data may be stored in any type of computer readable medium, such as storage devices including random access memory (RAM), disk drives, solid state drives, or another storage medium. .
コンピュータ読取可能媒体はまた、レジスタメモリ、プロセッサキャッシュ、およびＲＡＭのような、短期間にわたってデータを格納するコンピュータ読取可能媒体といった非一時的コンピュータ読取可能媒体を含み得る。コンピュータ読取可能媒体はまた、より長期間にわたってプログラムコードおよび／またはデータを格納する非一時的コンピュータ読取可能媒体を含み得る。このため、コンピュータ読取可能媒体は、たとえば読取専用メモリ（read only memory：ＲＯＭ）、光ディスクまたは磁気ディスク、ソリッドステートドライブ、コンパクトディスク読取専用メモリ（compact-disc read only memory：ＣＤ－ＲＯＭ）のような二次ストレージまたは永続的長期ストレージを含み得る。コンピュータ読取可能媒体はまた、任意の他の揮発性または不揮発性記憶システムであってもよい。コンピュータ読取可能媒体は、たとえば、コンピュータ読取可能記憶媒体、または有形の記憶装置と見なされてもよい。 Computer-readable media can also include non-transitory computer-readable media such as computer-readable media that store data for short periods of time, such as register memory, processor cache, and RAM. Computer-readable media may also include non-transitory computer-readable media that store program code and/or data for longer periods of time. Thus, a computer readable medium may be, for example, a read only memory (ROM), an optical or magnetic disk, a solid state drive, a compact-disc read only memory (CD-ROM). It may include secondary storage or permanent long term storage. A computer readable medium may also be any other volatile or nonvolatile storage system. Computer-readable media, for example, may be considered a computer-readable storage medium or a tangible storage device.
さらに、一つ以上の情報伝送を表わすステップまたはブロックは、同じ物理的デバイス内のソフトウェアモジュールおよび／またはハードウェアモジュール間の情報伝送に対応してもよい。しかしながら、他の情報伝送が、異なる物理的デバイス内のソフトウェアモジュールおよび／またはハードウェアモジュール間で行なわれてもよい。 Additionally, steps or blocks representing one or more information transfers may correspond to information transfers between software and/or hardware modules within the same physical device. However, other information transfers may occur between software and/or hardware modules within different physical devices.
図面に示される特定の構成は、限定的であると見なされるべきではない。他の実施形態は、所与の図に示される各要素をより多くまたはより少なく含み得ることが理解されるべきである。また、例示される要素のいくつかは、組み合わされるかまたは省略され得る。さらに、例示的な実施形態は、図面に示されていない要素を含み得る。 The specific configurations shown in the drawings should not be considered limiting. It should be understood that other embodiments may include more or less of each element shown in a given figure. Also, some of the illustrated elements may be combined or omitted. Additionally, example embodiments may include elements not shown in the drawings.
本明細書ではさまざまな局面および実施形態が開示されたが、当業者には他の局面および実施形態が明らかであろう。本明細書に開示されるさまざまな局面および実施形態は例示を目的としており、限定的であるよう意図されておらず、真の範囲は特許請求の範囲によって示される。 While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are intended to be illustrative and not intended to be limiting, the true scope being indicated by the claims.
コンピューティングシステム１００は、ディスプレイ１０６と、前面カメラ１０４、背面カメラ１１２、および／または前面赤外線カメラ１１４とを使用して、対象被写体の画像を取り込むように構成され得る。取り込まれた画像は、複数の静止画像であるか、またはビデオストリームであり得る。画像取り込みは、ボタン１０８を起動すること、ディスプレイ１０６上のソフトキーを押すこと、または何らかの他のメカニズムによってトリガされ得る。実現化例に依存して、画像取り込みは、たとえばボタン１０８を押すか、対象被写体の適切な照明条件に応じてか、コンピューティングシステム１００を予め定められた距離だけ動かすか、または予め定められた取り込みスケジュールに従って、特定の時間間隔で自動的に行なわれ得る。
距離投影モデル３１４は複数のマッピング３１６～３２６を含み、マッピング３１６～３２６のうちの一つ以上を介して推定物理的距離３３６を決定し得る。マッピング３１６～３２６の各々は、複数の被写体下端比率を、複数の対応する物理的被写体距離と関連付け得る。たとえば、マッピング３１６は、被写体下端比率３１８～３２２を、対応する物理的被写体距離３２０～３２４と関連付け得る。同様に、マッピング３２６は、被写体下端比率３２８～３３２を、対応する物理的被写体距離３３０～３３４と関連付け得る。マッピング３１６～３２６に関連付けられた被写体下端比率（たとえば、被写体下端比率３１８～３２２および３２８～３３２）は、候補被写体下端比率と呼ばれ得る。なぜなら、それらは各々、推定物理的距離３３６を決定するために使用される可能性があり得るためである。
Range projection model 314 may include multiple mappings 316-326 and determine estimated
具体的には、ユーザインターフェイス（user interface：ＵＩ）４１０は、０．０、０．２５、０．３５、０．４５、０．４７、および０．５を含むそれぞれの被写体下端比率に対応する複数の水平線を示す。とりわけ、被写体下端比率０．５に関連付けられた水平線は、ＵＩ４１０のほぼ中央に位置付けられて、ＵＩ４１０をほぼ等しい上半分と下半分とに分割する。ＵＩ４１２は、１メートル、５メートル、１０メートル、２０メートル、３０メートル、および無限遠を含む対応する物理的距離でラベル付けされた、ＵＩ４１０と同じ複数の線を示す。すなわち、被写体下端比率０．０、０．２５、０．３５、０．４５、０．４７、および０．５は、物理的距離１メートル、５メートル、１０メートル、２０メートル、３０メートル、および図４Ａの無限遠基準線に関連付けられた距離（たとえば無限遠）に、それぞれ対応する。被写体下端比率は、距離投影モデル３１４のマッピング３１６～３２６のうちの一つを表わし得る関数Ｆ（ｂ）によって、対応する距離にマッピングされ得る。
Specifically, the user interface (UI) 410 corresponds to each subject bottom edge ratio including 0.0, 0.25, 0.35 , 0.45, 0.47, and 0.5. Show multiple horizontal lines. Notably, the horizontal line associated with a subject-to-bottom ratio of 0.5 is positioned approximately in the center of the
Ｆ（ｂ）は、経験的な訓練データに基づいて決定され得る。たとえば、カメラに対して複数の物理的距離が測定され、環境内で視覚的にマークされ得る。カメラは、これらの視覚的にマークされた距離を表わす訓練画像データを取り込むために使用され得る。訓練画像データを取り込む間、カメラは、環境内の予め定められた高さに配置され得る。このため、この訓練画像データに基づいて訓練された関数またはマッピングは、同じカメラ、または、（ｉ）似ているかもしくはほぼ同じ一組のカメラパラメータを有するとともに、（ｉｉ）似ているかまたはほぼ同じ予め定められた高さに位置付けられた別のカメラを使用して、距離を測定するために有効であり得る。追加の関数またはマッピングは、異なる一組のカメラパラメータを有するカメラおよび／または異なる高さに位置付けられた同じカメラを使用して得られた訓練データに基づいて、同様の手順を使用して決定され得る。 F(b) can be determined based on empirical training data. For example, multiple physical distances can be measured relative to the camera and visually marked in the environment. A camera can be used to capture training image data representing these visually marked distances. While capturing training image data, the camera may be placed at a predetermined height within the environment. Thus, a function or mapping trained based on this training image data will have the same camera, or (i) a similar or nearly the same set of camera parameters , and (ii) a similar or nearly It may be useful to use another camera positioned at the same predetermined height to measure the distance . Additional functions or mappings are determined using similar procedures based on training data obtained using cameras with different sets of camera parameters and/or the same cameras positioned at different heights. obtain.
幾何学的モデル５１４は、画像センサ５００の配向５００Ｂおよび５００Ｃを示しており、カメラのピッチ角の変化を補償するために使用され得る数学的関係を決定するために使用され得る。具体的には、幾何学的モデル５１４は、配向５００Ｂが、負のピッチ角αmax depressionと、光軸５０４Ａの配向５０４Ｂへの回転と、ΔＬmax depressionによって無限遠基準線５２０に関連付けられた被写体下端比率のオフセットとに対応することを示す。このため、tan（αmax depression）＝ΔＬmax depression／ｆ、およびｆ＝ΔＬmax depression／tan（αmax depression）である。したがって、αmax depressionとφmax elevationとの間のピッチ角θ分のカメラ（すなわち、画像センサ５００およびアパーチャ５０２）の回転については、被写体下端比率のオフセットΔｂは、Δｂ＝ｆtan（θ）、言い換えればΔｂ＝（ΔＬmax depression／tan（αmax depression
））tan（θ）によってモデル化される。この式は、画像データ３００に関連付けられたカメラピッチ３０６を補償する推定オフセットを決定するために、オフセット計算器３１２によって使用または実現され得る。推定オフセットΔｂは（たとえば画素数ではなく）被写体下端比率に換算して計算されるため、推定オフセットΔｂは、被写体下端比率計算器３１０によって計算された被写体下端比率に直接加算され得る。
Claims (20)
前記画像データに基づいて、前記画像データ内の前記被写体の下端の垂直位置を決定することと、
前記垂直位置と前記画像データの高さとの間の被写体下端比率を決定することと、
距離投影モデルを介しておよび前記被写体下端比率に基づいて、前記カメラと前記被写体との間の物理的距離の推定値を決定することとを備え、前記距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）前記それぞれの候補被写体下端比率と（ｉｉ）前記環境における対応する物理的距離との間のマッピングを規定し、
前記カメラと前記被写体との間の前記物理的距離の前記推定値の指標を生成することを備える、コンピュータが実現する方法。 receiving image data representing an object in the environment from a camera;
determining a vertical position of a bottom edge of the object in the image data based on the image data;
determining a subject bottom edge ratio between the vertical position and the height of the image data;
determining an estimate of the physical distance between the camera and the subject via a distance projection model and based on the subject bottom edge ratios, the range projection model comprising a plurality of candidate subject bottom edge ratios. for each respective candidate subject bottom-ratio of , defining a mapping between (i) said respective candidate subject bottom-ratio and (ii) a corresponding physical distance in said environment;
A computer-implemented method comprising generating an indication of said estimate of said physical distance between said camera and said subject.
前記画像データを取り込んでいる間に前記カメラの前記物理的高さが前記予め定められた高さを下回る場合、前記カメラと前記被写体との間の前記物理的距離の前記推定値は過大推定値である、請求項２に記載のコンピュータが実現する方法。 The estimate of the physical distance between the camera and the subject is an underestimate if the physical height of the camera exceeds the predetermined height while capturing the image data. can be,
The estimate of the physical distance between the camera and the subject is an overestimate if the physical height of the camera is below the predetermined height while capturing the image data. 3. The computer-implemented method of claim 2, wherein:
前記予め定められた高さの前記指定に基づいて、前記カメラが前記予め定められた高さの前記指定に従って位置付けられていると仮定するように前記マッピングを修正することによって前記距離投影モデルを構成することとをさらに備える、請求項２に記載のコンピュータが実現する方法。 receiving the predetermined height designation via a user interface associated with the camera;
Based on the specification of the predetermined height, constructing the range projection model by modifying the mapping to assume that the camera is positioned according to the specification of the predetermined height. 3. The computer-implemented method of claim 2, further comprising:
前記複数の候補マッピングのうちの各それぞれのマッピングは、前記予め定められた高さの対応する指定に関連付けられる、請求項４に記載のコンピュータが実現する方法。 constructing the distance projection model includes selecting the mapping from a plurality of candidate mappings based on the specification of the predetermined height;
5. The computer-implemented method of claim 4, wherein each respective mapping of said plurality of candidate mappings is associated with a corresponding designation of said predetermined height.
前記距離投影モデルを構成することは、前記予め定められた高さの前記指定に基づいて前記機械学習モデルの少なくとも一つの入力パラメータを調節することを含む、請求項４に記載のコンピュータが実現する方法。 the distance projection model comprises a machine learning model;
5. The computer-implemented of claim 4, wherein configuring the distance projection model comprises adjusting at least one input parameter of the machine learning model based on the specification of the predetermined height. Method.
前記幾何学的モデルは、（ｉ）焦点距離を有し、前記環境内の前記予め定められた高さに配置された前記カメラと、（ｉｉ）前記環境の地面とほぼ平行に配向された前記カメラの光軸と、（ｉｉｉ）前記カメラの画像センサ上のそれぞれの点から前記環境の前記地面上の対応する点へ投影された複数の線のうちの各それぞれの線とを含み、
各それぞれの候補被写体下端比率は、前記幾何学的モデルに基づいて、前記環境における前記対応する物理的距離に関連付けられる、請求項２に記載のコンピュータが実現する方法。 the mapping is based on a geometric model of the camera;
The geometric model comprises: (i) the camera having a focal length and positioned at the predetermined height within the environment; and (ii) the camera oriented substantially parallel to the ground of the environment. (iii) each respective line of a plurality of lines projected from a respective point on the camera's image sensor to a corresponding point on the ground surface of the environment;
3. The computer-implemented method of claim 2, wherein each respective candidate subject bottom edge ratio is associated with the corresponding physical distance in the environment based on the geometric model.
前記第１のマッピングに関連付けられた各それぞれの候補被写体下端比率は、縦長画像データ内の対応する垂直位置と前記縦長画像データの高さとの間にあり、
前記第２のマッピングに関連付けられた各それぞれの候補被写体下端比率は、横長画像データ内の対応する垂直位置と前記横長画像データの高さとの間にあり、
前記方法は、
前記画像データを取り込んでいる間の前記カメラの配向に基づいて、前記画像データの前記高さを決定することと、
前記画像データを取り込んでいる間の前記カメラの前記配向に基づいて、前記カメラと前記被写体との間の前記物理的距離の前記推定値を決定する際に使用するための前記第１のマッピングまたは前記第２のマッピングを選択することとをさらに備える、請求項１に記載のコンピュータが実現する方法。 the mapping includes (i) a first mapping corresponding to a portrait orientation of the camera and (ii) a second mapping corresponding to a landscape orientation of the camera;
each respective candidate subject bottom edge ratio associated with the first mapping is between a corresponding vertical position in portrait image data and a height of the portrait image data;
each respective candidate subject bottom edge ratio associated with the second mapping is between a corresponding vertical position in landscape image data and a height of the landscape image data;
The method includes:
determining the height of the image data based on the orientation of the camera while capturing the image data;
said first mapping for use in determining said estimate of said physical distance between said camera and said subject based on said orientation of said camera while capturing said image data; 2. The computer-implemented method of claim 1, further comprising selecting the second mapping.
前記カメラの前記ピッチ角を示す前記センサデータに基づいて、ゼロのピッチ角に対する前記カメラの前記ピッチ角によって引き起こされた前記垂直位置の変化を勘案する前記被写体下端比率の推定オフセットを決定することと、
前記被写体下端比率と前記推定オフセットとの合計を決定することとをさらに備え、
前記距離投影モデルは、前記合計に基づいて、前記カメラと前記被写体との間の前記物理的距離の前記推定値を決定するように構成される、請求項１に記載のコンピュータが実現する方法。 obtaining sensor data indicative of a pitch angle of the camera from one or more sensors associated with the camera;
determining, based on the sensor data indicative of the pitch angle of the camera, an estimated offset of the subject bottom ratio that accounts for a change in the vertical position caused by the pitch angle of the camera relative to a pitch angle of zero; ,
determining a sum of the subject bottom edge ratio and the estimated offset;
2. The computer-implemented method of claim 1, wherein the distance projection model is configured to determine the estimate of the physical distance between the camera and the subject based on the sum.
前記カメラの上方傾斜に関連付けられた正のピッチ角は、前記合計が前記被写体下端比率よりも高くなるように正の値を有する推定オフセットをもたらし、
前記カメラの下方傾斜に関連付けられた負のピッチ角は、前記合計が前記被写体下端比率よりも低くなるように負の値を有する推定オフセットをもたらす、請求項９に記載のコンピュータが実現する方法。 Determining the estimated offset of the subject bottom ratio comprises multiplying an estimated focal length of the camera by a tangent of the pitch angle of the camera;
A positive pitch angle associated with an upward tilt of the camera results in an estimated offset with a positive value such that the sum is higher than the subject bottom ratio;
10. The computer-implemented method of claim 9, wherein a negative pitch angle associated with a downward tilt of the camera results in an estimated offset having a negative value such that the sum is less than the subject bottom ratio.
前記第１の画面比率と前記第２の画面比率との合計は１に等しい、請求項１０に記載のコンピュータが実現する方法。 The estimated focal length is determined by (i) determining a maximum pitch angle that offsets the infinity reference line from the camera's initial position on the image sensor to the top of the image sensor by a first screen ratio, or (ii) the infinity determining a minimum pitch angle that offsets the far reference line from the initial position on the image sensor to the bottom of the image sensor by a second screen ratio;
11. The computer-implemented method of claim 10, wherein the sum of the first screen ratio and the second screen ratio equals one.
一つ以上の被写体検出アルゴリズムを介して、前記画像データ内の前記被写体の位置に対応する前記画像データ内の関心領域を決定することと、
一つ以上の被写体下端検出アルゴリズムを介しておよび前記関心領域に基づいて、前記被写体の前記下端を識別することと、
前記被写体の前記下端を識別することに基づいて、前記被写体の前記下端が前記環境の地面と接していると決定することと、
前記被写体の前記下端が前記環境の前記地面と接していると決定することに基づいて、前記画像データ内の前記被写体の前記下端の前記垂直位置を決定することとを含む、請求項１に記載のコンピュータが実現する方法。 Determining the vertical position of the bottom edge of the object in the image data comprises:
determining, via one or more object detection algorithms, a region of interest within the image data corresponding to the location of the object within the image data;
identifying the bottom edge of the subject via one or more subject bottom edge detection algorithms and based on the region of interest;
determining that the bottom edge of the subject is in contact with the ground of the environment based on identifying the bottom edge of the subject;
determining the vertical position of the bottom edge of the subject in the image data based on determining that the bottom edge of the subject is in contact with the ground of the environment. computer implementation.
前記追加の画像データに基づいて、前記追加の被写体の下端が前記画像データ内で見えないと決定することと、
前記追加の被写体の前記下端が前記画像データ内で見えないと決定することに基づいて、前記カメラと前記追加の被写体との間の物理的距離の追加の推定値が、最小の測定可能な物理的距離を下回る予め定められた値であると決定することと、
前記カメラと前記追加の被写体との間の前記物理的距離の前記追加の推定値の追加の指標を生成することとをさらに備える、請求項１に記載のコンピュータが実現する方法。 receiving additional image data representing additional objects in the environment from the camera;
determining, based on the additional image data, that the bottom edge of the additional subject is not visible in the image data;
An additional estimate of the physical distance between the camera and the additional subject based on determining that the bottom edge of the additional subject is not visible in the image data is a minimum measurable physical distance. a predetermined value below the target distance;
2. The computer-implemented method of claim 1, further comprising generating an additional measure of the additional estimate of the physical distance between the camera and the additional subject.
前記被写体の少なくとも一部が前記カメラの前記視野の前記アクティブな部分内に含まれていると決定することと、
前記被写体の前記少なくとも一部が前記カメラの前記視野の前記アクティブな部分内に含まれていると決定することに基づいて、前記カメラと前記被写体との間の前記物理的距離の前記推定値の前記指標を生成することとをさらに備え、
前記カメラと対応する被写体との間の物理的距離のそれぞれの推定値の指標の生成は、前記カメラの前記視野の前記アクティブな部分の外側の被写体については省略される、請求項１に記載のコンピュータが実現する方法。 receiving a designation of an active portion of the camera's field of view;
determining that at least a portion of the subject is contained within the active portion of the field of view of the camera;
of the physical distance between the camera and the subject based on determining that the at least a portion of the subject is contained within the active portion of the field of view of the camera. generating the indicator;
2. The method of claim 1, wherein generating an indication of each estimate of physical distance between the camera and a corresponding object is omitted for objects outside the active portion of the field of view of the camera. A computer-implemented method.
前記被写体が前記一つ以上の被写体クラスのうちの第１の被写体クラスに属すると決定することと、
前記被写体が前記第１の被写体クラスに属すると決定することに基づいて、前記カメラと前記被写体との間の前記物理的距離の前記推定値の前記指標を生成することとをさらに備え、
前記カメラと対応する被写体との間の物理的距離のそれぞれの推定値の指標の生成は、前記第１の被写体クラスに属さない被写体については省略される、請求項１に記載のコンピュータが実現する方法。 receiving a selection of one or more object classes from a plurality of object classes;
determining that the object belongs to a first object class of the one or more object classes;
further comprising generating said indication of said estimate of said physical distance between said camera and said subject based on determining that said subject belongs to said first subject class;
2. The computer-implemented claim 1, wherein generating an indication of respective estimates of physical distances between said camera and corresponding objects is omitted for objects that do not belong to said first object class. Method.
プロセッサと、
前記プロセッサによって実行されると前記プロセッサに動作を行なわせる命令が格納された、非一時的コンピュータ読取可能記憶媒体とを備え、前記動作は、
前記カメラから、環境における被写体を表わす画像データを受信することと、
前記画像データに基づいて、前記画像データ内の前記被写体の下端の垂直位置を決定することと、
前記垂直位置と前記画像データの高さとの間の被写体下端比率を決定することと、
距離投影モデルを介しておよび前記被写体下端比率に基づいて、前記カメラと前記被写体との間の物理的距離の推定値を決定することとを含み、前記距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）前記それぞれの候補被写体下端比率と（ｉｉ）前記環境における対応する物理的距離との間のマッピングを規定し、前記動作はさらに、
前記カメラと前記被写体との間の前記物理的距離の前記推定値の指標を生成することを含む、コンピューティングシステム。 camera and
a processor;
a non-transitory computer-readable storage medium containing instructions that, when executed by the processor, cause the processor to perform an action, the action comprising:
receiving image data representing an object in an environment from the camera;
determining a vertical position of a bottom edge of the object in the image data based on the image data;
determining a subject bottom edge ratio between the vertical position and the height of the image data;
determining an estimate of the physical distance between the camera and the subject via a distance projection model and based on the subject bottom edge ratios, the range projection model comprising a plurality of candidate subject bottom edge ratios. , defining a mapping between (i) said respective candidate subject bottom edge ratio and (ii) a corresponding physical distance in said environment, said operation further comprising:
A computing system comprising generating an indication of said estimate of said physical distance between said camera and said subject.
前記動作はさらに、
前記一つ以上のセンサから、前記カメラの前記ピッチ角を示す前記センサデータを得ることと、
前記カメラの前記ピッチ角を示す前記センサデータに基づいて、ゼロのピッチ角に対する前記カメラの前記ピッチ角によって引き起こされた前記垂直位置の変化を勘案する前記被写体下端比率の推定オフセットを決定することと、
前記被写体下端比率と前記推定オフセットとの合計を決定することとを含み、
前記距離投影モデルは、前記合計に基づいて、前記カメラと前記被写体との間の前記物理的距離の前記推定値を決定するように構成される、請求項１８に記載のコンピューティングシステム。 further comprising one or more sensors configured to generate sensor data indicative of a pitch angle of the camera;
Said operation further comprises:
obtaining the sensor data indicative of the pitch angle of the camera from the one or more sensors;
determining, based on the sensor data indicative of the pitch angle of the camera, an estimated offset of the subject bottom ratio that accounts for a change in the vertical position caused by the pitch angle of the camera relative to a pitch angle of zero; ,
determining the sum of the subject bottom edge ratio and the estimated offset;
19. The computing system of claim 18, wherein the distance projection model is configured to determine the estimate of the physical distance between the camera and the subject based on the sum.
カメラから、環境における被写体を表わす画像データを受信することと、
前記画像データに基づいて、前記画像データ内の前記被写体の下端の垂直位置を決定することと、
前記垂直位置と前記画像データの高さとの間の被写体下端比率を決定することと、
距離投影モデルを介しておよび前記被写体下端比率に基づいて、前記カメラと前記被写体との間の物理的距離の推定値を決定することとを含み、前記距離投影モデルは、複数の候補被写体下端比率のうちの各それぞれの候補被写体下端比率について、（ｉ）前記それぞれの候補被写体下端比率と（ｉｉ）前記環境における対応する物理的距離との間のマッピングを規定し、前記動作はさらに、
前記カメラと前記被写体との間の前記物理的距離の前記推定値の指標を生成することを含む、非一時的コンピュータ読取可能記憶媒体。 A non-transitory computer-readable storage medium containing instructions that, when executed by a computing system, cause said computing system to perform an action, said action comprising:
receiving image data representing an object in the environment from a camera;
determining a vertical position of a bottom edge of the object in the image data based on the image data;
determining a subject bottom edge ratio between the vertical position and the height of the image data;
determining an estimate of the physical distance between the camera and the subject via a distance projection model and based on the subject bottom edge ratios, the range projection model comprising a plurality of candidate subject bottom edge ratios. , defining a mapping between (i) said respective candidate subject bottom edge ratio and (ii) a corresponding physical distance in said environment, said operation further comprising:
A non-transitory computer-readable storage medium comprising generating an indication of said estimate of said physical distance between said camera and said subject.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063033964P | 2020-06-03 | 2020-06-03 | |
US63/033,964 | 2020-06-03 | ||
PCT/US2021/032686 WO2021247228A1 (en) | 2020-06-03 | 2021-05-17 | Depth estimation based on object bottom position |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2023525066A true JP2023525066A (en) | 2023-06-14 |
JP7394240B2 JP7394240B2 (en) | 2023-12-07 |
Family
ID=76422038
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022567599A Active JP7394240B2 (en) | 2020-06-03 | 2021-05-17 | Depth estimation based on the bottom edge position of the subject |
Country Status (7)
Country | Link |
---|---|
US (1) | US20230127218A1 (en) |
EP (1) | EP4049242A1 (en) |
JP (1) | JP7394240B2 (en) |
KR (1) | KR20230007477A (en) |
CN (1) | CN115516509A (en) |
DE (1) | DE112021001436T5 (en) |
WO (1) | WO2021247228A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019164514A1 (en) * | 2018-02-23 | 2019-08-29 | Google Llc | Transitioning between map view and augmented reality view |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2006266848A (en) | 2005-03-23 | 2006-10-05 | Sharp Corp | Distance measuring device |
US8164628B2 (en) | 2006-01-04 | 2012-04-24 | Mobileye Technologies Ltd. | Estimating distance to an object using a sequence of images recorded by a monocular camera |
JP4848312B2 (en) | 2007-05-17 | 2011-12-28 | 綜合警備保障株式会社 | Height estimating apparatus and height estimating method |
JP5077088B2 (en) | 2008-06-17 | 2012-11-21 | 住友電気工業株式会社 | Image processing apparatus and image processing method |
KR101758735B1 (en) | 2012-12-03 | 2017-07-26 | 한화테크윈 주식회사 | Method for acquiring horizontal distance between camera and target, camera and surveillance system adopting the method |
JPWO2014171052A1 (en) | 2013-04-16 | 2017-02-16 | コニカミノルタ株式会社 | Image processing method, image processing apparatus, imaging apparatus, and image processing program |
JP6787102B2 (en) | 2016-12-14 | 2020-11-18 | 株式会社デンソー | Object detection device, object detection method |
CN108596116B (en) * | 2018-04-27 | 2021-11-05 | 深圳市商汤科技有限公司 | Distance measuring method, intelligent control method and device, electronic equipment and storage medium |
JP7224832B2 (en) | 2018-10-01 | 2023-02-20 | キヤノン株式会社 | Information processing device, information processing method, and program |
-
2021
- 2021-05-17 CN CN202180034017.7A patent/CN115516509A/en active Pending
- 2021-05-17 JP JP2022567599A patent/JP7394240B2/en active Active
- 2021-05-17 KR KR1020227042610A patent/KR20230007477A/en unknown
- 2021-05-17 WO PCT/US2021/032686 patent/WO2021247228A1/en unknown
- 2021-05-17 US US17/995,051 patent/US20230127218A1/en active Pending
- 2021-05-17 EP EP21732148.8A patent/EP4049242A1/en active Pending
- 2021-05-17 DE DE112021001436.6T patent/DE112021001436T5/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP7394240B2 (en) | 2023-12-07 |
US20230127218A1 (en) | 2023-04-27 |
EP4049242A1 (en) | 2022-08-31 |
CN115516509A (en) | 2022-12-23 |
KR20230007477A (en) | 2023-01-12 |
WO2021247228A1 (en) | 2021-12-09 |
DE112021001436T5 (en) | 2023-01-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6283152B2 (en) | Graphic assisted remote location with handheld geodetic device | |
JP2018535402A (en) | System and method for fusing outputs of sensors having different resolutions | |
US20120056887A1 (en) | Depth estimation system for two-dimensional images and method of operation thereof | |
US11585724B2 (en) | Fixtureless lensmeter system | |
US20230169686A1 (en) | Joint Environmental Reconstruction and Camera Calibration | |
CN105025284A (en) | Method and device for calibrating display error of integral imaging display device | |
US20230386065A1 (en) | Systems and methods for processing captured images | |
KR20140142441A (en) | Shooting Method for Three-Dimensional Modeling And Electrical Device Thereof | |
CN112150560A (en) | Method and device for determining vanishing point and computer storage medium | |
KR20210062708A (en) | Fixed lens meter system | |
JP7394240B2 (en) | Depth estimation based on the bottom edge position of the subject | |
US10866635B2 (en) | Systems and methods for capturing training data for a gaze estimation model | |
US20230245332A1 (en) | Systems and methods for updating continuous image alignment of separate cameras | |
US11450014B2 (en) | Systems and methods for continuous image alignment of separate cameras | |
WO2018161322A1 (en) | Depth-based image processing method, processing device and electronic device | |
CN114600162A (en) | Scene lock mode for capturing camera images | |
JP7477596B2 (en) | Method, depth estimation system, and computer program for depth estimation | |
WO2022244069A1 (en) | Imaging condition determination method, imaging condition determination system, imaging condition determination device and computer-readable medium | |
WO2023247015A1 (en) | Determining a scale factor | |
TWI516744B (en) | Distance estimation system, method and computer readable media | |
WO2023203530A1 (en) | Interpupillary distance estimation method | |
CN116708731A (en) | Projection picture adjusting method and device, electronic equipment and readable storage medium | |
JP2022021027A (en) | Information processing device, information processing method, and computer readable recording medium | |
CN117036318A (en) | Refractive topography measurement method, device, electronic equipment and readable storage medium | |
JP2016110597A (en) | Information processing system, information processing device, coordinate conversion method and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20221107 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20221107 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20231012 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20231031 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20231127 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7394240Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |